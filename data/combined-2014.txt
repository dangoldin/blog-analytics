{% include setup %} What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to  my GitHub . If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.  PS. It's great when web services allow users to export their data and wish more would start doing the same.    	                               	Cumulative distance. You can see a few flat areas in February and November when I took a break due to some minor injuries.                                                     	Distance run by month. Unexpected drop in November due to a break but pretty solid otherwise.                                                     	Distance run by week. Not much new information here that's not covered in the monthly graph.                                                     	Cumulative time. Very similar shape to that of cumulative distance.                                                     	Cumulative time vs distance. Superimpose one on top of the other to compare the shapes. Started off slowly but started getting faster in October.                                                     	Speed by run. I got significantly faster in October but slowed down again in December.                                                     	Speed by month by distance quantile. The idea here was to look at my improvement in speed but controlling for distance. Echoes the previous chart showing my speed improvement in Oct for the longer distances.                                                     	Speed distribution by distance quantile. Another view that looks at the distribution of my speeds for all runs in a given distance quantile. Not much here but I was expecting to see that I'd have a faster pace for shorter runs.                                                     	Speed vs Distance scatter plot. Another way to look at the relationship between speed and distance but not many new insights here. Slight correlation between speed and distance. This is pretty much because as I got faster I started doing longer runs. It'll be interesting to see how this changes in 2014.                                                     	Speed vs Distance scatter plot clustered. An attempt at clustering the runs by speed and distance. In this case they were basically clustered by distance since the speed didn't vary significantly.                       {% highlight r %} library(ggplot2) library(grid) library(gridExtra) library(reshape) library(scales) library(lattice) library(ggthemes)  data = read.csv("cardioActivities.csv", check.names=FALSE)  summary(data)  data <- data[order(data$Date),] # Sort ascending by date data$ymd <- as.Date(data$Date) data$month <- as.Date(cut(data$ymd, breaks = "month")) data$week <- as.Date(cut(data$ymd, breaks = "week")) + 1 data$distance <- data$'Distance (mi)' data$distance_total <- cumsum(data$distance) data$speed <- data$'Average Speed (mph)' data$time_hours <- data$distance/data$'Average Speed (mph)' data$time_hours_total <- cumsum(data$time_hours) data$time_mins <- data$time_hours * 60 data$time_mins_total <- cumsum(data$time_mins) data$distance_total_norm <- data$distance_total/sum(data$distance) data$time_hours_total_norm <- data$time_hours_total/sum(data$time_hours) data$qs <- cut(data$distance, breaks = quantile(data$distance), include.lowest=TRUE) # Quantile data by distance run  # Generate a new data frame by qs and month to make plotting easier data.qs_monthly <- ddply(data, .(qs, month), function(x) data.frame(distance=sum(x$distance), time_mins=sum(x$time_mins))) data.qs_monthly$speed <- data.qs_monthly$distance/(data.qs_monthly$time_mins/60)  # Summarize the data by qs to make plotting easier data.summary <- ddply(data,~qs,summarise,mean_speed=mean(speed),sd_speed=sd(speed),mean_distance=mean(distance),sd_distance=sd(distance))  # Cluster each of the runs by speed and data m <- as.matrix(cbind(data$speed, data$distance),ncol=2) cl <- kmeans(m,3) data$cluster <- factor(cl$cluster) centers <- as.data.frame(cl$centers)  # Normalize cumulative distance and time data.normalized <- melt(data, id.vars="ymd", measure.vars=c("distance_total_norm","time_hours_total_norm")) data.normalized$variable <- revalue(data.normalized$variable, c("distance_total_norm"="Distance", "time_hours_total_norm"="Time"))  png('rk-speed-vs-distance.png', width=800, height=800) ggplot(data=data, aes(x=speed, y=distance)) +   geom_point() +   theme_economist() +   scale_color_economist() +   geom_abline() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Distance') +   ggtitle("Speed vs Distance") dev.off()  png('rk-speed-vs-distance-clusters.png', width=800, height=800) ggplot(data=data, aes(x=speed, y=distance, color=cluster)) +   theme_economist() +   scale_color_economist() +   geom_point(legend=FALSE) +   geom_point(data=centers, aes(x=V1,y=V2, color='Center'), size=52, alpha=.3, legend=FALSE) +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Distance') +   ggtitle("Speed vs Distance - Clustered") dev.off()  png('rk-speed-month-qs.png',width = 800, height = 600) ggplot(data = data.qs_monthly,   aes(month, speed)) +   geom_line() +   facet_grid(qs ~ .) +   scale_x_date(     labels = date_format("%Y-%m"),     breaks = "1 month") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle("Speed by Month") dev.off()  png('rk-distance-month.png',width = 800, height = 600) ggplot(data = data,   aes(month, distance)) +   stat_summary(fun.y = sum,     geom = "bar") +   scale_x_date(     labels = date_format("%Y-%m"),     breaks = "1 month") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle("Distance by Month") dev.off()  png('rk-distance-week.png',width = 800, height = 600) ggplot(data = data,   aes(week, distance)) +   stat_summary(fun.y = sum,     geom = "bar") +   scale_x_date(     labels = date_format("%Y-%m-%d"),     breaks = "4 week") +   xlab('Week') +   ylab('Distance') +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Week') +   ylab('Distance') +   ggtitle("Distance by Week") dev.off()  png('rk-speed-distribution-qs.png',width = 800, height = 600) ggplot(data, aes(speed, fill=qs)) +   geom_density(alpha = 0.5) +   geom_vline(aes(xintercept=mean_speed), data=data.summary) +   facet_grid(qs ~ .) +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Density') +   ggtitle("Speed Distribution by Distance") dev.off()  png('rk-distance-cumulative.png',width = 800, height = 600) ggplot(data=data, aes(ymd, distance_total)) +   stat_summary(fun.y = sum, geom = "line") +   scale_x_date(     labels = date_format("%Y-%m"),     breaks = "1 month") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle("Cumulative distance") dev.off()  png('rk-speed-daily.png',width = 800, height = 600) ggplot(data=data, aes(ymd, speed)) +   stat_summary(fun.y = sum, geom = "line") +   scale_x_date(     labels = date_format("%Y-%m"),     breaks = "1 month") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Speed') +   ggtitle("Speed by Run") dev.off()  png('rk-time-cumulative.png',width = 800, height = 600) ggplot(data=data, aes(ymd, time_hours_total)) +   stat_summary(fun.y = sum, geom = "line") +   scale_x_date(     labels = date_format("%Y-%m"),     breaks = "1 month") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Time (hours)') +   ggtitle("Cumulative Time") dev.off()  png('rk-time-vs-distance-cumulative.png', width=800, height=800) ggplot(data.normalized,   aes(x=ymd, y=value, colour = variable, group=variable)) +   geom_line() +   theme_economist() +   scale_color_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   theme(legend.title=element_blank()) +   xlab('YMD') +   ylab('') +   ggtitle("Time vs Distance (normalized)") dev.off() {% endhighlight %}
{% include setup %} Every time there’s a big event or terrible weather, there’s a slew of complaints about Uber’s surge pricing. By now, you’d think that Uber customers would expect this to happen and yet they’re surprised when a $10 cab ride turns into a $100 Uber ride. I suspect Uber’s already done as much as it can on the messaging side; psychologically it’s just tough for someone to take a $10 ride one day and then a day later pay an order of magnitude more.  Every Uber transaction involves three parties - the customer, the driver, and Uber. In every case it’s up to Uber to set the prices in order to get the supply (drivers) equal to the demand (customers). In most cases, these are in alignment since people are willing to pay more for an Uber than a cab for the convenience. the problem occurs when the demand side gets too large and Uber needs to drastically increase prices in order to encourage more supply. Uber should have enough data by now to be able to determine the prices that will lead to supply being equal to demand for every demand level but that doesn’t solve the perception problem.  One  suggestion  I liked was having Uber drop their margin on these high demand days in order to maintain goodwill. Uber  supposedly takes ~20%  of each fare and that could remain the case for low fares but Uber can drop that on surge days in order to reduce the customer cost. This way a $10 drip gives Uber $2 but a $100 trip no longer needs to bring Uber $20 and can be set closer to $80. The issue is that people won’t care that a $100 trip now costs $80. Instead they’ll hear that a $10 trip now costs $80. The only way to make consumers happy would be for Uber to have a deep negative margin for the surge days.  Another option is to move to an auction model. Each customer would specify where they are, where they want to go, and what they want to pay and it would be up to a driver to either accept or ignore that offer. This way Uber can achieve  perfect price discrimination  with both drivers and customers getting what they want.  Uber must have considered both of these approaches. The latter one would require a significantly different product with more complicated logistics and a more difficult pitch but it would keep the various parties aligned to their reserve price. The former approach, on the other hand, would be much easier and cheaper to achieve. I believe Uber would be still be profitable if they took a negative margin on the rare surge day and they could offset it with a small increase in the margin on a normal day. The only thing I can think of is that they’re taking the long term view and are hoping to change the customer perception of what’s fair. I don’t know if they’ll succeed but if they do I expect many more services start adopting these “purer price” models.
{% include setup %} I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on  worldtaximeter.com  and combining that with the data from  taxiautofare.com  and  www.numbeo.com  in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute. Since each city’s fare came in local units I also had to convert to miles (sorry world) and US dollars (sorry again). Using R we generate the fares for the various combinations of distances and stoppage times and start diving into the data. As usual, the data and code are up on  GitHub  with contributions, corrections, and suggestions welcome. I’d also love to get the real rates for the cities so either do a pull request or let me know what they are in the comments and I’ll update the post.      City  Base  Inc Dist  Per Dist  Per Min  Dist Cvr  Fare Cvr  $ Base  $ per Mile  $ per Min  Ratio  $ per Hr      New York  2.50  0.00  2.50  0.50  1.00  1.00  2.50  2.50  0.50  5.00  30.00    Mumbai  19.00  1.50  12.35  0.50  1.61  0.02  0.32  0.33  0.01  39.77  0.50    London  2.20  2.00  1.70  0.52  1.61  1.64  3.61  4.49  0.85  5.31  50.71    Amsterdam  2.66  0.00  1.95  0.32  1.61  1.36  3.62  4.27  0.44  9.81  26.11    Tokyo  712.00  0.00  188.00  56.00  1.61  0.01  6.84  2.91  0.54  5.41  32.26    Aberdeen  2.40  0.90  1.10  0.37  1.61  1.64  3.94  2.90  0.61  4.79  36.41    Austin  2.54  0.20  1.30  0.67  1.61  1.00  2.54  2.09  0.67  3.12  40.20    Baltimore  1.80  0.15  1.36  0.44  1.61  1.00  1.80  2.19  0.44  4.98  26.40    Barcelona  2.05  0.00  0.98  0.38  1.61  1.36  2.79  2.15  0.52  4.15  31.01    Berlin  3.00  0.00  1.58  0.41  1.61  1.36  4.08  3.46  0.56  6.20  33.46    Boston  2.60  0.23  1.73  0.54  1.61  1.00  2.60  2.79  0.54  5.16  32.40    Chicago  2.25  0.18  1.11  0.37  1.61  1.00  2.25  1.79  0.37  4.83  22.20    Dublin  4.09  1.00  1.03  0.37  1.61  1.36  5.56  2.26  0.50  4.48  30.19    Edinburgh  3.00  0.52  1.20  0.36  1.61  1.64  4.92  3.17  0.59  5.41  35.13    Ibiza  3.25  0.00  0.98  0.35  1.61  1.36  4.42  2.15  0.48  4.51  28.56    Las Vegas  3.30  0.00  1.49  0.53  1.61  1.00  3.30  2.40  0.53  4.53  31.80    Los Angeles  2.85  0.18  1.67  0.50  1.61  1.00  2.85  2.69  0.50  5.38  30.00    Madrid  2.04  0.00  0.98  0.32  1.61  1.00  2.04  1.58  0.32  4.93  19.20    Malaga  1.42  0.00  0.84  0.34  1.61  1.36  1.93  1.84  0.46  3.98  27.74    Mallorca  3.00  0.00  0.80  0.29  1.61  1.36  4.08  1.75  0.39  4.44  23.66    Manchester  2.35  0.43  1.00  0.28  1.61  1.64  3.85  2.64  0.46  5.75  27.55    Melbourne  3.20  0.00  1.61  1.04  1.61  0.89  2.85  2.31  0.93  2.49  55.54    Montreal  3.45  0.00  1.70  0.63  1.61  0.93  3.21  2.55  0.59  4.34  35.15    New Delhi  40.00  0.00  15.00  1.67  1.61  0.02  0.67  0.40  0.03  14.46  1.67    Paris  2.20  0.00  1.14  0.75  1.61  1.36  2.99  2.50  1.02  2.45  61.20    Rome  2.80  0.00  1.52  0.44  1.61  1.36  3.81  3.33  0.60  5.56  35.90    San Diego  2.50  0.00  1.67  0.46  1.61  1.00  2.50  2.69  0.46  5.85  27.60    San Francisco  3.10  0.00  1.39  0.47  1.61  1.00  3.10  2.24  0.47  4.76  28.20    Seattle  2.50  0.16  1.55  0.52  1.61  1.00  2.50  2.50  0.52  4.80  31.20    Sydney  3.40  0.00  2.06  0.91  1.61  0.89  3.03  2.95  0.81  3.64  48.59    Toronto  4.25  0.14  1.74  0.53  1.61  0.93  3.95  2.61  0.49  5.29  29.57    Vancouver  3.20  1.00  1.85  0.50  1.61  0.93  2.98  2.77  0.47  5.96  27.90    Washington DC  3.00  0.00  0.93  0.26  1.61  1.00  3.00  1.50  0.26  5.76  15.60    Zurich  6.00  0.00  3.80  1.15  1.61  1.10  6.60  6.73  1.27  5.32  75.90    Beijing  13.00  3.00  2.30  0.30  1.61  0.17  2.21  0.63  0.05  12.34  3.06    Shanghai  14.00  0.00  2.40  0.50  1.61  0.17  2.38  0.66  0.09  7.73  5.10    Moscow  245.00  0.00  26.53  14.00  1.61  0.03  7.35  1.28  0.42  3.05  25.20    Bangkok  35.00  0.00  6.00  1.67  1.61  0.03  1.05  0.29  0.05  5.78  3.01    Buenos Aires  1.81  0.00  1.00  0.18  1.61  1.00  1.81  1.61  0.18  9.20  10.50    Cairo  2.50  0.00  1.25  0.28  1.61  0.14  0.35  0.28  0.04  7.19  2.35    Dhaka  250.00  0.00  35.00  4.17  1.61  0.01  3.25  0.73  0.05  13.51  3.25    Istanbul  2.80  0.00  1.73  0.33  1.61  0.46  1.29  1.28  0.15  8.44  9.11    Jakarta  6000.00  0.00  3550.00  500.00  1.61  0.00  0.49  0.47  0.04  11.43  2.46    Lagos  3.32  0.00  3.06  0.16  1.61  1.00  3.32  4.93  0.16  31.58  9.36    Manila  50.00  0.00  13.60  1.75  1.61  0.02  1.10  0.48  0.04  12.51  2.31    Rio de Janeiro  4.70  0.00  1.70  0.37  1.61  0.42  1.97  1.15  0.16  7.38  9.35    Seoul  2800.00  0.00  1050.00  206.00  1.61  0.00  2.63  1.59  0.19  8.21  11.62      Using this information we can run a few interesting analyses:                                                          USD per minute vs USD per mile.  The most obvious check is to see the most and least expensive cities by the two dimensions we have - distance and time. The results are expected - Asian and African cities tend to be the least expensive and European cities being the most expensive. Within Asia there's pretty significant variance with South and Southeast Asia being the cheapest but Seoul and Tokyo being more expensive. A city that stood out was Lagos - it has the one of the lowest per minute fares but one of the largest per mile fares. I don't know why this is the case but I suspect it has something to do with t sure why this is the case other than the roads being in poor condition and the price needing to take that into account.                                                                          Keep time fixed at 10 minutes but vary distance.  The idea here is to look at how quickly the prices increase by distance for different cities. This echoes the previous chart of Europe and Lagos having the highest per mile fares.                                                                          Keep distance fixed at 4 miles but vary time.  Similar to the previous plot but look at the way price will increase as a function of time. Not much new data here.                                                                          What does $10 get you?  Another way to look at expenses is to see the maximum time and distance $10 will get you in different cities. This is similar to looking at the inverse of the per minute and per mile prices.                                                                          What does $10 get you (zoomed)?  This zooms in the bottom left corner of the previous plot. Turns out that having $10 in an expensive city doesn't go very far.                                                                          Ratio of $ per mile vs $ per minute.  The goal was to see how many times a mile was more expensive than a minute for the different cities. The reason we see such high ratios is that the price of gas has a lower variance from city to city than the cost of labor - this leads to cities with low labor casts having significantly higher ratios.                                                                          Heatmap of fares as a function of time and distance.  I wanted this to be a bit more insightful in order to be able to compare all cities against each other across both dimensions but the extreme differences make it difficult to visualize. This highlights once more how expensive Zurich is compared to the other cities. The heatmaps below cluster the cities by the sum of price per mile and price per minute in order to visualize them along similar price scales.                                                                          Heatmap of the fares as a function of time and distance by city (1st quartile).                                                                           Heatmap of the fares as a function of time and distance by city (2ng quartile).                                                                           Heatmap of the fares as a function of time and distance by city (3rd quartile).                                                                           Heatmap of the fares as a function of time and distance by city (4th quartile).
{% include setup %} I'm an avid reader and have embraced the move to digital. An internet connection gives me access to thousands of books with a device that’s thinner than a single book. What I grapple with are the reading apps - I can’t find one that does everything I want.  On my iPad, I have iBooks, Readmill, Oyster, Kindle, and ShuBook with each having a separate use case. iBooks and the Kindle app are for books that I purchased from iTunes and Amazon, respectively. Oyster is a great ebook subscription service but I’m limited to the books available in their library. I discovered ShuBook when I wanted to host my own ebook server but have switched to Readmill due to the much nicer reading experience, a web interface to manage my library, and cross-device syncing.  Ideally, I’d be able to use Readmill for everything. I don’t mind paying for books but I do mind paying to be locked into a particular ecosystem. The creation of content should be decoupled from the distribution of content which should be decoupled from the consumption of content. Yet these days they’re tightly coupled. The only real way to overcome these restrictions is to become a digital pirate. It sucks that customers are forced to break laws in order to get the best experience.
{% include setup %} In 2013, I gave myself a goal of running 1000 miles. I used RunKeeper to record my runs and used its goal feature to track my progress and quickly see how much I had left. Two days before the new year, I was able to hit my goal and got a little notification from RunKeeper congratulating me on achieving my goal. This small notification got me thinking about how emotion is built into our products. RunKeeper doesn’t care whether it was a 1 mile or 1000 mile goal - the reaction I get would be the same. Yet if I shared these two achievements with my friends, the reactions I get would be completely different. Sure, an algorithm could be designed to treat accomplishments of various difficulties differently and can even be adapted to take into account that to some people, running one mile is equivalent to others running 1000 miles.  I doubt a smarter algorithm would actually make a difference. We might appreciate the intelligence of the algorithm but we’re not going to believe that this digital praise was authentic or that our software actually cares. We already have Google Now promising to give us the information we need when we need it and Amazon is trying to ship products to our doors before we even place the order. Yet as smart as these are, they’re not emotional. Even Siri is just an algorithm. As technology gets smarter, I wonder whether future generations will feel this way. We’ll continue to see these improvements as simply smarter software and better data but I doubt future generations will feel the same way. We’ve seen how dumb our technology has been and won’t be able to think of it as anything more than software. Future generations will be born and grow up in a world surrounded by smarter and better versions of what we have and won’t be saddled with this bias. Many believe the singularity will happen in our lifetimes but I think this will have the larger effect - that we’ll start viewing technology as our equal.
{% include setup %} One of the first things I was given when joining  TripleLift  was a Macbook Air and an Amazon EC2 instance to do my development work on. Before that, every company I worked at would give me a pretty powerful computer so that I’d be able to do my development work locally. At first, coding on a remote instance took some getting used to but now I'm a fan of this approach.    	 It allows me to work from any computer and paired with the highly portable Macbook Air I can work from virtually anywhere. On the flip side, it relies on a connection to the internet so if the internet ever cuts out it’s difficult to do work unless you also have it checked out locally.   	 It’s a great way to simulate a production environment. Especially on OS X where many packages require significant Stack Overflowing to figure out, being able to install libraries that will be used in production is a great way to work out the kinks and be confident that your code will run as expected.   	 Along these lines, the entire team will end up with a very similar environment which makes it very easy to give and receive help without having to get used to an entirely new environment.   	 If your application relies on EC2 it’s a great way to become familiar with the AWS ecosystem as well as reduce latency between Amazon’s various services. This is useful when you have a significant volume of data going to and from S3 and want to make it as quick as possible.   	 There are a ton of tools to make this easy. I’ve recently discovered the  SFTP plugin  for  Sublime Text  which lets you edit your files locally that are automatically synced to the remote instance. That paired with emacs or vim on the instance are all you need.     The biggest drawback is that you end up relying on the internet in order to get the most out of this set up. It’s possible to have your code synced locally for editing but getting set up to run locally defeats the purpose of having a remote instance since you have to install and configure the various packages. Given that there’s internet almost anywhere I go I think this trade off is worth it.
{% include setup %} Most developers are familiar with the FizzBuzz code test which is a quick way to filter out developers who can’t code. At Yodle, we had our own, slightly more challenging problem. The challenge was read in a text file and then print out the frequency each word appears in descending order. It’s more complicated than FizzBuzz but it assesses a variety of skills. The solution needs to do the following:       Read the file     Split the text into a list of words delimited by non-letter characters     Convert each word to lower case     Compute the frequency each word appears     Sort the results in descending order by frequency     Print this sorted list     I thought it would be fun to see if I could do it in PostgreSQL and was surprised by how quick and easy it was. The most challenging part was figuring out how to read the file - after that it was just using a few of the built in functions to clean and organize the text.  {% highlight sql %} DROP TABLE IF EXISTS temp_t; CREATE TABLE temp_t (c text);  COPY temp_t(c) FROM '/tmp/data.txt';  select lower(data.w) as word, count(1) as num_words from (   select regexp_split_to_table((select string_agg(c,' ') from temp_t), E'[^\\w]+') as w ) data where data.w <> '' group by word order by num_words desc, word; {% endhighlight %}  It also turns out to be very simple to do FizzBuzz in PostgreSQL. The nice part of the PostgreSQL solution is that it can easily scale to adding a 3rd combination. for example print Dozz if the number is divisible by 7. In the PostgreSQL solution, it would just require adding a row whereas in the standard solutions it would require a bit of work and would increase the chance of a bug.  {% highlight sql %} DROP TABLE IF EXISTS fizzbuzz; CREATE TABLE fizzbuzz (   num int,   text varchar(4),   priority smallint );  insert into fizzbuzz (num, text, priority) values (3, 'Fizz', 1), (5, 'Buzz', 2);  select coalesce(string_agg(fizzbuzz.text, '' order by fizzbuzz.priority asc), nums.num::text) as text from (   select generate_series(1,100) as num ) nums left join fizzbuzz on nums.num % fizzbuzz.num = 0 group by nums.num order by nums.num asc; {% endhighlight %}  Clearly PostgreSQL isn’t the right tool for every task but it’s surprising how powerful it can be given the right problem. It’s also a great way to think differently about a problem - even if you end up choosing a more standard solution.
{% include setup %} Given the recent news of Medium  raising $25M  and Svbtle  opening up  to the public I thought it would be an appropriate time to explain why I’m not using either of them. They’re both simple, clean products that allow writers to concentrate on their writing rather than configuring the dozens of options available in other blogging platforms. They’ve also done a great job with the typography that makes the content enjoyable to read. Compared to the other content websites out there, they’re incredible fast - they have a minimal structure and don’t load a ton of external content - especially when compared to the major publishers out there now such as the news sites and the social networks.  Yet I’m not writing on either of them, nor on Tumblr, Wordpress, or Quora even though I tried each one. For me, writing is about personal expression and being able to control the entire experience - both from the content generation up to the consumption - is important to me. I realize my design will never be as elegant as theirs but at least I can change it whenever I want. A year ago I wanted to include the D3 library for a  post  - this would have been impossible with Svbtle or Medium and I would have had to use static images. Recently I wanted to share  some charts  that I generated but on the first pass I realized they were too large for the content - with a few small tweaks to my theme I was able to incorporate them into my blog. I’ve also been thinking about using Mixpanel to track various events - something I’d never be able to do without full control.  The custom design is part of it but the other value lies in decoupling. I want to be able to decouple the creation of content from the presentation of content from the spreading of content. As an engineer, I like the fact that I’m not tied down to any platform - I know I can get additional pageviews by leveraging the built-in marketing networks these platforms provide but having marketing integrated into a creation tool feels dirty. I’d rather rely on Twitter and Hacker News to share my content. Sure it’s more difficult but it’s a more lasting way to get followers and readers for your content rather than the platform itself.  By being independent, I never have to think about how these platforms evolve and what the impact will be. They’ll have to monetize at some point and I don’t want to worry about that outcome. We’re already seeing massive changes in the way content is produced and consumed and being able to experiment with various approaches and technologies is important - especially for someone in technology. Relying on a third party that’s trying to do too much betrays that.  PS - I just realized I never mentioned how I host my blog. It’s currently hosted on Github pages using the  Jekyll-Bootstrap  plugin. At the moment, it gives me the control I want, deals with usage spikes, and is free. If anything ever changes, I can quickly pull everything down and host it on my own.
{% include setup %} Earlier today I read Nathan Yau’s  post  that had a quick  R script  to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were. I made a two quick changes to the script to make it more useful for my data: a map overlay to see where in the world I ran and an ability to view a zoomed in area of the map. I’ve included the updated script and the resulting plots below.  {% highlight r %} # From Nathan's script library(plotKML) library(ggplot2) library(maps)  # GPX files downloaded from Runkeeper files   lat_south & routes$long > long_west & routes$long                                                        A few specks here and there - clearly visible runs in the NY/NJ area as well as some in Virginia, New Orleans, and San Francisco. Can also see a few runs in India.                                                                         Zoom in on my runs in the Hoboken/NYC area. I don't have the lat/long coordinates here but if I had them it would be pretty easy to generate a map overlay.
{% include setup %} Now that I’ve started blogging I realize how important traveling is to creativity. After my trip to India I had a ton of different blog ideas. Some came from comparing the two cultures - for example cab rides and mobile phone business while others just came from realizations, such as the lack of truly global technology products. Many dismiss travel as a luxury but it’s a great way to bring a new perspective and let thoughts settle. In my case, it felt as if these connections formed subconsciously based on what I’ve been thinking about and doing actively for a year. It’s not surprising that our conscious experiences drive these subconscious connections but it’s interesting how stark this realization was. Prior to blogging, I never would have had an idea and immediately think of writing about it but it’s become a consistent thought. Travel encourages this serendipitous thought and companies should be encouraging it. Instead, many black ball employees who take a vacation and make employees feel guilty for taking some time off.
{% include setup %} One of my favorite things about Python is being able to use  virtualenv  to create isolated environments. It’s extremely simple to use and allows you to have different versions of Python libraries used by different projects.  The thing that's tricky is getting virtualenv set up on a production environment under different services since each one requires a slightly different configuration. I’ve gone through my projects and collected the various ways I’ve gotten it running for different services. I’m sure I could have done it differently but the following worked for me and will hopefully come in handy to others. If you have any questions or I'm not being clear enough let me know and I'll updat the post with more information.       Nginx and Gunicorn under Supervisor.      Nginx  - The configuration isn't anything different than normal except that you may need to specify some specific paths that are within your virtualenv  {% highlight nginx %}   Static files needs to point to virtualenv directory location /static/admin {   autoindex on;   root   /home/ubuntu/app/venv/lib/python2.7/site-packages/django/contrib/admin/; } {% endhighlight nginx %}      Gunicorn  - I have a shell script here that's used to set the various paths and options that configure Gunicorn  {% highlight bash %} #!/bin/bash set -e DJANGODIR=/home/ubuntu/app DJANGO_SETTINGS_MODULE=app.settings.prod  LOGFILE=/var/log/gunicorn/guni-app.log LOGDIR=$(dirname $LOGFILE) NUM_WORKERS=2 # user/group to run as USER=ubuntu GROUP=ubuntu cd /home/ubuntu/app source /home/ubuntu/app/venv/bin/activate  export DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE export PYTHONPATH=$DJANGODIR:$PYTHONPATH  test -d $LOGDIR || mkdir -p $LOGDIR exec /home/ubuntu/app/venv/bin/gunicorn_django -w $NUM_WORKERS \   --user=$USER --group=$GROUP --log-level=debug \   --log-file=$LOGFILE -b 0.0.0.0:8000 2>>$LOGFILE {% endhighlight bash %}      Supevisor  - Here we just point our configuration file to the shell script for Gunicorn  {% highlight ini %} [program:gunicorn-myapp] directory = /home/ubuntu/myapp user = ubuntu command = /home/ubuntu/myapp/scripts/start.sh stdout_logfile = /var/log/gunicorn/myapp-std.log stderr_logfile = /var/log/gunicorn/myapp-err.log {% endhighlight ini %}        Celery  under Supervisor.   In this case we just configure Supervisor to start virtualenv path for celery. A cool feature is being able to specify the environment variables - in my case to pass in the Django settings module.   {% highlight ini %} [program:celery] ; Set full path to celery program if using virtualenv command=/home/ubuntu/myapp/venv/bin/celery worker -A myapp --loglevel=INFO  directory=/home/ubuntu/myapp user=nobody numprocs=1 stdout_logfile=/var/log/celery/worker.log stderr_logfile=/var/log/celery/worker.log autostart=true autorestart=true startsecs=10  environment =   DJANGO_SETTINGS_MODULE=myapp.settings.prod {% endhighlight ini %}      Fabric .   The idea here is to make sure all our remote install commands are run after activiating the virtualenv.   {% highlight python %} from __future__ import with_statement from fabric.api import * from contextlib import contextmanager as _contextmanager  env.activate = 'source /home/ubuntu/myapp/venv/bin/activate' env.directory = '/home/ubuntu/myapp'  @_contextmanager def virtualenv():     with cd(env.directory):         with prefix(env.activate):             yield  @hosts(env.roledefs['db']) def rebuild_index():     with virtualenv():         run("python manage.py rebuild_index") {% endhighlight python %}
{% include setup %} A few days ago I discovered a new use case for Foursquare when I was meeting up with a friend. We were catching up and during the course of the conversation I realized I needed to introduce him to someone I had met earlier. Unfortunately, I completely blanked on his name and company. All I recalled was that he frequently checked into his company on Foursquare. Sure enough, when I opened up Foursquare I saw that he had checked in there that morning.  Clearly Foursquare isn’t a contact book app but it provided enough adjacent information and triggered enough thoughts to come in handy. It’s not surprising that we use apps and products in unintended ways but it’s always great being able to catch ourselves in the act.
{% include setup %} Something that’s come up frequently in my quantitative work is balancing heuristic and algorithmic approaches. It’s surprisingly difficult to get the first attempt at an algorithmic approach working properly - it’s not an academic exercise and real world issues will always appear. Over time I’ve found myself writing heuristic checks and tweaks to deal with the various edge cases the algorithmic approach encounters. For example, setting the min and max bounds on the results of a function or adjusting the slope of a curve if it ends up being set in the wrong direction.  It makes me wonder why I didn’t just start with a heuristic approach and worked on an algorithmic approach later after I’ve collected enough data and had a better understanding of the environment. The challenge is that a heuristic approach is only a temporary solution. It will be be difficult to maintain and improving it will require additional hacks and tweaks. A heuristic approach is great at setting a quick baseline but long term improvement will only come from a more rigorous approach.  An example would be writing an algorithm that bids on Google Adwords. A heuristic approach would take yesterday’s bids on a set of keywords, look at their performance, and adjust them or down based on a few simple rules. A simple heuristic might be to allocate budgets to different keywords based on their conversion rates. Unfortunately, this wouldn’t handle the case of different keywords having different costs and volume. Incorporating these would require additional rules and introduce more complexity.  An algorithmic approach would be to model the relationship between cost, impressions, and click through rates for each keyword and then optimize for total conversions. Each model would be designed to predict a dependent variable based on a set of independent variables and would require a statistical approach to make sure the results were statistically significant and safe to use. Each model would require its own research and set of tests but would lead to a more scalable system. Since the models would be independent of one another, you’d be able to improve them individually. It may turn out that our cost calculation model is great but the one that estimates impressions needs more improvement. Now you can focus on the model that needs the most work rather than trying to globally optimize the whole system.  As with anything, these are tools and their usage depends on the situation. It’s difficult to come up with a rule of when to use one over the other but I tend to favor heuristics when I need to do something quick and know it’s not going to require significant changes. If it’s a complicated problem that will require ongoing work, I’ll opt for the more rigorous, algorithmic approach. It will take more work initially but will be better in the long term.
{% include setup %}  There's a little known algorithm for constructing a  Sierpinski triangle  that is surprisingly easy to implement.  1. Start the three vertices that form a triangle 2. Pick a random point inside the triangle 3. Pick a random vertex 4. Go halfway from a the random point to the vertex and mark that point 5. Go to step 3 using the result of 4 as the starting point  I'm trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following  file . Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the  Sierpinski carpet  can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.       {% include D3 %}  {% include custom_js %}
{% include setup %}  As a follow up to my previous  post , I modified my Sierpinski generation  code  to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it's difficult to confirm given a fixed screen resolution. I'd love to know what's going on here and whether there's a relationship between the number of sides and the distance ratio.                                       N = 4, ratio = 0.4                                                      N = 10, ratio = 0.2                                 # of sides                                    Distance ratio                                            Generate!                    {% include D3 %}  {% include custom_js %}
{% include setup %} Turns out that tagging and categorizing blog posts is more difficult than I thought. I start with one set of tags but as I go through my posts I realize that my initial set of tags no longer make sense and I need to restart. The challenge is finding the set of tags that are specific enough to categorize a single post yet general enough that they can be applied to other ones. I haven’t found the perfect set of tags yet but did manage to go through and tag each of  my posts . Over time I hope to improve the tag taxonomy and update the existing posts. I’d love to hear suggestions on how to effectively organize my posts and examples of other blogs that are doing this well.
{% include setup %} Nearly every week I receive an $8 off $25 coupon from delivery.com. I’m sure the intent is to generate awareness and develop a habit but it’s having the opposite effect on me: I’m being trained to only order when I have a coupon. Couponing is tough - too little and it will have no impact but too much and you run the risk of training your customers to only react to deals which will result in you needing to have higher prices to maintain your margin.  I recall reading that the retail clothing industry is essentially sales and coupons. Since people are trained to only buy on sales, retailers will set an initial high price and use discounts to drop it to something that will appeal to consumers. No wonder clothing retailers have sales practically every week and definitely every holiday. Ron Johnson tried changing this when he became the CEO of JCPenney but wasn’t able to do it before his ouster. I suspect even if he had more time he wouldn’t have been able to do it without the support of other retailers. Even then, each would have a strong incentive to deviate, a la  Prisoner’s Dilemma , so they might just end up exactly where they started.  This begs the question of why other industries haven’t embraced the discounting model. My guess is that it would take a significant amount of effort to change consumer perception that a single company wouldn’t be able to apply and it’s just too complicated to orchestrate - especially when the payoffs are uncertain since competitors can quickly move to this model as well.
{% include setup %}  Earlier this week I received the following email from imo.im:      It’s amazing what $19 billion can do. For years imo.im has supported third party chat clients but within a couple of weeks of the WhatsApp acquisition they’ve abandoned that support to focus on their own network and become the next WhatsApp. For a while now they’ve been building features to support this move - videos in August, stickers in January - and I wonder what would have happened if they focused on their platform earlier. Now they’re just playing catch up to WhatsApp, Kik, Line, and countless others. Timing is critical and I suspect it’s too late for imo.im to be entering the first party messaging fray.
{% include setup %} Since writing the  Drowning in JavaScript  post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.                                                          Average load time.  To get a general sense of the data this plots the average time it took to load each URL. The interesting piece here is that multiple foreign sites take a while to load (163.com, qq.com, gmw.cn, ..) - I suspect a big reason is that there's quite a bit of latency since I'm based in the US. Another observation is that many news sites tended to load slowly (huffingpost.com, cnn.com, cnet.com). The Google sites loaded extremely quickly (all less than 1 sec) as did Wikipedia.                                                                          Load time boxplot.  This provides a bit more information on the load times by showing the min/max values as well as the median and the percentiles. Not a significant amount of new insight here.                                                                          Number of requests.  Huge variance here as well - rakuten.co.jp loaded almost 800 external files while the Google sites are all less than 10.                                                                          Number of request vs time to load.  This leads to the question of whether sites that are loading more files take a longer time to load. By plotting a scatter plot between the two it's pretty clear that all things being equal more files increase page load time.                                                                          Number of requests vs time to load linear fit.  A simple regression of load time as a function of the number of file requets confirms this. On average, each additional file leads to an additional 20 milliseconds of load.                                                                          File type frequency.  We can also take a look at the most common type of file requested. As expected, images are the majority of requests followed by JavaScript.                                                                          File types by url.  Not a lot of insight here but the colors sure are pretty. One thing that does standout is that if a site has a significant amount of file requests they tend to be of multiple types.                                                                          File type correlation.  We can plot a simple correlation of file type found on a page to see whether there are any file types that tend to be included together. Not much going on here.                                                                          Multiple linear regression.  And just for fun we can run a regression to see whether a particular file type leads to significantly worse load than others. I was hoping to show that having a lot of JavaScript hurts performance but that doesn't seem to be the case. I suspect it's due to the innate time differences it takes to load some sites (in particular sites outside the US) vs others.                      As usual, the code’s up on  GitHub .
{% include setup %} Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and  compare them  against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from  rahimnathwani  who ran the script in Beijing, using home ADSL, and wanted to share his dataset.  I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.  Below are some visualizations highlighting the differences in a few different ways. I’d love to get my hands on more data so if fee free to submit a  pull request  with your data and I’ll rerun the analysis. I’ve also included the R code that generate the plots below for those curious to see how they were done.                                                          Parallel plot.  The idea here is to see whether the lines are mostly horizontal or if they're steep. Horizontal lines would indicate that sites are universally slow (and fast) while steep lines indicate that some sites load much faster in one city compared to the other.                                                                          Load time differences.  Here we sort the sites by the difference in average load time, NYC minus Beijing. Most of the sites loaded faster in NYC but I suspect the biggest reason was due to internet speed differences. The sites that loaded faster in Beijing are for the most part in China.                                                                          Scatter plot.  A different perspective than the parallel plot but trying to answer the same question. We do notice a few outliers here which we can investigate by adding text labels.                                                                          Labeled scatter plot.  This provides a nice look at the outliers but makes it impossible to look at the sites that loaded quickly in both NYC and Beijing.                      {% highlight r %}times <- read.csv("out-times-beijing.csv", sep="\t", col.names=c("url", "time")) times$url <- as.character(times$url) final <- ddply(times,~url,summarise,mean_time_beijing=mean(time),sd_time_beijing=sd(time))  times2 <- read.csv("out-times.csv", sep="\t", col.names=c("url", "time")) times2$url <- as.character(times2$url) final2 <- ddply(times2,~url,summarise,mean_time_nyc=mean(time),sd_time_nyc=sd(time))  combined <- merge(final,final2,by="url") combined$time_diff <- combined$mean_time_nyc - combined$mean_time_beijing combined.m <- melt(combined, id.vars=c('url'), measure.vars=c('mean_time_beijing', 'mean_time_nyc'))  png('crawl-stats-comparison-parallel.png', width=600, height=600) ggplot(combined.m) +   geom_line(aes(x = variable, y = value, group = url)) +   theme_tufte() +   ylab("Load Time (ms)") + xlab("") dev.off()  png('crawl-stats-comparison-time-diff-bar.png', width=600, height=600) ggplot(combined, aes(x=reorder(url, -time_diff), y=time_diff)) +   geom_bar() +   theme_tufte() +   coord_flip() +   xlab("Load Time Diff (ms)") +   ylab("URL") dev.off()  png('crawl-stats-comparison-scatter.png', width=600, height=600) ggplot(combined, aes(x=mean_time_beijing, y=mean_time_nyc)) +   geom_point() +   theme_tufte() +   xlab("Beijing Load Time (ms)") +   ylab("NYC Load Time (ms)") dev.off()  png('crawl-stats-comparison-scatter-text.png', width=600, height=600) ggplot(combined, aes(x=mean_time_beijing, y=mean_time_nyc)) +   geom_text(aes(label=url), size=3) +   theme_tufte() +   xlab("Beijing Load Time (ms)") +   ylab("NYC Load Time (ms)") dev.off() {% endhighlight %}
{% include setup %} I’m an avid reader and signed up for  Oyster  as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.  One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.  These API endpoints turned out to be publicly accessible and it only took a short Python  script  to retrieve the books and dump them into a CSV file. This got me a little less than 3,000 books - turns out that the publicly accessible data is only a fraction of the entire collection and my endeavour wasn’t as fruitful as hoped.  I did manage to get a set of over 4,000 books and decided to have fun with it.                                                          Num of authors by num of books written.  Very few others appear more than once in the data set. This may be due to the limited data set or Oyster's job in editing the publicly accessible collections, maybe both.                                                                          Distribution of ratings.  Ratings are clustered around 4 with very few ratings under 3. This is most likely a biased set since the Oyster editors would have chosen the highest rated books to be featured in their sets.                                                                          Num of books by author.  Kurt Vonnegut has over 20 books available on Oyster with Shakespeare in the number 2 spot.                                                                          Ratings by author box plot.  Just a quick box plot to see the rating distribution by author.                                                                          Rating vs # of books.  Doesn't look as if the # of books an author has written on Oyster has any relationship with their rating. I thought maybe authors with higher average ratings would appear more frequently.                                                                          Rating over time by author.  This was a reach but I wanted to see whether an author was most likely to have better ratings earlier or later in his or her career. In this case it looks as if the publish date isn't the original authorship date so not a very useful analysis.                                                                          Publisher ratings.  Similar to authors, we can take a look to see whether some publishers have significantl higher ratings than others. This is a bit more useful since there's a lot more data per publisher than there is per author. I couldn't make much sense of the results here.                                                                          Avg number of pages by decade.  I wanted to see whether books were getting longer or shorter so did a quick plot of the average number of published pages by decade since the year was too fine. The publish dates aren't entirely accurate so I wouldn't read too much into this.                                                                          Avg rating by decade.  Similar to the previous plot but looking at the average rating rather than the number of pages. Seems to be pretty steady to me although this may be due to the dataset being a curated list of top books.                                                                          Rating vs date.  Another way to look at the previous plot but plotting each book rather than the average by decade. Not much going on here although this may be due to the biased dataset and flawed publish dates.                                                                          Rating vs number of pages.  This is an interesting one - are longer books more popular? Most of the books are clustered around a couple of hundred pages but longer books do tend to have a higher average rating. I'm not sure why this would be the case but would guess that only someone who's already interested in a long book would read it or stick with it enough to leave a review.                      As usual, the code's up on  GitHub .
{% include setup %}        Over the course of the past year I’ve become more and more pissed off at Gmail. I loved using Gmail when it launched - it made writing and reading email a pleasure. It was simple, clean, and responsive. Now it’s the opposite. All actions feel slow. The initial page load takes a substantial amount of time and then I get to wait for the various page elements to load - including a chat list that I’m almost never signed into and integration with a slew of other Google products. Loading emails or new tabs is noticeably slow and the search is sluggish for a company whose main product is a search engine.  This past weekend I decided to see what was out there and discovered  Fastmail . I’ve been using over the past few days and it’s been great. Emails are quick to load and send and the navigation feels snappy and responsive. I’m not sure how well it will work as my inbox grows but so far I’m impressed. It feels like Gmail when it launched almost 10 years ago. It feels odd to describe it in terms of Gmail since I’m bashing it but I can’t think of a better way.  I did a quick anecdotal test by looking at the networks tab in Google chrome as each loaded. Gmail loaded in 6 seconds after making nearly 150 requests and retrieving 338 kb while Fastmail loaded in a little over 300 milliseconds after making 18 requests and retrieving 128 kb. Repeating this a few times showed similar results. Others seem to be having the same issue since Google’s first auto-suggestion is “why is gmail so slow” when typing in “why is gmail”. The switch is also much simpler than I expected - I just have Gmail forwarding everything to my Fastmail account and it’s completely transparent to the outside world. In the future I plan on migrating everything to use my new email address but for now this is a good intermediate step. I haven’t heard much from others moving away from Gmail so I’d love to hear your experiences if you made the switch.
{% include setup %} I came across a neat Chrome extension called [Iconic History](http://shan-huang.com/browserdatavis/) that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search. My usage has also changed since I stopped using Gmail for my personal email and started using Fastmail. There’s the occasional new site but for the most part I’m a creature of habits - email, search, facebook, and Hacker News constitute the bulk of my internet activity. I’m honestly surprised by how much activity is taken up by a few sites. I suspect most people are similar - a few sites make up the majority of the page views. It would be great to see what this looks like for others and see if any general patterns emerge - I’m sure almost everyone people will have some mix of email, search, and Facebook but I’m curious to see what the outliers are.
{% include setup %} I’ve always been interested in hacker lore and have recently started compiling a list of tech-related stories and anecdotes that I found amusing. My ideal story includes an odd, somewhat ridiculous, situation that required a bit of technical ingenuity to solve while highlighting an arcane corner case and providing some glee.  So far, I’ve only been able to recall and find two such anecdotes but will add more as I discover them. Depending on how many I gather I may put together a permament list page. If you have any to contribute let me know and I’ll add them to this post.  The two so far:        Print this file, your printer will jam       The case of the 500-mile email
{% include setup %}      Every time Amazon announces a price drop there are always people pointing out that it’s still more expensive than other cloud computing services such as Linode or Digital Ocean. The Amazon fans then respond by saying sure AWS is more expensive but the value is the ability to scale quickly when needed.  For me, the biggest value behind AWS is the ecosystem and the included optionality. When building large scale web services it’s tough to know every issue you will run into and more often than not your needs and implementation will change. AWS provides a ton of available tools that make growing and scaling easier beyond the hardware itself. You may start with using EC2 for your server and S3 for hosting your static assets but over time you may start using Cloudfront as a CDN and Redshift for your analytics and EMR to process your various logs. That’s the biggest value in AWS - not being able to launch new machines quickly but having a set of infrastructure options that can be specialized to fit your needs.  It used to be that the physical hardware was orders of magnitude more expensive than engineers but this hasn’t been true for decades now - it’s perfectly reasonable to look for ways to reduce yours costs especially if it can be done quickly but obsessing over hardware costs, especially while you’re still growing, is a red herring. Building large systems is tough and the fewer things you have to worry about the better - using AWS reduces the chance that you will run into a scenario where you’re just not able to do something without changing your host and rewriting your architecture.
{% include setup %} Earlier today I was researching whether it was possible to generate Fibonacci numbers using a SQL query. A Google search turned up a  short PostgreSQL  query that uses a recursive approach. Since this is recursion, the query starts by defining a base case and then goes on to define a generation step with a stopping limit.  {% highlight sql %}with recursive f as (     select 0 as a, 1 as b     union all     select b as a, a+b from f where a < 100000 ) select a from f {% endhighlight %}  It’s interesting to see the edge features of a language and I find that query languages tend to have the most striking ones. My experience with the various SQLs has been that at the basic level they’re very similar but diverge significantly at the edges.
{% include setup %} It never bothered me when apps were acquired and shut down but the  Readmill news  hit me hard. It was one of the truly “free” ebook readers and never got in my way. It fit my behavior perfectly - I would download my books from wherever, drag them into the Readmill web app, and have them permanently accessible on my iPad after a quick sync.  My first reaction was wishing that it would be open sourced but that got me thinking about third party services. Numerous people have been saying how dangerous it is to rely on third party services but until Readmill it never really mattered to me. Sure, in the abstract it’s better to have everything hosted on your own but in reality it’s impossible to get to the same level of quality and experience for everything we use. We’re constantly balancing tradeoffs and we’re biased to favor the short term factors, such as ease of use and simplicity of set up, rather than long term ones, such as privacy, control, and data ownership.  I did some research on self hosting and came across  Sandstorm  - it’s pitched as a “personal cloud platform” and seems to be the solution to this reliance on third party apps. The idea behind it is that you have your own server and can download and install various cloud apps that will then have access to whatever data you give them. I’m eager to try this out. In the case of a Readmill replacement - I’d love to be able to host something on my own server to act as the backend and then download an iPad app that can connect to it. Both the iPad app and server can still be updated as new versions are rolled out but there’s no risk of the apps being shut down.  The business model would resemble Wordpress. The technology itself would be open source but if someone doesn’t want to run their own server they can pay to have their apps hosted somewhere else. There’s also room for a marketplace of premium or specialty apps that can be sold similar to the way themes and plugins are sold for Wordpress. People are already buying apps on the various app stores - it’s not a big leap to imagine people purchasing apps for their personal servers.
{% include setup %} I’ve been thinking about my history with computers and the impact they’ve had on me. I grew up just as computers were becoming mainstream, the spread of the internet coincided with my teens, saw the rise of “Web 2.0” during college, and got my first smart phone a few years after college. It’s fascinating to think about how much has happened to the world since the rise of computers and the varying experiences everyone’s had.  Nearly everyone has experienced the internet but at completely different points. Some experienced it when it was just text and had to use Archie, Gopher, and telnet to discover and consume content. Others joined through the AOL floppy discs and had to get multiple land lines in order to connect over dial up. Others avoided it for as long as possible but got dragged in when joining an office. And others are only getting seeing it now due to the spread of smartphones. Age is a huge part of the experience too. First using the internet as a child is different than using it as an adult. We have our own experiences that affect our interaction and dictate the experience we’ll have.  Each experience comes with its pros and cons but I’m happy where my experience fell on the spectrum. I got to deal with the joys of DOS, floppy disks and 16 colors and was able to experience the early days of the internet with a 14.4k modem and Lycos. I do wish I could see what it was like to code in the 70s and 80s when the engineering world was much smaller and one had to deal with a ton of constraints that we currently take for granted.  I wonder whether future generations will have similar experiences to what I had or whether technological advances will either be too predictable and make change appear gradual or not significant enough to warrant attention. I believe I got lucky since so many of the advances were consumer oriented and pro-hacker. Hopefully that the future brings more of the same.
{% include setup %} Inspired by yesterday’s post I decided to compile a list the memories I’ve had growing up with computers - hopefully they spark others. I tried to keep these in chronological order but I made some mistakes.  - The increase in the number of colors on a monitor and the various *GAs (CGA, EGA, VGA, XVGA) - Booting of one floppy disk and then running programs off another - Having both a 3.5 inch and a 5.25 inch floppy disk drives - Playing Prince of Persia for the first time - Using Norton Commander rather than the DOS command line - Shareware and the tons of paper catalogues selling games by mail - Upgrading to a 486 DX2 - Installing Windows 3.1 from a ton of floppy disks - The wonderful blue screen of death - Getting a second phone line in order to use dial up - Getting AOL instant messenger and my first screenname - Upgrading to a 56k modem - Using NetZero as an ISP - Hosting my first website at Geocities (wish I knew where it was and could dig it up) - Lycos and AltaVista - Finally getting a cable modem - Learning Pascal and C++ in high school - Warcraft 2, Starcraft, Diablo, Total Annihilation, Shattered Galaxy and LAN parties - Using Google for the first time - Signing up for the Gmail beta - Getting my first smartphone, a Motorola Droid I  Since then, I’ve had a ton of experiences but they feel incremental. I guess being steeped in tech for so long gives that perception.
{% include setup %} Lately, I’ve been thinking about tightly coupled systems and how prevalent JavaScript has become on the web.  Tightly coupled systems scare me. They will undoubtedly break and bring down big chunks of your infrastructure. The solution is to think about your system in terms of various independent services that are responsible for only doing a few things well that won’t bring down the rest of the system if they fail. This approach makes it easier to maintain your code as it grows and also reduces the risk of massive failure. The challenge is figuring out how to break your project down into these services and being sure to revisit that decision as you grow.  JavaScript is pervasive in the modern web. I’ve been using Ghostery for the past couple of months and am constantly amazed by how many external JavaScript libraries are loaded on popular sites. It’s not surprising to see dozens of libraries being included and evaluated. They range from advertising, to tracking, to adding functionality, and it’s incredibly rare to see just one.  On the surface, these two thoughts are different but their intersection is interesting. Similar to the way these sites include additional functionality by loading external scripts, we can compartmentalize various functionality into standalone components and make them available to our applications via simple APIs. In the case of a web app, we’d expose functionality through client side JavaScript libraries that would be coupled with a backend that does the heavy lifting. Rather than slicing horizontally, which is what typical apps do by having a separate UI and a s separate API, we can learn from these external libraries and slice vertically by function.  We integrate tools such as Google Analytics, Stripe, Disqus and MixPanel into our apps without a second thought and we should strive to write our code the same way. This allows us to choose the right tool for the job. If it’s a simple, low volume API that will be used internally, go ahead and do it quickly in a scripting language such as Python, Ruby, or PHP. If, on the other hand, the service will get a ton of requests, you can implement it in Node. In the extreme case of a site that’s using a ton of content, it may make sense to have the content hosted on S3 and just being retrieved by JavaScript called from the client - then the backend can be solely dedicated to providing the dynamic functionality.  This is a pretty extreme approach with it’s own set of challenges. It will definitely require more thought up front on how you want your application to work and will require a different approach than we’re used to but I feel this is the right approach if you’re building for scale. Every application should be broken down into components to see which would benefit from different approaches. If it turns out that two components have drastically different requirements, it might make sense to build them as completely standalone services and only communicate amongst each other via APIs.  This is nothing new, people have been preaching  service oriented architectures  for decades but I think we’ve forgotten it when thinking in terms of “web.” It feels more intuitive to split services in terms of frontend and backend but the right approach is to think in terms of actual functionality. It may turn out to be that tightly coupling the frontend and the backend is the right decision.
{% include setup %} Over the weekend I dug up an  old repository  I started to contain a running collection of  JavaScript tools  to make my life easier. Ever since I created it it had two tools - one to convert CSV/TSV text into a bootstrap table and the other to generate a “BCG style” matrix. Earlier today I coded up another script - a quick way to geocode a list of addresses. All you have to do is enter a list of address you want geocoded, one per line, and the script will use the Google Maps API to geocode each one with the resulting latitude/longitude being written to an HTML table. If you have any other suggestions for a quick tool let me know.
{% include setup %} Craigslist has become the fertilizer of the web. This realization came to me last week when I needed to get a replacement phone and decided to search for one on Craigslist.Filtering past the obvious scams I thought I find a legitimate offer and reached out. Within a few minutes I received the following response from "Kyle":   Hi a guy bought this from me, but I can tell you where I got it from. I got 3 of these from http://enetcweb.com/dibzees and I resold them for some extra money. The trick is to watch for bidding to slow down and then put in a bid. That's what I do and I win most of the time.    And this was from a listing that seemed legitimate! The vast majority of listings were clearly fraudulent that promised either amazing deals or was the same posting duplicated a dozen times with slightly different wording. Even beyond the fraud and scams there are probably tons of startups trying to take advantage of the network that Craigslist offers. Some are listing their products and services to validate their market and others are reaching out to owners of various listings trying to sell them on something.  At Makers Alley we posted a variety of products to Craigslist to understand the market. How many people would click through to our site? Would people more interested in buying custom furniture from individual makers or from a brand? Would anyone go through the entire checkout process?  We only received responses to ads that were positioned as independent makers and each of these responses was from another startup trying to get us to sign up for their platform. This feels like a perverse version of  "The Gift of the Magi"  - startups exchanging services with other startups - without any real consumers benefiting. I'd love to know what percentage of Craigslist is startups interacting with startups without either knowing the identity of the other. I suspect that for some verticals the number is shockingly high. This is also a massive example of how powerful the network effect is - the tiny sliver of value available in Craigslist is still enough to keep it useful.
{% include setup %} Last week I read an interesting article about  humans replacing robots  in Toyota's factory. The thesis being that only humans are creative enough (right now at least) to develop new skills and processes to deal with production inefficiencies. This rings true - in order to improve a manufacturing system you need to understand the entire process, from the raw ingredients up to the way consumers end up using the product. It's more difficult to do these days as products become more complicated with an increasing number of specialized components and I'm glad to see companies taking a longer term view and focusing on the value of human creativity rather than short term cost cutting.  Writing code is similar, it's easy to take off the shelf libraries and use them in an application. This is a perfectly reasonable approach when starting out and you need to get something built quickly; in fact this is the prefered approach so you don’t fall into the trap of premature optimization. But as you scale the big improvements will only come when you understand both the high level goals of what you’re writing and the low level details of how they are achieved. Then you can focus on removing and rewriting extraneous code and improve the components that are the bottlenecks. Without this understanding it’s very easy to waste time optimizing the wrong components rather than figuring out where the big wins will come from.
{% include setup %} Ever since I saw that Digital Ocean charged $5/mo, I’ve been meaning to migrate my sites and projects over from Linode but have been wary of dealing with the various issues that would ensue. I finally bit the bullet earlier this week and it went surprisingly smoothly.  My biggest concern was forgetting to copy some files that specified some esoteric settings I came up with when I first set up the projects. Luckily I didn’t run into this issue and most of the effort was spent in trying out my sites and looking at the log files to see which libraries were missing.  Here’s a quick overview of the process - the Digital Ocean  migration guide  was a big help.  - Follow the guide and install/configure rsync on both boxes - Use rsync to migrate the relevant files and folders. In my case it was everything in /var/www, the sites-enabled apache folder, and the MySQL dump - Install apache, MySQL and the other likely required packages. An overkill approach would have been to list every package on my old box and install it on the new one. - Load the MySQL dump into the new instance - Restart apache and go through the configuration settings. All the issues were due to disabled apache modules (headers, deflate, expires) and enabling them resolved them. - Go through each of the configured sites and make sure they worked. No downtime wasn’t a requirement for me so I ended up changing the DNS settings one by one confirming that each site ran properly. The PHP sites worked immediately but the python sites needed some packages installed via pip. - Copy the cronjobs from the old box to the new one  None of my projects were complicated and the migration went as smoothly as possible. Most of the time was spent waiting for the files to copy or packages to install and I never ran into an issue that didn’t have an immediately obvious fix. The error log was a big help - it gave me a quick way to identify problems and the missing packages. If you’re on the fence about migrating to Digital Ocean and the only thing holding you back is worrying about the migration I suggest just going for it. Worst case is you spend a few hours playing around with a new box.
{% include setup %} During my consulting years I’ve done a ton of Excel and noticed people getting frustrated by two seemingly simple operations. The first is getting a worksheet with gaps in a column and needing to fill it with values from the cells above and the second is doing a cross join between two sets of values.  The solution to the gap filling can be done by explaining the solution in such a way that it can be implemented via an Excel formula. The best I could come up with is “If a gap is a value, take the value of the closest non empty cell above it, otherwise keep its value.” We can create a formula in another column that takes this approach and after coming up with the new cell values and pasting them over the originals. In the image below, the formula in cell D2 is  =A1  and the formula in D3 is  =IF(A3="",D2,A3)  with D4 down being relative copies of D3.      The cross join problem is similar - we have two sets of values and need to enumerate each combination. The key point is realizing that we know what the values should be in a particular row and deriving the formula to get those values. My approach uses integer division to get the value in the first column and modulo to get the value in the second column although any function that’s deterministic should work. In the image below, the formula in cells D2 through D25 is  =INDEX($A$2:$A$5,(ROW()-2)/$H$2+1)  and the formula in cells E2 through E25 is  =INDEX($B$2:$B$7,MOD(ROW()-2,$H$2)+1) .      The file with the two approaches can be grabbed  here .
{% include setup %} An approach to scaling sites that I haven’t seen used much is using S3 as much as possible and falling back to it in case the dynamic elements are either not needed or unavailable. Many sites will host their static assets on S3 but there’s a lot more that can be pushed that way.  Reddit gives logged out users  cached content  rather than dynamically generating a page. That way logged in users get the full experience but logged out users may see a slightly out of date site. Content rich sites would benefit significantly from this approach - it would reduce cost and ensure uptime. If it turns out that the site does go down you can flip a switch and serve the cached/static content to everyone while the site is brought back up.  Current frameworks allow you to cache various elements of a page so they don’t need to be regenerated every time but they’re still dependent on the web server. If that goes down the page won’t be generated. An interesting idea might be to use client side JS to make a quick request to a server to see if it’s up and if not fall back to an HTML file on S3. I don’t know any sites that take this approach and would love to see some examples.
{% include setup %} I spend a large chunk of time working in the terminal and was curious to see what my most commonly used shell commands were. This also gave me an opportunity to practice writing one liners and learn a bit of awk.  {% highlight bash %}history | cut -d' ' -f4 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r{% endhighlight %}  The script is simple - look through my command history, extract the first word, and count the number of times that word appears. I was surprised to see git at the top but it makes sense - I tend to run it as a sequence (git status, git commit, git push) so it leads to an inflated count. The rest make sense - they’re a mix of the standard navigation commands as well as command related to my current projects. Next step is to set up a cron job to track this usage over time and see how it changes.      Command  Frequency      git  347    ls  103    python  89    fab  70    cd  49    ssh  28    cat  28    ping  23    emacs  22    stash  15    rm  15    rake  15    pip  14    cdblog  14    pwd  12    jekyll  12    connectec2  11    sudo  9    workon  7    wc  7    phantomjs  6    history  5    head  5    c_do  5    brew  5    sh  4    mv  4    make  4    grep  4    sass  3    redis-cli  3    open  3    mkvirtualenv  3    find  3    celery  3    source  2    sed  2    redis-server  2    mkdir  2    echo  2    dig  2    cp  2
{% include setup %} The Oculus acquisition got me thinking about the impact it would have on software development. We currently have a slew of editors and IDEs that are making us more productive and I wonder whether there's a place for VR. I don't think it's going to be as extreme as  Minority Report  (at least at first) but I do expect some things to get much easier.  Typing is currently much quicker than any other form of data entry and I don't imagine VR making this any better. While writing this I took a break and tried looking at the letters making up this sentence on my keyboard and it was slower - not to mention the mistakes that will likely occur during transcription. The only thing that would make data entry faster would be a direct neural connection which isn't going to be happening any time soon.  Navigation and context switching might become easier. I currently spend a fair amount of time tabbing through windows until I find the right one - a visual approach might make this process much better as long as it's implemented well. I'm also significantly more productive with an additional monitor - if VR is able to increase my working area I suspect I'd be more productive.  Debugging should get better. Being able to quickly examine various states during the course of debugging is extremely useful and I haven't seen a tool that makes this simple. An interesting, scifi-like solution would be to somehow provide a three-dimensional view of code execution and be able to view your code from an additional dimension. Being able to quickly go back and forth through time would make tracing code significantly easier.  Everyone's expecting VR to have a huge impact on gaming but I'm more interested in seeing the unforeseen use cases emerge. These will have an impact not only on entertainment and consumption but also on creativity and productivity.
{% include setup %} I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.  {% highlight bash %}grep "Invalid user " /var/log/auth.log | cut -d' ' -f8 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r | head -n 100{% endhighlight %}      Username  Count      test  141    postgres  116    oracle  88    web  75    test2  74    admin  59    jboss  49    ubuntu  45    webmaster  43    user  42    tech  40    debian  40    testuser  39    server  38    penguin  38    shoutcast  36    rdp  36    www  35    radio  35    ftp  33    test3  30    student  29    guest  29    toor  21    public  19    testing  15    tester  15    students  15    var  13    gov  9    adm  9    x  8    nagios  8    zabbix  7    z  7    y  7    w  7    vyatta  7    u  7    t  7    shell  7    s  7    r  7    q  7    p  7    o  7    n  7    michael  7    m  7    l  7    k  7    j  7    i  7    h  7    g  7    f  7    e  7    dup  7    d  7    ch  7    c  7    b  7    a  7    sales  6    office  6    home  6    data  6    bash  6    apache  6    administrator  6    v  5    test1  5    teamspeak  5    ssh  5    plesk  5    master  5    linux  5    ircd  5    http  5    walid  4    vnc  4    ust  4    ts  4    temp  4    telnet  4    smmsp  4    smart  4    samba  4    org  4    operator  4    net  4    named  4    mike  4    library  4    info  4    hacker  4    git  4    ftpuser  4    dan  4    cc  4      The usernames were all over the place - from generic ones (such as test, admin, ubuntu, guest) to the names used by various services (postgres, oracle, nagios) to letters of the alphabet. There was also a slew of common English first names. In total, there were ~1500 unique usernames that attempted to access my box.  The auth.log file also contains the IP address of each attempt and we can easily summarize by that.  {% highlight bash %}grep "Invalid user " /var/log/auth.log | cut -d' ' -f10 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r | head -n 100{% endhighlight %}      IP  Count      162.13.41.12  874    176.31.244.7  733    216.127.160.146  572    195.50.80.169  382    66.219.106.164  359    199.33.127.35  220    112.167.161.194  98    128.199.226.160  66    198.50.120.178  60    189.85.66.234  37    14.18.145.82  29    166.78.243.86  23    222.190.114.98  22    130.126.141.74  18    178.208.77.133  17    61.160.213.171  8    49.213.20.249  8    23.253.51.76  7    178.254.8.177  7    193.107.128.10  5    121.167.232.196  2    107.182.134.51  2    82.221.106.233  1    74.3.121.10  1    72.225.239.90  1    111.74.134.216  1      In this case, the total number of IP addresses is significantly smaller with only 26 unique IP addresses trying to connect. I took a look at a few and some of them look to be legitimate sites that may have been compromised.  If you have a box open to the world, you should make sure it’s secure. A small program that makes this easy is fail2ban - it scans log files and bans IPs that have had too many failed attempts. Two other quick fixes are to disable password authentication entirely and rely solely on public key authentication which is significantly harder to crack and change the default SSH port from 22 to something else. These should be enough to eliminate the bulk of attempts and keep your box secure.
{% include setup %} MixPanel has a clever way of handling failed login attempts. Instead of locking the user out of the account or forcing a password reset they send an email with two links - one to log in to the account directly and another to reset the password. I don’t recall ever seeing this approach before and wish more sites started doing it. This approach also obviates the need to even have a password - a site can just send a “login link” for an entered email address and the user can login via their inbox. This is similar to the way we login via the various social networks but instead of being sent to a social network for confirmation we are sent to our inbox. The only friction is having to go to your inbox to click on the link but since most people keep their inboxes open all day I don’t see this as a huge problem. The other advantage is security - most people use the same password across multiple sites so if one is compromised the others become vulnerable. Under this approach each site will have its own security controls and it becomes impossible for one site’s shoddy security to affect another’s. This is probably too drastic of a change for most users but I’d love to see sites start embracing this model.
{% include setup %} Fab recently  laid off  a third of their staff as they transition from designer flash sales into customized goods and their own private label. The business is tough and reminded me of our experiences building  Makers Alley . We initially set out to build a place where people can buy customized, personal furniture from local designers. The idea was was consumers would benefit from being able to get items that are custom made and can be customized to fit individual styles while supporting a local business and makers would have a new avenue to sell their products and build their brand.  Unfortunately, we faced huge obstacles on the consumer side. We thought that with such a feel good story and compelling vision we’d have no trouble attracting people to buy furniture but it was extremely tough. We launched during the flash sales era where everyone was on a hunt for deals and discounts. We considered showing discounted prices by inflating the original price but that just didn’t feel right and we didn’t want to diminish the work of our makers and designers.  I recall talking to one of our woodworkers who told us about being approached by Fab which wanted to include some of his pieces but they wanted him to sell his pieces at too steep of a discount in addition to Fab taking a cut that it made no economic sense to do it. This validated our belief that we did not want to go down the discount route but that still didn’t help us attract consumers - especially when they were being inundated with expiring deals and flash sales. Fab wanted to promote good design from new designers but their flash sales model prohibited them from working with the designers and makers who needed it the most.  Fab’s new direction feels bittersweet. It sucks for any entrepreneur to realize that a business model doesn’t work and it absolutely sucks to have to lay off amazing people who actually believed in your vision but at the same time I do think that their new model is more sustainable and better for the long haul, not to mention that it’s now more similar to what we were trying to do Makers Alley.  Building a custom label should be easier than starting from scratch since Fab’s already associated with good design but their challenge will be changing the mindset of their customers from expecting great deals to be willing to pay more for curated, unique designs. This is going to be tough and there’s a lot more competition in this space. I suspect their biggest competitor is Etsy and that’s why they’re focusing on their private label in order to move away from that model and become more like the Warby Parker for design.
{% include setup %} A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and  hacked one together  in JavaScript. It’s currently a part of my js-tools  GitHub repo  and suggestions are welcome. One thing I definitely need to add is the ability to specify the range of possible values for each field rather than using a hardcoded distribution.
{% include setup %} Data science has earned the reputation of being complicated and inaccessible to those without an advanced degree but it doesn't have to be this way. The goal of data science is simply to unlock insights and value from data. There's no need to make it more complicated than that. Of course, there are times where the data requires some domain knowledge or is just too big for someone without the necessary experience to work with but I believe that most places have enough low hanging fruit that anyone who can write a quick script can contribute and do data science.  This can be as simple as looking at a site's log files to figure out the most popular pages and how long they take to load in order to identify slow pages that can be sped up. Another quick task can be writing some queries to provide summary statistics across varying dimensions and visualizing them to see if any patterns emerge. A more advanced project can be going through a codebase and implementing a system to help track metrics in a way that makes future analysis easier. None of these require advanced quantitative knowledge and there's no reason that anyone should feel unqualified to dabble in data analysis. In my experience the most value has come from someone noticing something interesting and asking the right questions that led to a more thorough analysis. The more people that approach data with a curious mindset the more valuable a company's data becomes.  There's always the risk of discovering something  spurious  so it's important to validate discoveries but I'd rather have signals and noise than silence - especially if this encourages more people to become interested in data. At first, this can pose a problem for the people who need to deal with the noise but over time people will become more aware of what's valuable and can help identify areas of further analysis. This is the way to build a data driven culture - not by hiring a few data scientists.
{% include setup %} I ran into an odd bug today where a database entry was reverting itself after a seemingly simple update. For  Better404 , a customer can change the design of their 404 page but it turns out that every once in a while a change would go through but within a minute would revert back to the previous value. At the same time, update queries run directly via the MySQL client ran fine and were not being reverted - just the ones made through the site. To see what was going on, I enabled full query logging in MySQL (SET GLOBAL general_log = 'ON') and sure enough I saw a lagging query that would update the record to the prior values. Stepping through the code I was able to figure out the cause.  As with most  Heisenbugs , it turned out to be a timing/concurrency issue. As part of a user updating their settings, we would kick off a job to crawl and index the modified domain. After running, the job would update the record with the newly crawled timestamp. Unfortunately, the other fields were updated as well and since this indexing job was kicked off before the record was updated, stale data was written to the database. After figuring out the cause, the fix is easy. One option is to kick off the indexing job after the database is updated with the new values so that the indexing job will use the new values. The second option is to modify the indexing job to only update the relevant fields. Both options were trivial to implement so I played it safe and did both. The first required changing the order of some lines and the second was just specifying an optional parameter to the Django model’s save method. Below’s a visual representation of what was happening and the two fixes.      Unfortunately, concurrency and timing bugs tend to be the most difficult ones to figure out but whenever there’s non deterministic behavior they should be at the top of the suspect list. It’s important to know the tools we’re using and their default behavior - it’s possible that the approach they take differs from how we think they work and becomes a major source of bugs and frustrated debugging efforts.
{% include setup %} Although there’s been a ton of services launching trying to help people do everything under the sun I’ve been finding myself going back to simple tools. One of these has been replacing Evernote with text files that are synced via Dropbox after getting annoyed with Evernote one too many times. It’s great, I edit files in  Sublime Text  and came up with my own naming format to make search easier. If that fails, I just use grep and find and almost always find what I’m looking for. Since the files are plain text, every Linux command is a tool. It’s trivial to do bulk search/replace using sed or compose one liners to do various filters and counts. It’s easy to sort files by time or size and being able to do a regex search comes in handy when you only have a vague idea of what words you used when writing a note.  I wish more services would approach their products the same way - only focus on one thing, do it well, and allow other services to integrate with it in a simple way. This belief has been the foundation of the  Unix philosophy  but sadly most businesses haven’t embraced it due to a desire to encourage lock in and increase switching costs. Hopefully this changes in the future.
{% include setup %}      I recently needed to sell something on eBay and encountered an issue I thought they would have taken care of by now. Apparently you can pick the start time for an auction but it has to be in PDT - there’s no way to choose another time zone. The change is trivial and one would think that a $60B company would be able to support multiple time zones in their core product. Someone brought this up in  the forums  in 2012 and it turns out that time zone support is only present in the forum to allow users to see posts with a local time.  Whenever I see seemingly obvious UX anti-patterns it makes me think there must have been an ulterior motive. In this case, I suspect not having a time zone may lead to a smoother distribution of auction end times which keeps product demand high and can distributes bids more evenly throughout the day. Another reason may be the desire to reduce risk - time zones are difficult to get right and it’s possible that eBay doesn’t want to expose themselves to the liability. The most likely reason may be that they are just not investing heavily in the use experience. They’re already the market leaders and it may make more sense to focus on marketing and selling rather than on making it easy to list a product. If someone’s already started to list a product it’s unlikely that the lack of a timezone will cause him to change his mind - it didn’t in my case.
{% include setup %} Earlier today a I got a message on Twitter letting know that my Twitter account was hacked. Sure enough when I looked at my tweet history I saw a slew of weight loss tweets linking to a Pinterest pin. Turns out that my Pinterest account was compromised and since it was connected to Twitter every time someone pinned a weight loss link it got shared on Twitter.  The fix was simple - block the Pinterest app from within Twitter, disconnect Twitter from within Pinterest, and reset my Pinterest password. Unfortunately, none of these can be done via the apps nor the mobile sites. Instead, both provide a minimal settings page with no clear way of accessing the complete settings. Since I wasn't near a computer, I had to use the Twitter app to delete the spam tweets that were being posted a few times each hour.  This is a frustrating design pattern. Sites and apps should default to a mobile optimized experience but if any functionality is missing they should allow users to fall back to the web view. Arguably, that option should always be available since a user may have a specific flow in mind and shouldn't be forced to learn a new approach - especially if something's urgent. In my case, this wasn't that huge of a deal but I can see how it could have been.  The other lesson is that in this world of interconnected apps you're only as strong as the weakest app. Twitter followers don't care why they're seeing spam and having TFA enabled on Twitter won't help you if another account is compromised.
{% include setup %} A couple of weeks ago I attended a talk by  Professor Michael Stonebraker . For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.  This is already happening at the larger tech companies and it’s spreading downwards. Data is becoming increasingly important and having the tools available to leverage it is a critical advantage. It’s impossible to find a single database that can be used to run a transactional site, support complex yet quick analytics queries, scale to terabytes of data, and still maintain synchronization between its various nodes. Each of these use cases requires a database that’s optimized for that need and an application that knows how to leverage that database.  The neat thing is that many of these newer databases have embraced a SQL-like query syntax so it’s surprisingly easy to get started. The challenge is that this similarity is only skin deep and the implementations are drastically differently both in terms of how the data is stored as well as how the queries are executed. So although it’s simple to write a query that will execute on both Redshift and PostgreSQL it’s likely that this query isn’t as efficient as it can be on one or both of the databases.  This is the right approach for the specialized-database future. By providing a standard interface it makes us more comfortable with introducing a new database into our stack while providing very different functionality under the surface. It’s likely that the first implementations won’t be ideal but as teams become more comfortable with these new systems the implementations will evolve. I hope this pattern of standardizing around a simple interface becomes more popular. Then the backend can be designed for a variety of use cases without forcing the users to completely change the way they think.
{% include setup %} After reading Gilad Lotan’s  post  where Gilad bought 4,000 Twitter followers in order to analyze them, a  friend  of mine was inspired to analyze his followers to see if he could get any insight and come up with a neat visualization. The first step was downloading a dataset containing his followers and followees as well as the followers and followees for each of those accounts - the idea being that by going two levels deep you see how similar the various accounts are to each other based on who and what they follow and whether there are any patterns.  I offered to write a short script to help him pull the data and it turned out to be easier than I thought due to the excellent  Tweepy library . The biggest challenge was figuring out how to use Tweepy to deal with Twitter’s absurdly strict API limit (15 requests per 15 minutes) since the documentation was a bit sparse but after discovering the Cursor object it became surprisingly easy to iterate through the results and wait for 15 minutes for API errors.  The code currently works in a user id rather than username world since I wanted to avoid making additional calls but that can be implemented in the end to just pull the usernames for every user id in the dataset. The  code’s  up on Github so feel free to try it out and let me know if you run into any issues.
{% include setup %} As much as it pains me to admit it I’m really enjoying Google Now. I’m aware of how much information I'm sharing with Google to make it helpful but at the moment I find the tradeoff worth it.  It came in especially useful as I've been traveling over the past couple of weeks:  - Show the official exchange rate when traveling. This may not be perfect, especially in the case of "blue markets," but it's nice having a rough idea of how much a US dollar is worth. - Flight information. Since my flight details get sent to my Gmail account, I can quickly tell whether my flight's delayed and what terminal and gate it's scheduled to depart from and arrive to. This is useful to have when I need to make a transfer since I can quickly see where my next flight departs from. - Flight boarding passes. In addition to the flight information Google Now also shows the boarding passes for my checked in flights. I didn't have to do anything to board a plane other than activate my home screen and place it against a scanner. - Hotel information. Similar to flight information, I get a card telling me where my hotel is and how to get there. - Google calendar integration. This is an obvious one but I run my life through Google calendar. This gives me constant notifications of what I have to do when and as long as I enter an address for my events I also get an estimate for when I should leave.  A concern is that to actually make it useful I have to integrate more and more of my world with Google and I expect this to get worse as more Google Now cards are developed. The optimist in me hopes that Google Now will be opened up to third party developers in a future version of Android but the cynic suspects it's not going to happen.
{% include setup %} I’m a bit late to the Amazon Fire Phone party but wanted to chime in with a perspective I haven’t seen written about. Amazon is offering a  $6 CPM to mobile app developers  that launch an Amazon app during Auguster and September. Given that typical CPMs are  less than a dollar  with premium publishers like Facebook and Twitter getting  close to $6 , this is a very aggressive move by Amazon to build out their ecosystem.  A common refrain mentioned is that ecosystems drive smartphone adoption. This is why it's extremely difficult to compete against iOS and Android which combined have  72% share  of the smartphone market. Amazon is trying to jumpstart this by offering a potentially huge sum to developers if they're able to get the users. Amazon will have a difficult time getting a large market share but I suspect they will find a niche in a particular customer segment and a few app developers will be rewarded.  It's also interesting to contrast this with Microsoft's approach of paying developers a flat sum to create a Windows phone version of their app. On one hand, this approach allows Microsoft to be selective since they can pay to get the apps they want. On the other hand they run the risk of paying for an app without any users. The interests between Microsoft and it's app developers are not as aligned as those between Amazon and it's app developers.  One thing that may have influenced this decision is that Amazon's OS is Android based so it's significantly less work to port an app to work on Amazon's phone compared to a Windows Phone. The pitch to developers is akin to saying make a few changes to your app and potentially earn a bunch of money while Microsoft's would be write a new app in a new language and we'll pay you for your efforts.  Almost all Amazon phone coverage has been bearish and I wonder whether this move will have an impact. Apps to drive smartphone adoption but with so many good options already available replicating an ecosystem won't be enough.
{% include setup %}  Actually, as the artist gets more into his thing, and as he gets more successful, his number of tools tends to go down. He knows what works for him. Expending mental energy on stuff wastes time.  &nbsp;&nbsp;- Hugh MacLeod,  Ignore Everybody     This quote refers to art but it can just as easily apply to code. As developers, we’re constantly exposed to new tools and technologies and are curious to try them out. Everything new looks shiny and we imagine it will solve all the problems we’re facing. Yet almost always new tools bring their own set of problems and take time to learn. Instead of constantly chasing something new we should try to master what we’re already using - the value of that will most likely outweigh playing with a new toy. It’s better to rely on a small set of tools that we understand well rather than have a superficial knowledge of dozens of tools and technologies.  Of course it’s important to try out new tools since many of them are useful but it’s dangerous to rely on new tools exclusively and use them for a new project just because they’re the next big thing. To get some exposure to new tools, I will use them in toy projects or during hackathons so I can get a sense of how they work, what the strengths and weaknesses are, and how much I enjoy using them. Only then will I consider using them in a real project.
{% include setup %} I’m currently working on an application using  Netty , a low level network framework, and it’s given me a wonderful education of the HTTP protocol. Prior to this project, every web application I’ve worked on has leveraged a framework that removed the low level details. They built the HTTP requests from multiple packets, took care of various encoding issues, dealt with keep-alive connections, came with built-in support for sessions and cookies, and in general made it extremely easy to get a web server up and running.  Writing a Netty application is completely different than using a framework such as Django, Ruby on Rails, or Node. You get a much better understanding of how TCP and HTTP work and doesn’t actually take that much time once you get the hang of it. Building an HTTP request from individuals packets was completely novel and finally getting it to work made me feel the same way as when I built my first site. If you’re interested in or currently working in web development and you haven’t worked with a low level framework, take a weekend off and give it a try - it’ll give you a newfound appreciation of how the modern web works.
{% include setup %} I recently needed to set up HTTPS for my side project,  better404.com . Amazon makes it easy to  set up  by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.       Get an HTTPS certificate. I bought three certificates from Namecheap a couple of months ago when they were running a  promotion.     Go through the certificate generation process. I found  this guide  that explains how to do it detail and worked well for me. The only things I had to change where the  Nginx certificate configuration folder path (/opt/local/nginx/conf/certs => /etc/nginx/certs/) and replacing the filenames to be more specific (domain.* to better404.*). Note that this process is not immediate and you will need to send the contents of your CSR file to the SSL provider and they will respond back with the SSL certificate to use.     Enable SSL in Nginx. The previous guide provides some information here and I’m including the relevant parts from my configuration. I chose to redirect all traffic to HTTPS rather than supporting both simultaneously. {% highlight nginx %}#Nginx config server {     listen *:80;     server_name www.better404.com;     rewrite        ^ https://$server_name$request_uri? permanent; }  server {     listen *:80;     server_name better404.com;     rewrite        ^ https://$server_name$request_uri? permanent; }  server {     listen *:443 ssl;     server_name better404.com;      ssl on;     ssl_certificate certs/better404.pem;     ssl_certificate_key certs/better404.key;{% endhighlight nginx %}     Allow Nginx to bind to the IP address. One thing that’s not mentioned in the guide and required a bit of digging around is that you need to allow Nginx to bind to the non local IP address - otherwise it can only access the private IP address set by AWS. There’s a  quick guide  on how to do this I found on StackOverflow.     If you have any questions feel free to leave a comment and I’ll try to help out.
{% include setup %} A little known feature in AWS is an endpoint that allows you to retrieve various information about about the requesting instance. If you log in to one of your EC2 instance and make a simple request to http://169.254.169.254/latest/meta-data/instance-id you will get back the id of that instance. Similarly, you can get all sorts of  other instance information , including the public hostname, the instance region, and instance type.  This can be useful when you want to automate a simple deployment where you have a few instance with a variety of roles. A lightweight approach would be to use the  AWS CLI  to retrieve a list of all running instances along with their tag names, make a request to the meta-data/instance-id endpoint to get the id of the current instance, and then look up that id in the instance list in order to figure out what the role of this instance should be. Then execute the appropriate set of scripts to configure the instance properly.  More advanced solutions would involve using  Chef ,  Puppet , or  Opswork  but they come with a steep learning curve and overkill for a simple application. If it turns out your application is growing you can always upgrade to a more robust deployment solution.
{% include setup %}                             Source:           EcoVeganGal.com                       Food trucks have taken over every city I've been to. A decade ago the best you could find was a taco truck but now food trucks run the gamut from the simple taco up to experimental vegan. Priceonomics has a  great piece  on the rise of the food truck as well as a fascinating look at the economics of the food truck industry. If you haven't read it yet definitely check it out.  The summary is that the decline of the housing market in 2008 led to a drop in construction which caused many food trucks to sold at a discount. These food trucks were then bought by aspiring chefs who wanted a low risk way to start a restaurant. The cheap trucks gave chefs a way to experiment with varying cuisines and locations without incurring the massive costs of starting a restaurant.  This is a perfect example of how capitalism should work. Industries that are no longer profitable make way for new ones that leverage existing infrastructure. In the future these food trucks will transform into miniature carriers that will carry drones that will deliver food throughout cities. When one door closes, another door opens.
{% include setup %} I thought I've seen every design anti-pattern out there but had the luck to run into a new one a couple of days ago. I was buying domains on  Namecheap  and ended up going through checkout without verifying the payment details. Turns out that I had an old credit card on file which led to a declined payment. I was redirected to a page that told me to update my payment methods but instead of doing that I ended up hitting back and refreshed the page which triggered another failed charge attempt. One more and I'm locked out of my account.  Ironically, other than speaking to a rep the only way to unlock my account was by entering the last 4 digits of the credit card which I no longer have. It only took a few minutes to clear that up with the rep and it was basically my  fault but it's still interesting to see security questions based on ephemeral information. Old accounts are likely to have outdated credit cards, phone numbers, and addresses. In those cases it's too easy to get locked out and be stuck with having to speak to a service rep - and I suspect most companies won't be as responsive as Namecheap.
{% include setup %} In the quest to reduce the amount of stuff I own I've been going through various cabinets and boxes and trying to list everything on eBay. The most common items are old cables with no corresponding devices (or any ideas what these devices even are) and old DVDs.  Looking at the historic prices for these items doesn't make me happy - a Lenovo laptop charger is less than $10 while a Raging Bull DVD is a couple of bucks. But this entire process got me thinking about bundling. Bundling makes sense when selling cheap products. It's not worth the time to list these individually and it's likely that there are only a few people interested in each item. Bundling them makes it more likely that various items will appeal to a variety of buyers and increase competition. The Lenovo adapter may appeal to one person while a Game Boy charger may appeal to another. By having them in the same lot they are competing against each other and are willing to bid higher to get what they want.  High quality items are competitive on their own. The sum of the individual sales will be greater than the value of the bundle. The intuition is that bundling premium items when buyers only want a single item will decrease buyers' willingness to pay more for the extra items.  HBO's the standard example - it has enough consumer demand that it can stay independent and charge a premium. Other channels need to bundle and subsidize each other. I suspect many would be more successful breaking out, such as ESPN, but they're either stuck in contracts or fear change.
{% include setup %} While working on  Pressi , we found a niche selling "social media mashup pages" to colleges and small universities. Once we discovered it we needed a quick way to find these colleges and identify the contact details of their marketing or social media directors. Searching for this information was not the most efficient use of time for our small team so we went looking for other options.  Two options that stood out were  Mechanical Turk  and  Odesk  but they were designed for quick and simple tasks. Using them for complex tasks would result in poor quality results. One advantage that Odesk had was that it allowed us to work with the same person for many tasks - something we couldn't figure out how to do using Mechanical Turk. This allowed us to come up with a set of potential candidates based on their project interest and skillset. We gave each of them the same set of problems to do and compared the results. Using this approach we discovered someone who was the right balance of cost and quality and we ended up working with her over the next few months to compile this list.  There's a lot of talent on platforms such as Odesk and Mechanical Turk but it's not easy to find. A good approach is to develop a set of tasks that you're looking for someone to do and use that as a proxy for an interview. Giving this set of tasks to a few dozen people will lead to a few that stand out and can hired on for longer term work.
{% include setup %} In honor of their NYC launch,  Lyft  came up with an awesome promotion - 50 free rides, up to $25 each, over the next couple of weeks. This had the desired effect - a bunch of my friends are giving it a shot but since everyone else is doing the same it's difficult to find an available car. And when you do get a car you end up paying the peak demand rate rate.  One way to get around this is to break down a long trip into a series of shorter trips. The driver will have to agree to this but they have an incentive to do so since they will not need to find another passenger and will earn a base fare every trip. A minor inefficiency is that the driver will need to stop to mark the trip as completed and confirm the next trip. It's also easy to go over $25, especially in prime time, so you need to stop more frequently than you think you need to. If only Lyft or Uber showed the real time cost of a trip you'd be able to stop at the optimal times.  We used this approach yesterday when going from midtown Manhattan to Brighton Beach and were able to do it with 6 Lyft trips. The first two ended up going over the $25 rate so we became aggressive stoppers after that. The driver mentioned that a bunch of his passengers used the same approach to get free rides to Long Island and Connecticut. I'm just waiting for Lyft to plug this loophole - a simple way would be to not allow the same consecutive driver/passenger pair.
{% include setup %} I’ve been working on various tech related projects for over a decade now and have gone through a variety of approaches to deploying code. I’m far from an expert but though it would be helpful to jot down what I’ve seen and where I'm hoping to get.  - FTP upload, no version control: I developed my first few sites locally and then just copied them over to the host server via FTP. This worked well for simple projects where I was the only contributor.  - Version control, single branch: Once I discovered version control I immediately found it helpful. Version control made it easy to work with others but our deployment was still manual. When we were ready to deploy we would log in to our server, run the necessary commands to update the database schema, and then do pull/update to get a new version of our code base.  - Version control, single branch, more automated deployment: Logging in every time to do a deployment was a pain so we started using  Fabric  to automate deployments. Fabric allowed us to execute scripts on multiple machines without having to manually log in to each one. Since each box had a set of roles we were able to set up Fabric to deploy by role (ie deploy this change to the DB server, deploy this change to all webservers).  - Version control, multiple branches, more automated deployment: Another improvement was following git best practices and setting up a production branch with everyone working on development branches that would then be merged into master. When the deployment was ready to go out it would be merged into production. The value here was that when we ran into a bug on production, we were able to fix it without having to merge in a bunch of new features.  - Version control, multiple branches, automated testing, automated deployment: This is the ideal state. Each of our repositories is tested enough that code changes are automatically tested, merged, and deployed to production. The process should also be smart enough to handle db migrations and would be to revert changes if any problems arise. In addition, each box may have a different set of required systems libraries and packages and an automated deployment should be able to automatically configure a server with the necessary packages. I know  Chef  and  Puppet  are used for this but I’m only exploring them now.  Something to add is that there’s a huge incentive to make your stack as stateless as possible - for example having multiple web servers behind a load balancer that don’t need to share any state with other webservers directly. This makes it simple to spin up new servers when there’s more demand and improves scalability. Unfortunately, it’s not always possible and complicated deployments end up having coupling - especially when high performance is required. In that case adopting a declarative approach when configuring your instances helps bring some sort of statelessness - for example using AWS tags to declare an instance to be of a particular type and using the region information to dictate what other instances it needs to connect to. Otherwise you’re stuck trying to define a complicated topology via config files. I’d love to know how massive companies manage their deployments - I know Facebook has a  custom process  that will deploy new code to a set of boxes and then use BitTorrent to share it to others but I’d love to be able to compare that with those of others, for example Google and Amazon.
{% include setup %} While the pricing battle between Amazon and Hachette rages on, I’ve been thinking about the relationship between price and value. A typical ebook on Amazon costs $9.99 while a movie in a theater, especially one in New York, can cost more than $10. And yet the book takes longer to experience - a movie is over within 2 hours while a book can be enjoyed for hours. Or how about a beer or coffee, they’re two to three times cheaper than an ebook but are consumed an order of magnitude faster book and only provide immediate gratification.  Amazon released data indicating that dropping the price of an ebook from $14.99 to $9.99 (33%) leads to a 74% increase in number of books sold. I understand that a lower price increases demand but it’s still ridiculous that we’re that concerned about a $5 price difference for a book when we’re spending that much on a coffee.  We’re so used to spending that much on a coffee that we’ll pay it without question and we’re so convinced that ebooks should be less than $10 that we refuse to pay more. It’s amazing what habit and expectations can do. I understand this bias and yet I still have a hard time believing that by skipping two cups of coffee I can buy a book. It definitely makes me appreciate the effort required to change people’s perceptions of what’s a fair price.
{% include setup %} I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with  TEALS , a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the  Coalition 4 Queens  program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it. The general idea is that it will consist of 3 or 4 sessions with each session lasting a couple of hours. The class will be opt-in and the students should have some technology background.  Session I  - Overview of MySQL and relational databases. What are they? How are they used? What are the alternatives? - Provide a quick overview of the normal forms and what they mean. What impact does it have when they’re violated and go over what well designed databases have. - Introduce the dataset we will be working with. This will mostly likely be a dataset I’ll pull from some of my side projects that will hopefully be relevant. Currently, I’m thinking of using a database containing some fantasy football data that I’ve scraped. - Make sure everyone has MySQL installed or can get it installed.  Session II  - Revist the dataset we’re working with and explain the relationships between the various tables and columns. - Go over the basic syntax of a query: SELECT, FROM, and WHERE. - Go over the basic INSERT statement.  Session III  - Review the basic syntax of a query and introduce the JOIN operations. Use joins to answer some simple questions from our dataset. - Introduce the GROUP BY functionality and the ways it can be used to summarize data. Use this in conjunction with joins to explore our dataset. - Develop some complicated and slow queries and introduce the idea of INDICES so everyone is aware of why they are useful.  Session IV  - Go over table creation and have the students come up with some interesting aggregate tables. - Provide a quick overview of how to diagnose a query for performance and how to test a query to make sure it was written correctly. - Discuss the various system tables (information_schema schema) and the various system commands that can be used to get a better understanding of MySQL
{% include setup %} It’s just too easy to rant against LinkedIn but I can’t help it. They recently offered me a free month of business plus so I took them up on it. Little did I know (although I should have expected it) that canceling would be a maze that I still may not have escaped.      The cancel screen hides the downgrade to free option and automatically chooses a paid “recommended account” with a bright clickable “Downgrade Account” button. And then, when you actually do manage to downgrade, it’s not clear from the account settings page that you downgraded since it still displays as the premium account option. Maybe when my free month is up it will downgrade or maybe I’ll get charged - how am I supposed to know? I do see a note that says canceled and I suspect I’m in the clear but there’s no way to actually confirm other than contacting support.      It’s one thing to optimize your funnels to get more conversions and revenue is one thing but tricking your users to subscribe is entirely different. If your product relies on this sort of “optimization” you really should think of another business model.
{% include setup %} Countless people have written about cofounder conflicts in a startup but I rarely see anyone talk about how important a similar situation is - financial and personal. There are no problems when things are going well and it's only when things start going poorly, which they inevitably will, that these issues surface.  A founder that doesn't have a lot of savings will have a different relationship to fundraising than the founder who has enough savings to keep going. The former will push to fundraise early while the latter will want to wait and search for the best opportunity.  A founder that's single will have a different lifestyle and priorities than someone who is married or in a serious relationship. The founders will have a different approach to work and may have a hard time agreeing on a culture that fits them as well as the rest of the team.  A founder that wants to start a company is different than a founder who wants to strike it rich is different from a founder who believes in changing the world. Each of them are valid perspectives but put them in a room together and it will be impossible to make even the simplest decisions.  At  Pressi , we had our fair share of founder conflicts and in hindsight a big part of it was how different our situations were - it's definitely beneficial to have a team come from diverse background and provide multiple perspectives but it's also critical that everyone understands where everyone else is coming from. And it's important to do this when things are going well, otherwise they will become bigger issues when things are going poorly and every emotion and event is magnified.
{% include setup %} This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and  updated it to work  for the 2014 season. The old version used the great  Scrapy  framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses  Selenium  to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.  Note that the code was designed around my league’s settings and that the column order in Yahoo will depend on the scoring categories of your league. If that's the case you need to make sure to update the code (primarily the xpath expressions) to map to the columns in your view. Definitely feel free to submit a pull request that makes the code a bit more flexible since my goal was to get something out quick in time for a draft later this week.  And if all you care about is the data, here’s the  projected 2014 data  as of August 25, 2014.
{% include setup %} I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.  This got me thinking of the various configurations I’ve gone through over the years and what my latest approach has been. If you have additional variations and suggestions that you’ve been happy with I’d love to hear them.     My first real project had a single settings.py file that defined a different set of options based on the hostname. The benefit here was that every configuration setting was in one file and it was pretty easy to see what the differences were between development and production. The problem is that if the hostname ever changes you may break your entire application. This also makes it difficult to share you code with a team since the hostnames will be different and you end up with a massive file full of different configuration settings. {% highlight python %}import socket  if socket.gethostname() == 'ubuntu':     CONFIG_OPTION = 'prod-setting' else:     CONFIG_OPTION = 'dev-setting' {% endhighlight python %}     A simple improvement was splitting this single file into a common settings file along with separate files for each environment that were then imported based on the hostname. The common settings file would contain the shared settings while the individual environment files would contain the settings unique to each environment. This kept the files cleaner and made it clear what setting applied to which environment. {% highlight python %}import socket  if socket.gethostname() == 'ubuntu':     from settings_prod import * else:     from settings_dev import * {% endhighlight python %}     The previous options still suffered from relying on the hostname so a simple improvement was using symbolic links to point to the appropriate file. With this approach we can still have a common file as well as the individual files but the environment-specific files are importing the shared settings. The big advantage to this approach is that the the symbolic link command only needs to be run once on each server and will always point to the correct file. {% highlight python %}# settings_prod.py  from settings_common import *  # Now set the prod only options CONFIG_OPTION = 'prod-setting' {% endhighlight python %}  {% highlight sh %}ln -s settings_prod.py settings.py {% endhighlight sh %}     Another option that I started using is using the DJANGO_SETTINGS_MODULE environment variable to point to the appropriate settings file. I adopted this approach after reading  Two Scoops of Django  which has a ton of other useful tips that improved my development approach. This approach isn’t significantly different than the symbolic link one but it feels less hacky since it’s an approach supported by the official Django documentation and it’s easier to examine environment variables than looking at the symbolic links across your directory. {% highlight sh %}export DJANGO_SETTINGS_MODULE = project.settings.prod{% endhighlight sh %}
{% include setup %} When building a SAAS product geared towards developers the quickest way to start is to build an API. One can even make the argument that the MVP should just be the API documentation. This benefits both sides. Potential users of the API will know exactly what to expect and have a clear understanding of the functionality and limitations and you can quickly see if there are any issues or inconsistencies in what you’re building. Some non-fiction authors will share a table of contents with potential readers in order to get feedback and this extends that idea to companies and their products. Especially when your primary users are developers this is a simple way to share your idea and approach without resorting to buzzwords or even relying on a beautiful site design.  The best example of this approach is  Stripe . At launch, they had a beautiful API that you were able to use without creating an account. After seeing how it well it worked it was an easy decision to register for a full account. Tons of companies adopt an API-first approach for their internal systems and it’s not a lot of work to extend this to the external world. There’s definitely a risk in doing it since you’re exposing more of your internals but if you claim to be developer friendly it’s the best way to actually prove it.
{% include setup %} A  recent article  on our attachment to social media got me thinking about my most commonly used services and their relative importance. The goal is to answer the question of how I’d feel if various services suddenly disappeared. After going through this process it feels as if these services moved from being necessities to feeling like luxuries. They either have a substitute that will do what they do or only have value due to the network - clearly these are important but I just have no attachment to the product itself.  - Facebook: I don’t care much for it and at a certain level I want it to disappear. The best reason I use it is the only reason I use it - everyone I know is on it.  - Twitter: I enjoy Twitter and use it more frequently than Facebook but don’t think I’d feel the loss terribly. I use it as a content source primarily and in the end the stuff I’m interested in can most likely be found via Hacker News.  - Hacker News: Years ago, I remember participating in the discussions but lately I’ve just been using it as a source of news. My current use case would be taken care of via Reddit or any other tech news aggregator.  - Foursquare: This would be the biggest loss but purely from nostalgia. I started using it when it launched and saw it evolve through the various products. I’m not a fan of the recent split into Swarm and Foursqare but still suspect I’d be most upset if Foursquare disappeared.  It’s sad that I feel so cynical about the apps and I’m trying to understand why. I suspect part of it is that the novelty of these has worn off and part is that there’s so many new tech products out there that it may just be overload. We’ll see how I feel about them after the next five years.
{% include setup %} Apparently Twitter is considering curation user’s timelines. A perspective people haven’t really discussed is the impact on the tech side. Right now each user has a unique timeline that needs to be presented in near-real time in case they need to see it. This results in a massive storage operation using Redis where these timelines are  continuously generated and cached . By moving to a model where every user can be categorized into a group that sees a particular set of tweets Twitter can drastically reduce the amount of data they need to store per user. I’m sure Twitter already has a way of categorizing users in order to support the ad product and this approach would extend it to the “stream” product. In a way it’s akin to how compression works - find repeated patterns and replace every occurrence with something shorter. Then when you want to uncompress you just reverse the process.  I doubt this is the primary driver of the curation discussion and there are clearly more important issues at stake but this may be the proverbial “cherry on top” that will get Twitter to move to the curated model.
{% include setup %} Over the years, I’ve noticed two distinct coding styles. Some approach problems top down and will stub out the entire solution using dummy values and methods and come up with a naive solution before fleshing everything out properly. Others will instead take a bottom up approach and try to complete each method entirely before moving on to the next one.  Especially for larger problems, I prefer the top down approach. By stubbing out the various pieces it’s easy to see how everything fits together and makes it easy to identify and solve potential issues before investing a ton of effort into a poor implementation. The other benefit is that I start thinking at a systems level and come up with implementations that tend to be more extensible.  The only time I find myself taking a bottom up approach is when the problem is very well defined and I know exactly what the solution is or when I’m working on HTML and CSS. In that case, and especially with my limited skill, there’s so much coupling between the various components that I can’t avoid going linearly through the components. It does make me wonder whether people who have more frontend experience have also adopted the top down approach.
{% include setup %} I recently attended two web development workshop “meet and greet” sessions where recent graduates presented their projects and chatted with potential employers. I’m honestly surprised by how polished the projects were. Sure there were a few simple ones but most were solid; they were good ideas, well designed, and had functional backends. It’s amazing what it’s possible to do in 12 weeks.  These programs focus on a single frontend framework, such as Backbone or Angular, and a backend framework, usually Ruby on Rails. With the number of plugins and public APIs available it’s easier to get an app up and running than ever before. Of course these programs won’t provide the same level of knowledge as a degree or years of experience will but for many projects that’s not important. Being able to get something functional and private is more important than perfect and private and these bootcamps provide enough skills to do that. More importantly, they make code accessible to an entirely new group of people and provide enough skills to allow them to continue learning on their own.  It does make you think where tech skills are headed. As it becomes easier to build a larger variety of apps it will be interesting to see where software engineering will end up. Software engineering is a young industry and I suspect it’ll become increasingly specialized as the base set of tools and knowledge become widespread.
{% include setup %} Earlier this week we encountered an odd RDS issue that I’ve never seen before. An AWS hiccup caused a database replication query to fail which stopped the replication process. We discovered this the following day when we saw weird results during after running an analysis query. The nice thing was that this wasn't a huge deal since our production system relies on the master database but we did have to spend time dealing with this.  When we discovered this issue we did a few online searches to see how to resolve the issue and resume the replication. Turns out there's a command, "CALL mysql.rds_skip_repl_error", that will skip the current replication error and move on. In our case, the errors occurred when creating temporary table for a legacy job so we were able to skip it. Otherwise, we'd run the risk of breaking the sync between our master and replica databases.  Unfortunately, running this query once wasn't enough since the error keep on reappearing. After speaking with an AWS rep, we realized we could keep on running that command until we skipped past the replication errors. Another useful tip was to look at the ReplicaLag CloudWatch metric to see how far behind the replica database was from the master. In our case after going through a couple of dozen of these skip error calls replication resumed but the replica database was still more than a day behind.  While the replication caught up, we made a quick update to our scripts to point to our master database instead of replica so that our jobs would reference the correct data. After replication caught up we simply reverted this change.  To prevent this issue in the future, we're going to revisit the jobs that were using the temporary tables. We've also added a CloudWatch alert to notify us if replica gets too far behind. In a way we got lucky since these errors were recoverable. Without that we would have had to recreate the replica database which may have had a performance impact on our master database.
{% include setup %}     Earlier this morning I watched a Steve Jobs talk from 1980 where he discusses Apple and the relationship between hardware and software. An interesting piece comes at the 12:30 mark where he addresses the question “Right now software is powerful enough, what impact will improvements in hardware have on software?” His answer is great: “[We will] start chewing up power specifically to help that one on one interaction go smoothly and specifically not to help the calculation...  start applying that power to remove that barrier”  Sure the response is very Jobsian but the underlying point is significant. It’s only a tiny bit about what the software actually does; the majority is realizing that people will actually be using the software to solve problems and building tools for that experience. I remember starting with DOS on the family computer and being blown away when I first used Norton Commander. Similarly to when I saw Windows for the first time and saw my first smartphone.  Most hardware improvements over the past 30 years led to improvements in usability, not functionality. Processors in 2014 are 100,000 times more powerful than those in the early 1980s and a majority of the improvement went into user experience - better UIs packed into smaller devices. Without usability improvements computers wouldn’t be nearly as ubiquitous as they are now and would primarily stay a hobby for engineers. Each usability improvement brings aboard a whole new set of people. You can make the case that the same thing occurs with programming languages - very few people were writing assembly code at its peak compared to C code, and fewer people were writing C code at its peak than JavaScript.  Usability improvements are still happening but they’re taking the form of cloud and background services - akin to the way Google Now provides contextual information and the way Siri handles voice recognition. As sophisticated as they are, they will only get better as hardware improves.
{% include setup %} It’s amazing the impact tools have on productivity and enjoyment. I remember my first foray into Java using a combination of text editors and Ant. Setting up and configuring a simple project was a nightmare and without the internet I don’t know how I would have figured it out. This initial experience made me associate Java with an unnecessarily complicated approach that I wanted to avoid.  After Java, Python felt like a breath of fresh air. The code was simpler, more compact, and I was able to just dive in. Discovering pip and virtualenv made me enjoy it even more. But no language is perfect and with enough you uncover the imperfections. Performance became a bottleneck when I started working on serious code and I missed the benefits of static typing - especially when refactoring a large projects.  Recently, I started using Java again and it’s a completely different experience. I’m not sure whether it’s due to hardware or software improvements but Eclipse feels faster and more responsive. It makes Java nearly as fun as Python. The static, strong typing makes it easy to do large scale refactorings, Gradle and the open source ecosystem make it trivial to leverage all sorts of libraries, and the performance/coding ease is great - especially when dealing with concurrency. Good tools can make a world of difference to the accessibility and joy of writing a language. I read a while ago that Facebook has the strongest people  working on internal tools  and I’m not surprised; it may be one of the most effective way to make everyone happier and more productive.
{% include setup %} Something that’s bothered me ever since I started using a smartphone is the link opening behavior. Whenever I’m in an app and click on a web link it would immediately open up that page in a browser window. And when I’m already in a mobile browser and click on a link it would open that page up in a new tab. Compare this with the desktop environment. Clicking on a link within an app does open up a new browser window immediately but since there are shortcuts to quickly switch between programs it’s not a huge deal. And when I’m already looking at a webpage and want to open a new link it’s possible to open it in the background using command+click.  The most common reason I click on a link is as a bookmark so that I can go through it after I’m done with what I’m currently doing. The default behavior is the opposite of that - it turns something that I want to consume asynchronously into a synchronous process. Sure there are times where I do want to switch gears but the vast majority of the time opening a new window is a distraction. That’s why it’s so surprising that mobile web browsers have adopted this behavior. Smartphones are both slower than desktops and make it more difficult to switch between programs. Why couldn’t the default behavior be to open all new links in the background?  I finally found a browser that lets me do just that. It’s called  Javelin  and has a feature called “Stacks” that lets you click links while allowing you to continue using the phone. These links are loaded in the background and are ready to be consumed whenever you want to go through them by clicking on a little overlaid icon. This is quickly becoming one of my favorite new apps since switching to Android and I’m hopeful we’ll see other desktop behaviors translated into a mobile-friendly versions. I think this is where Android has an edge by providing a more open environment for developers to work with. With that openness you do end up with more crap but also more gold.
{% include setup %} As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database. I thought it would be useful to share the commands below and provide some context for those interested in learning more about MySQL and the data import/cleanup process.  The first step is to create the table that we'll be loading the CSV file into {% highlight sql %}create database stats; use database stats;  create table orig_stats (   week int,   name varchar(100),   position varchar(20),   opp varchar(50),   passing_yds float,   passing_tds float,   passing_int float,   rushing_att float,   rushing_yds float,   rushing_tds float,   receiving_tgt float,   receiving_rec float,   receiving_yds float,   receiving_tds float,   return_tds float,   twopt float,   fumbles float,   points float ); {% endhighlight %}  Now we load the CSV file into the table making sure to specify the options properly. In my case this took a few attempts to deal with the line endings. {% highlight sql %}LOAD DATA INFILE '/tmp/stats-2014.csv' INTO TABLE orig_stats FIELDS TERMINATED BY ',' LINES TERMINATED BY '\r\n' IGNORE 1 LINES ; {% endhighlight %}  Next step is to create the tables we want to end up with. In my case I wanted to normalize the data which required designed a new set of tables. A big assumption made here was that a player will not get traded from one team to another. This is definitely not correct in the real world but it is good enough for this exercise. If we wanted to allow for trades we would have a separate table that would map a player to a team by week. {% highlight sql %}create table teams (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(100),   PRIMARY KEY (id),   UNIQUE (name) );  create table positions (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(10),   PRIMARY KEY (id),   UNIQUE (name) );  create table players (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(100),   position_id int,   team_id int,   PRIMARY KEY (id) );  create table schedule (   week int,   home_id int,   away_id int,   UNIQUE (week, home_id, away_id) );  create table stats (   week int,   player_id int,   passing_yds float,   passing_tds float,   passing_int float,   rushing_att float,   rushing_yds float,   rushing_tds float,   receiving_tgt float,   receiving_rec float,   receiving_yds float,   receiving_tds float,   return_tds float,   twopt float,   fumbles float,   points float ); {% endhighlight %}  Now it's on to the hard part. We want to take the data in the original stats table and convert into a properly normalized data set. The strategy here is to start with the simple tables and work our way up to the more complicated ones leveraging the normalized data we created at each step. The first two tables are teams and positions and we can derive them from the "position" field in the original stats table by splitting the position field into two and realizing that the left side is the team and the right side is the position of given the player. {% highlight sql %}insert into teams   (name)   select distinct(substring_index(position, ' - ', 1))   from orig_stats order by position;  insert into positions   (name)   select distinct(substring_index(position, ' - ', -1)) as pos   from orig_stats order by pos; {% endhighlight %}  To generate the players table, we get the player position and team from the stats table and then find the associated ids from the teams and positions tables. The key assumption here is that there are no two players with the same name, on the same team, and the same position. {% highlight sql %}insert into players   (name, position_id, team_id)   select p.name, pos.id, t.id   from (     select name, position,       substring_index(position, ' - ', -1) as pos,       substring_index(position, ' - ', 1) as team     from orig_stats     group by name, position, pos, team   ) p   join teams t on t.name = p.team   join positions pos on pos.name = p.pos; {% endhighlight %}  We can figure out the schedule by getting a list of the unique matchups in the original stats table. Since the games are symmetric we only need to look at the rows that are home games. {% highlight sql %}insert into schedule   (week, home_id, away_id)   select s.week, t1.id, t2.id   from (     select week,       substring_index(position, ' - ', 1) as home_team,       substring_index(opp, ' vs ', -1) as away_team     from orig_stats s     where opp like '%vs%'     group by week, home_team, away_team   ) s   join teams t1 on t1.name = s.home_team   join teams t2 on t2.name = s.away_team   order by s.week, t1.id, t2.id; {% endhighlight %}  Putting everything together we generate the new stats table by doing the relevant lookups in the tables we created. We can ignore the redundant fields (name, position, opponent) and the only thing we need to watch out for is duplicate players. In this case there are two names, Zach Miller and Alex Smith, that need to be made "unique" by also looking at their team. {% highlight sql %}insert into stats   (week, player_id,   passing_yds, passing_tds, passing_int, rushing_att, rushing_yds, rushing_tds,   receiving_tgt, receiving_rec, receiving_yds, receiving_tds, return_tds,   twopt, fumbles, points)   select s.week, p.id,   passing_yds, passing_tds, passing_int, rushing_att, rushing_yds, rushing_tds,   receiving_tgt, receiving_rec, receiving_yds, receiving_tds, return_tds,   twopt, fumbles, points   from orig_stats s   join teams t on substring_index(s.position, ' - ', 1) = t.name   join players p on s.name = p.name and p.team_id = t.id; {% endhighlight %}
{% include setup %} Earlier this week XKCD featured a  comic  where oft-quoted movie quotes are autocompleted by iOS keyboard predictions. I decided to do replicate the exercise using Android and Swype. Some are similar while others are completely different. I suspect a big part of the difference is that Swype uses my history when offering the suggestions and since I’ve been travelling recently many of them tend to be airport related.       Say hello to my little  brother and sister and the other hand.        Toto, I've a feeling we're not  going to be a good time.        Bond, James Bond  with the Eagles to the airport.        I'm a leaf on the wind. Watch the video game console and I will get there early.        Goonies never say never been to the airport.        You have my sword. And my bow. And my wife.        Hello, my name is Inigo Montoya. You can send you a call to discuss the details.        Revenge is a dish best served from the other side of the terminals.        They may take our lives, but they'll never take our word for it.
{% include setup %} This weekend I felt discouraged after getting cold but dragged myself outside to go for a walk and clear my head. I ended up getting a cup of coffee and just sat on a bench for 30 minutes and letting random thoughts go in and out of my head. It turned into a form of personal meditation where some ideas crystallized and stuck around while others quickly disappeared. It felt great (so great that I ended up going back to my apartment and doing a bunch of chores I’ve been putting of!), especially considering that I had to drag myself away from the couch to even go for the walk and would have much preferred to just sit on the couch and watch football.  While sitting on the bench I made three resolutions that are making me feel more inspired and productive. They’re all simple and only require commitment but I’m definitely benefiting from them.      Stop procrastinating . We all have a ton of things to do and I got into the habit of delaying simple ones like dropping off clothes for dry cleaning or selling some old electronics on eBay. Lately I’ve started going through my list of todos and it feels great. Each small achievement is something that I can cross off and encourages me to move on to the next one. For most of these priority doesn’t even matter since the important stuff happens anyway; it’s the small items that tend to get stuck in a state of never done.    Focus on one thing at a time . This is the hardest one for me but I realized that I tend to get distracted too easily. Whether it’s checking my email or browsing Hacker News it’s a huge inefficiency. I’ve also found myself trying to do work passively while watching a football game and it’s noticeable how little I get done that there’s no point in even attempting to work. It’s not possible to achieve  flow  without being able to concentrate on one thing and quality suffers. It’s not even a good use of time since you’ll just end up taking a longer time to do two things poorly. It’s better off to get the stuff that requires your attention out of the way and then spend the rest of the time relaxing.    Take distraction free breaks . Given how useful going for a aimless walk was I decided to make this a habit and try to go for at least three a week. So far I tried going before work but it wasn’t the same. Either the subconscious knowledge that I needed to track my time in order to get to the office or being in a highly trafficked city intersection made it harder to get lost in my thoughts. In any case I might end up making this a once-a-week activity and do it on the weekends in a quieter place where I can be distraction free.     I’d love to know of other resolutions that people have embraced to make them feel happier, inspired, and productive so if you have any ideas definitely let me know.
{% include setup %} I’ve been trying to reduce the amount of stuff I have and a big part of it is old electronics. I’ve been selling off old headphones and random cables but the one thing that’s been more difficult to get rid of is older hard drives. I know that most of the stuff on them is junk that I’ll never see again but it’s still tough to just throw it away. They’re reminders of previous jobs and old projects that are a part of my identity that are tough to permanently delete with a click. Many of them are unique in the world and only exist on an old hard drive. I realize it’s foolish to keep them around but it’s tough to let go.  I thought about moving the stuff over to Dropbox but upgrading to a paid plan to store a bunch of files I’d never touch again seemed wasteful. I wanted something that was a one time upload, was cheap, and gave me peace of mind. I recently read about  Amazon’s Glacier  product and it fit the bill perfectly. It’s a penny a month for each GB stored but with additional fees for retrieving old files. Looking at the amount of files I want to store this would cost me less than 20 cents a month and allows me to get rid of a ton of old drives. I spent a couple of evenings this past week tarring up these old files and transferring them to Glacier using the  Simple Amazon Glacier Uploader  app.  Glacier is a great fit for data that’s difficult to throw away but unlikely to be accessed and is significantly cheaper than Dropbox. I’m in the process of going through more and more of my data and seeing which of it would be a good fit for Glacier.
{% include setup %} A couple of days ago I saw a mic.com article with the title “A European country is now offering free college education to Americans” but the only way to find out which country this was (Germany) was by clicking through to the actual page.      I understand that content sites make the bulk of their revenue through advertising but resorting to a link-bait approach seems like a terrible idea. It’s a shortsighted attempt that increases page views at the cost of insulting your audience and cheapening your effort that will not work as a sustainable strategy. Relying on headlines to generate traffic without any meaningful content is a great way to get to become a commodity. I hope that there are enough people out there that care about the content they’re producing and have a passionate audience that can be monetized based on quality of engagement rather than on quantity of page views. Otherwise we’ll all end up in a race to zero.
{% include setup %} Despite being a huge proponent of open source I’ve never made a contribution to a  third party project  until this weekend. The project was a simple scraper that downloads each Jeopardy game from  j-archive.com , parses the data, and loads into a SQLite database. The project had one issue open that was to make the download code threaded in order to reduce the time of downloading nearly 4700 games from over 7 hours to less than 30 minutes. I gave this a stab on Saturday and submitted a pull request that was merged in by the author on Sunday.  The process was surprisingly simple and it felt good making an improvement to an already useful project. An idea I’ve been toying with is making a contribution to every open source project I use. Some would be simple and may not even be code while others might be significant improvements. The idea is to continue continue the cycle of improvement to projects that have made me more productive. I can only imagine what would happen if everyone who uses open source adopted this approach.
{% include setup %} Ever since I’ve started blogging I’ve been getting around one spam blog comment a week.  Disqus  does a nearly perfect job of flagging them so I don’t understand the motivation behind it. They’re obviously spam and my readers are suave enough to never click on any of the links. There’s also little, possibly none, SEO value since they’re all loaded asynchronously and every link has a rel=”nofollow” property. And if the goal is to spark a discussiong and raise awareness they're so poorly worded that no reader will take them seriously. The only thing I can think of is that these companies pay a third party service to grace tangentially related blogs with content on their behalf and these third party services go the cheapest possible route in both effort and quality.
{% include setup %} Last Thursday was the last lesson of the four part Introduction to MySQL class I’ve been teaching at  Coalition for Queens  and I wanted to summarize my thoughts while they’re still fresh.  The diversity of the class was amazing and shows how useful affordable technology programs are. You get a mix of people from different backgrounds and different ages that all want to improve themselves and can all contribute in their own ways. Everyone has a unique experience and introducing technology into it may open up new opportunities.  It’s tough to get a curriculum that works for everyone but it’s important to try. Some people grasp concepts quicker than others. Some want to see more hands-on exercises. Some want homework assignments. Some want to be able to split up into groups and work with others on more complicated assignments. The dataset itself needs to be relevant and realistic or people will lose interest. I put the  curriculum up on GitHub  for suggestions but didn’t get any - hopefully others will use it in their classes.  Tools matter. For the first two sessions I used Keynote to generate the presentation and then exported it to a PDF. This approach lacked syntax highlighting which I wanted given the technical nature of the course. I switched to the wonderful  Remark.js  which allowed me to create slideshows using GitHub flavored markdown. This allowed me to integrate exercises into the lecture while incorporating syntax-highlighted examples. One issue was that it wasn’t as straightforward to export it as a PDF and I had to print it to a PDF file via Chrome.  Volunteer teaching is a great way to “teach to fish” rather than just giving a fish while helping develop public speaking skills and meeting a ton of awesome people. If you’re in New York City, work in technology, and believe technical skills are increasingly important you should take a look and try teaching a class at Coalition for Queens.
{% include setup %}      A.D White Library @ Cornell University by eflon    For the first time in almost a decade I checked out a book from a library. I don’t know why I ever stopped - the experience is extremely simple and you’re able to read a book for free. I had a book on my Amazon wishlist for a couple of weeks that I held off on buying but was able to read it over the past week after a quick visit to the library.  As a kid I used to go the library all the time and would go through multiple books a week and this brought back all those memories. I feel guilty for abandoning libraries in favor of Amazon and my iPad and think many people are missing out by consuming everything digitally. It’s not just the physical element but also the nostalgia and the knowledge that there were dozens of people who have read the exact same book you’re reading now. Reading a library book makes you feel that you’re a step in the book’s journey as it grows from home to home and person to person. That’s completely lost in a digital world and it will only get worse as libraries change to stay relevant. Until then I look forward to going back and checking out another book.
{% include setup %} I keep on discovering new use cases for  Markdown  the more I use it. My first exposure was when I migrated my blog to GitHub pages from Wordpress and Tumblr. Since then I’ve discovered GitHub flavored markdown which supports syntax highlighting which has been amazingly useful when blogging on tech topics or putting together notes for a tech talk. Just recently I wanted to include some MySQL snippets in a Keynote presentation and discovered the  Remark.js  library which lets you generate in-browser slideshows in Markdown with syntax highlighting.  There’s this desire to turn tools into products and it’s refreshing to see tools stay tools. They are able to stay much more flexible and evolve organically based on the needs of users rather than a preset direction. Markdown is a perfect example of this - it started simple but has evolved to have multiple variations and is used as the base for many other projects. At this point it has become such a standard that it can’t be co-opted by any single product or company.
{% include setup %} I recently read a FiveThirtyEight article on the  sneaker resale market . The concept is extremely foreign to me since I tend to not collect anything other than old notes and have a tendency of grossly mistreating my shoes and clothes. Nonetheless, I found it fascinating as it discusses the incentives of the various parties involved and comparing them against standard economic theory. One passage in particular was so insightful that I had to save it:    That differential allows people to buy something on the cheap but feel like they’re wearing a luxury item.    “So even if you paid $100, you’ve got $800 on your feet. It’s like having Gucci,” Taylor said   Everyone loves getting a deal but I think this is slightly different since it’s not so much about getting the deal as it is about being able to afford luxury and show it off. Sure it’s vain and has a bit of conspicuous consumption but if it gives someone self confidence and makes them feel like a million bucks I can’t complain.  We should strive to provide this type of experience when building apps and services. It’s not just adding some cheap gamification tricks or rewarding early users as much as it is making people proud to use your product. gamification tricks or rewarding early users as much as it is making people proud to use your product.
{% include setup %} Last night I took an old bash script I wrote that simplified connecting to an EC2 instance in an AWS account and implemented the same code in Python. The old code worked by listing a set of AWS instances and then prompting to pick a single one to connect to. The problem was that it wasn’t always easy to find the index of the desired instance and the code took a bit of time to run.  The new code leverages the Python AWS library to pull down the list of instances for a given region and then filters it down based on the name, IP address, or public DNS. If it turns out there’s a match then it will only return the public IP address which makes it easy to connect using ssh. For example, to list all servers containing the name “web server” you would run the following:  {% highlight sh %}python list_hosts.py --region=us-east-1 --filter="web server" {% endhighlight %}  And if you know there will only be one you can connect to it directly by using ssh and running the script inside two backticks: {% highlight sh %}ssh `python list_hosts.py --region=us-east-1 --filter="web server"` {% endhighlight %}  The code’s up on  GitHub  but at the moment there’s just this single script. I’ll keep adding more as I run into various issues working with AWS.
{% include setup %} A pretty trivial post but something I’ve been doing for a while now is keeping my dock as a vertical bar on the right of my screen. I started doing this years ago when I was working on Windows and it was too difficult to track every single program that was running. At that point I was in finance and would have a dozen Excel workbooks open and needed to be able to quickly switch between them. The only way I could do this with a bottom toolbar was by making it extremely thick which would take up too much space. Moving it to the side solved that problem and I stuck with it as I moved to Ubuntu and now OS X.  These days I have significantly fewer programs running at once and I’m much better at hopping between programs but it’s surprising what a good dock location can do for efficiency. Getting everything right only saves a fraction of a second but doing this countless times a day adds up. As important as tools are, the way they are access and used is just as important. The challenge is getting stuck with a suboptimal routine that you’re used to and not moving to a more optimal one since the short term cost is high. The greatest example of this is probably the  Dvorak keyboard layout  - in theory it’s significantly faster than QWERTY and yet virtually everyone is using the slower QWERTY approach. It’s just good enough.
{% include setup %} I’ve been a fan of GitHub pages ever since I started using them to host my blog a couple of years ago and a thought that’s been constantly popping up is why there haven’t been any products or services that help small businesses host their sites on GitHub. GitHub’s  terms of service  forbid a third party from hosting pages on behalf of customers but it doesn’t seem as if there’s anything stopping someone from building a tool or documenting the set of steps to help someone create a simple site and have it hosted on GitHub. That way the business only has to pay a domain registration fee while still getting fast and robust hosting with a fairly solid CMS.  Going further this site can be made significantly more dynamic by integrating third party services via client side JavaScript. Use Facebook to handle authentication, Google Analytics to provide analytics, Disqus to provide comments, and Firebase to provide a data layer. There’s no backend to maintain and you get to use a set of free and powerful tools. This isn’t going to work in every use case but over time we’ll see more and more applications built using this approach.  The simplest code is the code that you don’t have to write and I think we’ll start seeing more and more of these third party services that provide specialized functionality via JavaScript snippets. Many of them will also be free when starting out since their marginal cost will be virtually zero for small projects and will encourage tons of people to create these pseudo static sites that provide dynamic functionality without a backend. I suspect that as these tools become more popular we’ll see them packaged together by other companies and services that will make web development accessible to a wider audience. One of these days I’ll see what kind of application I can build using free services and client side JavaScript.
{% include setup %} Google recently launched a program,  Contributor , that offers an ad-free monetization model to publishers. The idea is that a user pays Google up to $3 a month and in return Google will not show that user any display ads on a website that’s a participant in the program. The monthly payment will then be distributed across the participating sites - most likely based on how many times you’ve visited that site.  I like the idea - not because most ads are terrible but because it shows that both publishers and Google are willing to experiment with another approach. Ads, as much as we dislike them, are the primary way content producers make money since web users expect free content everywhere.  The biggest audience for Contributor will be those who currently run adblock but feel guilty about it. Most people want the content free on principle and refuse to pay but the people that are using adblock but do want to support the publisher may be willing to pay the $3 a month to feel noble - in fact they might keep on running adblock and treat this as a way to reward the sites they visit.
{% include setup %} The recent news that PCH is set to acquire Fab reiterates how difficult startups are. So many startups strive to get an investment and believe that once they raise a round everything will get easier. That’s when things get difficult. Instead of focusing on achieving product market fit you start worrying about market share, competition, company culture, recruiting, process, which require a completely different skillset than what you started with. And to add to that you’re now accountable to a growing list of employees, shareholders, and customers. When I was working on my first startup I really thought that being able to get funding was the measure of success. Now I realize how naive that view was and how much more there actually is. Fab raised over $330M and wasn’t able to grow into a successful business despite undertaking massive pivots. There  are probably thousands of founders claiming that they’d be able to succeed with that kind of money without realizing how difficult it actually is.
{% include setup %}           I never thought about this until [Shaun](https://twitter.com/szach) brought it up but now I see it all the time: AWS ships code quicker than any other company that size. Some are simple feature improvements to existing products, such as an advanced instance search, additional configuration options for ELBs, and new instance types, but others are entirely new products, such as Lambda, EC2 Container Service, and the newly announced code management suite.  What’s even more impressive is that they provide infrastructure to thousands of other companies and the cost of failure is massive. A breaking feature will affect thousands of companies and cost a ton of money and hurt their reputation. Yet they keep on shipping and launching as if they were a newly launched startup.  I’d love to know the process Amazon has in place that drives these releases - both how they’re able to code at such scale as well as the test process they use to make sure no existing functionality breaks due to new features.  The half serious joke here is that Amazon is always shipping - physical items from Amazon.com and digital services from AWS.
{% include setup %} A month or so ago I read Rob Ewaschuk’s  philosophy on alerting  and since then I’ve been trying to be more aware of the alerts we have and whether any can be improved. The most actionable insight was to start thinking in terms of “symptom-based monitoring” where the alerts should reflect what the users are experiencing rather than various issues along the tech stack. This aligns your alerts with user expectations and can also simplify alerting since they will all be running at a high level. It may take longer to diagnose what the underlying problem is but it will reduce the total number of alerts required.  One of our alerts checks for faulty instances that are attached to a load balancer. We're notified whenever one goes down with the goal of investigating the cause and getting it back up and running. While serious, it's not critical since there's a fair amount of redundancy and the user won't notice any impact unless a large enough number of instances fail. Using the symptom-based monitoring approach we were able to tweak the alert to monitor the  latency of requests  made by the load balancer to the instance and trigger a warning if it gets too high for too long. This reduces the number of non-critical alerts while making critical alerts more in line with customer expectations.  The larger an application gets the more essential it is to have a firm overview of the system with a solid set of alerts. Too many and you end up either missing important ones or wasting too much team dealing with false positives. Too few and you discover problems too late. Alerts are typically, and rightly, ignored on smaller projects but when you have multiple applications distributed across dozens of instances it's increasingly important, and simultaneously more difficult, to understand what's happening. Something I've started doing to identify potential improvements is tracking every single alert and tracking it's false-positive rate with the goal of finding alerts that aren't meaningful and either get rid of them or replace them with something more actionable. Over time we'll hopefully get the right balance.
{% include setup %} Whenever I fly I try to be at least somewhat productive. This time it entailed finishing up an old blog post and messing around with Node.js on a side project. They say the only way to appreciate something is when it’s gone and that’s how I feel about developing without internet access. It’s such a common occurrence to need to look up the documentation for a particular function or library or search for novel error messages that my approach is completely altered without the internet. Where before a few visits to Google or Stack Overflow would have taken care of the problem now I get to rely on man pages as well as dozens small experiments to figure out what’s happening.  It’s definitely not as efficient as having everything at your fingertips but it’s definitely fun in small doses. In my case I get to brush up on some rusty skills and also get to be a detective when diagnosing bugs. Knowing that you will be coding without internet also forces you to set up a standalone development environment. This requires making sure all the static assets you use are available locally and can be served by your application as well as making sure you have a local database that contains realistic data.  It’s tough to avoid the internet when coding but it’s a worthwhile exercise to attempt a few times a month - it will make you appreciate what you have but also introduce you to a whole new set of skills.
{% include setup %} I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions. This allows the reader to get a feel for the data and makes the ideas much more tangible than an abstract formula. Piketty couples this with the economic data from the past two centuries to craft a persuasive argument for the causes of wealth accumulation.  Countless others have looked through the data, identified issues, and provided counterarguments so I don't want to get into that but I do want to highlight how important having data is for all types of research. If we're serious about these topics we should strive to collect as much data as possible while making it as accessible as possible. Piketty spent numerous hours collecting and transcribing the data from various paper sources and it's amazing what came out; I can only imagine how much other valuable research would come out if there was more publicly available data.  Governments should be responsible for collecting data and releasing it publicly. Many are starting to do this already although it still tends to be obfuscated behind a navigational maze and hidden in esoterically formatted PDFs. Over time we should see it become more transparent as the data formats standardize and we develop better tools to dig through the existing data.  Another issue we need to address is data correctness. On one hand it's great that people are going through Piketty's data and making sure it's valid but on the other if it's extremely confrontational and used to invalidate his work it serves as a warning to others that plan on releasing their data. Why would a researcher spend thousands of hours collecting data and making it accessible and then have to deal with the critics who find a few issues? Much easier to keep the data hidden and only provide the high level numbers that can't easily be challenged without doing the hard work. This perverse incentive needs to be resolved if we expect to see high quality researched being produced with open sourced data.  I'm hopeful that these larger scope theories with potential societal-impact become more common as we move into the 21st century. We have an increasing variety of tools to start making sense of this data with both individuals and institutions being more involved in organizing the world's data. No theory will ever be perfect or explain every case but having more data will serve as a guide for governments to hopefully improve life for their citizens. And if data is collected along the way it will fuel more analysis with actionable insights.
{% include setup %} If you’re constantly watching something grow it’s hard to notice the magnitude while those further away see it immediately. This is well known for parents not seeing how quickly their children are growing but obvious for distant relatives and friends who get a glimpse once every few months.  I have the same relationship with technology. I’m surrounded by it each day that it’s hard to tell how much it’s changed but a way to combat this bias is by traveling, especially to developing countries.  I’m writing this blog post on a Macbook Air that has a ridiculous amount of battery life at the Delhi airport while tethered to a mobile hotspot running off of my US phone. It's not as fast nor as comfortable as what I get at home or in the office but it's incredible, especially when compared to my two prior trips, a year ago and four years ago. During last year’s trip I had to buy a cheap Android phone and spent countless  hours running around  trying to get a working SIM card. This time I didn’t even have change SIM cards due to T-Mobile’s global roaming plan. Four years ago I didn’t even bother doing anything on my phone and had a 3G USB dongle that required it’s own proprietary software to even connect to the internet.  New apps and products launching is just the surface, the biggest changes are happening to infrastructures that make these experiences possible. These aren’t visible day to day but are unmistakable when seen year to year and traveling makes them apparent.
{% include setup %} Something I’ve encountered is being stuck on a difficult problem but then taking a break until an “aha moment” just materializes. This happened throughout college on difficult problem sets as well as countless engineering projects at work. Sometimes instead of getting hung up on a tough problem the best thing to do is to forget about it and go for a walk and let the subconscious take over. I don’t know why this works but it seems to be common with others as well.  Bill Gates takes an  annual reading vacation  where he reads books across a variety of disciplines in order to have their themes cross pollinate and spawn new thoughts and ideas. This isn’t a conscious process and he relies on his subconscious to do the organization and connect the different ideas together. This is akin to what happens when we stop thinking about a tough problem and focus on something else: the mind is free to wander and may combine them into something that’s useful - or at least inspire another thought that may be relevant.
{% include setup %} Now that fantasy football season is over for me and I have no risk of angering the fantasy football gods I can complain about an interface decision in the Yahoo Fantasy Football Android app. Every once in a while the app will sign me out, which I suspect is a security feature, but I can log back in without having to re-enter a password. The only effect this “feature” has is getting me annoyed. The app has clean and simple visual design but that shouldn’t be prioritized over actual usability. Hopping between apps is such a common task that developers should strive to make it as painless as possible. This may involve changing the views around to make them more light weight or figuring out a way to simulate behavior without having to show a loading screen but it definitely makes the app feel snappier and more responsive.
{% include setup %}           Photo by  Rancho de la Luna      Earlier this week I set up Amazon’s [Fire TV Stick](http://www.amazon.com/Amazon-W87CUN-Fire-TV-Stick/dp/B00GDQ0RMG) and wanted to jot down some thoughts while they’re still fresh. For the $20 promotion price, it’s a great deal. My alternative was an Xbox 360 along with a Raspberry Pi running XBMC. The Xbox would be used for streaming shows on Netflix and watching older DVDs while the Raspberry Pi would let me watch various files off of a USB stick. I’ve never tried a Chromecast so don’t know how the Fire Stick compares but so far it’s been much quicker to startup and navigate than either the Xbox or the Raspberry Pi. When all you want to do is watch a quick show during dinner it’s a bit frustrating when you’re done eating by the time the Netflix app is ready to use on the Xbox.  Other than speed, the other big benefit has been the ability to use my phone as a remote. This allows both voice search, which Amazon has been doing a good job of transcribing, as well as a real keyboard. Having to type by navigating an alphabetically-sorted on screen keyboard with a laggy controller is not fun.  I haven’t had a chance to install any games or explore any of the other apps but I’m still impressed. It might be time to finally get rid of the Xbox and the old DVDs.
{% include setup %} I just discovered that Google launched a new  AdWords feature  to help brick and mortar store owners track the effect their online spending is having in the offline world. The way it works is that if a user sees an ad for a particular store or product on their phone and then ends up close (based on the location sharing option in iOS and Android) to the store in question, Google will use that information as a signal that the ad was the cause of the store visit. It’s not supposed to be perfectly accurate but the idea is that with enough data Google can come up with models that can estimate the actual numbers.  Mapping online spend to offline conversions has been the holy grail ever since advertisers started spending online and with the proliferation of smart phones that track everything we’re getting closer and closer to solving that problem. For centuries advertisers had to estimate and have faith that their spend in newspapers, magazines, and public spaces was having an impact with no good way of measuring the results. The ability to track a person’s location is immensely powerful and we’ll start seeing more and more use cases. Using a similar approach it may even be possible to see what effect a billboard ad has: monitor the locations of your ads and if anyone walks by them assign a probability that they’ve seen it. Then if they end up in your store you can assume they got there by looking at the ad. It’s significantly more difficult than that since people see hundreds of ads a day and the result is not always immediate but even having a tiny bit of data is better than none at all.  This should make everyone a lot more concerned about their privacy. In the past it was simple to have distinct lives - home versus work, inside versus outside, online versus offline - but with our attachment to modern gadgets the lines are rapidly blurring. Being aware is the first step in avoiding being tracked but it’s only a short term solution. As technology improves we’ll rely more and more on passive benefits which when coupled with better and faster data mining algorithms will make it very hard to live “off-grid.” We rely on government to preserve our privacy but I worry that we’re moving too quickly for the legislative process to have any real impact. Bitcoin and other distributed systems may be able to counter this decrease in privacy and I’m curious to see what sort of counter-systems they’re able to produce.
{% include setup %} I’m becoming increasingly convinced that DevOps is a necessary skill for any software engineer to have. It gets you closer to the hardware and helps you understand the way your code will actually run and where it fits within the tech stack. It also provides independence when working on new projects since it gives you both the knowledge to understand the needs as well as empowers you to make them happen. This is especially important when working at a small company where there’s immense value in having general skills that can be used to make progress independently without need to disrupt others. My first job writing code I only had to worry about my small patch of software but over time I’ve slowly picked up a variety of DevOps skills that help me write better code. Below are the skills that every software engineer should know - they may not all fall under traditional DevOps but I believe they’re essential for anyone writing code. If you have any others let me know and I’ll add them to the list.  - **Shell commands**. Almost any task can be done efficiently using a series of shell commands. Being aware of the various commands and their options makes it simple to find specific files, summarize data, or just monitor a server. The most important ones are ls, cd, pwd, rm, wc, find, grep, head, tail, less, sort, cut, sed, and curl. - **Configuration of various applications**. This might be specific to Linux/Unix but knowing the default configurations of common applications and where the settings are is important when setting up an application and diagnosing problems. There have been countless times I installed something on my box only to run into issues with unexpected behavior. Being able to examine the configuration and grep through the logs is critical to understanding what’s actually happening. - **Package managers**. Both the ones by provided by the operating system (apt and yum) as well as the language specific ones (pip, npm, gem, CPAN). There already so many powerful open source tools and libraries available that it’s rare to find something that can’t at least serve as a starting point for whatever you’re building. - **Web architecture**. This is a big one but it’s crucial to understand how the internet works from the time you type in a web address to a browser to what’s going to happen on the server. A good starting point is to look at Amazon Web Services and be able to explain each of the services offered, how they’re used, and how they fit together. Even better is to play with them to get a sense of how they can be used. - **Setting up and deploying an application on a brand new VPS**. Using AWS or Digital Ocean it’s trivial to get a virtual private server (VPS) but it will come completely empty. Being able to log in, install the necessary software, and get it responding to external requests is vital for anyone writing web code. Even better is deploying multiple applications on the same VPS and configuring DNS to make it work. - **Back of the envelope calculations**. Having a sense of how long various types of requests and operations take is important in understanding the type of hardware you need and where optimizations can be made. Another good skill is thinking in terms of “bytes.” This helps you estimate how much memory your code is using and makes it easy to understand what solutions are scalable and which will end up causing problems.
{% include setup %} A friend sent me  an article  where the author discusses the recent news of an AI finally beating the Turing test and how he himself was clearly able to determine that the AI was not a human. The most common explanation of the Turing test is where someone communicates with both a human and an AI and is not able to tell which is the machine and which is the human. It’s almost always phrased in the way that a human will act normally and the AI will try to act as a human, mistakes, typos, and imperfect information.  Regardless of whether modern AIs can beat the Turing test I think it’s inevitable that an AI will conclusively beat the Turing test in the coming years. A more interesting question is whether a human can trick another human into thinking he or she is an AI. It’s similar to the Turing test in that it’s supposed to make the AI and human indistinguishable to a judge but instead of making the AI smarter we’re dumbing down the human.  The nice thing about this approach is that historically it was very easy for a human to act as an AI by making dumb mistakes and responding with non-sequiturs. I suspect it’s currently quite difficult to respond in a way that would convince someone you’re an AI, even after enough time speaking with one, and I’d love to see this attempted. In the end both of these approaches converge to the same goal of making AIs and humans indistinguishable and this is just another way of looking at it.
{% include setup %} I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.  {% highlight javascript %} var express  = require('express'),     port = 4000;  var app = express();  app.get('/sum', function(req, res) {   var n = parseInt(req.param('n'),10) || 0,       a = parseInt(req.param('a'),10) || 0;   if (n === 0) {       res.status(200).send('Sum: ' + a);   } else {       var url = "/sum?n=" + (n-1) + "&a=" + (a+n);       res.redirect(url);   } });  app.get('/fact', function(req, res) {   var n = parseInt(req.param('n'),10) || 1,           a = parseInt(req.param('a'),10) || 1;   if (n === 1) {       res.status(200).send('Factorial: ' + a);   } else {       var url = "/fact?n=" + (n-1) + "&a=" + (a*n);       res.redirect(url);   } });  app.get('/isPrime', function(req, res) {   var n = parseInt(req.param('n'),10),       f = parseInt(req.param('f'),10) || 2;   if (f > Math.sqrt(n)) {       res.status(200).send('Prime');   } else if (n % f === 0) {       res.status(200).send('Composite');   } else {       res.redirect('/isPrime?n=' + n + '&f=' + (f+1));   } });  app.listen(port); console.log('Server started on port ' + port); {% endhighlight %}  The only cases I can think of where it’s even remotely useful is if your servers are behind a CDN and you want to cache every intermediate result without having to write the application logic to do it or you need to reduce the amount of work done by a single HTTP request. It’s just not an efficient approach otherwise - the overhead of making new HTTP connections and handling arguments for every recursive step is usually more expensive than doing the actual logic within a single request.  The other use case I can think of is purely educational - it forces you to write your recursive code in a tail recursive style and forces you to think about the state you need to share between redirect requests. And if you’re ever told to solve a problem without using for loops or recursion you can violate the spirit of the request by using a series of HTTP redirects.  I’m genuinely curious if there’s an actual use case for this and whether anyone’s had to do this.