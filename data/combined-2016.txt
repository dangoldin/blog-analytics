{% include setup %} Part of my 2015 goals was to have a weekly retrospective where I’d be distraction free and force myself to just sit and think. I usually did this on a Sunday morning by going outside and sitting on a bench overlooking the river or inside a quiet park. At the end of each of the retrospectives I’d sit down and jot down my thoughts in order to consistently revisit the list in order to keep improving. Below are the lessons of 2015 that I’m adopting going into 2016.  - Scheduling time for a task rather than just a goal. In the past I’d add tasks as a day event to my calendar. The better approach is to block specific time for a task - this ensures I’ll at least get something done and makes it more difficult to push things back. - Minimize the amount of physical things I own and focus quality over quantity. Maybe this was due to the apartment move but I’ve come to the conclusion that I would rather have fewer things of higher quality. This is a bit tough for me to act on since I tend to like getting deals and am pretty cynical towards trends and fashions - I can’t tell what’s actually high quality and what’s just marketing. - Sleeping more and better. Some people can get away with little sleep but I’m unfortunately not one of them. I need to get at least 7 hours to be productive. - Tracking my time usage better. It’s amazing how much time we actually have and how much of it we waste. For me it’s due to a variety of distractions and I need to be better at understanding how I spend my time in order to improve my behavior. - Don't procrastinate. A simple lesson here but I need to stop pushing things to tomorrow that I can do today. Especially when delaying something ends up snowballing and delaying a bunch of other things. - Focus on one thing at a time. Multitasking doesn’t actually work and I end up doing multiple things poorly and slowly rather than one thing well. I typically fall into this habit when watching some TV while doing some work - in those cases I’m almost always slower at my work and it would have just been better to finish the work and reward myself with some leisure time. - Distraction free walks to think. This is a rephrasing of the introductory paragraph but it’s important to get away from distractions and just force your mind to wander and think. It’s difficult at first with the desire to look at a phone or a random website but it’s worth it. - Knowing at every point why I'm doing something. Another lesson here in understanding how I use my time better. If I’m doing something I should know exactly why I’m doing it since everything comes with an opportunity cost. This doesn’t mean that I need to be productive at all times and can never relax but I should understand the tradeoffs I’m making. - Having and evaluating short and long term goals. I wish I did this when I was younger but it’s important to have goals we’re constantly working towards since it provides direction and allows us to measure our progress. - Watching less TV. A no brainer here but TV is a pretty big waste of time and I should watch less of it. I already don’t have cable but still find myself wasting time watching Netflix or some football games. - Focus on making versus consuming. This is all about productivity but I need to get into the habit of not consuming as much (TV, blogs, games, etc) and instead using that time to create. I’m already decent at this but need to get to the point where creating actually gives me more pleasure and relaxation than consuming. - Focusing and dedicating time to finance/investing/routine/research. As I’ve gotten older I can’t help but think about my later life and a big part of is figuring out how to invest my savings now to prepare for the future. I need to be more active in my investments and make sure the money I have isn’t just sitting around depreciating. - It’s okay to not have any new insights. During one of my walks I just wasn’t able to think of anything new and that’s perfectly okay. Not everything is about productivity and novelty and it’s fine to just relax and enjoy the moment. - Having a behavior consistent with views. A philosophy of life one here but if there are certain things you feel strongly about you need to make sure you act in alignment with it. It’s tough to do given outside constraints but something I’ve been more keen on. This sounds a bit abstract but an example is fighting peer pressure - sometimes it’s better to just skip an event and focus on what you want to do. - Just get started with something small, work your way up. Oftentimes embarking on something new feels like a gargantuan undertaking but it’s better to just start and take it one step at a time. The point above on scheduling time for tasks rather than goals helps address this. - Identifying bad habits and working on eliminating them. This is all about self-awareness but we all have bad habits and if we acknowledge them and work on eliminating them we’ll all be better off. - Abstinence versus moderation. I don’t recall where I read this but it rung true to me. The point was that we’re all wired differently and that some people have a hard time doing moderation and for them abstinence is necessary. A lot of my bad behaviors fall into this territory and I’d be better off completely abstaining rather than walk the fine line of moderation. - Thinking about personal brand. I’m not sure this is relevant to everything but I think it’s important to think about the personal brands we have and fostering it. Who knows how the world will look in the future but it’s important to have a good reputation and understand how you’re seen and perceived. - Having constant list of todos. I maintain an ever-growing list of todos that I will try to knock out when I have some spare time. It helps take care of a few items while keeping me productive. - Finding entertainment from within, not outside sources. Rather than rely on the outside world to entertain us we should find that within - that way we can always be entertained and don’t need to be blocked by anything. - 1% better each day. Just a thought here but if we all got 1% better each day and that compounded then at the end of a single year we’d be nearly 38 times better. This is tough to achieve but there’s just so much potential that we at least have to try. - Expectations are oftentimes better than the reality. Many times I’ll do something because i have the expectations and thought that I’ll enjoy it but after the fact I realize that it was a waste of time. The biggest example of this for me is drinking - I come in with the notion that it’ll be fun but more often than not it’s the same as any other time. It would have been better to save the money and calories and just have a fun time with friends. - When making spelling mistakes, retype the entire word. A small one here but my spelling has gotten worse with the advent of built in spellcheckers and my way of fighting it is to retype the entire word without using the spellchecker whenever I make a spelling mistake. This at least gets me into the habit of spelling words properly. - Investing time and value into things that compound. Similar to many earlier points but we should be focused on investing our time into things that matter and help lay the foundation for the long term. In my case these are knowledge and health - investing in both of them now provides compounding effects for nearly everything later. - Taking care of the small things. These days it’s easy to get inundated with tons of small things that all eat up small amounts of time. It’s easy to dismiss these but I still strive to take care of the small details. - Figure out habits and rituals. Rather than trying to do too much at once it’s better to focus on a few things and do them until they become habits and rituals. Only then should we pick up new habits to adopt. - Running in the morning changes mood the rest of the day. It may be tough to wake up early in order to go for a run but it sets the tone for the entire day so I need to just do it. - Exceptions are never exceptions. It’s easy to skip something you don’t want to do by writing it off as an exception but it never is. It’s just a rational trick to make us feel better but it’s easy to destroy a habit by constantly thinking of exceptions.
{% include setup %} I just got back from a 10 day vacation in Paris and couldn't help but compare it against New York. That's what traveling does - forces you to compare what you're comfortable with the novelty you're exposed to. Some make you appreciate what you have while others make you want more. In any case I wanted to share my thoughts while they're still fresh.  - Public transit: One of the first things you notice after living in New York are the public transit systems in other cities. New York has a reputation for having one of the best (one of the best?) in the world and I was curious to see how Paris handled it. The first thing I noticed was how short the platform was - rather than the multiple block stops in New York the Paris platform is enough for a 5 car train - and sure enough that's the size of the Paris trains. Each station I've been to had accurate time estimates and it felt as if the trains ran frequently and I never had to wait longer than 6 minutes although I've only taken it during the day. One thing that's struck me as odd was that it seemed as if every train had their own method of opening the door. In New York the doors open automatically but in Paris you need to either hit a button or pull some sort of level to get the doors to open. The way the stations were labeled felt friendly to tourists as well - each time you had to decide on an uptown or downtown train it would list each of the stops along with the potential transfers along each route which made it very easy to orient ourselves. The last thing I want to mention is price: the NYC subway costs $2.75 right now and you have to pay a fee for the metrocard itself. In Paris the fee is €1.80 which is just under $2 at current rates and you can buy 10 at a time for €1.40 each - significantly cheaper than the NYC subway. - Bike and car share programs: New York has Citibike and Paris has an equivalent version called [Vélib](http://en.velib.paris.fr/). I didn't get a chance to use it so don't have much of an opinion but the rates they offered were significantly lower than a non-annual Citibike pass. A daily Citibike pass is close to $10 whereas you can get day of Vélib for €1.70 and a week for €8. In addition to a bike share program, Paris has an electric car share program with stations prevalent across Paris. I didn't get a chance to use these but it seemed like a really neat idea that reminded me of ZipCar without the burden of needing to return the car to the original destination. - Neighborhoods, not districts: This might be entirely due to where I stayed and wandered but each neighborhood felt like it's own little city. We'd walk around in a neighborhood and it would have everything one would need - a bakery, a cafe, some grocery stores, a few restaurants and bars, a dry cleaning place, and a few boutique shops. It made it seem that one only needs to walk a few blocks to have everything they need. In New York it feels as if there are districts - the flower district on 28th, the diamond district in midtown, the theater district near Time Square, the rug district on 31st, the lighting stores in chinatown - but it didn't feel as if Paris was structured the same way. Paris of course is known for the shopping on Champ-Elysees but that's more the exception than the rule. The only other area that felt like a district was a series of falafel shops in the Marais. Of course this may be completely wrong and only visible through my tourist lens. - Architecture: Compared to New York Paris is ancient and its architecture and layout reflects that. Due to [Baron Haussmann](https://en.wikipedia.org/wiki/Georges-Eug%C3%A8ne_Haussmann)’s work during the 19th century Paris has a consistent look and feel which adds to the beauty. Paris barely has any skyscrapers since the majority of the buildings were construct before the elevator era. I was also struck by how mixed use the buildings were - many of them were businesses on the ground floor while the higher floors were residential. New York definitely has a bit of that but still feels as if it has some areas that are resident focused while others are commercially focused. - Price: Based on my conversions and research I expected Paris to be a lot more expensive than it actually was. The biggest reason was that the exchange rate was hugely in my favor ($1.1 per €1) but even then the cost felt offset by the listed price including tips and taxes. For example, if I go to a restaurant in NYC and have a $14 dish it’ll end up costing me close to $18 due to the tax (8.875%) and tip (~15-20%). At an exchange rate of 1.1 dollars per euro that’s equivalent to a €16.37 dish. We went to a few grocery stores and the prices for fresh food felt reasonable and only a tad bit higher than what we were used to. We also got a chance to look at some posted real estate listings and they seemed cheaper than NYC - but the apartments are generally smaller. This is a pretty biased view since we spent it as tourists and didn’t have to buy clothes or any real house items but I suspect all in all it would be pretty comparable, if not cheaper, than New York. - Panhandlers: In NYC it’s typical for people to look away and rush by someone panhandling but what struck me about Paris was that people would stop and have conversations with them. Even more, people were stopping with their children to chat and seemed to be engaging in meaningful conversations. My French wasn’t good enough to pick up the contents but the fact that people actually stopped and had conversations struck a chord with me. We talk about treating poverty and homelessness but unless we treat them as people and provide proper respect it will be for naught. - Restaurants: Not too much here but one thing I wanted to point out was how diverse the streets of Paris were compared to the “front” of the restaurants. The host and waitresses at nearly every restaurant we ate at had the “classically French” look - I’m not sure whether this was intentional but it struck me as odd given how much diversity we have in NYC. - Public restrooms: I haven’t seen this anywhere yet but Paris has free, public, self cleaning restrooms. It’s a bit slow since you have to wait through the washing cycle for each person but the fact that it’s publically available and free amazes me. - Cabs: For the most part we used the subway but we had an interesting experience when we used a cab. The driver suggested an alternate route to the one provided by his GPS and it took us a bit longer than expected to get where we were headed. Instead of charging us what the meter showed he admitted fault and told us to pay a lower amount. Despite our protests he stuck to the lower amount. I’m not sure if this is a common experience but I’m extremely doubtful something like this would ever happen in NY.  Combined, these make it seem that I prefer Paris to New York but I honestly haven’t figured that out. Paris seems to have more progressive policies than New York but I’m basing that purely on my 10 day trip and actually living and working there may be entirely different. It feels as if Paris takes public services more seriously - the public transit is cheaper, more frequent, and more robust since I didn’t experience a single stall or failure which is sadly a common occurrence in New York. I’m also aware that I’ve only spent 10 tourist days in Paris and may be approaching it through rose-colored classes. I’d love to get thoughts from people that have lived for significant periods in both.
{% include setup %} An important lesson I’ve picked up is to have a consistent development environment across your computers. These days it’s common to have a home computer, a work computer, as well as a series of VPSs that we use for development. The more similar they are the easier life gets. Having the same code and libraries reduces the risk of an application working on one machine but not the other and avoid the hassle of upgrading esoteric libraries. I’ve run into numerous issues where small version difference led to weird behaviors that ended up taking a long time to debug. Consistent tools help as well - using emacs on one machine but vim on another slows you down when you have to context switch and figure out which one you’re using. By committing to one you become more efficient as you develop the shortcuts and flows that are possible. Using virtual environments and containers helps get at this point - they’re both ways to ensure that the code you’re writing and testing is going to be the same code that’s running on production. Without this every time you deploy new code you’re risking failure. More often than not it will work as expected but it’s those rare cases that will be problematic and anything that can be done to avoid them should be done. One of the simplest ways is to align your development environments with your production ones.
{% include setup %} Python’s my goto language for doing quick tasks and analyses with the majority of them being quick scripts to analyze a file or pull some data. I’m constantly looking to improve my code and lately have developed the following approach. The goal isn’t to make it as short as possible but to make it as expressive and clean as possible. They're related but not synonymous.  {% highlight python %}#!/usr/bin/python  import csv from collections import namedtuple  # Can add whatever columns you want to parse here # Can also generate this via the header (skipped in this example) Row = namedtuple('Row', ('ymd', 'state', 'size', 'count'))  with open('file.csv', 'r') as f:     r = csv.reader(f, delimiter=',')     r.next() # Skip header     rows = [Row(*l) for l in r]     # Do whatever you want with rows {% endhighlight %}  The reason I like this approach is that it’s obvious what’s happening and it’s being done in a Pythonic way. There’s no traditional for loop that spans multiple lines and it’s simple to update the loop to manipulate the values during the handling of reach row. This approach also leverages the namedtuple collection which is one of my favorite types - a class-like structure that's significantly more memory efficient but provides easy named access the fields (row.ymd, row.state). With this basic structure in place we can add all the bells and whistles that manipulate and tweak the rows. One thing to be aware of is that the namedtuple generates if immutable so you either need to manipulate the values before construction or use additional structures to transform the data.
{% include setup %} Over the past year I’ve been collecting personal stats nearly every day in order to see if I can spot any patterns and just understand myself better. These ranged from the time I spent sleeping to my mood (both physical and mental) to what I ate and drank. Over the weekend I hope to dive deeper into them and work out some relationships and patterns but for now I wanted to share just some basic summary stats. The script to analyze the data is up on [GitHub](https://github.com/dangoldin/annual-stats-analysis) but note that it’s designed for my file format.         Avg  Min  Max  Std Dev      Sleep (Hours)  7.35  3  11.5  0.96    Alcohol (Drinks)  1.79  0  14  1.97    Coffee (Cups)  1.38  0  2.5  0.66      I also have data for my physical and mental moods three times each day. I haven't gotten the chance to get anything meaningful out of it yet but for the most part I'm a pretty happy person! I've had a few colds and headaches but I categorized over 90% of the days as being in a good mood but just under 80% where I'm a good physical mood due to some congestion or allergies which end up improving by the end of the day.  These summaries are interesting despite being simple and I can't wait to see what I discover when I take a deeper look. The goal is also to use this exercise to tweak the what and how of what I'm going to collect in 2016. Definitely let me know if you have ideas.
{% include setup %} Nearly all the conversations with my family is in Russian and phone calls are no different. The fun happens when I miss a call and it goes to voicemail. Turns out that despite the amazing job Google does in transcribing English calls it fails terribly at Russian. Instead of realizing that it’s not English it ends up with transcriptions such as “douche nozzle booster.”      Given Google’s expertise in machine learning and their massive data sets I’d expect them to at least be able to identify a non-English language. My guess is that Google Voice is no longer a priority and may not even be under development at all. I had a little over a hundred unread messages I needed to mark as read. With Gmail you get the option of applying an action to the entire selection - not just what’s visible - but with Google Voice you have to go through it page by page. And there’s no way to include more items per page. A tiny bit of modern web functionality did make it through though and I was able to use shortcuts to get the job done relatively quickly. I realize self driving cars are both more exciting and have more potential but I wish there was something being done to improve Google Voice - there’s a ton of us still using it.
{% include setup %} Great engineers assume end to end ownership of their products. Rather than focusing on one feature at a time they understand how it fits in with the rest of the product and think about the impact it will have on users and the business. This leads to code that scales with the product while being able to be maintained and developed by a small team. But you can only have this with everyone embracing full ownership over a product.  This idea can be expressed via an ownership hierarchy. The idea is that all engineers are responsible for writing code but the best ones want their code in products that’s loved by the end users. By moving up this hierarchy you develop a larger sense of ownership than someone who just wants to knock out some tasks.  - I opened a pull request: This is the start for every engineer. We all write code and some may consider it done when they open a pull request - leaving the rest up to someone else. - My code’s merged into master: The next level is a tiny bit beyond - in this case it’s not just that the code was written but that it has also been merged into the main branch. - My code’s deployed to production: At this point we’re at least aware that the code isn’t the end goal and that we want to make sure it’s out in the real world. - My code is being used in production: We’re finally at the point where we care that our code is actually being used. Code that’s deployed but unused doesn’t matter and we strive to write code that’s actually used. - Users love my code: The peak is building products that are loved by users. This is what drives great products and should be the goal for every bit of code that’s written and deployed.
{% include setup %} If you’re in the software engineering world you’ve probably heard of the 10x developer. They’re an order of magnitude more productive than everyone else and can make all sorts of problems go away. The 10x number is completely arbitrary but I’ve worked with numerous developers who were notably more productive than others. A big part of it is just being able to write more code - a combination of knowing the right tools for the job and moving quickly while avoiding mistakes. But a bigger part in the productivity comes from making the appropriate decisions that are able to stand the test of time. If your code needs rewriting every time a new feature comes out it’s going to be tough to be as productive as someone whose code can be easily expanded and maintained as the product evolves. Great developers make design decisions that are able to solve the immediate problem but also leave a clear path for the improvements that will inevitably come. If you know what’s coming in a couple of months or in a year it’s simple to account for it in the current design but the real skill comes in being able to think of the unanticipated cases and be able to support them with minimal effort. Beyond that some choices end up unlocking opportunities that would have been difficult to fathom in the first place. Imagine coming up with an elegant implementation that solves an urgent problem and a couple of months later you realize that with minimal tweaking that implementation can turn into something that is transformative to the product. It’s impossible to think through every decision since you’ll end up stuck in a world of “analysis paralysis” but great engineers either have a gut feel or enough experience to make these high leverage decisions more frequently than others.
{% include setup %} I just saw that Apple has [acquired](http://techcrunch.com/2015/11/24/apple-faceshift/) Faceshift, a VR based startup that makes it easier to create realistic animated characters. Two years ago Facebook [acquired](https://www.facebook.com/zuck/posts/10101319050523971) Oculus VR and Google soon followed by an [investment](http://venturebeat.com/2014/10/13/google-counters-facebooks-oculus-buy-with-500m-investment-in-vr-startup-magic-leap/) in Magic Leap. Apple is rumored to working on a self driving car and we all know Google is doing the same thing. And around the time that Facebook acquired Oculus Uber was [poaching](http://www.nytimes.com/2015/09/13/magazine/uber-would-like-to-buy-your-robotics-department.html?_r=0) a good chunk of the robotics department at Carnegie Mellon.  VR and self driving cars have been hailed as the next big thing but it’s still shocking how so many of these large tech companies that started off in different industries are converging and out spending each other on these new technologies. It seems as if in their desire to own a new market they’re all joining the fray hoping to become monopolies in these new industries.  Coupled with the news of how [dominant](http://www.wsj.com/articles/the-only-six-stocks-that-matter-1437942926) the larger tech companies have been in the stock market compared to the other players it’s hard not to think that we’re moving into a world dominated by a few big companies that can outspend others and take over brand new industries. We love the idea that a small group of people in a garage can become the next Google but I wonder whether that’s likely to remain true. I hope so but maybe we really are just setting us up for another Gilded Age.
{% include setup %} When starting to spec out a new feature a good habit is to think about what it won’t do. This forces you to focus on the problems that aren’t being solved and makes you aware of the tradeoffs you’re making. Rather than focusing on the problems being solved it’s equally important to know what you’re not doing as well as what your implementation will preclude you from doing in the future. To be effective we need to make tradeoffs or we’d never be able to launch anything but we shouldn’t make them blindly. We need to be aware of the tradeoffs we’re making and understand the paths that will be closed off by a given implementation. By thinking of the negatives of a particular approach we’re able to surface many of these dormant issues. This helps avoids surprises later on and ensures the code has been dissected and thought through in a variety of ways.  Another great thing to do is to share this list with the end users of the product. We’re known for having a variety of biases and a common one is risk aversion. In this case if we just list the problems we’re solving everyone’s glad to endorse it but as soon as we highlight the negatives people will start speaking up. We’re never going to ship perfect code but it’s something that we should strive for and getting actionable feedback early in the process is one of the best ways to get closer to that goal.  Everyone picks up this skill naturally through experience after being bit too many times by a crappy implementation but imagine how much better we’d be if we understood the tradeoffs we’re making with every decision. Focusing on the cases our solution doesn’t work for and prevents us from handling is a great way to get this experience earlier.
{% include setup %} Years ago, one of my projects at Yodle involved building out an automated reporting system that would consolidate all the existing reports being run via SQL queries and consolidate them into a unified application that would take care of the execution and the delivery. During the design process I spoke with existing users to see what else they’d like and it quickly morphed from a cron-job like application that just emailed CSV files based on SQL queries into a full fledged business intelligence tool that users could use to pull arbitrary data formatted in a multitude of ways. While thinking through the design of this application I spoke with the CTO and he gave me a phrase I keep going back to: “To get the expressiveness of SQL you have to write SQL.”                      While simple and glib I like how relevant this statement is to building software. When asked users will push for the most flexible and powerful system that comes with all the bells and whistles but at that point it’s equivalent to them just writing the code themselves. We have to know where to draw the line and understand what the use cases our product needs to support and not just everyone’s wishes. Otherwise we run the risk of building a [Homer](http://simpsons.wikia.com/wiki/The_Homer).
{% include setup %} Retargeting ads work by checking to see a product you’ve looked at and then showing you that product over and over again with the hope that at some point you buy it. There are entire companies dedicated to this with extremely sophisticated algorithms so I’m surprised when I see inefficient behavior. In my case it was an Amazon ad that kept following me around even after I already purchased the product, a precision cooker. Given that Amazon knows my purchase history and sees that I’ve already bought the cooker it makes no sense to keep showing it to me. It seems that their algorithm figured this out as well and started showing me the same product in different packages and at different price points. The fact that they have logic that’s smart enough to show me different variations of the same product but not take into account my purchase history shocks me. What makes this even worse is that I own some Amazon stock and realize that this inefficiency has an impact, albeit a tiny one, on my shares.
{% include setup %} Many have written about society’s inability to enact laws quickly enough to deal with the current pace of technological innovation. Governments are still trying to figure out how to regulate the sharing economy with both AirBnB and Uber being reacted to rather than being effectively regulated. This leads to different treatment in different locations and causes confusion for consumers, the businesses, and regulators.  A potential way to rectify this is to actually increase term limits for people in government. With politicians focusing on reelection every few years and constantly moving in and out of office it’s tough to develop a consistent regulatory approach. This worked well a hundred years ago when new industries would take a decade to develop and you could regulate them as they grew. Now it can take a year for entirely new businesses to be created before governments can react to what’s happening. By then the new consumer behavior has become entrenched and becomes difficult to change. Rather than worrying about reelection and undoing prior policies politicians should be focused on the future and how to adapt government for an increasingly changing society.  It’s counterintuitive that to deal with rapid change you want slower government turnover but it makes sense. Imagine a football team changing coaches for every game or a company switching CEOs every year. They’d be too busy dealing with the leadership change to win games or grow as businesses. Stability is necessary in rapidly changing environments and governments need to adapt to provide that in modern times.
{% include setup %} One of the biggest lessons I’ve learned is to spend extra effort thinking about the database when setting out to build something new. Compared to changing a database schema, changing code is trivial. The database structure defines how you think about your business and either provides the flexibility as you grow or impedes you when forced to support something it wasn’t designed to handle.  With code you can do a deploy which can replace all behavior at once while with data you’re forced to acknowledge and handle the data you have. If this is a large table you have to figure out how to migrate the data to a new schema. The simple way is to deal with the downtime and hope the migration works. The more complex way is to support two database schemas at once with your code while the migration occurs. Neither of these would be necessary if you think through the database design choices you’re making. It’s going to be impossible to address every future need but there’s incredible value in at least thinking through potential changes and how they’d be supported.  A simple question is the relationship between tables - are you ever assuming a one-to-one relationship that may be one-to-many in the future? If that’s the case you’re probably better off designing the database to support the more advanced case but having your application only support the one-to-one case. This keeps the flexibility in place if you need but doesn’t complicate the code too much.  Another question to ask is whether there’s anything redundant. It may be easier to denormalize your data a bit for the sake of improving a query but don’t. If a database can support an inconsistent state it will support an inconsistent state. Whether due to a bug, a timing issue during a deploy, or someone making a manual update you’ll end up with an inconsistent state in the database which will likely lay dormant for too long. Avoid this issue entirely by removing all redundancies and potentially conflicting fields.  Beyond the tactical questions, thinking about your business and product roadmap a year from now is a great way to influence your schema now. If you suspect you’ll need to support a particular feature or flow you should imagine what your data would need to look like. It’s important to do this when writing code but it’s more important to do this when designing your database. Code can be changed with a deploy but database changes require more.
{% include setup %} Nearly every major tech company is pursuing a self driving car future and it’s inevitable that at some point most cars on the road will be completely autonomous. Cheap and easy transportation is the immediate change but there will be massive secondary effects to the shapes of cities and society.  A [college professor](https://www.johnson.cornell.edu/Faculty-And-Research/Profile?id=lvo2 ) used the example of the invention of the car to highlight these sort of effects - if told that cars would be successful most people could have guessed that they’d replace horses and clean up cities. But very few would have been able to predict the rise of highways which led to the development of suburbs and the current structure of the United States.  Self driving cars have that same potential and it’s an interesting exercise to think through the impact. The short term is that fewer people will own cars, our roads will be safer, and that there will clearly be some disruption among the auto manufacturers. The medium and long term are where it gets tricky.  The increases in safety will lead to faster cars which may lead to another shift in where people live. One idea is that people will be able to live further and further away from cities which will lead to a decline in suburbs with more people opting to live in more remote areas while also leading to a boost in urban living.  Self driving cars will not just be ferrying people and one can imagine nearly everything being able to be delivered by self driving car or truck. I like the image of a “carrier truck” that drives around neighborhoods with a series of drones taking off and landing to deliver items along the way. In this sort of world there’s not a great need for physical stores. Taken to the extreme this means that cities will be designed to focus on the social elements. Convenience stores will disappear but restaurants will thrive. Most people aren’t buying everything online but I suspect it’s only a matter of time.  Public transit will have to change. I worry that if the price of self driving cars drops low enough to appeal to most people but high enough to not be affordable by some it will lead to a decline in public transit. At that point since most people wouldn’t care about public transition it would end up in a self destructive loop as more and more people decide to go for the self driving car route which in turn leads to less and less funds being allocated to public transit.  These are just scratching the surface and we’ll have to wait to see what happens.
{% include setup %} I’ve been meaning to mess around with [Let’s Encrypt](https://letsencrypt.org/) since they launched their public beta but haven’t had the chance until earlier today. As an proof of concept I had a bunch of old projects running on a Digital Ocean instance and decided to try converting them to HTTPS using the Let’s Encrypt project.  Despite the usual complexity of getting and integrating an SSL certificate Let’s Encrypt made it extremely easy. It was smart enough to go through each of my Apache configuration files and prompted me to see which domains I wanted to switch over to HTTPS. After selecting a few and continuing to the next step it generated new configuration files with the appropriate setting to enable SSL support.  The only issue I ran into was handling a WSGI configuration properly. Let’s Encrypt works by copying an existing configuration file and adding a few lines to specify the SSL certificate. This works great for simple configurations but can lead to an issue when you have the same WSGI configuration across two files. The fix was straightforward - temporarily comment out the conflict lines, run the Let’s Encrypt script, and then uncomment the lines in the new SSL version of the file.  Overall an extremely simple way to enable HTTPS on your projects. In the past I would have never set SSL up on toy projects due to both the cost of buying one as well as the cost of dealing with a bunch of esoteric commands to set it up. Let’s Encrypt makes it incredibly easy - especially if you’re running Apache.
{% include setup %} After reading the positive reviews I got past the gimmick factor and jumped aboard the Amazon Echo train and got it set up yesterday. After going through the obvious examples (what’s the weather, tell me a joke, add x to my shopping list, play song y) and playing around with it I’m past the gimmick stage. The always on listening is really a different way to interact with our devices. Conceptually it’s no different than using Siri or Google Now but in practice it’s a world of difference. I don’t always have my phone with me and for some things it just feels more natural to start speaking and see an immediate effect. Whether that’s playing some specific songs or playlists, changing the volume, or adding items to a shopping list it feels more natural than having to go through a phone. One of my favorite use cases so far has been using the Echo to keep track of my shopping list. In the past I’d be in the kitchen and realize we needed something and would forget as soon as I switch tasks. With the Echo I can immediately call out what to add and have the list readily available next time I go to buy something.  To be honest, 99% of our Echo usage has been playing music and adding things to a shopping list but I can see the potential there. There are a ton of apps, that Amazon calls skills, with new ones constantly being developed and I look forward to seeing what kind of cool stuff gets developed.
{% include setup %} I [set up](http://dangoldin.com/2016/02/21/amazon-echo) the Amazon Echo over the weekend and have been an active user of my wife’s Spotify account which comes integrated with the Echo. I would have preferred to use my Apple Music account but the Echo currently only supports Spotify. I suspect the biggest reasons are competitive - Amazon and Apple are competing for the home and it’s likely that either Amazon doesn’t want to integrate Apple or Apple is preventing Amazon from getting the integration done. At the same time Amazon has a music offering yet they specifically call out the Spotify integration. Is this because Spotify is only a competitor for music and the value of an Echo trumps this? Is it because Spotify has more reach and this is a necessary integration? I’m sure the answer is a bit of both but it’s fascinating to see how these partnerships develop.  Ideally every company would provide an open way for others to integrate their apps but we live in a competitive, capitalist world where every company wants to get an edge over their competitors. As consumers it’s up to us to push for the integrations we want and make sure these platforms stay as open as possible - otherwise we’ll end up making the rich richer and prevent new entrants from even having a chance.
{% include setup %} Whenever I watch some online lectures or listen to a podcast one of the first things I do is change the speed to either 1.5x or 2x the original. Sometimes I’ll have to skip back or reduce it back to the normal speed but for the most part this approach saves me tons of time and I like to think that I absorb the same amount of information. But the fact that I can absorb and process information at twice the speed makes me wonder how much more productive I’d be if every conversation I had occured at twice the speed. Is there some physiological reason we don’t speak at twice the speed? Is there a cultural factor? Does this information density vary based on language?  We can train ourselves to get faster and faster at processing aural information but there must be some limit and I suspect there’s a wide range in the information density of various languages. If this is the case I wonder if there’s some conclusion that can be drawn about that culture or society. Most likely the bottleneck is on the transmission side - the effort to produce language is more than listening and it requires both our brains to form thoughts and our mouths to turn them into sounds which are the limiting reagent.
{% include setup %} It’s obvious in hindsight but incredible when you experience it but every successful company has to iterate through a variety of tools as it, and its problems, grow. A typical modern tech startup starts by identifying a problem and using a common web framework to quickly come up with the first pass. But as this company grows new problems and situations arise that the initial solution no longer supports. They may end up having a series of asynchronous tasks and need to start using RabbitMQ with that use case. MySQL may no longer be enough and they start offloading their data to Redshift. That off the shelf web framework is no longer performant enough so they have to split it into multiple components and start embracing strong, statically typed languages.  This tool specialization also goes hand in hand with team specialization. A single engineer doesn’t have the time to do everything so startups need to make the short term decisions and focus on the next couple of months in order to grow. Only when they grow does t make sense to find the critical problems and dedicate time to fixing them. And hopefully by that point you have a larger team that can focus on the deeper problems.  For most of us the problems have already been solved and it’s about figuring out how to adapt the solutions for our systems. Sometimes this requires getting an open source library to work. Other times it may require implementing the code from an obscure academic paper. But the real success comes when you run into problems that no one else has encountered. At that point you’re dealing with extremely specialized problems that you were the first to encounter. These are the scale problems that Google and Facebook are solving and every startup hopes to get there some day. In fact, that is the ultimate mission engineering based companies - solving problems that haven’t been encountered yet.
{% include setup %} While reserving some EC2 instances earlier this week I discovered that Amazon allows you to [sell](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-buying-guide.html) reserved instances you’re no longer using. Usually the prices the third parties are offering are very close to the fair market value but I wondered if there was an arbitrage opportunity by reserving a longer term instance and selling it for a series of shorter term leases. The [typical discount](https://aws.amazon.com/ec2/pricing/) for buying a 1 year reserved instance is 30% while buying one for 3 years can get over 60%. The idea being that if you can get an instance for a 60% discount over 3 years and then sell it for 3 one year terms at a 25% discount you end up coming out ahead. Of course the challenge is that Amazon constantly drops prices so a 60% discount now may be equivalent to something much smaller three years later. There’s also the risk of no one purchasing your instances but that seems unlikely since you can always undercut Amazon’s official price. The other factor is the discount rate since you’re paying up front for 3 years worth of an instance. During that time you could have taken that money and invested it elsewhere which could have led to a better return but which would have been unlikely when you’re getting a 30% discount over the course of a year.  Unfortunately, based on the Amazon seller [documentation](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-selling-guide.html) it looks as if you can't actually split a 3 year reservation into 3 single year reservations and you'll be charged a 12% fee. There's always the option of using an instance and then selling it at a premium for a term that Amazon is not offering but I doubt it's worth it given the risks and the restrictions placed by Amazon.
{% include setup %} Yesterday I read an [interesting piece](http://www.nytimes.com/2016/03/06/business/airbnb-pits-neighbor-against-neighbor-in-tourist-friendly-new-orleans.html) on Airbnb in New Orleans. The content itself isn't new - it highlights the typical conflict between those that view Airbnb as violating local ordinances and ruining the city and others who believe that Airbnb brings value and is helping New Orleans rebuild after Katrina.  But what was interesting was the repeated claims of Airbnb and the other rental marketplaces that it’s just not scalable to follow local policies for every city and it’s up to the users to know their local regulations and follow them. I understand it’s difficult to localize complex products but these responses just feel like an excuse.  > Representatives of the larger home-sharing companies have met with New Orleans officials, but they are seldom heard from in more public forums. Officials of Airbnb and VRBO (Vacation Rentals by Owner, a HomeAway brand that is popular in New Orleans) point out that they operate in so many places they cannot possibly get into the specifics of local policy; they are merely private businesses offering services to consumers. So it is up to New Orleans and other cities to devise their own regulations, and up to users to follow them.  > According to Mr. Rivers, Airbnb and VRBO told his staff that it would be too onerous to adjust their software to accommodate every regulatory arrangement for thousands of municipalities around the world. Spokesmen for Airbnb and VRBO confirm that rewriting their platforms in this way is not practical.  Contrast this with Uber. They also run a marketplace that’s highly sensitive to local regulation but work within the confines of the law (including pushing to change legislation). Both Uber the company as well as the Uber app have adapted a localized view. When I open up the Uber app in New York City I see a variety of options that I don’t see in other places. In fact, Uber can even push idiosyncratic updates that may only last a couple of days - for example a special [“De Blasio” ride option](http://techcrunch.com/2015/07/16/uber-launches-de-blasios-uber-feature-in-nyc-with-25-minute-wait-times/) that came with a 25 minute wait time.  The goal of technology companies is to come up with elegant solutions to real world constraints. Uber has embraced it by building their company and product to embrace local differences while Airbnb adopted the attitude of a single product for the whole world. I’m confident if Airbnb wanted to build a flexible product that worked for local markets they’d be able to and it would actually be a fun and interesting product and engineering challenge. Startups need to embrace their challenges and this feels like Airbnb being complacent. I understand there’s a high cost to localize Airbnb and it comes with a world of risks but if they do it right they’ll be able to capture significantly more share and markets.
{% include setup %} The MySQL enum field provides a nice compromise - the space efficiency of using an integer, the human readability of text, and basic type safety. Yet I had this vague recollection of reading something that made it seem enums carried a ton of risks when changing the column definition so wanted to see if I could “break” it. Turns out it’s a lot more resilient than I thought. I went through a series of combinations - ranging from changing the order of the enums in the definition to trying to insert values that didn’t exist but in every case it handled it as expected. Doing a bit of research I discovered how MySQL represents the enum type. Rather than storing the values in a specific order MySQL supposedly creates a map-like structure to relate the integer values with their enum counterparts. This allows you to change the order of the enum definition without changing the underlying map or any of the stored values. I still wouldn’t use enums for anything that would require a join but for storing small and simple sets of data it works great.  {% highlight sql %} drop table if exists test;  -- Create the toy table create table test (   id int auto_increment primary key,   e enum('a','b','c') );  -- Populate it with some sample values insert into test (e) values ('a'), ('b'), ('c');  -- Confirm they look good select * from test;  -- Now let's add another possible enum value alter table test modify column e enum('a','b','c','d');  -- Looks good select * from test;  -- Add some more values insert into test (e) values ('d'),('a'), ('b'), ('c');  -- Looks good select * from test;  -- Change the order around alter table test modify column e enum('a','b','e','c','d');  -- Looks the same select * from test;  -- Change it again alter table test modify column e enum('a','b','c','d','e');  -- Looks the same select * from test;  -- Add and change the order alter table test modify column e enum('b','c','d','e','f','a');  -- Looks the same select * from test;  -- Fails since 'g' is not a valid value insert into test (e) values ('g');  -- Replace 'a' with 'f' update test set e = 'f' where e = 'a';  -- Now get rid of 'a' alter table test modify column e enum('b','c','d','e','f','g');  -- Now add 'a' back in alter table test modify column e enum('a', 'b','c','d','e','f','g');  -- Now swap 'f' back with 'a' update test set e = 'a' where e = 'f';  -- Looks just like before select * from test; {% endhighlight %}
{% include setup %} To get the most benefit from working out it’s important to have a plan and consistently measure yourself and keep pushing your goals and yet it’s surprising how rarely that’s done in a professional setting. We spend over 40 hours a week working but the majority of us view it as a chore and something that we just have to do. Imagine if everyone approached work the same way they approach the gym. People would have much clearer ideas of what they want to do and what challenges they face. They would be able to measure how well they’re doing and understanding what they need to start doing to get to the next level. Instead most approach it as something that they need to do rather than something they want to do.  Consistently going to the gym without a plan will definitely improve your shape and is better than not going at all. But it pales with what would happen if you went to the gym with a plan in mind. It took me a long time to realize this and I suspect most people approach work passively - they’ll just put in the time, do a good job, and see where things will go. But true success and joy come from constantly reevaluating your goals and thinking through the means to achieve them.  It’s depressing to think that most people spend a third of their day on something that they’re not actively engaged with when an attitude change can change the entire perception of work. Rather than putting in the bare minimum we should be thinking of what we want in our lives and how work can make that happen.  Many people seem to idealize retirement but I want to be so engaged in my work that I never want to stop. This is the vision we should all be working towards. I understand that this is a privileged perspective and not everyone has this choice and there are always constraints but it’s something we should all strive to attain.
{% include setup %} Last week the big news was that Google’s AlphaGo was able to win 4 of the 5 games against Lee Sedol in Go. As we’ve gotten better and better hardware it’s not surprising that an AI was finally able to win in a well defined environment. AIs will continue to improve and we’ll start seeing more and more of this behavior across a wide range of problems and not just games. The most significant part for me was that this was achieved by Google and not by IBM. IBM had two recent notable achievements in AI - one was building Deep Blue in 1997 which beat Gary Kasparov in chess and the other was building Watson in 2001 which dominated at Jeopardy. Yet just five years later Google has claimed the AI victory with AlphaGo.  It’s tough to not see this is as the passing of the torch. Google has tons of incredible AI that’s used to power the business but building an AI to focus on Go was taking something out of IBM’s playbook. Luckily, we’re seeing a ton of AI work being open sourced - from Google’s [TensorFlow](https://www.tensorflow.org/) to Facebook’s [FAIR](https://research.facebook.com/blog/fair-open-sources-deep-learning-modules-for-torch/) to [OpenAI](https://openai.com/blog/introducing-openai/) - and I’m excited to see what we come up with.
{% include setup %} In order to do any meaningful data analysis you need to have fun doing it. Otherwise it becomes a chore that’s extended by each additional analysis you run and each additional failed attempt at an insight. This requires a positive attitude and enjoying the slow, methodical process of discovery and appreciating each iteration while getting closer to the end goal. The vast majority of analyses lead to no new insight, especially when all the easy stuff has already been figured out, and it’s critical to remain the optimist while appreciating the present.  A key part of this is tools. I have a set of tools I’m intimately familiar with and can manipulate them without much thought. It’s this passive approach and behavior that lets me go through the rote work while simultaneously focusing in on the challenging elements of the problems I’m facing. Fast tools are also a requirement. Tools that allow you to quickly get a result prevent you from leaving the zone and leave you ready for the next attempt.  When 80% of the work is rote data manipulation it’s important to not burn out while getting to the 20%. To be successful you need to find the fun in both the data manipulation and the analysis.
{% include setup %} I rarely write about politics but it’s an election year and I had an interesting realization. Political parties are just like product bundles. We each have our own issues and policies we’re passionate about but it’s impossible to find a politician, less a party, that has the same views we do. Instead we have political parties that take a few issues and policies and try to wrap them up in a bundle hoping to appeal to enough people to win an election.  Reading the Wikipedia [article for product bundling](https://en.wikipedia.org/wiki/Product_bundling) makes it obvious how closely it fits political parties. From Wikipedia:    Bundling is most successful when:    There are economies of scale in production.   There are economies of scope in distribution.   Marginal costs of bundling are low.   Production set-up costs are high.   Customer acquisition costs are high.   Consumers appreciate the resulting simplification of the purchase decision and benefit from the joint performance of the combined product.   Consumers have heterogeneous demands and such demands for different parts of the bundle product are inversely correlated. For example, assume consumer A values word processor at $100 and spreadsheet processor at $60, while consumer B values word processor at $60 and spreadsheet at $100. Seller can generate maximum revenue of only $240 by setting $60 price for each product—both consumers will buy both products. Revenue cannot be increased without bundling because as seller increases the price above $60 for one of the goods, one of the consumers will refuse to buy it. With bundling, seller can generate revenue of $320 by bundling the products together and selling the bundle at $160.       Each of these is a perfect fit for politics. There are huge economies of scale and distribution for political parties. They’re purely information so there’s no marginal cost and the brunt of the cost is in the formation of a party which is incredibly difficult due to the massive network effects and infrastructure required. People have diverse beliefs with great variance on the most important issues and don’t have the depth to know every issue.  It’s no wonder that political parties are so entrenched but this also provides insights on how to dismantle these bundles. We need to examine history and see how previous bundles have been broken down and see whether those solutions can apply to our political system.
{% include setup %} Yesterday I made the case that the current political system consists of a [series of product bundles](/2016/03/26/political-parties-are-product-bundles/) and I’ve been thinking of ways unbundling would work. And what better way than to look at existing products and industries that have been unbundled.  As numerous people have pointed out, the music industry is a clear example. Initially music was sold on CDs and there was no idea of buying solo songs. But with the launch of the iPod, iTunes, and internet proliferation it became possible to buy individual songs. Lately we’ve been back in the bundling phase with the various monthly music subscription services, such as Spotify and Apple Music.  Music debundling was driven by technological changes. It made no sense to package individual songs for sale when they required physical packaging. But as soon as the majority of households got reasonably fast internet it became possible to start selling individual songs.  But how does this apply to the political system? It’s not really a technology problem. We have the ability to share and disseminate information to anyone with an internet connection. We have the ability to allow everyone to vote through a smartphone. We have the ability for anyone to start a cause and share it with millions of people. Unfortunately, having the ability doesn’t mean much without follow through.  In the case of politics there’s so much entrenchment (think recording studios) that change occurs at a glacial pace. The reason the recording studios signed with Apple was because of the rampant piracy - not due to their desire to improve the consumer experience. We need the political equivalent of piracy to spur this unbundling. Ideally it comes dressed as a white knight ready to save the system but leads to unexpected secondary effects that lead to significant changes in the system.  My gut is that we need a few small changes that open to the door to these unintended effects. Something akin to allowing people to request or report services via an app which leads to people asking what else? That will open the door to voting for issues and politics from our phones and maybe even filing taxes.
{% include setup %} Last weekend I finished the [The Everything Store](http://www.amazon.com/The-Everything-Store-Bezos-Amazon-ebook/dp/B00BWQW73E), which details the rise of Amazon from a fledgling online book retailer to its current form. One pattern that stood out for me was how Amazon was able to continuously push into new business areas due to the infrastructure that they had in place based on previous decisions and commitments.  They started with books but were able to grow into other smaller products once they figured out the logistics behind shipping smaller items. Once Amazon had that in place they kept tweaking their distribution system to expand the variety of products offered while improving the speed of delivery. This allowed them to keep amassing a list of products which they used to open up their platform to third party sellers. And as Amazon improved their infrastructure they were able to open that up to these third party sellers as well. In parallel, they built AWS to provide computer services to internal Amazon teams but were able to turn it into a brand new line of business that powers the majority of new startups. And now their are rumors of Amazon building out a shipping service to bypass FedEx and UPS.  Ben Thompson coined this the "[ladder-up strategy](https://stratechery.com/2016/snapchats-ladder/)" and I’d argue it’s the only way companies can keep consistently growing. Relentlessly focusing on a few things and then using them to attack adjacent markets is how you grow from a struggling startup to a powerhouse. The challenge is making the short term decisions that set you up for success in the future as well as knowing when to leverage that infrastructure to move into the next thing. The former is incredibly difficult - it requires thinking beyond the immediate step and understanding the opportunities that become available after a successful execution of the initial step. Then follow up by thinking of what the next strategic step will be and what doors that will open up. After a few iterations of this exercise you may get a glimpse of your company’s future. But the ideas are the easy part - the execution is an order of magnitude more difficult. Scenarios will consistently come up that necessitate changing your approach but each change will pull you further and further away from your initial vision. You then need to either steer the company back towards the original direction or adapt your plan based on these new directions.  It’s incredible seeing this successfully execute though. Companies that are able to do this consistently increase the size of their moat and become nearly impossible to dislodge. Each of their activities complements and reinforces the others which when coupled with their benefits of scale grant them monopoly-like powers.
{% include setup %} The best code isn’t code that’s elegant or code that’s brilliant it’s code that doesn’t need to be written. One of the best feelings is when you can take a new problem and turn it into an existing problem that already has a solution. Sometimes that requires making a few tweaks and compromises to the problem or the code but the time and effort saved can be massive. This requires a deep understanding of the problem being solved as well as the existing code. Someone knowing the code but not the problem won’t be able to transform the problem into something applicable. And someone having a deep knowledge of the problem but not the code won’t be able to see how the code can be adapted to solve this scenario. The optimal result comes from someone who can strip away the cruft from both of them while still maintaining the spirit of both in order to combine them.  To make this work you need code that’s clean, well architected, and accessible. Such code is a pleasure to work with and is transparent enough that a decent programmer can see how it can be tweaked to solve new problems that arise. This requires massive amounts of discipline to go back and refactor your code when necessary to keep it in a pristine state so it can be easily transformed when needed. And that transformation with introduce wrinkles that will need to be ironed out to set it up for the next wave of changes.  While writing this post I was reminded of a joke that emphasises this idea of minimizing work by focusing on what you’ve already done:     A mathematician was interviewing for a job. The interviewer asks him - "You are walking towards your office and running late for a very important meeting and you glimpse a building on fire with people screaming for help. What will you do?". The mathematician thinks for a while and replies : "People's lives are more important than an office meeting. I would immediately call for a fire brigade and help the trapped to the best of my abilities". The interviewer seems to be impressed with the mathematician's answer and moves on to the last question. Just to check his sanity, she asks: "And what if the building is not on fire?"  After a moment of thought, the mathematician replies with confidence: "I will set the building on fire. Now, I have reduced it to a problem that I have already solved before!"    -  ScottElliot  on  reddit
{% include setup %} Engineers strive to write code that’s general and flexible enough to adapt to support a variety of cases with minimal changes. Unfortunately, writing general code isn’t easy and requires significant thought, effort, and experimentation. The challenge is figuring out the appropriate time to generalize your code.  If you do it too early you may spend unnecessary time writing generalized code that will never be used again. Even worse you may write code that you think is generalizable but ends up collapsing under its own weight under future scenarios. In this case writing minimal code would have served you better since it would have been much easier to adapt or throw away to support the new case.  If you do it too late you most likely spent time doing repetitive work that could have been better spent building a scalable solution that you may end up doing anyway.  My rule of thumb is to generalize at n=3. The first two times I have to support a new scenario or process I'll just do it manually or hacked together. But as soon as I need to do it for the third time I'll start looking for a more generalized solution. At this point it's likely that the third is not the last time I'm going to have to do it and I also have 3 cases to base and test my solution on.  This isn’t a trivial approach but works surprisingly well. It’s incredibly difficult to predict whether a simple script will morph into something more or end up being used once. The easiest way to predict whether it will be repetitive is to wait until it is repetitive - for me that magic number is 3. High enough to weed out the edge cases but low enough to get enough value from being generalized.
{% include setup %} Earlier today I read an [article about MaxMind](http://fusion.net/story/287592/internet-mapping-glitch-kansas-farm/), a company that offers an IP address to geographic location mapping service, making a seemingly minor decision in 2002 that that led to unintended consequences that have been going on since then. The article goes into detail about the decision and the effect but the main idea is that it’s not a prefect system and they needed a way to approximate some IP addresses to particular locations. Lo and behold these locations are now seeing tons of harassment from law enforcement and various strangers online.  This is a perfect example of how a quick fix to a seemingly simple problem can lead to a world of problems that can impact others without you even knowing. I can imagine myself running into that problem and making the same decision. It’s unlikely I would have thought about the people that may have lived at those coordinates or that people would actually be using this information to track people down.  There’s a lesson here for everyone who’s writing software: at the end of the day all the code we write will have some effect on people and we need to be mindful of that. We’re not going to stop making mistakes but we should take the time to consider the impact of every line of code we write.
{% include setup %} One of the latest trends I’ve noticed is B2B companies is allowing you to sign up with a company email address and automatically linking you with the rest of the organization. This is a definite no-brainer and a really simple way of getting new users setup without having to be bottlenecked by a burdensome administrative process. No one on the HR team has to enter employees into the system nor send anyone their username or account info. Instead they just provide a link to the service and have people sign up with their company email address. Once this is done they immediately have access to whatever the base employee account should have. Only later one does an admin need to grant additional permissions and privileges.  The companies off the top of my mind that have done this are [Slack](https://slack.com/), [Greenhouse](https://www.greenhouse.io/), and [Tallie](https://tallie.com/) but there are countless others. If you’re building a B2B product that’s designed around teams working together this should be at the top of the product queue. It’s a great way to get on the good side of the HR team while getting your users onboarded quicker.
{% include setup %}      Last night, Amazon [announced](http://www.nytimes.com/2016/04/18/business/amazon-challenges-netflix-by-opening-prime-to-monthly-subscribers.html) that in addition to the annual plan they’re going to start offering Prime as a monthly service. Sure enough, investors interpreted this as good move by Amazon (up 1.51% at end of day) while hurting Netflix (down 2.79% at end of day and even more post earnings). These percentages translate into a $1.34B decrease to the Netflix valuation and a $4.49B increase in valuation for Amazon. As a shareholder of both I find this behavior interesting for its irrationality.  Companies are constantly innovating and have a constant stream of ongoing initiatives and experiments. I’m surprised such a simple move can impact the markets so much - it seems like an obvious move that would have happened at some point and should have been baked into the current price. The fact that there was such a sudden stock price move attributed to the news strikes as proof in the irrationality of the markets - trivial decisions shouldn’t be moving the needle and people should be investing in long term plans and visions.  I’m bullish on both and view them both as compelling replacements to cable and legacy TV consumption. Netflix has better content portfolio and is worth the $7.99 I pay each month. Amazon provides some new shows but I’m a Prime member for the free 2 day shipping. I’d love to see the numbers but I suspect there’s a large overlap between households that have Netflix and those that have Prime. The real competition is existing cable networks that are going to get punished as the younger cord-cutter generations move out of their parents’ homes.
{% include setup %} A combination of bots being in vogue and Telegram offering $1M in [bot prizes](https://telegram.org/blog/botprize) got me to spend a little bit of time writing a bot last week. To get my feet wet I created a simple, self-serving bot that would reply with a random blog post when sent a /blogme command. The code itself is extremely straightforward and most of the time was spent going through the Telegram bot docs and getting the deployment and HTTPS setup. A nice feature that Telegram has is the ability to write a bot that can respond to both polling and webhooks. The polling approach is a much trivial to get started with since you don’t need to worry about any of the devops work and can work on the core interaction. The cons are that it won’t respond immediately and you need a way to track messages your bot has already replied to. Changing it to a webhook provided real time responses but made it a bit more difficult to test and wrapping everything inside a minimal web framework. The biggest hiccup was the requirement of HTTPS for a webhook integration but [Let’s Encrypt](https://letsencrypt.org/) made it simple to get up and running. A year ago I wouldn’t have bothered prototyping anything that required HTTPS but these days it’s incredibly easy to set up. The [code is up](https://github.com/dangoldin/bots) on GitHub and if you’re interested in bots definitely take a look. And if you have Telegram installed try messaging “danblog” with /blogme to get a random blog post.
{% include setup %} When working on new features it’s easy to keep increasing scope until you end up doing a full rewrite of your code. Don’t. It’s healthy to refactor code as you go but you need to be wary of how many things you’re changing and the risks those changes carry. Code will get stale unless it’s constantly maintained and updated as the rest of the product evolves but trying to change too much at once will make it difficult to diagnose issues and increase the odds of bugs in production.  The analogy is that of an extremely sick patient. That person may need a variety of transplants but it’s dangerous and stupid to replace multiple organs at once. Instead you should find the most critical one to replace and do that. After the body adjusts to that transplant you move on to the next most critical one. Otherwise the body will go into shock and reject the organs.  Bad code is similar to this patient. There are countless things that can be improved but if it’s doing a critical job keeping a product alive you need to treat it carefully. Replacing everything at once may end up working but more likely it will cause a slew of problems that will be tough to diagnose given the various changes. It’s much better to approach code like a sick patient - make a change, release, and monitor to make sure everything is going well. Once you’re confident that the code is functioning as expected you can move on to the next most critical item. Over time you end up replacing the critical components while reducing risk.
{% include setup %} As many people know despite being bullish on tech I’m spartan and utilitarian with my technology usage. This expresses itself as a strong bias for text above nearly another format. There are tons of apps that try to help me organize my tasks and todos but I prefer simple text files and an intelligent folder structure. This is true when it comes to blogging as well - rather than using a fancy CMS or hosted application I rely on Jekyll which exposes my content in Markdown.  On the surface this seems inefficient - why build your own tools when perfectly good apps exist that will be maintained and improved over time? Unless I spend a ton of time there’s no way I’m going to be able to build a blogging platform that competes with Medium or Wordpress nor will I ever make a to do application that is better than Todoist, Wunderlist, or Google Calendar.  For me it’s less about the tool and more about the problem. Sure, a tool helps with that but I’m more about figuring out a process that works for me. Despite how great an app is it’s extremely unlikely that it will change to accommodate my evolving needs. Having my own process optimized around text gives me the flexibility to do things my way as well as easily change both the process and the underlying data.  Just last week I realized that I forgot to add metadata to a few of my blog posts. Had the content been squirrelled away in a web app there’s no way I would have been able to easily find which posts were affected other than writing a crawler and examining the DOM. But having everything in simple structured Jekyll text files made it as simple as writing a simple command line regular expression to identify these posts. And this can easily scale to any other blog maintenance task I have - whether it’s adding some additional information to a subset of posts or just searching for various words or phrases.  The success of this system depends on building out and committing to a structured approach when dealing with text. Text is innately extremely flexible but by imposing a semi-structured system of tags and folder structures it makes it extremely easy to navigate and manage. And if anything does change it only requires a small script to update everything to fit the new format. Replication is also simple - I can either keep it in a version control system or have it synced via Dropbox. If you’re undisciplined or have a static workflow definitely leverage an existing tool but if you’re constantly trying to improve your system and want the ability to go back and analyze content you produced there’s not much better than text. It unlocks the power of the command line while giving you the option to write whatever esoteric script you need to solve your own problem. And if you do want to export your data anywhere else it can be as simple as turning your simple, semi-structured text into an API request to whatever service is in vogue at the moment.
{% include setup %} Last Friday, Fred Wilson [wrote a post](http://avc.com/2016/05/feature-friday-photo-search/) lauding Google’s photo search. I’ve had the same experiences. In the past couple of months I’ve made numerous searches without expecting a useful result but in nearly every case I was pleasantly surprised. Just in the past week I wanted to search for a short story I wrote while in middle school that I digitized at some point over the past few years. My first attempt was to search for “paper” which got me too many results to parse through. But for my second attempt I tried “essay” and was able to find a photo of one of the hand-written pages. It was simple to look at the date I uploaded that one page to find the others. A couple of days ago I was out of town but needed my passport information to fill out an online government form. Turns out that I have a photo of my passport on my Google account - I backed it up years ago as I was traveling so I had proof in case anything happened to it.  On one hand I’m clearly impressed by how accurate the searches are but does make me worry about how much information we inadvertently share that can be indexed. It’s hugely convenient now but it’s impossible to predict the future and see how it will be used. I wonder how many photos we’re currently sharing that we assume are indiscernible to these automated systems. The vast majority are safe for now but given the pace of technological progress I’ll be shocked if software isn’t more accurate and faster than humans in 20 years. And photos are just a small piece of the puzzle - every bit of digital content we produce will be data mined until it can’t reveal any more. All this will benefit us in the short term but I wonder the world will look like when everything we produce can and will be analyzed and understood by machines.
{% include setup %} When writing code it’s very easy to accumulate deprecated database tables that end up as zombies - they’re still around and may even be populated and used by a variety of side scripts but if they disappeared and the dependent code was removed nothing would be different. In fact you’d have a smaller code base, a smaller database, and would hopefully improve everyone’s productivity a tiny bit.  Dealing with the tables are are still being populated and read requires a bit of investigative work and knowledge of the product since there’s no simple way of identifying them. But there are a simple ways to identify tables that are no longer updated.  ### 1. The metadata Some databases provide a last updated flag as part of the metadata tables. For example, MySQL contains an update_time field inside the information_schema.tables table for MyISAM tables. Reading the MySQL documentation it also looks as if recent versions will have this set for some InnoDB tables as well.  ### 2. The temporal columns In the case where there’s no metadata for a table you have to resort to a bit of trickery. If your table has any form of a time column then you can write a very simple query - **select min(timestamp), max(timestamp) from table** - to spot the most recent data in a given table. If this date is old you may be able to safely assume that this table is no longer being populated or maintained. Combining this quick trick with data from the informatino_schema.columns table and you can write a very neat query that can run this check across the entire database. For example, you can first run **select table_schema, table_name from information_schema.columns where column_name = 'timestamp'** to identify every table that contains the timestamp column. Then you can automate the creation of a monster query that will generate a checking query for each of the tables and then union them all together. So then you end up with something akin to **select 'table_1' as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_1 union all select 'table_2' as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_2 union all...** The query may take a while depending on the indices but once it does you can quickly sort by the max timestamp to quickly spot the potentially unused tables. A small adjustment you can make to deal with tables that may still be getting populated is to look at the number of rows that exist by day. If you see a huge decline it can be a good indicator that this table may just be getting some noise from an older job and is safe to remove, but only after removing the deprecated job.  ### 3. Snapshot and monitor But what do you do if there’s no metadata and no timestamp column? Ideally you’d have created and updated timestamps in every table. If not you can either add these to be automatically set and see whether anything changes over time or you can just take a manual snapshot today and a few days or weeks later to see whether anything changed. If the table is too large you can compare the number of rows or some summary statistics. The general idea is to compare it against multiple periods of time to see if, and how much, it’s changing.  These are just a few tricks I’ve picked up over the years trying to keep database schemas clean. Most companies do a good job managing the deployment process when generating new tables and writing new code but it’s rare to find companies that tend to their database garden. I believe maintaining a clean database is underrated - it’s valuable to know that everything in your database is used and that you don’t have to worry worry about an obscure script touching an obscure table you’ve never heard of. I’d love to know if people have other tips that can be used to both keep, and get, a database clean.
{% include setup %}         Wanting to avoid a busy lunch rush but hankering for Chipotle I decided to download their app to order ahead. It’s a straightforward app and everything went as expected until I had to enter the expiration date for my credit card. The way the app is set up is that you’re expected to choose the month first followed by the year. Unfortunately it prevents you from picking a month in the past. One can probably guess what problem this leads to: if the expiration date is in the future but the expiration month is before today’s month the app rejects the month change until you change the year. The screenshot illustrates the design.  I tend to be more passionate about usability issues than most - especially ones that are obviously wrong and trivial to fix. I suspect in this case in the desire to make the user experience better by not allowing a user to select a date in the past it actually had the opposite effect and decreased the usability.
{% include setup %} Despite my [aversion](http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog/) to walled gardens and platforms I’ve seen a ton of people make the switch to [Medium](https://medium.com/). Within the past month I’ve seen a variety of bloggers move over to Medium, both big and small: [Mark Suster](https://bothsidesofthetable.com/finding-a-new-medium-aa0f882815d#.s4y1c45ky), [Semil Shah](http://blog.semilshah.com/2016/04/30/medium-rare/), [Andrew Parker](http://thegongshow.tumblr.com/post/143602596745/corporate-governance-dictatorships-vs-democracy), and a former coworker, [Dillon Forrest](https://medium.com/@dillonforrest). I’m still not convinced that Medium is for me but it definitely feels as if it’s at that inflection point with more and more people moving to Medium. And from what I’ve heard it does wonders for reach and promotion - something that I’ve been relying on Google search and Twitter for.  To that end I’m going to try an experiment and start publishing on Medium ([https://medium.com/@dangoldin](https://medium.com/@dangoldin)) as well as on my primary blog. The goal is to experiment with Medium and see how much engagement it can actually drive. To start I’m going to copy some of my posts over to Medium and see how they fare.  So far, one of the nice things about Medium is that it comes with a simple API that allows you to take either Markdown or a subset of HTML and turn into a Medium post via a quick API call. In fact, earlier today I wrote a [small script](https://github.com/dangoldin/medium-tools) that that takes the raw Jekyll markdown and posts it as a draft to Medium. It won’t work on every single post yet but for the ones that are pure markdown it works perfectly (example: the [original](http://dangoldin.com/2016/05/11/identifying-unused-database-tables/) vs on [Medium](https://medium.com/@dangoldin/identifying-unused-database-tables-f1e969039f6c#.1n6p1g1jw)).
{% include setup %} In 2012 I did a [simple analysis of IMDB](http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/) to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great [post by CuriousGnu](https://www.curiousgnu.com/imdb-age-distribution) that did a similar analysis across a larger set of movies but limited to movies since 2000. I reached out and they were kind enough to give me a DigitalOcean instance containing the data already loaded into MySQL. The analysis should be finished up tomorrow but I wanted to write this post up to share the mundane parts of the process. The janitorial part is critically important to an analysis and it’s important to get it right or the results will may be meaningless or even completely wrong. The [NY Times interviewed](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0) a variety of data scientists and came away with the conclusion that 50 to 80 percent of a data scientist’s time is spent cleaning the data. This is no exception and I wanted to provide a sense of the effort and thought that goes into getting data into a state that’s actually useful.              Lucky for me I already had the data loaded and queryable in MySQL. Most of the time the data is scattered all over the place in a variety of different formats that require a slew of scripts to wrangle and manipulate the data into a useful format.  The first task was to get familiar with the data and I started by looking at sample rows from each of the tables. The table names were descriptive but it turned out that some of them were empty. Running a query that calculated the size of each provided a good idea of where the valuable data was - for my analysis the useful data lived in the title, name, cast_info, and person_info tables.  {% highlight sql %} SELECT TABLE_NAME, table_rows, data_length, index_length, round(((data_length + index_length) / 1024 / 1024),2) "Size in MB" FROM information_schema.TABLES WHERE table_schema = "imdb";{% endhighlight %}              The next step was figuring out the way the tables related to one another. Since the field names were obvious this was extremely straightforward. The only nuances came due to an unconventional naming scheme - for example the title table contains the list of movies but the other tables map to it via a movie_id column. Similarly, the name table contains people but it’s referenced via person_id in other tables. They key part here was starting with a movie I know and confirming that the results made sense. In my case I chose my favorite movie, The Rock, and made sure that the results of my query made sense.  {% highlight sql %}select * from title t join cast_info ci on t.id = ci.movie_id join name n on ci.person_id = n.id where t.id = 3569260;{% endhighlight %}  After getting a feel for the data it was time to actually think about the data necessary for the analysis. To see what was possible I examined the person_info table which contains a variety of information about each person - anywhere from birth and death dates, to spouse, to various names, to height. In my case looking at the birth and height gave me some ideas but I needed to extract these to make them useful. I ended up creating a table for each one and writing a series of queries to populate each one. This required looking at the format of the data in each of the rows and leveraging various combinations of the locate, substring, and cast commands to transform the text fields into something numeric. The birth date was straightforward since it came in two styles - one was just a year and the other was the full birth day with day and month.  {% highlight sql %}insert into person_birth     SELECT person_id, cast(info as UNSIGNED)     FROM person_info     WHERE info_type_id = 21     AND length(info) = 4;  -- Birthdate is full date so just take the year insert into person_birth     SELECT person_id, cast(substring(info, locate(' ', info, 4) + 1, 4) as unsigned)     FROM person_info     WHERE info_type_id = 21     AND length(info) > 4;{% endhighlight %}  Height was a bit more difficult since it came in a variety of formats. Some were in centimeters, while others were in feet, while others were in feet and inches, with a small fraction having partial inches. Each of these required a complicated series of MySQL commands to convert to inches.  {% highlight sql %}insert into person_height     SELECT person_id, cast(replace(info, ' cm','') as unsigned) * 0.393701     FROM person_info     WHERE info_type_id = 22     AND info like '%cm';  -- No inches insert into person_height     SELECT person_id, substring(info, 1, locate('\'', info) - 1) * 12     FROM person_info     WHERE info_type_id = 22     AND info not like '%cm'     AND info not like '%/%'     AND info not like '%"%';  -- No fractional inches (would also work for no inches but playing it safe) insert into person_height     SELECT person_id, substring(info, 1, locate('\'', info) - 1) * 12 + substring(info, locate('\'', info) + 1, locate('"', info) - locate('\'', info) - 1)     FROM person_info     WHERE info_type_id = 22     AND info not like '%cm'     AND info not like '%/%'     AND info like '%"%';  -- Fractional inches insert into person_height     select person_id, cast(base_height as decimal) + cast(numerator as decimal)/cast(denominator as decimal)     from (     SELECT person_id, info, substring(info, 1, locate('\'', info) - 1) * 12 + substring(info, locate('\'', info) + 1, locate('"', info) - locate('\'', info) - 1) as base_height,         substring(substring(info, locate(' ', info, 5) + 1, 3), 1, locate('/', substring(info, locate(' ', info, 5) + 1, 3))-1) as numerator,         substring(substring(info, locate(' ', info, 5) + 1, 3), locate('/', substring(info, locate(' ', info, 5) + 1, 3)) +1 ) as denominator         FROM person_info         WHERE info_type_id = 22         AND info not like '%cm'         AND info like '%/%'         AND info like '%"%'     ) temp;{% endhighlight %}  Finally it was time to dive into the data. The first query I decided to write was to look at the average age of actors and actresses by year. Writing the query and doing a quick explain caused me to add a few indices to improve the performance but even then it still took over 20 minutes to execute. Having used Vertica and Redshift in the past I knew a columnar database would help but I wanted to keep it free. This led me to [MonetDB](https://www.monetdb.org/).  Somewhat remarkably, installing and setting up MonetDB was a breeze but I had a two hiccups migrating the data. One was creating the equivalent tables in MonetDB which had a slightly different syntax from MySQL and required a bit of trial and error to work through. The other was the actual export of data from MySQL in a way that was also easy to load into MonetDB. I ended up settling on a CSV export that also took into account the various ways to delimit, escape, and enclose the different fields. After getting the migration to work on one table it was just a series of copy and pastes to get the other tables over.  {% highlight sql %}-- MySQL export select * from title into outfile '/tmp/title.csv' fields terminated by ',' enclosed by '"' escaped by "\\" lines terminated by '\n';  -- MonetDB import COPY INTO title from '/tmp/title.csv' USING DELIMITERS ',','\n','"' NULL AS '\\N'; {% endhighlight %}  I had no experience with MonetDB and didn’t know what to expect with this entire series of steps being a waste of time. I expected some improvement and it turns out the query that took over 20 minutes to run in MySQL was able to run in just over 30 seconds in MonetDB. I was off to the races. I spent the next bit of time QAing the data and dealing with outliers and edge cases. Some were due to mistakes I made - for example not filtering cast members to only include actors and actresses which manifested itself in an actor that lived to be over 2000 years old. This turned out to be a movie about [Socrates](http://www.imdb.com/title/tt1560702/) with one of the writers being Plato. Some simply uncovered weird data - there's a movie, [100 Years](http://www.imdb.com/title/tt5174640/), which is scheduled to be released in 2115 and led to some old actors and actresses. While others were clearly data mistakes - actors who were born after they died, for example [Walter Beck](http://www.imdb.com/name/nm2917761/) who was born in 1988 but passed away in 1964.          Dealing with these was an iterative process. I ended up settling on removing all non actors and actresses from the queries as well as limiting my dataset to movies produced between 1920 and 2015 while also eliminating all combinations where a movie was produced before a birth. These edge cases are infrequent enough that they most likely wouldn’t have had any impact on the results but going through this process gives us confidence in what we’re doing. The next step is actually going through the analysis which I hope to finish up tomorrow.  If you’re interested in the code, it’s up on [GitHub](https://github.com/dangoldin/imdb); and if you’re interested in the data contact me and I can share a snapshot of the DigitalOcean instance that contains the data in both MySQL and MonetDB.
{% include setup %} After getting the [IMDB data loaded](http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/) it was time to dive in and start looking at the data. In 2012, I did an [analysis](http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/) to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the [help of CuriousGnu](https://www.curiousgnu.com/imdb-age-distribution), I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on [GitHub](https://github.com/dangoldin/imdb) and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.                              A replication of  CuriousGnu's chart  as a sanity check to make sure the data was loaded correctly. As one can guess, actresses skew younger compared to actors with an average of 34.6 compared with 41 for actors.                                       The previous chart examined the distribution across the entire date range but we can see how this shift occurred over time. Before the 1940's actors and actresses were much closer in age. Another interesting point is that both actors and actresses have been getting older on average. One theory is that this is a function of the movie industry being new at the beginning of the 20th century with very few actors and actresses at the start that have aged along with the industry. Another reason may be lack of accurate data prior to the 1940s in the IMDB dataset which skews the results toward more recently-born actors and actresses.                                       Similar to the above but focused on actors and actresses that have appeared in at least 100 movies. The goal here was eliminate some of the noise and focus on the high volume actors and actress. This tells a similar story to the previous chart.                                       Combining both the distribution and trend over time we can look at the distribution changes over time. This also highlights the bias in the early years - in the 1920s it looks as if no one was older than 30 according to the IMDB data. After some digging around it's due to the lack of birth dates for many of the early 20th century actors and actresses. For example, for movies produced in 1920 we have close to 19,770 actor/actress movie combinations but only 1,060 (~5%) with a birth date. For 2010 the respective numbers are 269,645 and 52,262 (~19%). This causes our distribution to look heavily truncated but ends up correcting itself once we get into the 30s and 40s. In this case the average ages are inaccurate until the 1940s but I suspect the relationship between the genders still holds.                                                        This is just a timelapse of the data above that makes it much easier to see the shift of the average actor getting older at a faster pace than the average actress.                                       In addition to birth date the data also contained the height so I decided to have some fun and see how that looked. This is just a plot of actor and actress height by year of production. My takeaway is that actor heights stayed roughly flat while actress heights have been increasing. Note that since I only had a single height for each person this wouldn't be able to accurately represent children growing up but I imagine those are a small fraction and wouldn't influence the results.                                       This is an interesting one. Instead of looking at the heights by movie production year this examines heights by birth date of the actor and actresses. In this case we see that actors have stayed roughly the same height while actresses have increased in height over time. There's also a huge looking drop at the end - going from a bit over 70 inches to less than 65 for actors and from 65 inches to less than 63 for actresses. This drop off is in the late 90s which also indicates these are teenagers just growing up.                                       For the last one I wanted to get a sense of whether actors are more likely to be in more movies than actresses. The chart here is a bit tough to read but it looks at the distribution of actors and actresses by the number of movies made. in this case the scale was massive since there were tons of people who've only been in a few movies so I had to normalize by taking the log. The effect is subtle but the fact that the tail for actors goes wider than the tail for actresses indicates that an average actor is more likely to appear in multiple movies than the average actress.
{% include setup %} Self driving cars are inevitable and yet I’m surprised by how aggressive Uber is in contributing to the space. Uber is winning right now due to massive network effects. For most drivers and passengers Uber is the primary option and they only switch when Uber is either in surge if you’re a passenger or if you’re a driver when no passengers are available. Self driving cars eliminate half of the market. They won’t need to balance multiple apps on their phones and won’t need to go back and forth trying to find a passenger. It will all happen behind the scenes and do a much better job than any human would. They’d be as likely to work with Uber as any of their competitors. In fact, the entire protocol may evolve to be open with owners setting up their cars to start picking up and dropping off passengers when they’re not in use. The equivalent of how you can sell electricity back into the grid without having to do a ton of extra work. Imagine being able to own a car and just let it roam so it starts earning.  It’s unclear why Uber is driving this change - self driving pose a risk and diminish their competitive advantage. Maybe the outcome will eliminate individual car ownership and Uber wants to own a fleet of these cars. In that case pushing for this result makes sense but carries a world of risks - why wouldn’t car manufacturers both produce the car and have it part of a fleet? The other option is that they accept it’s not ideal but feel as if they have no choice since if others achieve it first they’ll be in an even worse position. Or maybe Uber does think they’ll own the market by the time self driving cars are a reality and at that point no one else will even bother to compete.
{% include setup %} I started working on a project to investigate my blog posts and see how my writing has evolved over time. I’m still working on it and will definitely write up the results but the entire process got me thinking about my blog and some of the highlights. I started blogging to improve my writing, improve my thinking, and grow my personal brand. Despite being a large time commitment I enjoy doing it and there have been a variety of small episodes that have made it even better:  - In 2013 I wrote a [short post](http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/) with an excerpt from a book I was reading about the lack of a dial tone in cell phones. This took off on Hacker News and ended up being covered in [Gizmodo](http://gizmodo.com/5994589/why-your-cell-phone-doesnt-have-a-dial-tone), [Mental Floss](http://mentalfloss.com/article/50185/why-don%E2%80%99t-cell-phones-have-dial-tones), and even made an appearance in the NY Times tech ticker. - I built a small community. I have a small number of repeat visitors who will comment on the occasional post and I actually ended up meeting up with a frequent contributor, [Ted](https://twitter.com/tedder42), when I visited Portland for the first time. - When Turo was called RelayRides I did an [analysis](http://dangoldin.com/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/) to figure out the optimal car to get that will generate the biggest return. This led to a few people reaching out and me doing a little bit of consulting work to analyze their market. - I’m a big fan of Citibike and came up with a [small web app](http://dangoldin.com/citibike-station-directions/station-to-station.html) to that translates every New York City trip into a walk to a Citibike station, a station to station bike ride, and then a walk to the final destination. After posting this a few people reached out to ask whether a smartphone app was available as well as ideas to make it even better. I unfortunately haven’t had the chance to work on it but it’s great seeing people finding value in something I’ve done. - Cities opening up their data and I had some fun visualizing the [routes of Jersey City’s garbage trucks](http://dangoldin.com/2015/12/12/jersey-city-garbage-truck-routes/). This led to me connecting to our councliwoman who then introduced me to the head of Jersey City’s tech innovation team. - An interesting one was when a journalist from [FiveThirtyEight](http://fivethirtyeight.com/) reached out to ask about an old GItHub project I was working on. This ended up not leading anywhere but did provide a glimpse into modern journalism and the desire to highlight and surface content from the tail. - You know you’ve made it when you have “SEO experts” reaching out and either offering their site optimization services or a payment to post an article with a link to another site. I’ve received dozens of offers so far but haven’t accepted any yet!  These are just the highlights and at this point I’m happy to receive any inquiry. None of these have been massive but they’re all small highs that are a reminder that what I write is being read. Their lack wouldn’t stop me from blogging but it’s always nice to receive a surprise comment or email.
{% include setup %} Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a [quick script](http://www.r-bloggers.com/building-wordclouds-in-r/) I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.csv and am generating two word clouds - one for keywords and the other for tags of my blog posts.  {% highlight R %} # Install and the libraries install.packages("tm") install.packages("SnowballC") install.packages("wordcloud")  library(tm) library(SnowballC) library(wordcloud)  # Read the file df = read.csv("/tmp/out.csv", header=TRUE)  # Now generate the two word clouds. # Most of the work here is removing the unncessary and common words # as well as optionally stemming each of the words. In my case since # I'm plotting the keywords and tags I ignore this step.  corpus     Word cloud of the tags I use.       Word cloud of the keywords I use.
{% include setup %} I’ve been getting a stream of offers to help “optimize” my site and decided to follow through with one and see where it went. The general pitch is to call out existing errors and problems and offer a service to help fix the variety of errors and improve my search ranking. Here’s the text of the most recent email:     Dear business owner of dangoldin.com,   How is it possible that your website is having so many errors? Yes, most of the people share their anger and frustration once they get my email.   Now, I will show you the number of broken links, pages that returned 4XX status code upon request, images with no ALT text, pages with no meta description tag, not having an unique meta description, having too long title, etc., found in your dangoldin.com.   I have a large professional team who can fix all the above issues immediately at an affordable price. I guarantee you will see a drastic change in your Google search ranking once these are fixed.   If this is something you are interested in, then allow me to send you a no obligation audit report.   Best Regards,   XXXXXX     Clearly this is not personalized as every mention of dangoldin.com can be replaced with another domain and have the same effect. The language doesn’t feel natural and is awkward but the author does include a series of technical words and phrases to showcase his knowledge. I wonder if they have A/B tested the hell out of different copies and ended up coming up with this. I recall reading that Nigerian scammers purposely use non-standard English as a way to identify even better marks. If they wrote in perfect prose they’d end up luring many more people into the top of their funnel that would end up backing out later. Much better to get a smaller set of people hooked that have a higher conversion rate.  The day after my reply I received a PDF titled “ Website Analysis for dangoldin.com .” It’s surprisingly well-fleshed out and contains a series of best practices and stats that my site is ranked on. It has the obvious ones such as number of pages indexed by Google as well as some esoteric ones, such as whether it’s listed on “DMOZ.” The analysis ended with a search ranking plan as well as the pricing page with 3 potential plans ranging from $300 to $900 a month. My gut is that this was a dual effort between code and humans with the bulk automatically generated and a human polishing it up. I’m confident that the human component was outsourced given the language and the fact that the firm has presence in India. Generating this was probably cheap but not insignificant and does make me wonder what their conversion rate is. The $300 price point seems high but is in line with the website optimization services out there so maybe these guys have figured out their customer acquisition model.
{% include setup %} I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple [Python script](https://github.com/dangoldin/blog-analytics/blob/master/analyze.py) that went through each post and dumped it into a CSV file with a series of columns that would be easy to [analyze via R](https://github.com/dangoldin/blog-analytics/blob/master/analyze.R). The columns ranged from numeric stats - such as how many words, tags, images, and links - to the actual text of the post itself. The goal was to put in a structured enough shape that the rest of the analysis could be handled in R. I started by collecting some summary statistics and looking at them over time but got carried away and ended up digging deeper into my evolution as a blogger.  Some high level stats to start it off:  - 412 total posts with 54 of them before 2013 - 725 total links - 537 total tags - 1,379 total keywords - 9,705 total words in the meta descriptions - 145,499 total words of content                              As mentioned I started actively blogging in 2013 so there's no surprise here.                                       Given that I've written the same number of posts in 2013, 2014, and 2015 it looks as if my posts have gotten shorter and shorter.                                       Similar to the point above - I'm sharing fewer and fewer links.                                       Yet I'm still tagging the posts at roughly the same rate. This makes sense since I'll do anywhere from 1 to 3 tags per post.                                       By month there's a bit more noise due to vacations but am keeping pace with 2 a week.                                       Nothing obvious here.                                       Just for fun but this is the total number of words by week. I also did this by day but it was even noisier.                                       Clearly I write more during the weekend. Note that I had to prepend a number to the day of week to get the sort working.                                       Similarly, the number of words is also higher on weekends.                                       Another way to look at it is to see the distribution by year. In 2013 I was actually pretty on-point with my Tuesday/Friday writing schedule but since then have regressed to mostly writing on the weekends.                                       The same information as above but switching the X and Y axes. I find this one not as easy to interpret as the previous one.                                       This examines the various companies I mentioned over time. Google's dominant and it looks as if I haven't written about microsoft since 2014. You can also see the rise of Uber and Snapchat.                                       Looks as if 2015 was the year of languages with Python and JavaScript dominating the others.                                       Word cloud of the various tags I used on my posts. Clearly I like engineering and startups.                                       Tag wordcloud for 2013. All about startups and design here.                                       Tag wordcloud for 2014. This gets deeper into technology with strong representation by AWS, devops, coding, as well as a variety of programming languages.                                       Tag wordcloud for 2015. Welcome to engineering management. In 2015 I developed into a manager and start writing about the various lessons I've learned on the journey.                                       Tag wordcloud for 2016. Nothing significant yet and looks like a pretty healthy mix of the prior years. We'll see how this looks after the year is over.
{% include setup %} I have been subscribed to [Stratechery](https://stratechery.com/) for almost a year now but have recently started listening to the [Exponent podcasts](http://exponent.fm/). One of them, titled [Pickaxe Retailers](http://exponent.fm/episode-071-pickaxe-retailers-2/), makes the case that WeWork has an appropriate valuation due to their ability to leverage their strong brand and become the utility layer for real estate as well as provide a slew of products to their tenants. Similar to the way AWS has eliminated the need to run your own data center and Stripe has eliminated the need to acquire merchant accounts and negotiate with vendors, WeWork may do the same for physical space - both commercial and [residential](http://www.fastcompany.com/3055325/from-wework-to-welive-company-moves-members-into-its-first-residential-building).  While the explanation is reasonable it’s tough for me to buy into it. The decision to use any product boils down to how easy is it to switch and what’s the cost/revenue potential. In the case of AWS it’s incredibly costly to switch. You have to incur the cost of updating your code and deployment to make sure it will run on a new platform, retraining your team, and if you plan on switching to your own datacenter then hiring for roles you’ve never had to deal with. Added to this you have Amazon constantly cutting costs while innovating on new products. The value in switching only comes at massive scale - even Dropbox is getting beaten up over their move away from Amazon instead of focusing on building a more compelling product.  Stripe is in a similar situation. Despite providing a seemingly simple service it’s difficult and costly to replace. Stripe contains customer data and has a slew of products, for example subscriptions, that make it tough to switch. Imagine having to ask every customer to re-enter their credit card information. At the same time, Stripe gets cheaper and cheaper as your volume increases which makes it less and less compelling to replace.  I just don’t see these sort of arguments holding true for WeWork. AWS and Stripe both run services that start of cheap and get even cheaper as you scale. They’re both unbelievably sticky and have a growing cost of switching. WeWork has neither of these. The actual office space may be great but over time it gets easier to make the decision to rent your space directly rather than pay WeWork’s margins. WeWork does provide additional services that make their space great when you’re small. Unfortunately, these same services can easily be outsourced as you grow. For unlimited coffee you go with [Joyride](http://www.joyridecoffeedistributors.com/service/page/cold-brew-iced-coffee-kegerators-coffee-kegs/). For office cleaning and maintenance you go with [Managed by Q](https://managedbyq.com/). For food you can go with one of the hundreds of delivery startups. Every service WeWork provides can be had via a separate company..  Picture a small company starting out on AWS, using Stripe, and renting an office at WeWork. As they grow it’s easy to imagine them still using AWS, still using Stripe, but no longer at WeWork. Netflix is the perfect example. They’re a public company with a current market cap of just under $41 billion. Yet they’re still on AWS. And Amazon is a competitor! I can’t imagine any public company using WeWork as their primary office space solution.
{% include setup %} A few weeks ago I had to run some errands at the mall and ended up having some free time. I was also a few blog posts behind so decided to see how much I could actually do via phone. Surprisingly, I got a fair amount done. The posts still required a fair amount of editing when I was back on my computer but for getting the bulk of the content and structure down on my phone was nearly as good as via a real keyboard. What it lacked in speed it made up for by not having real multitasking which made it more difficult to get distracted. It wouldn’t work for posts that require search or significant research but for quick blurbs or jotting down thoughts it works remarkably well and I suspect it will only improve with time. Years ago I viewed phones and tablets as being purely designed for consumption rather than creation so this has been a pleasant surprise and I’m coming around to the idea that one can be productive without an actual computer. Next is to try attempting to write a blog post via voice dictation.
{% include setup %} The messaging space is fascinating. There are probably hundreds of apps available with pretty massive fragmentation. Onavo collected the following data to indicate the reach of the various messaging apps by country and while WhatsApp (owned by Facebook) is clearly dominant there are some countries that WhatsApp is a fringe player, especially among Asian countries.       via TechCrunch    It’s shocking how dominant the local companies are in Asia. WeChat is the behemoth in China. In Japan it’s Line. And in Korea it’s KakaoTalk. I don’t know whether it’s as simple as nationalism or that the local companies just have a much better understanding of the market and were able to build better products.  I just view messaging apps as utilities.There’s no need to restrict myself to a single app and I use them reactively. If someone messages out via iMessage I’ll use that. If someone uses WhatsApp I’ll use that. And so on. If a friend asks me to use a particular app I have no problem downloading it and giving it a shot. They have limited network effects and there’s no reason to restrict yourself to one.  I suspect most people feel the same way. They probably have an app that’s the goto with their most frequently messaged group but if they’re part of another group that has their own principal app there’s nothing stopping them from using it.  The most dominant apps will be the ones that are able to leverage them to become utilities. Tencent has built a massive business on top of WeChat which acts as the digital hub in China. WeChat is not just for messaging but is essentially the operating system for mobile in China. It can be used to interact with a litany of services in china - including payment at physical stores, booking ridesharing services, and serving as an authority on identity. Nothing like this exists in the US or Europe and it’ll be interesting to see what comes out.
{% include setup %} Trying to launch a new browser seems like a fool’s errand and yet if there’s anyone that can do it it’s [Brendan Eich](https://en.wikipedia.org/wiki/Brendan_Eich), who in addition to creating JavaScript also ran Mozilla. Given his pedigree I decided to give his new browser, [Brave](https://brave.com/), a shot. It’s definitely a bit on the rough side compared to the mainstream browsers but it’s surprisingly fast. The speed improvement comes from a built in adblocker rather than having it implemented via slower browser extensions. At the same time Brave wants to pay publishers for their content by partnering with higher quality advertisers in order to serve benevolent ads that should also be priced at a premium.  The difficulty with this approach is that tracking users is what makes the advertising so valuable. Being able to track users allows advertisers to see what users care about, their purchase intent, as well as a whole slew of demographic information based on their consumption behavior. Eliminating this will cause advertisers to be shooting in the dark. There’s a reason Google and Facebook are eating up nearly 80% of every advertising dollar - they’re leveraging their data to provide extremely targeted and effective advertising that will be tough to do without the ability to track users.  I can see the case that Brave can centralize the tracking and allow users to opt into sharing this data with various partners. The challenge is getting advertisers on board with this as they would have to trust Brave for their reporting and getting users to opt in to this. I know very few people who’ve used adblock and then decided to switch back to a full ad experience. Brave has a tough road ahead.
{% include setup %} The Wall Street Journal had a [great piece](http://www.wsj.com/amp/articles/why-fruits-and-veggies-are-so-crazy-cheap-in-chinatown-1466762400) on why produce is so cheap in Chinatown. The conclusion:       Her discovery: Chinatown’s 80-plus produce markets are cheap because they are connected to a web of small farms and wholesalers that operate independently of the network supplying most mainstream supermarkets.     Most of the city’s fruits and vegetables come from wholesalers at the Hunts Point Produce Market, the South Bronx distribution hub boasting all the color and accessibility of La Guardia Airport. Chinatown’s green grocers, in contrast, buy their stock from a handful of small wholesalers operating from tiny warehouses right in the neighborhood.     Because the wholesalers are in Chinatown, they can deliver fresh produce several times a day, eliminating the need for retailers to maintain storage space or refrigeration, said Ms. Imbruce.     I love this. It runs counter to the common belief that cheaper prices can also be achieved through massive scale. Yet in what I suspect is one of the hardest industries, food distribution in NYC, small scale seems to be doing the better job. Produce has an extremely short shelf life and combined with the cost of real estate in NYC it must require some incredible management to be able to sell it for the half the price of produce found at the supermarket. And everyone involved ends up winning - consumers get cheap prices and a great selection, the stands are able to turn around a ton of inventory due to the low prices, and the farms benefit from the variety of crops they’re able to grow.  This is a perfect example of being able to build a successful business by focusing on activities that complement each other (à la [Michael Porter](https://hbr.org/1996/11/what-is-strategy)): they have their own local wholesalers that get constant replenishment that can then be priced incredibly cheaply which encourages high turnover and feeds back into the need for quick replenishment. This also allows them to focus on produce that doesn’t need to be kept on the shelf as long and is expected to be sold and eaten within a short amount of time. They embraced the idea of “making it up in volume” by setting up every activity to drive that goal.
{% include setup %} The more I use Snapchat the more obvious the potential. The way the product has evolved reminds me of Facebook’s history. Facebook started simply as a profile page for Ivy League college students but due to strong execution and brilliant product decisions has grown into the current behemoth. Snapchat is on a similar path - the initial version was a simple ephemeral photo sharing app but the recent updates seem frequent and massively impactful.            Movie tickets within Snapchat     Earlier today I was messing around and spotted a movie trailer ad for Swiss Army Man that was followed by an option to swipe up to buy the movie tickets that felt native to Snapchat. I didn’t have to click to go to another app and it felt natural to just swipe to get to the next step. And this was a simple case of buying movie tickets. I can imagine this flow expanding to other scenarios that follow a powerful ad with an immediate transaction. This echoes WeChat - the powerhouse app in China that’s an unholy mix of a [social network, a payments platform, and an app ecosystem](http://a16z.com/2015/08/06/wechat-china-mobile-first/). There’s no equivalent of WeChat outside of China and I suspect most think the replacement will look like a WeChat clone. Snapchat feels completely different yet has the potential to be more. WeChat’s foundation is a third party app ecosystem that’s built on top of text while Snapchat is almost entirely visual and asserts a high bar for third party experiences.  This is huge. Interacting with Snapchat is a joy, whether it’s taking photos, playing with the filters, checking our your friends’ stories, or watching the Discover videos and this is due to the masterful job they did with the interactions. They’re not immediately obvious but once discovered are intuitive and consistent across the variety of experiences. Want to go next? Just tap. Want to dig deeper? Swipe up. Want to go back? Swipe down. This is incredibly powerful. By learning these shortcuts Snapchat is able to offer a variety of adventures that users can easily engage with without taking up any additional screen space. This allows every experience Snapchat offers to take up the full screen which keeps us in the moment and makes it easy for us to keep going.  Snapchat is already taking the baby steps of becoming a platform by enabling external parties to build on top of Snapchat. The obvious case are the content producers Snapchat is partnering with but a more telling example is the way they’re approaching geofilters. [Geofilters](https://snapchat.com/geofilters) are created offline, are then uploaded to the Snapchat site, and after approval become accessible in the app. This is a foreshadowing of the Snapchat formula - build out a compelling in-app experience and then follow it up with tools for outsiders to craft their own.  The movie trailer ad can be extended to highlight products - a compelling video of a product that can then be followed up with an option to buy. This can extend into multi-touch - imagine being able to tap on different sections of the screen that drive different experiences. If I’m watching the Olympic trials I can tap on the different players to get some more information about each one or if I’m watching some NBA highlights I can tap on LeBron’s jersey or sneakers to get taken to the Snapchat-integrated Nike Store. And this is just scratching the surface - by focusing on new highly engaging user experiences and setting up the tools to create these compelling stories Snapchat can transition into an incredibly powerful platform.  Snapchat should have no problems monetizing. The advertising industry can be broken down into two major types - brand advertising and direct response. Brand advertising is the typical TV ad that’s focused on building awareness and selling a story and lifestyle. Direct response, on the other hand, is about getting the customer to “convert” and requires every dollar spent to back out into a measurable return. Think of Google Adwords - you search Google for a book, click the sponsored Amazon link, and then buy it - Amazon will then know how much you paid for the book as well as how much the click cost. Using this data they can then optimize their campaigns to maximize profit. Snapchat may be able to sit at that intersection. It’s the perfect platform for high quality brand videos that take up the full screen but can also drop down into transactions - think of my movie trailer/ticket experience earlier today. This will help them get closer to the holy grail of advertising by attributing purchases to brand advertising.  Snapchat is tiny compared to Facebook but major shifts in the tech world have never been direct. New platforms start at the fringes but keep growing until they supplant the incumbents. Google superseded Microsoft by becoming the entry point to the web. Facebook is supplanting Google by bypassing the open web and providing app experiences on every platform. How would Snapchat unseat Facebook? I don’t know but I’m sure Facebook’s watching.
{% include setup %}        Potential energy : the energy of a body or a system with respect to the position of the body or the arrangement of the particles of the system.     Dictionary.com              Kinetic energy : the energy of a body or a system with respect to the motion of the body or of the particles in the system.     Dictionary.com       I’m constantly striving to discover new ways of thinking about code and my latest is thinking about it through what many of us learned in high school physics - potential and kinetic energy. The definitions are above but a simple way to think about it that potential energy is what your system is capable of while kinetic is exercising that option. One can look at code the same way. Code that has a high potential energy can be turned into a vast amount of kinetic energy that can deliver new products and features at an amazing pace. This is code that is well architected and tested and is designed in such a way that it can be easily modified to handle whatever it comes its way. Code with low potential energy, on the other hand, is poorly designed with small changes leading to unintended side effects such that most of the time is spent fixing the code up. The comparison here between a rocket and an old, rickety car is appropriate. The rocket expends the bulk of its energy in minutes and travels hundreds of miles. The car breaks down every couple of miles and requires a skilled mechanic just to keep it going for another few miles.  But in physics we have the law of the [Conservation of Energy](https://en.wikipedia.org/wiki/Conservation_of_energy) stating that energy can’t be created nor destroyed. This also applies to code! Taking code with a high potential energy and quickly modifying to solve a need may reduce its potential energy. In this case it’s up to us as developers to exert effort to bring it back up its high potential energy state.  This metaphor is an obvious exaggeration but it does strike at what makes for good code and something we should all strive to write. It’s not about being brilliant or elegant or simple but about being flexible enough to support whatever the world throws at it and it’s up to us to keep it at that level. To keep pushing the physics - every new feature adds [entropy](https://en.wikipedia.org/wiki/Entropy_(order_and_disorder)) to our codebase and unless we actively clean it up it only gets worse.
{% include setup %} I haven’t seen much written about how consumer protection relates to a product’s user experience but it’s a topic that’s worth exploring. I was reminded of this when my mortgage loan was sold to a new servicer. I came home to find a letter in the mail notifying me that my loan has been sold and that going forward I’d have to use a different payment portal and system. It was simple enough to register but the payment process became less efficient and there was no support for a Mint integration.  This is clearly a first world problem and there are a lot of benefits that come with being able to buy and sell loans. It’s the foundation of our financial system and allows companies to specialize across the entire loan business - some are designed for loan origination while others focus on servicing. This also encourages companies to improve their loan valuation models since if they’re able to identify an arbitrage opportunity they can trade on it and profit.  At the same time it’s frustrating that as a consumer I have no say in what happens and it’s a commitment made on my behalf for multiple decades. I don’t know what the right answer is here. User experience is highly subjective and what works for me may not work for someone else and products should hopefully improve over time yet I think there is something here. As an engineer the simple answer would be to force every consumer facing company to expose all functionality and data via an open API which would allow any experience to be crafted around it but I can’t imagine that actually happening. And maybe none of this will actually matter since AI will get to the point where we won’t need to interact with any of these services directly.
{% include setup %}           I’m a huge Twitter fan so it’s especially frustrating when I encounter issues. The latest one was discovering a "This content is not available in your country" message when trying to catch up on some Euro Cup highlights in a moment. I understand that in today’s digital rights world there’s always a chance for some content to be unavailable but there’s no reason it should have been included in Twitter’s flagship product that’s supposed to attract and engage new users. The fact that it’s manually curated makes it even worse - how could this have slipped through? One explanation is that the curator was not based in the US and had access to the video. The other is that the video was available initially but was pulled later on. In both cases Twitter should have had the appropriate safeguards to identify this was happening and amend the moment. An even better approach would have been to have different versions of the moment depending on the user's location. The current implementation just feels sloppy and I can’t stand to see it in a product I love using.
{% include setup %} When I started building sites one of the accepted principles was to give customers what they want as soon as you can. This manifested itself by taking users to the logged in view whenever they navigated to the site’s homepage. This makes sense - if you know a user’s logged in why waste their time by showing them a homepage that’s designed to sell the product?  Yet recently I encountered two sites, [Greenhouse](https://www.greenhouse.io/) and [Tallie](https://tallie.com/), that will default to the homepage and only load the logged in view when I click the sign in link. One argument is that they both have separate domains for the logged in experience - app.greenhouse.io rather than www.greenhouse.io and usetallie.com rather than tallie.com - but there’s nothing stopping them from redirecting to those as soon as they recognize that a user is logged in.  One explanation is that they’re using different domains for the landing page versus the app but it’s still odd. Greenhouse can set cookies at the wildcard domain (*.greenhouse.io) and Tallie can make the necessary redirect or client side check to see whether a user is logged in. In fact if you actually go to usetallie.com first it will redirect you to tallie.com which, after clicking on “Client Login”, will take you back to usetalie.com.  Another explanation is that they want to save on hosting costs and serve a purely static webpage at first. This way they don’t need any dynamic content and only need to have the dynamic logic for when a user wants to login. This seems like a reach though - these are both enterprise apps and can’t possibly have the traffic load to warrant this degradation of the user experience. Even then the cost of doing a simple login check should be enough for any modern web application to handle.  The last explanation I can think of is that there’s something on the homepage that they want every user to experience. And the only thing that would make sense is tracking and advertising. One potential reason is that the content is so sensitive that they either legally can’t or just don’t want to drop third party trackers inside the app yet still want the ability to target and track users who’ve landed on the home page. A preliminary look using Ghostery bears this out - the homepage for Tallie drops 20 trackers while the in app page drops 6, most of which are for analytics. For Greenhouse it’s not as direct with the homepage dropping 17 while the in app page drops 13, most of which are advertising related. If this is the case I’m disappointed, but not surprised, that user experience was sacrificed to drop some third party JavaScript trackers.                              Ghostery trackers: Tallie home + in app, Greenhouse home + in app               I’m searching for other explanations but can’t think of anything that else that would encourage this “anti-pattern” to make a comeback. If anyone has any ideas I’d love to hear them.
{% include setup %} A fun engineering puzzle I heard this week was to write an algorithm that finds the shortest path between two words of the same length where you’re only allowed to change a single letter each step and every word needs to be valid. This morning I decided to have some fun with it and wanted to jot down my thought process going through the exercise in the hope that it provides a bit of perspective on how I approach code.  The first step was to just do an example in my head to visualize the problem. I started with two short words, dog and cat, and went through the manual transition. The optimal solution is where each letter changed is the final letter - in the case of dog to cat it was simply dog -> dot -> cot -> cat. Now that I had a baseline (and a test), I decided to dive into the actual code.  The immediate realization was that since this was asking for the shortest path I’d need to do a breadth first search, something I haven’t had to touch since some early job interviews. The other realization was that the graph would need to be constructed on the fly. With these two in mind I dove right in.  I broke the problem down into three parts - one was loading the dictionary, two was writing a function that would get the “adjacent” words, and three was doing the search itself. The first function was straightforward since I just loaded in the built in OS X dictionary:  {% highlight python %} def load_dictionary(path = '/usr/share/dict/words'):   dictionary = set()   with open('/usr/share/dict/words', 'r') as f:     for line in f:       dictionary.add(line.strip().lower())   return dictionary {% endhighlight python %}  While thinking about the adjacent word function I thought back to [Peter Norvig’s spell checker](http://norvig.com/spell-correct.html) and remembered how simple yet powerful it was (if you haven’t seen it yet you should take a look - one of the most elegant code examples I’ve seen). All his code needed was a tiny tweak to filter the list of generated words to those in the dictionary.  {% highlight python %} def adjacent_words(word, alphabet):   splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]   replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]   return [r for r in replaces if r in dictionary] {% endhighlight python %}  Now it was time to do the actual search which took me a bit of time. I knew the theory but it took me a bit of time to translate it into code. And even then I wasn’t happy with how it looked so ended up finding a pretty simple [Python implementation](http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/).  {% highlight python %} def bfs_paths(source, target, dictionary, alphabet):   queue = deque(((source, [source]),))   while queue:     v, path = queue.popleft()     for n in [w for w in adjacent_words(v, alphabet) if w not in set(path)]:       if n == target:         yield path + [n]       else:         queue.append((n, path + [n])) {% endhighlight python %}  The last part was cleaning up the code and improving its efficiency. The key parts here were using string.lowercase as the universe of letters, replacing a standard list with a collections.dequeue to significantly speed up the “pop” operation, and making the dictionary and alphabet variables locally scoped. As a final test I ran through the dog to cat example and got two additional transformations: dog->cog->cag->cat and dog->cog->cot->cat. The complete code is below but note that I left it open-ended so it will print every path it finds rather than just the shortest one.  {% highlight python %} #!/usr/bin/env python  import string from collections import deque  def load_dictionary(path = '/usr/share/dict/words'):   dictionary = set()   with open('/usr/share/dict/words', 'r') as f:     for line in f:       dictionary.add(line.strip().lower())   return dictionary  # Peter Norvig's spellcheck code is amazing: # http://norvig.com/spell-correct.html # Just use the replace part of it def adjacent_words(word, alphabet):   splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]   replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]   return [r for r in replaces if r in dictionary]  # Had to remember how to get this working again # Took a bunch from http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/ def bfs_paths(source, target, dictionary, alphabet):   queue = deque(((source, [source]),))   while queue:     v, path = queue.popleft()     for n in [w for w in adjacent_words(v, alphabet) if w not in set(path)]:       if n == target:         yield path + [n]       else:         queue.append((n, path + [n]))  if __name__ == '__main__':   alphabet = string.lowercase   dictionary = load_dictionary()    for x in bfs_paths('dog', 'cat', dictionary, alphabet):     print x {% endhighlight python %}
{% include setup %} Pokemon Go is huge right now. People across all age ranges, demographics, and geographies are getting involved and it’s hard to imagine this sort of adoption for any other game. What I find fascinating is that Pokemon Go was based on the same augmented reality mechanics as Ingress, another game developed by Niantic Labs. Ingress has a loyal following but pales in comparison against Pokemon Go when looking at the user numbers, despite Pokemon Go being less than 2 weeks old while Ingress has been around for almost 4 years.  The Pokemon brand has a huge following and it’s incredible what a strong brand can do in the modern world. With zero distribution costs and instant global reach an existing brand can grow faster than at any previous point. Now more than ever do brands matters. Modern technology has made it easier than ever to enter new markets and quickly launch apps but in this world of commoditization and heavy competition a strong brand can do wonders.  The companies best positioned to take advantage of this new world are the ones with strong intellectual property and brands that can leverage whatever innovation comes along. Right now it's augmented reality but in the future it will be something else. The business will be adapting new innovations that allow companies to magnify and enhance their brands.  This model reminds me of the pharmaceutical industry. I started my professional career working for a pharmaceutical consulting company which gave me a crash course in how the industry works. One of the more interesting insights was that the biggest advantage large pharmaceutical companies have is their sales force rather than their R&D. This allows them to just acquire small biotech companies for their newly developed drugs and have their own sales force selling it. This approach makes sense - you find what you’re great at and focus on applying that as much as you can. This is what Nintendo is doing with Pokemon Go and every brand with global scale IP should be doing.
{% include setup %} Targeting is one of the best ways to improve the return on an advertising campaign. By identifying potential customers you're able to focus your advertising on them rather than someone random. And one of the best simplest ways is to set up your advertising campaigns to focus on a specific geography. Maybe your product is only sold in the United States and advertising it elsewhere is a waste. Or maybe your product is sold everywhere but the messaging and copy needs to vary by region. Or maybe it's sold everywhere with the same exact copy but the price varies by region. Being able to change your campaigns by geography is a simple way to improve the performance of any campaign.            Yet most geographic targeting is dumb. Earlier today I was on Twitter and noticed an ad for a Dodge Ram sponsored by Ram Trucks Canada. It's true that I'm on vacation in Canada but it's definitely not the case that I'll be buying a car in Canada. The solution to this isn't complicated. Every social network should have a good idea of my patterns and where home and work are. And if I'm outside those areas it should be easy to determine whether it's a quick trip out of town or a longer vacation. For all I know these platforms have this information but they should be exposing it to advertisers. Of course these inaccurate mismatches are a tiny percentage of the total advertising spend but it adds up and more importantly having more fleshed out profiles will improve the ad optimization.  Imagine being able to determine whether someone drives to work or takes the train. Every heavily used social network has the data to derive this but I suspect few have. We're already placed in various customer segments based on our behavioral and consumption history yet geography is still assumed to be the current location. I suspect the more advanced companies are using geographic information to craft better profiles but I'd love to see this opened up to advertisers.
{% include setup %} While working on a [small programming puzzle](/2016/07/17/coding-puzzle-word-transformation-through-valid-words/) I remembered Peter Norvig’s [spell checker](http://norvig.com/spell-correct.html) and how blown away I was after seeing it for the first. It’s one of my favorite examples of code that’s clean and elegant while being extremely expressive and powerful. If you haven’t seen it yet I encourage you take a look and step through it since he does a much better job of explaining both the code and theory than I ever could.  I don’t want to attribute my improvement as a coder to a single program but this program forced me to think much deeper about the code I write and provided a glimpse of good code. It serves as a goal and pushes me to be more aware of the code I write and whether it’s as simple and expressive as it can be. It’s not easy but approaching development through a lens of self improvement has been instrumental in helping me become a better coder. Good programmers are never happy with the code they wrote a year ago which is a sure sign that they’ve improved over the past year. Dissatisfaction is what drives people to improve and code is no different. It’s rare to find code that’s shocking in its brilliance and I’d love to see more examples so please share.
{% include setup %} A key part of management is getting out of the way and building out processes that help your team be as productive as possible. At the same time, you can’t change what you can’t measure. Combining these two makes it clear that to improve, whether people or process, you need to start measuring and tracking the appropriate metrics.  In software engineering, some things are easy to track: how many bugs there are, how quickly they’re resolved, how much code are written - but rarely tell the whole story and may lead to perverse incentives. The common example is measuring developer productivity through number of lines of code written: a smart developer would purposefully write verbose and long winded code to get their metric up.  Then there are the items that are hard to measure but actually drive productivity: improvement as an engineer, simple and expressive code, code that’s easily changed. These are incredibly difficult to measure, especially at scale, but if you’re able to focus on improving these you’ve found the holy grail.  By being creative it’s possible to come up with proxy metrics and approximations despite not being able to find easy ways of measuring the actual performance drivers. Think of these as traits that productive teams have and should be encouraged. There will always be exceptions and many are susceptible to gaming but they’re much better than nothing.  Besides the usual suspects (velocity, bugs, test coverage), I plan on tracking the following proxy metrics. Individually they don’t tell the whole story but taken together I hope they’ll be a good way to help improve the productivity of an engineering team. Note that a requirement for these was that they would be easy to collect, ideally automated.  - Pull request size: I believe pull requests should be as small as they can be. Larger pull requests are harder to code review and carry more risk. - Pull request file variance: Not a 100% sure about this one but I suspect there’s a difference in pull requests that are isolated to a small set of files rather than dozens. It may indicate that our code is not as cleanly laid out or architected as it should be and may be worth cleaning up. - Pull request activity: Another soft one but I want to see whether the amount of comments and changes a pull request has carries any meaning. I think junior engineers tend to have more feedback on their code versus more senior developers and measuring this may be a good way of discovering that. The challenge is that this one is easily gamed and we should all want to encourage discussion of code in order to come up with as high quality code as we can. - Deploy frequency: The more we deploy the more useful code makes it out into the real world and we should strive to deploy as often as we can while maintaining a high quality bar. We’re not at continuous deployment yet but hopefully this will help us get there.
{% include setup %} The iPhone is the top selling consumer product of all time and a variety of podcasts and articles makes it seem that this is the peak of consumer technology and we’ll never see anything as popular. This is shortsighted. Every new technology achieved wider and wider adoption and eclipsed the previous generation - [laptops eclipsed desktops and smartphones eclipsed laptops](http://ben-evans.com/benedictevans/2014/4/25/ipad-growth). One thing that’s clear is that each generation of tech gets closer and closer to us. Initially we were exposed to computers when we went into the office. Soon we started buying desktops for our homes. After that we decided we wanted laptops that we could carry around with us. Smartphones gave us the ability to carry computers around in our pockets with a full day’s worth of charge.  Smartwatches aren’t very popular now but cellular connectivity may make them even more popular than smartphones. The interactions and designs will need to improve to handle the novel form factor but that itself is an opportunity to get closer to to the senses other than sight. Smartphones we carry but smartwatches we wear. Beyond smartwatches we may end up with technology that gets us closer and closer to becoming cyborgs. At this point we get very close to sci-fi territory with chips that are implanted under our skins or technology that can interface directly with our brains. At that point I can’t even imagine what sci-fi novels will be - everything will seem possible.
{% include setup %} This is an annual tradition now but I just updated my old script that crawls and extracts the projected fantasy football data from Yahoo to work with the 2016-2017 season. The changes were incredibly minor: Yahoo broke the the login page into two steps and there was a minor change in the order of the columns. Both of these were trivial to implement and the code is up on [GitHub](https://github.com/dangoldin/yahoo-ffl). If all you care about is the raw data you can just download the [CSV](https://raw.githubusercontent.com/dangoldin/yahoo-ffl/master/stats-2017.csv).  Every year I intend to use the data to come up with a drafting algorithm yet I’ve failed to do anything with it over the past couple of years. I’m hoping this year is different.
{% include setup %} Open source is great: if you find the right library you’re able to save a ton of time and get code that’s been through the gauntlet that you can confidently incorporate into your system. Unfortunately many open source libraries are partially baked with documentation that doesn’t always accompany the rapid development of the code. This leads developers to repeatedly cross reference their code with some archaic documentation and then wonder why it’s not working as expected. This is proportional to the obscurity of the library - popular libraries will have most of their kinks worked out but esoteric ones that are likely maintained by one person won’t have the polish.  Yet it would be nice to take one of these libraries and build off of it. The simple answer is to reach out to the maintainer and ask questions. I always get excited when someone reaches out with a question about how to use one of my open source libraries; I’m not at that scale where this is burdensome and it’s encouraging that someone is actually using my code. When this doesn’t work a neat trick is to [https://github.com/search](search GitHub) for usage of that code. Most documentation provides a simple starting tutorial and assumes the user can take it from there. More often than not this doesn’t work well and you have to look at the source code to understand how the code works, what arguments the methods expects, and the order in which they should be called. By looking at actual usage of the code you can see how others have integrated these libraries in actual applications rather than toy examples. This works incredibly well for open source libraries in that middle area where they’re not popular enough to have everything figured out yet are useful enough to have had numerous developers wrangle them into their code. Many new and popular libraries fall into this bucket so if you want to use code that’s just becoming popular leveraging GitHub’s code search is a great way to start.
{% include setup %} I've been using [Turo](https://turo.com/) to rent our car out for the past couple of months and have been using a simple spreadsheet to track the revenue. Being a lazy engineer doing this manually became a bit tiresome so I finally automated it. Unfortunately Turo does not have a simple way of downloading the data and there’s no open API so I had to resort my usual solution: [scraping](https://github.com/dangoldin/turo-automation). Luckily for me I just came off of updating my Yahoo fantasy football scraping script and was ready to do the same for Turo.  The entire process took a few hours and the [result](https://github.com/dangoldin/turo-automation) is decent - it goes through every one of your completed trips and scrapes the receipt page for the total paid, total earned, the various reimbursements, and the start and end times. As of this writing it still doesn’t handle cancelled trips or trips that have not yet been taken. Another thing I noticed when writing the script is that Turo changed the representation of a trip - some of the older receipts had reimbursements in a different section from the newer ones so that needs a bit of tweaking. I’m sure there are some other edge cases I’m not handling properly since I could only code against the data I have; if it ends up not working for you let me know and I’ll see what I can do.  The process to build the scraper was standard: use Chrome’s source inspector to examine the structure of the page and then try using a few different selectors in an interactive Python section running [Selenium](http://www.seleniumhq.org/) to see whether they worked as expected. Once I had the various selectors and code figured it out it took a little bit of refactoring to get into a somewhat clean state.
{% include setup %} While building the [Turo scraper](http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history/) I became annoyed that there was no API to make my job significantly easier. Then I wouldn’t have had to go through a variety of hoops and iterations to get the data I needed and would also not have to worry about changes to their page design breaking the script. This got me thinking about an idea to write my scraper in such a way that it’s exposed as an API. In that case I can architect the code so that the retrieval and manipulation of the ride data is completely separate from the scraping code. Then if and when Turo does decide to release an official API all I’d need to do is swap my unofficial implementation out for the official one.  This chain of thought led to me to the challenges of building this on the engineering side. There’s something neat about being able to specify a bit of data through a series of steps. For example, to get the details for a ride the steps may be: 1) login to Turo, 2) navigate to that ride’s receipt page, 3) parse the details, 4) return them as JSON. Another API endpoint may be to retrieve all the rides. This one would be 1) login to Turo, 2) navigate to the first page, 3) fetch all the rides, 4) if there’s a next page, go to it and repeat step 3, otherwise 5) return the list of rides as JSON. For almost every request the first and last steps will be the same but the intermediate step will vary. This becomes even more interesting since we can now start to think about caching the results at the intermediate levels so you can avoid the steps if you’ve already done them in the past. This way we’re incrementally building a “shadow” version of the site and use that for everything we need but keep augmenting it when needed.  Pushing this further we can imagine a scraping specific language that represents the steps involved during a scraping session. The goal here is to replace the code that does the DOM traversal and instead come up with a cleaner and more expressive way that can be applied through code. Sometimes the application will be going to our cache but other times it will require actually navigating to the appropriate page.  I’m excited to try this approach out since it turns a rote scraping exercise into a higher order solution that can scale to other scraping jobs. I only wish I thought of it sooner since by the time I went down this rabbit hole I was mostly done with the actual code so I’ll have to give this a shot on the next scraping job.
{% include setup %} I’ve been a happy Instapaper user for years but the news that it was being acquired by Pinterest got me thinking about some alternatives. Not because I have anything against Pinterest; in fact I think this is a great fit and they’ll be able to complement each other but because it’s a reminder that no third party product is guaranteed to last and I wanted to see what open source alternatives are out there.  I discovered [wallabag](https://www.wallabag.org/) and got it setup earlier today. The documentation to install and get it running was incredibly straightforward and I was able to get it operational within an hour. Unfortunately it took a bit of wrestling to understand the various configuration options and I’m still unable to get it working across both the web and an iPhone. There’s a series of steps you need to do - from generating a unique RSS token to setting up an oAuth application that make it difficult to just get up and running. I understand that it’s designed for developers and offers a ton of customization but it should be simpler to get get the base installation - every user would want an extension to easily add articles and a way to access them offline on a phone and automatically generating the necessary settings would make it much easier to get started.  Trying out an open source alternatives is an eye-opening experience. You don’t realize how much polish it takes to build something usable. We love claiming that we can build anything in a day but it’s the relentless polishish that makes a successful product. I suspect this is why it’s incredibly hard to find open source products that require a cross platform approach. It’s difficult to think of successful open source applications that span across multiple environments. That requires multiple developers each agreeing on a unified vision and then making sure each of the components fits together. This is a tough combination and may be why so many popular open source projects are incredibly focused: it’s a lot easier to get multiple people working on a single product when it’s simple and they all share the same pain. But as soon as the scope expands there’s no single vision holding everything together and it shows in the final product.  The nice thing about open source is that anyone can add functionality and I’m already thinking of ways to improve wallabag. Hopefully I’ll have some time over the next few weeks.
{% include setup %} Something that I haven’t quite figured out is how to avoid wasting food. I like to think I keep good track of everything in my fridge but too often I end up finding something in the corner that spoiled and needs to be thrown out. Earlier today I was talking to someone at the office about this problem and how nice it would be if you could just have something that knows everything that’s in the fridge and can track how long it’s been there and an estimate of how long it will last. I’m sure refrigerators in 10 years will have this built in but I wanted to see what I could cobble together in an evening.  Luckily for me Google released a Cloud Vision API and I decided to give it a shot. Turns out implementing it was extremely straightforward, despite Google’s poor documentation, with a quick code search on GitHub that led to me [https://github.com/ramhiser/serverless-cloud-vision](https://github.com/ramhiser/serverless-cloud-vision). Unfortunately, the results were not promising. I ran on three images and while the categorization was surprisingly accurate it was too general. I expected to at least accurate identification for the bottles and cans - milk, ketchup, yogurt but the closest it got was food, ice cream, and gelato. Granted, the photos weren’t staged well and it took me about 15 minutes to get it working but I was still disappointed. The Cloud VIsion service doesn’t offer much customization so I’m going to see how much better I can make it by improving the photos. I’ve included the original photos along with the classification results below. As usual my code is up on [GitHub](https://github.com/dangoldin/fridge-vision) although it was really just a straight up copy and paste from [ramhiser’s code](https://github.com/ramhiser/serverless-cloud-vision) above.                              {% highlight json %}[         {           "score": 0.90114909,           "mid": "/m/02wbm",           "description": "food"         },         {           "score": 0.88251483,           "mid": "/m/02phwj2",           "description": "display window"         },         {           "score": 0.81870794,           "mid": "/m/0cxn2",           "description": "ice cream"         },         {           "score": 0.76996088,           "mid": "/m/0270h",           "description": "dessert"         },         {           "score": 0.75129372,           "mid": "/m/02fz11",           "description": "gelato"         },         {           "score": 0.69974077,           "mid": "/m/02rfdq",           "description": "interior design"         },         {           "score": 0.57035172,           "mid": "/m/02q08p0",           "description": "dish"         },         {           "score": 0.54961139,           "mid": "/m/0191_7",           "description": "retail store"         },         {           "score": 0.53331912,           "mid": "/m/031bff",           "description": "window covering"         },         {           "score": 0.51523668,           "mid": "/m/01_bhs",           "description": "fast food"         }       ]{% endhighlight %}                                     {% highlight json %}[         {           "score": 0.87785435,           "mid": "/m/07yv9",           "description": "vehicle"         },         {           "score": 0.78110605,           "mid": "/m/0k5j",           "description": "aircraft"         },         {           "score": 0.77443254,           "mid": "/m/0cmf2",           "description": "airplane"         },         {           "score": 0.7131173,           "mid": "/m/02pkr5",           "description": "plumbing fixture"         },         {           "score": 0.71218145,           "mid": "/m/015y8h",           "description": "jet aircraft"         },         {           "score": 0.66169083,           "mid": "/m/06ht1",           "description": "room"         },         {           "score": 0.5944497,           "mid": "/m/01lgkm",           "description": "recreational vehicle"         },         {           "score": 0.54411179,           "mid": "/m/041x_j",           "description": "public toilet"         },         {           "score": 0.53394085,           "mid": "/m/017_cz",           "description": "major appliance"         }       ]{% endhighlight %}                                      {% highlight json %}[         {           "score": 0.78413528,           "mid": "/m/02phwj2",           "description": "display window"         },         {           "score": 0.647836,           "mid": "/m/02rfdq",           "description": "interior design"         },         {           "score": 0.59498113,           "mid": "/m/0c_jw",           "description": "furniture"         },         {           "score": 0.57692927,           "mid": "/m/0191_7",           "description": "retail store"         },         {           "score": 0.54954523,           "mid": "/m/08790l",           "description": "boutique"         }       ]{% endhighlight %}
{% include setup %} Social networks carry extreme network effects and have massive winner-take-all dynamics. This makes it impossible for two social networks that have the same pitch to co-exist and leads to pretty strong differentiation. Facebook owns relationships. Twitter owns interests. Instagram owns lifestyle. Snapchat is starting to own experience. This is why I find it fascinating when the content from one social network or medium bleeds into another. Twitter doesn’t allow for tweets longer than 140 characters so people overcome that by sharing screengrabs of long form text. I’ve seen the same on Imgur - it’s primarily used for images but often you’ll see someone posting an image of a long story. We have our own unique relationships across each of these networks so it’s not surprising that we’ll sometimes want to communicate something that’s best expressed with a specific medium yet it’s still fascinating seeing it in action. I get the feeling that they’re publicly exploiting a loophole and adding a tiny bit of chaos to the universe.
{% include setup %} Ever since AMP was announced I’ve been meaning to migrate my blog but hesitated due to the fear that it would take an inordinate amount of time and would be laden with edge cases. But over the Labor Day weekend I decided to give it a shot and see how far i could get. A quick GitHub search showed two promising repos - [amp-jekyll](https://github.com/juusaw/amp-jekyll) and [amplify](https://github.com/ageitgey/amplify) - and I gave them both a shot. They approach AMP integration in two different ways - amp-jekyll creates an AMP version of every post and has it live in a separate folder structure while amplify is a comprehensive theme. This made the amp-jekyll integration much easier since it’s designed to work parallel to the existing blog but I wanted to do a full rewrite.  I ended up cloning the amplify repository and manually importing a few of my blog posts to see how they’d render, handle the images, and look under the different style. After playing around with amplify I realized it would actually be straightforward to integrate directly into my blog as an additional theme. After copying over the design files and the libraries required to inline the SCSS I was left with making a few changes to the CSS to get it to resemble the previous design. All in it took a few hours to get my blog migrated to AMP and it’s incredibly quick - especially on mobile (if you haven’t given it a shot please do).  There are still a few things I need to take care of but I’m pleasantly surprised by the ease and simplicity of the transition and the resulting performance. The major problems I need to take care of before I call this a success:  - Disqus integration. I’ve been using Disqus to manage comments and it would be a shame if I had to ditch it. Based on a few StackOverflow and forum posts it looks as if it’s possible to get Disqus working by forcing an iframe with the comment section but I’ll have to figure out how this works. - Various styling fixes: Since I ended up starting with the amplify CSS there are a few inconsistencies that I still need to take care of - especially on some of my older posts that have some ugly inline CSS. - Img to amp-img: To be AMP compliant you cannot have any img tags and instead must use amp-img. This sounds straightforward but amp-img requires you to specify the dimensions of the image which I have not been doing. It looks as if there’s a plugin for this in amp-jekyll and I got it working locally but need to get it working on GitHub pages. - JavaScript heavy posts: I have a few older posts that depend on D3 for visualizations and I’m going to have to rewrite those posts to include the animations as amp-iframe elements. This seems straightforward but I’m sure I’ll run into some hiccups when I actually sit down to do this. - Build times take forever: This is my biggest issue so far. Since AMP requires all CSS to be inlined it means that every CSS change causes the entire site to be regenerated. Before AMPifying, Jekyll would build the site in around 10 seconds and now it takes nearly 2 minutes. I don’t have a good fix for this but a simple solution may be to have different CSS for the different types of pages to avoid a full site regeneration with every style change. While developing I solve this problem by moving all but a few posts out of the _posts folder in order to reduce the number of pages that need to be generated. Then when I’m happy with the outcome I’ll move the other posts back and let it go through the full generation. This is extremely hacky and I wish there was a better solution here.  I’d love to know what the readers of the blog think and whether they’re noticing any improvement so if you have any feedback please let me know. And I’m aware that I have yet to get Disqus set up to work with AMP but in the meantime let me know via [Twitter](https://twitter.com/dangoldin).
{% include setup %} In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code. As usual, the code is up on [GitHub](https://github.com/dangoldin/yahoo-ffl/blob/master/analyze.R) and I’ll keep updating it as I keep adding newer visualizations and analyses.                              Pretty simple here but highlights how much more valuable the QB position is compared to the others.                                     The  box plot  is a quick way of looking at distributions since it highlights a few metrics at once - the median, the quartiles, as well the outliers. What's interesting here is how many outliers there are at the QB and WR positions, especially how uneven it is for WRs.                                       The density plot shows how the points are distributed by position. This shows a similar story to what we saw in the boxplot but visualizes each of the data points. I suspect the symmetry in the QB position is not unique and is just an artifact of the fact that QBs are heavily represented in the top 100 players each week and if were to expand our dataset we'd see similar distributions for the other positions.                                       Similar exercise to the above but by team. I didn't find a ton interesting here other than Pittsburgh is dominant when it comes to top fantasy players and that Denver and Philadalphia are lacking.                                       This isn't the most useful due to the biased dataset but it does highlight the dominance of some teams compared to others but not much more than that - at least with a quick glance.                                       A bit tough to read due to the volume of teams but paired with the previous one does show that there are a few outliers but many of the distributions are similar.
{% include setup %} Over Labor Day weekend I migrated my blog to use [AMP](https://www.ampproject.org/) but the first version was definitely a work in progress. One big item I needed to take care of was converting all my images to be AMP compatible by replacing &lt;img&gt; tag with &lt;amp-img&gt; along with the image width and height. I ended up writing a quick Python script to go through each of my posts, find each &lt;img&gt; tag, get the image’s dimensions, and then replace the original tag wit the AMP version. Unfortunately, I ran the script without too much testing and forgot to add closing tags which caused some of the content to go missing.  The solution was to write another script that once again went through every post but instead of replacing every img tag with an amp-img tag it found every amp-img referenced and added a closing tag in case it didn’t have one. These two scripts combined ended up fixing most of the AMP issues but I’m sure there are still a few posts that got warped so if you notice any please let me know.  In the spirit of constantly shipping the code is up on [GitHub](https://github.com/dangoldin/ampification) but is simple enough to not need a ton of polishing. Note that it’s not very robust and has some assumptions based on my blog structure so I would test it thoroughly before applying it to your posts.
{% include setup %} While going through and making sure each of my old posts was AMP compatible I came across a [post from 2012](/2012/06/07/achieving-browser-autocomplete/) where I tried to list the first autocomplete suggestion for each letter. This naturally made me think of what the results would be if I did the same exercise now. Comparing the information 4 years apart is an interesting way to see how my habits have changed but also provide a glimpse into the evolution of companies, products, and technology. The biggest surprise is how much of the list is work related - it’s somewhat expected given how much time we spend working and how many more cloud services there are but it’s still shocking that almost half the list is work related. The other major realization is that much of my consumption has shifted to mobile - many of the sites that are no longer on the list I actively use on my phone; I may actually use Instapaper, Google Maps, and Twitter more frequently now but it’s mostly on mobile via an app. Given how interesting this exercise was I plan on doing this annually and encourage others to do the same - it’s an extremely simple way to see how technology and our relationship to it changes over time.      2012  2016  Notes      analytics.google.com  amazon.com  I don't care as much as I used to about site metrics but to make up for it I'm shopping more frequently.    bankofamerica.com  betterworks.com  We've been using BetterWorks to manage team and personal OKRs.    cad-comic.com/cad  console.aws.amazon.com  Another work product - need to make sure everything's still up and running.    docs.google.com  drive.google.com  Just a domain change.    eventbrite.com  -  An internal Sentry installation to help us track errors.    facebook.com  football.fantasysports.yahoo.com  Start of football season but not sure what would have replaced this.    glos.si  github.com  Glossi is no longer around but I spend a ton of time on GitHub now.    heroku.com  hellofresh.com  Might be based on recency since I was cancelling my account last week.    instapaper.com  interactivebrokers.com  I'm mostly using the Instapaper app now.    joinblended.com  jira.com  Need to maintain our agility.    klout.com  kafka.apache.org  Apparently I spend a lot of time read Kafka docs.    linkedin.com  localhost:4000  This is Jekyll which powers my blog.    maps.google.com  mint.com  This is an interesting one. I use Google Mpas more frequently than Mint but it's mostly on mobile or typing an address in directly or incognito.    news.ycombinator.com  news.ycombinator.com  One of the few that stayed the same.    optimum.com  opentable.com  We no longer have Optimum and booked a dinner reservation recently.    plus.google.com  -  This is a work domain that's just not very well secured.    questionablecontent.net  questionablecontent.net  Another one that stayed the same.    reader.google.com  -  Another inernal work domain.    startupmullings.com  suntrust.com  I went from blogging about startups to paying a mortgage.    twitter.com  triplelift.atlassian.net  Our Atlassian installation.    udacity.com  usetallie.com  Another work site to submit expense reports.    voice.google.com  vettery.com  A work site to help recruiting.    wixlounge.com  wrike.com  Not many sites starting with w. We tried Wrike out before using JIRA.    xkcd.com  xkcd.com  Another one that stayed the same. I'm loyal to my comics.    youtube.com  youtube.com  Same here. I don't use YouTube much but not many other sites starting with a y.    zerply.com  zillow.com  I'm such an adult.
{% include setup %} After migrating my blog to AMP the last task was getting [Disqus](https://disqus.com/) working again. The crux of the issue is that in order to improve page performance AMP disallows blanket script tags (which the Disqus integration leverages) but to make up for it comes with a variety of helpers to include officially support functionality. Examples of this include an amp-youtube tag to include YouTube videos and the amp-vimeo tag to include Vimeo videos. As a generic solution, AMP provides the amp-iframe tag which allows you to include a restricted iframe.  Doing the research it turned out there was no out of the box solution but after a bunch of false starts I came across a great [post](https://labs.tomasino.org/disqus-in-amp) by [James Tomasino](https://twitter.com/mr_ino) where he ran into similar issue and came up with a workaround that was simply creating an additional HTML page for each post that contained the appropriate Disqus code which could then be included via the amp-iframe tag. Unfortunately this approach wouldn’t work in my case since amp-iframe requires HTTPS and my blog is solely HTTP due to being hosted on GitHub pages with a custom domain.  The workaround I came up with is to take the script James came up with and make a few tweaks to it that allow it to be hosted on an S3 bucket. I also wanted to avoid having to build an additional comment HTML page for each post for each new post and made a small change that allowed me to pass the relevant details as GET arguments into the comment iframe page. If you’re interested in the implementation, just take a look at the source of this page or check out [https://s3.amazonaws.com/dangoldin.com/amp-disqus.html](https://s3.amazonaws.com/dangoldin.com/amp-disqus.html).
{% include setup %} One of the biggest lessons I learned when I became an engineering manager was how important the basic operational elements. These are all the things that need to get done outside of code and allow the whole team to be as productive as possible and range from reminding people to do code reviews to creating dashboards to highlight key metrics to enforcing an on-call process. These tasks are important yet repetitive so being a good engineer I’ve spent some time automating them. There’s still a long way to go but strong engineers have a mindset that they want to automate as much repetitive work as possible in order to focus on unique and novel challenges.  This attitude can be applied to management as well. By automating the menial stuff you’re able to focus on the tasks that require a human touch. Nearly every product geared towards developers exposes some sort of API which can be used to automate most rote work. The approach I’ve taken so far is extracting data from Redmine and GitHub via their APIs and exposing the results in a simple dashboard powered by [freeboard](https://github.com/Freeboard/freeboard) as well as on Slack. Since every company has a unique setup with their own set of tools and processes it’s difficult to come up with a universal solution but modern day tools make it incredibly easy to get started with some sort of automation.
{% include setup %} Apple’s Touch ID is great but one thing it doesn't handle well is wet fingers. Even if my hands are a little bit sweaty or not completely dry it's difficult to unlock the phone. Yet as soon as they’re dry the phone immediately unlocks. What’s surprising, especially given Apple's focus on delivering the perfect user experience, is that this is still a problem. I'm not familiar with the hardware behind Touch ID but even if there's some sort of warped fingerprint it should be good enough. The fact that there are a few unsuccessful attempts with the wet thumb followed by successful attempt should be enough to develop a profile for the wet version which can be used on future attempts. Modern products succeed by delivering optimized experiences; future products will need to adapt and grow along with us until they become eerily predictive.
{% include setup %} In April, Twitter [announced a deal](http://www.bloomberg.com/news/articles/2016-04-05/twitter-said-to-win-nfl-deal-for-thursday-night-streaming-rights) with the NFL to broadcast Thursday night games and I gave it a shot this past Thursday via the Twitter app on my FireTV. The primary motivation was to watch the game but I was also curious to see Twitter’s implementation. I was pleasantly surprised by how smooth and clean the overall experience was: you could watch the entire game without knowing it was via Twitter but the tweets added a please, yet optional, touch. The only real difference between the Twitter app and any other FireTV streaming app was that Twitter augmented the experience with twitter content - tweets, images, and scopes.  This NFL product makes me optimistic about Twitter and does feel as if they finally stumbled unto a product that works and reinforces their strengths. By focusing on live events and building on top of them with content that’s unique to Twitter they have the potential to change the way we consume live TV. At the moment the feed seems to be chronological but if Twitter can figure out how to make it a bit more relevant it can make the feed section standard.  The feed section currently takes up close to a third of the screen which causes the video to be scaled down. The sidebar is an obvious first attempt but I can think of other ways the tweets can be shown - maybe a ticker tape or even a translucent overlay - would make it likelier that people keep the feed on throughout the game.  There’s also a ton of opportunity in opening this platform up to developers. Twitter developed the reputation of betraying the developer community during their growth but this can be a chance to redeem themselves. Imagine being able to build an app that lives within the TV app and can show you how your fantasy team is doing or just displaying a more targeted subset of the tweets or even pulling in additional stats. All Twitter would need to do is provide the platform and the community can build on top of it to deliver custom experiences. The incentive is already there - being able to have an app that’s used while people are watching TV is hugely motivating and will get developers supporting Twitter.  Disclosure: I own a small number of Twitter shares.
{% include setup %} One of the first things felt by a fast growing company is the lack of meeting space. The first few weeks at a new office it’s wonderful to know you can find a room whenever you need it. Yet after a few months and a bunch of extra people you realize you have to book meetings days in advance. And what makes this worse is seeing more than one room booked for the same meeting.  After seeing this happening I decided to do something about it and wrote a quick script to pull the meeting calendar for every room from [Google Calendar](https://developers.google.com/google-apps/calendar/) and then flag the ones having the same start time, end time, and creator.  This isn’t foolproof since it won’t identify cases where someone books multiple rooms for the same time with different times but it's a solid start and already caught a few cases. The code is up on [GitHub](https://github.com/dangoldin/gcal-shaming) so feel free to take a look and provide suggestions.
{% include setup %}           While exploring the city earlier today I ended up wandering too close to the Google building and somehow got connected to their guest wifi network, GoogleGuest, and noticed that iOS 10 gave me an “Security Recommendation” notification. My first reaction was that this was an Apple jab at Google but It turns out that iOS 10 introduced a [new feature](https://www.engadget.com/2016/07/22/ios-10-unsecured-networks/) to let people know that they were connecting to an open network. The intent seems to be to warn users that they may not be on a secure connection but it’s a bit hidden away and didn’t actually prevent me from connecting: it was more of an FYI.  It’s clearly a minor feature but I think it reinforces the stance Apple has been taking in favor of [user privacy and encryption](http://www.apple.com/customer-letter/). They’re positioning themselves to be the antithesis of Google and this is a small way of driving that point home.
{% include setup %} One of the best arguments I’ve heard against mass surveillance is that the marginal cost has dropped to nearly zero which warps the system. Since so much of our world is digital it costs the government nothing extra to collect each additional data point. Given these incentives it’s no surprise that the government was able to get the major companies to provide a dedicated feed of the data they were collecting - modern technology has enabled both the collection and analysis of massive amounts of data.  This infrastructure is something we’ve never had before. In the past surveillance carried a sizable cost - beyond the warrant one would need to either install wiretaps, manually intercept mail, have people followed, and generally hire people to do both the data collection as well as the analysis. These constraints necessitated making tradeoffs and prioritized those that carried the largest risk.  It’s impossible to undo the technological advances and we wouldn’t want to. At the same time we need to do more to introduce friction back to surveillance. The goal isn’t to achieve 100% privacy but to make it costly enough that governments need to think about who and what they’re tracking. The obvious way is to start using end to end encryption - I’m sure isolated cases can be cracked but cracking it at scale would be a monumental task.
{% include setup %} Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that [secor](https://github.com/pinterest/secor), a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly. I fastidiously tweaked the Kafka configuration to get as much out of it as I could to no avail. I spent hours experiment with the various secor options to see whether there was a simple solution I was missing. No matter what I tried I was unable to consume more than 50mb/min - despite the fact that both the Kafka cluster and the instance running secor could support an order of magnitude more than that. I confirmed that there was something fishy by running the same exact code on a massive c3.8xlarge instance to see how much better it would fare. And sure enough I still couldn’t get past 50mb/min.      The blue is an c4.xlarge and the orange is a c4.8xlarge. Clearly they should not both be consuming at the same rate. Also, the large spike in the middle is when the offsets start dropping off and secor keeps attempting to catch up.       The flip side is that the uploads to S3 are throttled and drop of when we're behind Kafka.   At this point I was extremely frustrated and figured I might as well revisted the Kafka docs and found this [wonderful gem](http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact):  > The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.  The light immediately went off and sure enough, secor was configured to use a Kafka 0.8 client. As soon as I [upgraded secor](https://github.com/pinterest/secor/pull/262) to use Kafka 0.10 the consumption rate shot up to over 2.5gb/min. Despite feeling incredibly stupid it felt good to finally get to the bottom of it and only wish I read the docs more thoroughly before diving in. The benefit to all this is that I have a much better understanding of how  Kafka, ZooKeeper, and secor need to be configured and the value of actually reading the documentation, something that I still haven’t internalized.      After the upgrade we see a healthy spike of data going in as we're trying to catch up.       Similarly we see us writing it all out to S3.
{% include setup %}           The past couple of weeks I’ve had a big case of writer’s block. I haven’t been able to motivate myself to write as much as I used to and when I did get to write it felt more like a chore than a joy. I didn’t know how to break out of it but this past weekend I kicked off the OS X upgrade without realizing how much time it would take.  Since I made a commitment to write two posts a week and I was computerless I had to do something. Lucky for me I have an bluetooth keyboard lying around a neat lap desk with a phone slot so I decided to give it a shot and see what I could muster.  It turned out remarkably well. The small screen made it a lot easier to focus which was magnified by the inability to easy switch to another app - something I’m prone to doing when I’m on an actual computer.  I still need the command line to commit the text and handle the image upload but it was incredibly liberating to write using a keyboard, a lap desk, and a phone. The change of environment itself may have gotten me over the writer’s block but I can also see myself using this setup whenever I travel or am outside. It’s also portable which makes it simple to write where I am.
{% include setup %} While going through my old GitHub repos I discovered that the most starred repo was [twitter-archive-analysis](https://github.com/dangoldin/twitter-archive-analysis), a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working. Compared to three years ago, the results are surprisingly not that different - I definitely tweet less frequently than I used to and my activity has shifted into being more about replies rather than general tweets.                              No tweets while I'm asleep but tend to be the most active in the evenings.                                       Pretty even distribution but more active on the weekends than the weekdays.                                       Hit my peak in 2013 and have been declining since.                                       I tried to get at the idea of how much I tweet over time and by day - the weekends have remianed steady but my weekday tweeting has dropped off.                                       Definitely not taking advantage of the full 140 characters.                                       The next visualization provides a much better idea of my tweet type distribution.                                       A clear trend to being more about replies and engagement rather than just posting thoughts and ideas.
{% include setup %} After Friday’s DNS DDOS attack I’ve been thinking of approaches that could prevent this from happening in the future. In a perfect world every device would be up to date with the latest updates and it would be difficult to compromise anything that’s connected to the internet. Unfortunately, this is not the case and there’s an ever growing number of devices that are quickly hacked together and sold without any focus placed on security. Akamai did a [study that shows](https://www.wired.com/2016/10/akamai-finds-longtime-security-flaw-2-million-devices/) over 2 million internet connected devices have been compromised which allows them to be used to run DDOS attacks, very similar to the one that took down a big chunk of the internet on Friday. The challenge is that most owners both don’t know and don’t bother to do any security audits when setting up these devices and very likely never upgrade the firmware nor the software to make them more secure.  The solution is either to have much stronger regulation on what’s able to be sold to force manufacturers to secure their devices but I suspect this is a non starter - it’s tough to control the global world and there will always be incentives to deviate. A better solution would be one that assumes the internet will be filled with these malicious devices but can still handle them.  One idea is to make our routers smarter. They’re our homes’ gateway to the internet and improving the way they handle outbound traffic can reduce the impact these faulty devices have. Imagine them being smart enough to know the typical pattern of every connected device and throttle atypical traffic. Or have them serve as a both a cache and a throttler of DNS requests. The risk here is that the router itself becomes compromised or ends up accidentally rejecting valid traffic. I suspect most people have a router that was given to them by their ISP and ISPs have a strong incentive to keep their routers secure. And even if the router does get compromised we can push this sort of “smart throttling” unto the ISPs. In the case of the accidental throttling we’ll either need to deal with a small delay or provide the ability for a human to override the throttling - something that they would not unknowingly do to support a random device.  The solution here is to accept that we will always have bad actors and that we’ll never have total security. In that sort of world the network itself needs to be robust and resilient enough to handle whatever is thrown it’s way. Making the network more intelligent is one way but other ways include building in more resiliency into the protocol itself or making more and more of the internet distributed. This problem will only get worse as our entire homes connect to the internet and we need to find a solution before then.
{% include setup %} Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.  Earlier today I did yet another one of these analysis and got annoyed by how repetitive the process was and wrote a quick script to handle simplest case: a single line chart derived from two columns - each representing an axis. The script works by taking tab delimited data via stdin and then using matplotlib to do a standard line chart. There’s a ton of room for improvement but it fits my standard workflow of using [SQL Workbench/J](http://www.sql-workbench.net/) to execute the query and then quickly copy it over to my clipboard in a tab delimited format.  The [code is up on GitHub](https://github.com/dangoldin/python-tools/blob/master/plotter.py) and it can be executed from the command line by piping the raw data directly into the script. If the data is in the clipboard it’s as simple as typing in “pbpaste &#124; ./plotter.py”. Using this approach I was able to generate the image below as well as the Excel version for comparison. The major improvements are cleaning up the styling so it looks nicer as well as supporting multiple series.
{% include setup %} NFL viewership is [down 10%](http://www.wsj.com/articles/ratings-fumble-for-nfl-surprises-networks-advertisers-1475764108) this season and I understand the desire to grow the brand and the sport [abroad](https://en.wikipedia.org/wiki/NFL_International_Series#Long-term_deals_and_the_NFL.27s_return_to_Mexico:_2016.E2.80.93present). It seems misguided to take a product that’s declining in popularity and rather than fixing the core problems to try to grow it as is. This is akin to a tech startup marketing the hell out of a product that’s unable to retain its existing customers. The proper approach is to nail the product before trying to push it into the market.   In this case it’s actually worse since the NFL is not something isolated and the experience of one fan will influence the experience of another. By having some games played internationally the NFL runs the risk of alienating some fans that have to wake up much earlier to watch the game. The London games have been airing at 9:30 AM EST, which is 1:30 PM in London and 6:30 AM in California. This requires the California fan to be up at dawn to watch the game. And on the flipside, imagine a London fan actually adopting an NFL team: an 8:30 PM EST game would start at 12:30 in London and go on for a few hours.   Given the time difference it’s tough to imagine the NFL expanding internationally in its current form.  The only events with this sort of mass appeal are international competitions - the Olympics and the World Cup come to mind - and they occur every 4 years. They should instead focus on attaining the perfect experience for their existing users - that includes making the league more balanced, ditching the blackout rules and embracing full digital distribution, and actually dealing with the concussion and violence issues. Instead it feels as if the NFL is diluting themselves for a small change at some short-lived growth.
{% include setup %} I love to read so it took me a surprisingly long time to get a Kindle. Before then I felt fine either just grabbing a physical book or using a tablet or a phone. LCD displays never bothered me so I figured I might as well get the responsiveness and additional functionality of a tablet rather than a single-use device. But earlier this year I spent some time using my wife’s Kindle and loved the form factor as well as the battery life. I also started to buy a lot more ebooks so finally took the plunge and got myself a Kindle.  If you’re not an avid reader you can get away with a tablet or phone. But if you enjoy reading the Kindle is great and makes it easy to fall into a reading addiction. I’m a bit odd in that I will refuse to write in a book; I remember having a half dozen SAT practice books and rather than circling the choices I would do it in a separate notebook. I don’t know how this habit stemmed but I won’t do any mutilation of the book, including folding a corner to save a spot. I like to think I value the sanctity of a book but I’m sure it’s due to a habit I picked up as a kid.  With a Kindle I don’t have this aversion and get a kick out of highlighting interesting passages or just words I don’t know. I’m also reviewing the draft of a friend’s book and this made it incredibly easy to take notes. Unfortunately, since the book was a draft and was not purchased through Amazon my highlights aren’t accessible through the website which makes it difficult to actually go through and flesh them out.  This seems like an opportunity for Amazon to improve the experience. It may be an anti-piracy decision which offers an inferior experience to pirated books but this seems misguided and ruins the experience for the majority in order to penalize the minority. Amazon is dominant in ebooks and digital publishing and has already won the space; they should be doing everything they can to encourage authors to write and a big part of that is giving them an easy way to get feedback on their drafts.
{% include setup %} Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.  An idea I’ve been toying with is a “poor man’s data pipeline” that could be built in a serverless way and can scale to massive volumes. It turns out that a pretty simple data pipeline can be built using two AWS services: Elastic Load Balancer (ELB) and Lambda. This data pipeline doesn’t have the true streaming that Kafka provides but for simple aggregations and a tolerable 5 minute delay it’s extremely cheap and robust.  The way it works is by setting up an Elastic Load Balancer with [access logs enabled](http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html) but not actually associating it with any EC2 instances. Then every time an event is generated it would just be making a request to the ELB with the data specified as query string parameters. Note that this is extremely cheap since at this point we’re not paying for any computer and are just paying for traffic to the load balancer. Note that this is a hack since it will cause every single response to have a 503 status code but can be easily remedied with an simple web service that does no actual processing and responds with a simple 204.  Once we have access logs enabled we set up a Lambda function that gets executed every time a new S3 log file is generated. This lambda function downloads the S3 file and rolls it up via a custom function which can then be setup to export the resulting data wherever it needs to go. Note that at the moment Lambda still has a series of [limits](http://docs.aws.amazon.com/lambda/latest/dg/limits.html) that may prevent this from working at incredibly high volumes but even then one can set up Lambda to make a simple HTTP request to an external service with the log file path which can then be processed.  The [code](https://github.com/dangoldin/poor-mans-data-pipeline) is short and sweet and is up on GitHub along with a guide on getting started. If you have any questions or suggestions I’d love to hear them.
{% include setup %} Two years ago I [toyed around](http://dangoldin.com/2014/12/31/redirect-recursion/) with an odd idea of implementing recursion over HTTP redirects. The idea is that the state is managed through the query string arguments and at each recursive step we just redirect to the URL for the next one. I still can’t think of a legitimate use case for this approach but have been on an AWS [Lambda](https://aws.amazon.com/lambda/) binge lately and wanted to see whether I can get this “redirect recursion” working under Lambda. Turns out it’s incredibly easy.  The only question was exposing the Lambda function to the outside world but AWS offers the [API Gateway](https://aws.amazon.com/api-gateway/) service to make this happen. This also gave me a chance to mess around with the API Gateway for the first time and definitely has me thinking about entire tools and applications that can be done in a “serverless” way.  {% highlight javascript %} # A simple Lambda function to calculate the factorial 'use strict';  exports.handler = (event, context, callback) => {      const done = (err, res) => callback(null, {         statusCode: err ? '400' : '200',         body: err ? err.message : JSON.stringify(res),         headers: {             'Content-Type': 'application/json',         },     });      switch (event.httpMethod) {         // Calculate the factorial         case 'GET':             var n = parseInt(event.queryStringParameters.n,10) || 1;             var a = parseInt(event.queryStringParameters.a,10) || 1;             if (n   20) {                 done(null, {'status': 'try a smaller number'});             } else {                 var url = 'https://rrouzys2ra.execute-api.us-east-1.amazonaws.com/prod/redirect-recursion?';                 var args = 'n=' + (n-1) + '&a=' + (a*n);                 callback(null, {                     statusCode: 302,                     headers: {                         'Location': url + args                     }                 });             }             break;         default:             done(new Error(`Unsupported method "${event.httpMethod}"`));     } }; {% endhighlight %}        This connects any request to the /redirect-recursion endpoint to the Lambda function.         This shows the URL that needs to be invoked to run the recursion.
{% include setup %} A [few years ago](http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/) I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they're making.                                                          Average load time.  Pretty similar to last year with most of the top platform sites being incredibly quick. The surprising thing is that on average sites seem to have gotten slower but this can be entirely due to me having a different internet connection - something that on its own is an issue.                                                                          Load time boxplot.  Similar distribution to two years ago but so much more variance. No idea why this would be the case.                                                                          Number of requests.  Many more requests being made in 2016 than in 2014. In 2014 no site made over 1000 requests but in 2016 we see it happening with 3 sites.                                                                          Number of request vs time to load.  Expected and similar results to 2014. The more requests a site is making the longer it takes to load.                                                                          File type frequency.  Pretty similar distribution to 2014 but we do see much higher numbers across the board and a relative decrease in JavaScript and an increase in JSON and HTML.                                                                          File types by url.  Not much here but seems that there's a bit more variety of content types compared to 2014 although still heavily dominated by images.                      As usual, the code’s up on  GitHub  but you'll need to go back in the revision history to get access to the old data files.
{% include setup %} I have over [50 repositories](https://github.com/dangoldin?tab=repositories) on GitHub with the majority being one time projects that were either me exploring a new technology, writing a small script, or doing a quick data analysis and visualization project. Every once in awhile when I’m a bit nostalgic I’ll go through these old projects and mend some of the code.  What’s surprising is discovering old projects and scripts that work as is without me having to do anything to update the underlying code. I’m used to so working with so many open source libraries and cryptic documentation that it’s rare to find a public library that works exactly as you expect. Of course my projects are much simpler than the typical open source library but I find it remarkable that I can get code up and running within a few minutes of a checkout.  This is something that we should aspire to when writing code - writing it in such a way that if someone were to use it in a few years it would be easy to follow and understand while being able to be run without requiring any modification. There’s an urge to constantly improve and extend everything we write but in the world of software it’s possible that we may end up making it worse. Rather than adding bells and whistles to everything we write we should take a step back and think how we would react if we were to discover it in a few years.
{% include setup %} There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, [Trusted Advisor](https://aws.amazon.com/premiumsupport/trustedadvisor/), that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month. External vendors are cheaper but can still get expensive depending on your bill: [Cloudability](https://www.cloudability.com) starts at 1% of your AWS costs which compared to Trusted Advisor is significantly cheaper is still 1% of your AWS bill.  One option is to sign up for a single month and use that to take the necessary steps to improve your cloud configuration. If your infrastructure is stable month to month this is a simple and cheap way to revamp your setup. But if your infrastructure is constantly evolving you need a way to revisit your environment when necessary.  I spent some time looking at our AWS infrastructure last week and it turns out AWS provides an option to export a [detailed billing report](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html) to S3 which is what the external vendors use to provide their recommendations. AWS offers a few reporting options but the most detailed one contains every resource and tag and in my case was over a million rows and nearly 500MB. There’s a wealth of information here and I wrote a small script to slice, dice, and visualize the data along a few dimensions to help provide some transparency into what is the biggest cost. The code is extremely simple since it’s just grouping and visualizing the data by different dimensions. The only real enhancement I made was to translate the OpsWorks tags into a layer dimension to make the visualization more useful. The two big things I still need to do are provide recommendations around reserved instance usage and do a better job of grouping the usage types since they’re too specific. As usual the [code is up](https://github.com/dangoldin/aws-billing-details-analysis) on GitHub and I’d love to hear any suggestions or feedback. Below are some graphs the script generates but note that I removed the axes labels to avoid revealing our costs and configuration.                                                          By product name.  A simple summary of cost by AWS product/service.                                                                          By product name.  This shows every type of usage AWS has in the billing report. To deal with the long tail the script also generates a plot for the top 25 but one thing I need to do is a better job of grouping these - for example data transfer has different values depending on region and type and I want to consolidate them into a one in order to see total costs due to data transfer.                                                                          By layer and usage type.  To me this is the most interesting one since it's looking at data for multiple dimensions - in this case layer and usage type. The goal here was to see which application/usage pairs result in the largest costs and allow me to prioritize investigation effort. Once again this will be more useful when I do a better job of grouping the usage types.
{% include setup %} Yesterday I had the privilege of giving a talk at [HackReactor](http://www.hackreactor.com/) titled “Things I wish I knew” which was an amalgam of the various themes and topics I’ve been blogging and thinking about. While going through the blog I came up with two themes for the topic - the first was tactics that would make someone a better programmer immediately and the second was how to improve as a developer over time.  ### Short term tips to become a better programmer - **[Generalize at n=3](http://dangoldin.com/2016/04/07/generalize-at-n3/)**. Rather than coming up with the perfect abstract solution right away my rule of thumb is to start thinking about that on your third iteration of solving the same problem. This will ensure you’re solving a problem that will recur while giving you enough perspective to actually develop a useful abstract solution. - **[Think carefully about your database](http://dangoldin.com/2016/02/15/design-your-database-for-flexibility/)**. Compared to changing a database changing code is much simpler. Code is mostly stateless and you don’t need to worry about backfills or migrations. - **[Focus on interfaces, not implementations](http://dangoldin.com/2015/12/02/think-interfaces-not-implementation/)**. Instead of obsessing over the perfect implementation it’s more important to think about how your application works and the way it’s architected. This way you can always change the implementation of a single method or function to make it better without having to gut and rewrite the entire application. - **For dates and times, just use UTC**. A very common refrain online but only worry about timezones when displaying data to users. - **[Use GitHub for documentation](http://dangoldin.com/2016/08/14/integrating-poorly-documented-open-source-libraries/)**. Sometimes Documentation and StackOverflow don’t have exactly what you need. A good resource is to use GitHub’s code search and find actual examples of the relevant code being used.  ### Getting better over time - **[Learn to appreciate DevOps](http://dangoldin.com/2014/12/26/devops-for-the-rest-of-us/)**. Not many people love DevOps but I’m a strong believer in understanding how your code will run and be deployed. It gets you more familiar with the entire lifecycle and allows you to be more creative with your solutions. - **[Have a sample project to learn new languages](http://dangoldin.com/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/)**. In addition to tutorials I like having a project that I will implement in new languages to learn them. This repetition highlights the major differences between the languages and allows me to work on a sample application that mirrors my interests. - **[Approach code like you approach the gym](http://dangoldin.com/2016/03/13/approach-work-like-the-gym/)**. We spend more than 8 hours a day working but imagine if we approached it like we do the gym. Sure people that go to the gym every day without a plan are better off than those who don’t go at all but they pale in comparison to those that go with an agenda. How do we turn every line of code we write into something that’s as focused as a workout? - **Read the classics**. Despite being a relatively young field software engineering has had a ton of great books written and rather than spending time reading blog posts (including this one!) it’s worthwhile to go read the classics.
{% include setup %} I often find myself upgrading an open source to a newer version but I have a bad habit to only skim the release notes. More often than not an upgrade will work out of the box and you’ll get the immediate benefits of the newer version but every once in a while things blow up and you need to revert or scramble to get a fix out. Reading documentation tends to be dry with only a few relevant parts but when working on large systems it’s paramount to go through and understand the nuances of every upgrade. During my career I’ve run into a variety of issues that could have been avoided by a thorough reading of the release notes. There’s still a chance you’ll miss something and that’s why you should always have a sandbox environment and try to containerize as much as you can. Below are a few examples of issues I’ve run into upgrading various applications over the past few months:  - **Kafka 0.8 to 0.10**. This wasn’t a true upgrade but we wanted to spin up a parallel Kafka cluster that was a significant departure from our previous version. Kafka is a complicated application and we assumed that our code was backwards compatible. This was half-true. The code worked but it took a major performance hit that was [clearly document](http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact) in the release notes. - **Mongo/PHP**: This was a simple case of not reading the compatibility chart between the different versions of Mongo, PHP, and the associated drivers. If you’re running an old version of PHP you are limited to a subset of drivers that don’t support the latest version of Mongo. Once again this was discovered when messing around on our sand environment. - **Sentry**: Sentry’s a wonderful product that collects errors from every application you’re running and consolidates them into a single, slick UI. We wanted to get the benefit of a few additional plugins and decided to upgrade it to the latest version. Lucky for us there were some significant changes that required us to install a variety of build tools, including the C compiler. Unexpected but quickly remedied.  In our desire to get the latest and greatest we should be taking a step back to weight the benefits and the risks and looking at the release notes is a great way of understanding the potential impact. Even then it’s critical to have a separate environment to test different versions and a plan to roll back since it’s impossible to know what may actually happen on your unique system and configuration.
| |Array generation | 63.96| |Naive simple | 78.74| |Naive smart | 71.13| |Filter single | 82.19| |Filter multiple | 81.86| |Filter lambda single | 109.44|
{% include setup %} It’s that time of the year when many organizations are ramping up their donation efforts and I wanted to share the organizations I donate money to. I feel incredibly lucky to be where I am and being able to donate to worthy causes is a great way to pay it forward. Everyone is passionate about something and donating to that cause is incredibly worthwhile and valuable.  [Wikipedia](https://www.wikipedia.org/). The need for education is critical to a functioning society and unfortunately this has been magnified recently by the explosion of fake news. Wikipedia is incredible at providing factual information and I find myself visiting it multiple times a day. It’s both education and entertainment since it’s just so easy to get lost in its labyrinth. Out of all the tools and services I pay for Wikipedia offers by far the highest return.   [American Civil Liberties Union](https://www.aclu.org/). Especially over the next few years our rights will be incredibly important and the work the ACLU is doing is a key part in making sure they exist in the future. Democracy is being challenge around the world and keeping it safe is something we need to do for future generations. Rather than losing our liberties one small piece at a time the ACLU needs the ability to engage in the small skirmishes and battles that strengthen our freedoms.   [Electronic Frontier Foundation](https://www.eff.org/). Being in the tech industry I like to think I understand the dangers of technology better than most and the EFF has both an increasingly important and increasingly difficult job ahead. Technology has eliminated friction across the globe and democratized a wealth of information but the flipside is that it makes it incredibly easy for governments and agencies to monitor our digital worlds. This will be a bigger and bigger issue as more and more of our lives are captured digitally and the EFF is the bulwark keeping them secure.