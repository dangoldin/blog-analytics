{% include setup %} I'm finally in a position to do a "Year in Review" post that I’m comfortable writing. In March I left my full time job to pursue  Glossi , our startup, full time. In May, we were accepted into an  incubator  and had an amazingly productive four months. Unfortunately, none of us were passionate about the direction Glossi was headed and we’ve ended the year pursuing a new venture,  Makers Alley .  I learned a ton about myself but along with that came the realization of how much I don't know. Working as one of three cofounders on a startup forced me to develop a much broader set of skills. I understand the whole technology stack better and can actually make a passable website now (mostly thanks to  Bootstrap ) but I can also discuss trademark and copyright law with lawyers as well as do some tax planning with an accountant. I'm not too interested in pursuing law or accounting as a profession but it's great being able to follow along and chime in every once in a while with something useful.  More important than the skills, I've learned more about myself in the past year than I have in my entire professional life. I've discovered that money is not as important to me as I thought and that the control over my day is important to me, even if it does mean more work for less pay. I have a better understanding of my strengths and how they are best applied. I also know what my weaknesses are and am working on a few goals for 2013 which I’ll publish over the next few days. I'm still trying to figure out my exact passions but at least I'm starting to acknowledge that they need to be discovered.  Lastly, I want to acknowledge how lucky I am to even be in this position. It's wonderful having a  wife  and family who support me through all this, I know it can't be easy.
{% include setup %} Now’s the time people are making resolutions for 2013 so I’m going to join the club. I’m publishing them publicly since that should help my motivation. I’m also calling them goals since a goal seems harder to abandon than a resolution. I’m hoping that having these goals be specific, however arbitrary, will also help me in achieving them.  Here goes:    	  	 Run 1000 miles in 2013   	I’ve gotten out of shape over the past couple of months and that’s a bad place to be in the late 20s. I think it’s important to get good habits now since that will help me maintain my health as I get older. Not to mention that being in better physical health will improve my acuity. 	  	  	 Write at least 2 blog posts a week   	The more I work with various people the more I realize the importance of communication. Writing doesn’t come easily to me and I spend the majority of time editing but I’m hoping that it’ll be easier by the end of 2013. And although writing is just one aspect of communication, improving that will lay a solid foundation for the others. 	  	  	 Meet up with 2 old acquaintances each month   	Going from a company with hundreds of employees to working with a cofounder reduces the number of people you have contact with. By restoring my old relationships I’ll be able to connect with old friends and strengthen my network. Something I’ve learned over the past year of meeting with various folk in the NYC startup community is to end every meeting with an offer to help and I’m going to adopt that attitude as well. 	  	  	 Develop 6 side projects   	This one’s here for a few reasons. One, I want to keep on improving and starting a project from scratch is a great way to work with new technologies and explore different approaches. There are many times that I want to go back to my existing code and rewrite it but why fix something that isn’t broken? Isn’t it better to put that energy into something new? Two, I want to give back to the community and putting these projects on GitHub will hopefully help someone. Three, this will just be a good outlet for when I need a break from the main gig. Four, I want to build my personal brand and having more more of my work publicly available will hopefully help. 	  	  	 Start a hands-on hobby   	This one stems from a personal belief that I just need to do something with my hands since I spend so much time in front of the computer. This may end up being drawing, painting or woodworking but the goal is to find something that allows me create something physical and not digital. I’ve already dug up some colored pencils and drawing paper and am in the process of signing up for a woodworking class at a hands-on coworking space in Brooklyn called  3rd Ward .
{% include setup %} I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.  Hope they’re useful!      Avoid it if possible   It is a bit odd that I’m starting off with this as the first tip but if there are alternatives definitely take a look at those; many sites come with an API and that may be a much better approach. Otherwise, every time there’s a change in the HTML structure you run a risk of breaking your scraper which will leave you scrambling to fix your code. It’s also a good idea to organize your code such that a change in the HTML for one of the scraped items does not break the others. For example, if you want to get the name and address of a restaurant from Yelp, have one method that will get the name and another that will get the address. This will most likely be less efficient so you’ll need to use your judgement to see whether the risk-speed tradeoff is worth it.      Use a library   Unless you’re doing a one off job, use a library. Every major language has one: Python has  BeautifulSoup , Perl has  HTML::TreeBuilder , Javascript has  htmlparser , and there’s no excuse to not use one. If you ever need to go back to make some changes (which you most likely will need to), you’ll be glad you did. You can also find libraries that let you simulate browser behavior by storing cookies and letting you submit forms. This gives you the ability to scrape sites that require a login. Some sites try to prevent scraping by obfuscating their HTML a bit in which case you’ll need to do either a string replacement or a basic regular expression to get it parsed by the library.      HTML/DOM inspectors are a must   Since scraping requires getting specific elements from a web page, we need to understand the HTML structure of that page. For me, doing this work within the browser works best since it gives you the ability to both see the HTML that’s responsible for a certain element and also gives you a console window which lets you test a scraping approach. The two browsers I’ve used successfully for this are  Google Chrome  and  Firefox  with the  Firebug  plugin.      User agent spoofing   Every time your browser visits a website, it submits a request that contains information about the browser. This is why some sites show a different page when you’re using a phone versus a computer. Every once in awhile you will need to trick the site into sending back the proper page by “spoofing” the user agent. A simple way to check if you need to do this is to view the source of a page in a browser and compare it with what you’re retrieving in your code. If they’re different, try changing the user agent and see if that fixes it.      Be clever   Looking at the source of a page may be a bit overwhelming and there may be easier ways of getting at that information so be clever! An example of two approaches that I stumbled across were to spoof a mobile browser and to call the AJAX url directly. Spoofing a mobile browser tends to give you simpler and more lightweight HTML which is easier to parse. Loading the content via AJAX lets you get at the content quicker and usually in a more structured format, like JSON or XML. These approaches won’t work on every site so you need to do some research and experiment a little to understand how each site is setup. After that you can figure out the best approach for your scraper.      Be specific   When scraping, you want to make your scraping code rigorous enough to not fail if the page structure ever changes. A good rule of thumb is to be specific when you write your scraper. Use a specific id rather than a class since the id is guaranteed to be unique. Similarly, avoid an ordinal approach where you reference the 2nd or 3rd div. Sometimes this is unavoidable but try to see if there’s another approach. Another useful tidbit is to use the more content-descriptive identifier in the page. For example, if you see a div with the address you want to scrape and that div has two classes, “location-address” and “blue-highlight”, use the “location-address” one since that’s defining what the content is, not how it’s displayed.      Save the HTML of the retrieved pages   It’s helpful to save each HTML page you’ve retrieved. It takes a few iterations to get your scraping code working and it’s quicker to just have the HTML on disk so you don’t have to download it every time the script runs. Another advantage is that if you discover a mistake in your code, you don’t have to redownload all the pages you’ve already processed. It only takes a few minutes of work and worth doing.      Monitor actively   Scraping is prone to breaking so make sure you monitor the job as it runs. It’s likely that your code will work well on one page but will fail on others. I tend to write my code to be a bit picky at first while I work out the kinks and once I’m confident in it I will build in some logic to deal with a missing value to make sure it continues to run. As I mentioned earlier, storing the HTML of the page will save you time if you need to update your scraper and need to rerun it.      Throttle your requests   If you don’t want your roommates pissed pissed at you, which will happen when Yelp blocks you for 6 months, throttle your requests. The simple way to do this is to have your code wait in between downloading pages and another approach is to use proxies to hide your true IP address. This will make it seem that the requests are coming from a variety of computers and keep your roommates happy.
{% include setup %} The recent revolt around Instagram’s TOS changes got me thinking about the revolt against SOPA/PIPA and the impact social media is having on cultural participation. We’re wired to want to improve things and when we come across what we feel is an injustice we want to change it. Unfortunately, social media has made us lazy. Sharing something on Twitter or Facebook gives us the nice, warm feeling that we’re actively contributing to a cause. Instead of going out and demonstrating in public, snail mailing our representatives, or providing financial support, we’re clicking a link and think we’re making a difference. PIPA/SOPA wasn’t stopped due to internet outrage but from people calling their representatives and doing more than just mentioning their opposition. Wikipedia and Reddit didn’t just put a message up saying they oppose PIPA/SOPA but blacked out their site. Would the result be the same if they just had a message stating they opposed it?  Social media is great at raising awareness, it’s just not very useful until someone down the line acts on it. Does the increase in awareness lead to actual change? This simplicity and reach also leads to a massive number of causes being championed. I can’t login to Facebook without seeing some cause being shared and promoted. Causes now need to market themselves as much as a consumer product. Are we really better off?  I can’t help but think of the pen and sword metaphor. The pen is mightier than the sword because the pen is able to get many swords. Does social media just give everyone pens or does it lead to more swords?  My way of dealing with this is to take a real action every time I share something on social media. If I share a Kickstarter project then I will donate to it. If I share opposition or support of a bill, I will call my representative. We’d be in much better shape if everyone did the same.
{% include setup %}    Groupon  has fascinated me since they’ve launched. It popularized an entirely new business model, encouraged the launch of hundreds of competitors, and was able to IPO three years after being founded. This sounds great until you look at the performance after the IPO: the stock is down 80% and it’s consistently missing the quarterly goals.    The daily deals space isn’t as profitable as it used to be and they’re trying to become a tool platform for small businesses. To grow beyond daily deals, they’ve been on an acquisition spree. Over the past two years they’ve acquired a  scheduling startup , a  social shopping startup , a  POS system , and a  restaurant reservation system . I don’t think this will be enough for them to get seen as something bigger than daily deals.  Groupon grew by providing steep discounts to consumers but sacrificed businesses in the process. It will be a hard sell trying to get a business to use your tools when a few months ago you were telling them to discount their products more than 50%. I understand that they needed to do this to grow and I’m sure they even had a choice: the space was so competitive that if they didn’t do this someone else surely would have. It just puts them in a pretty tough spot.  Recent fast growing ecommerce businesses have also favored the consumer over the business. This leads to quick initial growth but causes problems in the long term.  Fab  is taking this approach as well by providing steep discounts on designer products. Consumers are loving it but what happens when there aren’t any businesses left who are willing to agree to such a discount? Sure, using Groupon and Fab can be viewed as a marketing expense but I suspect they and their investors want to be seen as more.  It’s difficult to balance the needs of the various sides of a marketplace. You do want to  subsidize one side  but it’s dangerous to favor the consumer side so much since it’s difficult to distance yourself from. Maybe it is the proper approach in the beginning but there needs to be a way to get out and I think that’s a tough problem.  Etsy  and  AirBnB  had slower growth but were able to align the incentives of the various sides of the market early on. It was easier for them since there’s overlap between the two sides (sellers are buyers and buyers are sellers) but I suspect this is still the right approach for long term success.
{% include setup %} I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with  matplotlib . The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one "gotcha" was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I'm more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I'd like to explore how my usage of Twitter has evolved and also get to play around with the  NLTK library .  I've committed by ugly code to github if anyone wants to play around with it:  https://github.com/dangoldin/twitter-archive-analysis . I know the code is ugly. I'll clean it up one of these days.                              Apparently, I like to tweet evenings and nights.                                       You can see I like to take my Fridays and Saturdays easy. Since I also tend to tweet more frequently at night this indicates I'll go out Friday and Saturday nights.                                       I was pretty much quiet since I got on Twitter in 2008 but have been more consistent since 2012.                                       I must admit this one's here mostly because I wanted to do a heatmap but this does reinforce that I've been a more active on Twitter since 2012 and that I'm less active on Friday and Saturday.                                       I started off barely saying anything but it looks as if I'm consistently around ~90 characters per tweet.                                       I wanted to see whether my behavior around tweeting, retweeting, or replying has changed over time but this doesn't make it very clear due to the number of lines going on so I decided to normalize it - see next chart.                                       Now we're on to something. In the beginning I was basically posting short tweets about my life but more recently I have been more involved in the community aspects.
{% include setup %} At Aaron Swartz’s memorial service in New York, Doc Searle said something that struck a chord: Aaron was one of the few tech people who would get involved in legal and political issues. It’s true - we hackers aren’t into it. We claim we’d be better off if there were more engineers in charge and yet we’re not making an effort to be those engineers. I’ve heard a variety of unconvincing reasons: it’s just not interesting; there’s too much bullshit; it’s more about selling than creating. I think the real reason is that we’re just too impatient.  Our roles and jobs have made us this way. Our work tends to have well structured problems that are solved through individual effort. Only when we have to rely on someone else do we become aware of how slow things move and how long things take. Even the agile methodology, for all its wonders, focuses on the short term and encourages small, easy achievable tasks. It’s no surprise that when we encounter something that takes longer than we’re used to that we dismiss it as not for us.  Impatience is also why I had difficulty as a product manager after coding for 5 years. I had a grand vision of what needed to be done but wasn’t able to execute it. I blamed it on the politics but it was really my impatience and immaturity. It was easier to work with developers to build the product features and just release them than it was to work with the actual users and get them on board. That would have required understanding their use cases, listening to everyone’s concerns, having a trial period, and all sorts of other things that would take too long.  Most of us do want to make the world better and do make an effort to contribute; we give up too soon. During the service, Roy Singham quoted Frederick Douglass:  “If there is no struggle, there is no progress.” Real progress takes time and we need to get comfortable with that if we want to see it happen.
{% include setup %} In November, I migrated my Tumblr and Wordpress blogs over to GitHub pages and have been making a few tweaks here and there. I started with the awesome  Jekyll-Bootstrap library  but wanted to share the changes I’ve made. It’s all hosted on GitHub so feel free to fork it.           Design changes              The version I started with didn’t have the Bootstrap responsiveness library so I added that in       Since I’m using it primarily as a blog, I updated the design to emphasize the blog aspect       Consolidated the pagination and social sharing widget to fit on one line       Incorporated some best practices from  Kaikkonen's blog typography guide                  Backend changes              Small improvements to SEO by giving ability to add keywords to each page       Added Open Graph meta tags to control what’s displayed when people share the page on Facebook       Made a few tweaks to the way the sitemap was being generated
{% include setup %}            A Patent Troll        Newegg recently defended  itself against a patent troll that sued them over a shopping cart patent. As a result, the patent was invalidated and Soverain Software will lose $2.5M from this and the $18M they won in 2011 from Victoria’s Secret and Avon. Unfortunately, they’ll still keep the tens of millions of dollars they “earned” in earlier years. Since virtually every ecommerce site has a shopping cart feature you’d think that this patent would have been invalidated sooner.   The reason it takes this long is that most companies settle when faced with a lawsuit and only a few fight back. Over time, companies that have a reputation for fighting back are sued less frequently and companies that do settle just pass the cost onto the consumer. It’s no surprise that these patents end up sticking around. Unfortunately, it’s a shitty situation for smaller businesses: they can’t afford a lawsuit and can’t afford to raise their prices.  What can we do to change these incentives around? Right now, a big advantage patent trolls have is that they make the first move and can choose who to sue and who to avoid. Why not bring the fight to them? A simple approach would be to find these these flawed patents and file for a  reexamination  with the USPTO.  Ask Patents  has already started collecting a database for prior art to challenge patent applications but this information can also be leveraged to challenge already granted patents. Another option would be to sue the patent troll directly, as Microsoft and Google  have done , which has an added benefit of a jurisdiction other than  East Texas . An extreme approach would be to create shell companies that intentionally violate these patents in order to challenge them. Imagine a hackathon whose sole purpose is to create sites and companies that violate these patents in order to troll the troll. Why not bring the fight to them?
{% include setup %} I’m making an effort to freshen up and improve my data skills so when I found out that two of my friends were going to take an  R class  on Coursera, I joined them. The class is pretty typical for an online programming class: each week there are a set of lectures to watch, a quiz to take, and a programming exercise to do. In addition to this, we also have a weekly Google Hangout to discuss the lectures, go over our programs, and share our R questions.  I realize we’re still at the dawn of online education but it feels as if the class has simply been moved from the classroom to the web, without any thought as to how the class can be structured to make the best use of the web. So far, the Google Hangout paired with the programming exercises is the most valuable. The programming exercises provide the structure and the Google Hangouts help us absorb the material better. We are able to share our solutions, analyze the pros and cons of the different approaches, and by explaining why we solved them a certain way we end up understanding the material better ourselves. Why can’t online classes be designed to take advantage of this? I understand that not everyone has someone to take a class with and yet having a partner provides a big benefit.  These online programming classes should take a lesson from  Project Euler . Project Euler is a series of puzzles that require both a mathematical and programming insight to solve. The brilliance is that they have forums for each problem that you can only access after solving the problem. But once you gain access, you can see other solutions, learn from them, and pick up tricks and approaches that you’ll need to solve future problems.  Pairing the structure provided by a class with this Project Euler community would create better online programming classes than we have now. My ideal online programming class would have the following structure:      No video lectures . The content can be better presented through text and visuals and helps people work on it at their own pace.    Each lesson would be focused on a particular problem . Start by describing a problem and then spend time going over different ways of thinking about it as well as the different tools available. This should be as interactive as possible where students can follow along by running the code on their own computer.    Limited time spent on defining terms . People are sitting in front of the computer and can do a search on Google or Stackoverflow to get more and better definitions than can be covered in a class.    Leveraging the volume of people taking the class . When you have thousands of people taking a single class you can do things that you just wouldn’t be able to do normally. For example, you can analyze people’s programming solutions to see which solutions are the most common, which run the quickest, or which are the shortest. Having that information available to students would be much better than just getting a numerical score.     This approach isn’t for everyone but that’s the point. Moving online will give us the ability to have classes custom tailored for each student but we need to start by thinking about building classes for the web from scratch rather than copying them from the classroom.
{% include setup %} This past weekend I participated in the  Bicoastal Datafest  hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn't be very easy, especially in two days, but I did meet  Llewellyn , a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.  We ended up with the following approach that we were able to code up during the weekend: 1. Use the OpenStates API to get the URL of the bills 2. Download the bills and convert each to raw text - from PDF and HTML 3. Extract 8 word phrases from each bill, excluding stopwords 4. See which phrases were duplicated across states 5. Examine the duplicate phrases to see which bills are most likely duplicates  Somewhat surprisingly, this approach led us to discover the following duplicate bills:   Firearms Freedom Acts   Shared the phrase:  manufactured without inclusion significant parts imported another state                                Indiana                                                        Michigan                                 Prohibit US government officials from enforcing firearm-related acts   Shared the phrase:  accessory ammunition owned manufactured commercially privately state remains                                Arizona                                                        Tennessee                                 Prevent pharmaceutical substitution of opioid drugs   Shared the phrase:  bear labeling claim respect reduction tampering abuse abuse                                New York                                                        New Jersey                                The code's up on  Github  so if you have any ideas or improvements - contribute and help out. In two days we were able to get something useful done and it's exciting to see what we can discover if we stick with it.
{% include setup %} I was excited to read that Coursera can now offer classes for college credit. I’m optimistic that this is a start of a trend that will change higher education. At first, this will be adopted by the motivated student - the one who took AP classes in high school and the price sensitive student - the one who took community college classes before transferring to a university. But over time, this will spread until it’s the dominant approach. It’s simply better. It may be more expensive to get a digital class together but it’s primarily an upfront cost that will be spent on getting the best lecturers, developing engaging lectures, and creating varied course content that’s optimized for different learning methods. This content can be accessed from anywhere and is always available.  These classes will serve as building blocks for entirely new curriculums and courses of study. Dependencies between classes will be created so if you’re having trouble with a part of one class you can quickly go to another class that covers that topic in more depth. Companies can get involved as well and help bring the costs down. For example, a company can subsidize a classes and in return be allowed to recruit students who score highest on various sections. Companies can even create their own funnels based on the performance over a set of classes. I don’t know if this is the right approach since it will lead to corporate influence in education but I’m excited by the opportunities. This is something prior generations never had and just a shadow of what future generations will have. I'm just glad it's so easy to keep on learning after college.
{% include setup %}  On Saturday, I finished  Team of Rivals  and while looking at my calendar noticed that it was also  Lincoln's  birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the  7 volume set  containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.  I was able to get 1,458 documents containing almost 16,500 sentences and a little over 547,000 words. I tried getting the date each letter was written or speech was given but was only able to get it for 60% of the documents. That was enough to get some insights.   Number of speeches/letters by year   I suspect a lot of his early writing and speeches and were lost since they just weren't preserved as well as his later speeches and letters        Trend of phrases   I wanted to examine the phrases that he most commonly used over time in order to see whether there were any noticeable changes and whether they meant something. Turns out there was some interesting stuff here that's highlighted in green.           Slavery  - There are references to slavery across the entire date range with the  Dred Scott decision  and the  Missouri Compromise  appearing as common phrases in the 1850s.        Civil War Generals  - You can trace the career of the generals during the Civil War based on their mentions.  General Hooker  was mentioned in 1862 and 1863;  General Meade  in 1863 and 1864; and  General Grant  in 1864 and 1865. This echoes history: General Hooker was replaced by General Meade in 1863 with General Grant being in command of the Union Army in October of 1863.        The Presidency  - When Lincoln was elected president in 1860, he started finishing his letters with the phrase "Lincoln, President of." During the presidency we also see mentions of his cabinet:  Stanton  and  Seward .      *The table below was generated by looking at the top 20 three word phrases used in each year range and then consolidated into a top 100 list across the entire dataset. The X indicates that the phrase was in the top 20 three word phrases for that year range. I highlighted the interesting rows in green.            phrase   1832-1845    1846-1853    1854-1859    1860     1861     1862     1863     1864     1865               the united states    X    X    X    X    X    X    X    X    X           of the united    X    X    X    X    X    X    X    X    X           i do not     X    X    X    X    X    X    X    X    X           the secretary of          X    X         X    X    X    X    X           secretary of war          X              X    X    X    X    X           in regard to     X    X    X         X    X    X    X    X           the people of    X    X    X    X    X    X    X    X    X           of the people    X    X    X         X    X    X    X    X           president of the     X    X    X    X    X    X    X    X    X           in favor of      X    X    X         X    X    X    X    X           my dear sir      X    X    X    X    X    X    X    X                as well as   X    X    X         X    X    X    X    X           so far as    X    X    X    X    X    X    X    X    X           dred scott decision                X                                         there is no      X    X    X    X    X    X    X    X    X           by the president          X    X    X    X    X    X    X    X           the supreme court    X    X    X         X         X                     united states and         X    X    X    X    X    X    X    X           of the union     X    X    X    X    X    X    X    X    X           that it is   X    X    X    X    X    X    X    X    X           it is a      X    X    X         X    X    X    X    X           that judge douglas             X                                         the dred scott             X                                         that there is    X    X    X    X    X    X    X    X    X           institution of slavery   X         X         X    X                          secretary of state        X    X    X    X    X    X    X    X           the missouri compromise                X         X                               to say that      X    X    X         X    X    X    X    X           of the state     X    X    X    X    X    X    X    X    X           the state of     X    X    X    X    X    X    X    X    X           of the government    X    X    X    X    X    X    X    X    X           major general mcclellan                          X    X    X                     of the country   X    X    X         X    X    X    X    X           secretary of the          X    X         X    X    X    X    X           of the army           X         X    X    X    X    X                it is not    X    X    X    X    X    X    X    X    X           of the potomac                       X    X    X    X    X           part of the      X    X    X         X    X    X    X    X           one of the   X    X    X    X    X    X    X    X                united states to               X    X    X    X    X    X    X           washington d c                       X    X    X    X    X           house of representatives     X    X    X    X    X    X    X    X    X           as to the    X    X    X    X    X    X    X    X    X           harper s ferry                       X    X    X    X    X           the public safety                        X    X    X    X                major general hooker                              X    X                     the gentleman from   X    X    X                                         lieutenant general grant                                        X    X           major general halleck                             X    X    X                major general meade                                    X    X                of the enemy          X              X    X    X    X    X           the union and         X    X         X    X    X    X    X           the day of   X    X    X    X    X    X    X    X    X           the president of          X    X    X    X    X    X    X    X           the rio grande        X                                              the senate and   X         X         X    X    X    X    X           to the senate              X         X    X    X    X    X           army of the           X              X    X    X    X    X           city point va                                       X    X           and house of                         X    X    X    X    X           executive mansion washington                         X    X    X    X    X           of the treasury           X              X    X    X    X    X           of the secretary                         X    X    X    X    X           of the bank      X         X                                         of the public    X    X              X    X    X    X                of the war        X    X         X    X    X    X    X           yours very truly               X    X    X    X    X    X                as may be    X    X    X         X    X    X    X    X           he did not   X    X    X              X                          lincoln president of                    X    X    X    X    X    X           m stanton secretary                               X    X    X    X           stanton secretary of                              X    X    X    X           the war department        X              X    X    X    X    X           i shall be   X    X    X    X    X    X    X    X    X           william h seward                    X    X    X    X    X    X           edwin m stanton                               X    X    X    X           for the purpose      X    X    X    X    X    X    X    X    X           general grant city                                      X    X           i have been      X    X    X         X    X    X    X    X           is to be     X    X    X         X    X    X    X                it will be   X    X    X         X    X    X    X    X           it would be      X    X    X         X    X    X    X    X           of all the   X    X    X    X    X    X    X    X    X           of the department         X              X    X    X    X                the post office      X    X              X    X    X    X                the public lands     X    X                   X    X    X                yours of the     X         X    X    X    X    X    X                at p m                            X    X    X    X           grant city point                                        X    X           h seward secretary                  X    X    X    X    X    X           i have no    X    X    X    X    X    X    X    X    X           in relation to   X    X    X         X    X    X    X    X           seward secretary of                     X    X    X    X    X    X           that i have      X    X    X    X    X    X    X    X    X           as follows to    X    X                   X    X         X           dear sir yours             X    X    X    X    X    X                sir yours of     X         X    X    X    X    X    X                dear sir i        X    X    X    X    X    X    X                ought to be      X    X    X    X    X    X    X    X    X           of the is    X    X    X    X    X    X    X    X                Phrase word clouds   I tried visualizing the table above as word clouds but in hindsight don't think it was the best way to display the data. It did give me an excuse to play around with  D3 library  though.       As usual, the code's up on  Github .
{% include setup %} What’s lost in the  Tesla/NY Times discussion  is how much information Tesla is collecting. Tesla collected the location, the speed, and the battery charge throughout the journey and referenced it during the rebuttal. Is Tesla collecting this data for every car sold? Do the drivers know this data is being collected? If John Broder knew Tesla had this data from his drive  his review  would have turned out differently. We’re all in favor of truth and honesty in reporting but should it be this easy to share data? What prevents Elon Musk from digging into the driving data of a politician who proposes some legislation that will adversely impact Tesla and finds likely unethical behavior?  As  software eats the world , data will be collected from more and more areas of our lives. Target is already  figuring out  whether you’re pregnant and this is just from using your purchase history. Combine that with other data sources, increased computation power, and cheaper data storage and companies end up knowing us better than we know ourselves. We need to make sure that our privacy evolves alongside the data. Currently, the concept of data privacy is too abstract to make us care. We need to see the actual data and the derived results in order to see how valuable it is. Only then will we want to protect it.  Disclosure: I love what Tesla is doing and own Tesla stock. I also realize that this data is used to offer a better, cheaper product. At the same time, I believe we need to find the right approach to privacy when it comes to our data.
{% include setup %}                                    Last week, my wife and I took a vacation to New Orleans and it was the first time we used Foursquare to plan a trip. I asked friends for suggestions, looked at other Foursquare lists, and did some online research to create  my list  of 25 places that I wanted to visit while there. These places ranged from tourist magnets such as Bourbon Street and Cafe Du Monde to the more local places like Cafe Envie and Port of Call. Our typical approach in New Orleans was to go to a neighborhood we wanted to explore and then visit the places that were nearby on our Foursquare lists. Out of the 25 places I had on my list, I ended up visiting 16 which is a bit low but I made up for that by visiting a bunch of local places that I wouldn’t have discovered otherwise.  Before I had a smartphone, I remember drawing a map every time I went somewhere new so I wouldn’t get lost. I’m no longer doing that but still need to come up with a rough plan of when to visit the various places. The next step would be something that takes my list and applies a route finding algorithm to come up with an agenda that gets me to visit all the places while taking into account distance, open hours, and the venue type. Combine this with  Google Glass  and you get  a pretty awesome way of exploring a new city. I’m excited.
{% include setup %}            Over the past year, I cofounded two startups and launched a bunch of side projects. Since they all had potential, I wanted to make each as standalone as possible and ended up with dedicated accounts for each. This meant that I had a flood of accounts for each, ranging from the various Google products to Sendgrid and AWS to Freshbooks and Quickbooks. Unsurprisingly, this turned out to be an unmanageable pain in the ass.  It gets worse. We ran into a trademark issue and had to change our company name from Glossi to  Pressi  and transfer our branded assets. This meant handing over our domain and since we’re heavy users of Google’s products, losing access to our email, our documents, and our calendars. To migrate, we had the great fortune of having to forward the important emails and share the important documents to our new account.  This led to me an epiphany that we’re using today. Only have unique email. Everything else can be managed through individual accounts until it’s necessary to create company accounts. And even then, only create accounts that are absolutely necessary, which will typically be the financially dependent ones (Freshbooks, Stripe, etc). This allows us to not worry about having a flood of Google tabs open and we get to avoid the adventure of figuring out whether a doc we’re looking for has been shared on a personal or company account. When something does need to be shared with someone outside the company, we share it with our corporate clone and manage it from there.  By no means is this a perfect solution but it works for me and I only wish I stumbled unto it sooner. How do you make it work?
{% include setup %}            I’ve been a fan of Bruce Schneier ever since I read his  post about security theater  in the post 9/11 world. As soon as I discovered that he wrote a book,  Liars and Outliers , I added it to my to-read list and just finished reading it over the weekend. It’s one of those books that is obvious as you read it but spawns a ton of thoughts. He develops a framework that he uses to analyze security and trust in individuals, organizations, and differently-sized societies.  Trust is the foundation that’s allowing the world to become faster paced and interconnected. We’re interacting with people all across the globe, our organizations and businesses are larger than ever, and we’re more dependent on technology than ever. Modern life depends on these complex trust systems and Schneier does a great job explaining the various interactions and the impact technology is having. As others have said, the 21st century will be about data and the rise of social networks, wearable computers, and the quantified self movement are an indicator of the type of data that will be collected. We need to make sure proper systems are in place to prevent abuse and Liars and Outliers provides a great framework to think about these issues and prepare us for the data century.
{% include setup %} In old versions of the iOS App Store, every time you downloaded a new app it would close the App Store and navigate to the screen with the now-downloading app. Recent versions of the App Store keep it open and force you explicitly exit. I’m surprised that the App Store didn’t launch with the new behavior - it must have been a conscious decision since the development effort for both seems similar.  The only time the original approach is faster is when users intend to download a single app. I suspect this is actually the most common scenario and someone at Apple decided to design the App Store to optimize for it. Unfortunately, they didn’t consider the frustration of having to download multiple apps - even if one only does this a fraction of the time. Training yourself to hit the home button after downloading a new app is a lot easier than training yourself to scroll to the App Store icon, clicking on it, and resuming the app search. Even if 90% of the time a user is only downloading a single app, the other 10% matters is significant given a large enough cost. One can forgive Apple for launching the iPhone with this behavior - no one knew how people would use the App Store. But why did it take years to release the update?
{% include setup %} I recently came across Jeff Jordan’s  post  about revamping the post office so it’s no longer losing more than $5 billion a year. Jeff suggests the obvious solution of raising prices but I think a more clever approach would be to start price discriminating. Everyone who needs to mail a letter has to pay 46 cents for a stamp but why not come up with tiered pricing. People who need to send something urgently can pay more than a dollar while others who only care that the letter arrives can pay 20 cents. The postal service would need to ensure their systems are able to track how full or empty each shipment is but this would allow them to ship the cheaper, less urgent mail with the more urgent mail to maximize the shipping space. Another way to price discriminate would be to give a discount for mail that’s picked up at the post office within a few days rather than being delivered to the home.  I took a quick look at the USPS  financials for 2012  and if the average price of a first class delivery increases from 42 cents to 52 cents, the post office would be profitable given the same volume. I realize that’s a 24% jump in price but if it’s done via a price discriminatory approach, such as introducing multiple price points based on delivery guarantees, it won’t feel as drastic.                                 Service Line               Revenue               Volume               Unit Price               New Price               New Revenue                                               First Class               $28,867               68,696               $0.42               $0.52               $35,721                                   Standard               $16,428               79,496               $0.21               $0.21               $16,428                                   Shipping + Packages               $11,596               3,502               $3.31               $3.31               $11,596                                   International               $2,816               926               $3.04               $3.04               $2,816                                   Periodicals               $1,731               6,741               $0.26               $0.26               $1,731                                   Other               $3,785               498               $7.60               $7.60               $3,785                                   Total               $65,223               159,859               $0.41               $0.45               $72,078                     Airlines have been price discriminating since Sabre launched in the 60’s, coupons have been around for 100 years now, and retailers have been offering discounts on out of season items for even longer. Hardware and software improvements are streamlining operations all over the place and are allowing companies to price more efficiently than ever. I’d love to see the government do the same.
{% include setup %} Entrepreneurs are familiar with the elevator pitch. The idea is to give a pitch in 30 seconds (the duration of an elevator ride) that is compelling enough to an investor that it leads to a follow up meeting where you can go through your pitch deck. An entrepreneur coming up with an elevator pitch is similar to a politician trying to come up with sound bites that are easily digestible, look good on the news, and stick in people’s minds.  Why are we so intent on diluting our message? So much substance is lost when we simplify and condense. We mock politicians when they speak in sound bites and yet we do the same thing when we pitch investors. We both want to draw attention to ourselves and stay top of mind but why take shortcuts? Investors will come to you if you build a great product, get customers, and generate revenue. Voters will support you if you empathize with them and support their community. We need to stop looking for the easy way out and just do the work, success will follow.
{% include setup %} Reading  Katie Zhu’s post  on NPR’s news app architecture got me curious about a setup where most of the content is static and can be hosted on S3 and EC2 is primarily used to generate the static content which is then uploaded to S3. The benefits were obvious:      Cost:  S3 is cheaper than EC2.    Reliable:  S3 doesn’t go down near as frequently as EC2.    Scalable:  Since it’s primarily static you don’t have to worry about additional capacity or dealing with caching, databases, and all the other fun things.    Simpler:  There are no weird server issues here. As long as you generate the right content and your rendering is good, you don’t need to worry about a web server acting up.     I’ve been meaning to write a script that would scrape Hacker News in order to show me the top content I missed while sleeping. I had some time this weekend and decided to give it a go using this “pseudo-static” approach. The result is called Yet Another Hacker News Reader ( YAHNR ) and you can take a look at the code on  GitHub . Turns out it was pretty simple to write and the most difficult part was thinking differently about the problem. Whereas I’d keep the content in a database I ended up storing them in static JSON files and instead of having the logic to generate the HTML page live on a web server I have it using Mustache templates.  I’ve become a fan of this approach and think every developer should try it out. It offers a new perspective and most apps will have some components that’ll be able to leverage this sort of setup. Right now, if you run a static blog and want comments, you can use  Disqus . You can use  Firebase  to build entire web apps that do all the work on the client side. As more and more services become available via Javascript, this approach becomes more and more practical.
{% include setup %}            A brief one today.  This past week, Sandy and I have been super busy getting a new version of  Makers Alley  out that allows you to customize and buy furniture. We’re launching with two makers that have items for sale but we’re busy adding more.  Withers &amp; Grain  specialize in using reclaimed wood from the 5 boroughs and do their own wood and metal work.  Mark Grattan  is a furniture designer who has designed a furniture collection for Makers Alley in a geometry-inspired style. Take a look at their pages, watch their videos, and customize their pieces. If you have any feedback let me know.
{% include setup %} At the beginning of 2013, I set a  goal  to do something with my hands to contrast with the constant life in front of the screen. I finally finished my first “art” project this past weekend and documented the result. I had a stash of old, torn jeans that were just taking up space and instead of throwing them out I decided to have some fun. Here’s the process and end result.                              1. Find a silhouette of the NYC skyline                                      2. Replicate the silhouette by cutting pieces out of old jeans and arranging them on a piece of cardboard wrapped in another shade of jeans.                                      3. Use some fabric glue to attach the pieces to the canvas                                      4. The result - now all it needs is a frame
{% include setup %} Last week, Andrew “weev” Aurenheimer, was sentenced to 41 months for going through publicly accessible AT&T URLs which exposed the email address of 114,000 iPad owners. I don’t want to talk get into the absurdity of the sentence or how AT&T should be the one held accountable for this “ security .”  I’m more interested in the fact that people still consider an email address to be private information (although I do realize that the leak also revealed iPad ownership information). This may have been the case years ago when we arrived on the internet but right now, our email addresses are everywhere. We give it to every new website we sign up for and we display it proudly on our websites. I’m sure my email address appears on dozens of spam lists for sale on the internet. Google already gives 3000 results when I search for my email address.  The definition of what is and is not private changes as a society evolves. Technology has been increasing the pace and society has yet to catch up. Most of the people in the tech world are pretty aware of the trends but the majority of people are surprised by how much information they’re sharing whenever they touch a digital device. And it’s only going to get worse. If we’re this concerned about our email addresses, how will we feel when people use Google Glass to look up our Facebook or LinkedIn profiles just by looking at us?
{% include setup %} A recent trend in website design I’ve been seeing is the long single page. My first distinct memory of seeing it is from  Karma  but I’ve started noticing it everywhere. It runs the gamut from non-profit  causes  to video game  PR firms . In fact, we’re even using this approach for the Makers Alley  homepage .  Surprisingly, it’s starting to make inroads on news sites as well. Whereas before news sites would have an article spread across 20 pages (looking at you Business Insider) in order to increase page views and show more ads, some news outlets are actually improving the user experience. Both  NPR  and the  Washington Post  have posted pieces that leverage this approach and it creates a significantly more engaging read. It’s a pleasure reading long form content this way, richer media adds to the experience and minimizing mouse clicks avoids the disruption of a page load.  I hope it stays.
{% include setup %}           After a couple of weeks on the waiting list I finally got access to the Mailbox App. It’s a huge improvement over the standard mail app and my mobile email consumption habits have improved significantly. I’m still not at “inbox zero” but am making my way there.  I don’t know much about iOS development but one thing I’ve been wondering about is whether they could have written it to not use a remote server. Regarding scaling Mailbox, they  wrote :    A critical part of Mailbox scaling is its brand new infrastructure. Mailbox relies on servers in the cloud to do things like send push notifications, download email as fast as possible, and handle “snoozed” messages.    From reading developer docs, it does seem you need a server to do push notifications but I wonder if there’s a way to schedule notifications on the client side. Conceptually, there’s nothing the server needs to do that can’t be done client side via simple polling. This way, the server load becomes non existent and scaling issues are avoided. Even a hybrid approach could have worked: use the server approach when possible but fall back to polling if the servers are overwhelmed. I just can’t help but think that there must have been a way to have the same functionality client-side. The cynic in me wants to say that it was done to build up hype but it’s equally likely that I just don’t know iOS development. It seems odd that crippling an app would help with marketing. I hope that’s not what it takes to sell for $100M.
{% include setup %}            A few days ago, Google made the new compose default on Gmail. It went from a separate page to a popup that’s accessible from anywhere in Gmail. And for the vast majority of the time, it’s better: it’s quicker to get to and makes it easy to reference other emails while writing a new one. Unfortunately, for attaching an image (not embedding it inline) or doing some heavy formatting, it’s a huge step backwards and makes me want the old compose back.  I’m sure the data backed up the decision. Only a tiny fraction of all messages needed this additional functionality so why worry about it? The problem with this approach is that even taking into account the infrequency, the cost of the workaround is large enough to cause a usability problem for the power users. It’s akin to the  old version  of the iOS App Store that would close itself every time you downloaded a new app. Sure that was great when you only wanted to download a single app but it made every other scenario significantly worse.  In the rush to be data driven, we shouldn’t forget the actual users and what they’re trying to do. A data driven approach should be used to improve our understanding, not replace it. Otherwise, we run the risk of “nice to have” features replacing the “must have” ones.
{% include setup %}           During Passover, Coke and Pepsi sell sugar based versions of their sodas in order to stay kosher for Passover. These high fructose corn syrup (HFCS) free sodas are extremely popular and people stock up while they can. I don’t know whether this is due to the better taste, the nostalgia, or the limited supply but these sugar based versions are definitely more popular. I wonder what would happen if either Coke or Pepsi decided to go “all in” on sugar and launch a marketing campaign against HFCS based food and drinks. I’d love to look at the margins of sugar vs HFCS based sodas and see what the market share increase would need to be in order to offset the switch to sugar. My gut tells me that pursuing this strategy would be a win but the companies are too entrenched in their current process that it’s just not going to happen. Smaller soda manufacturers, such as Boylan, GuS, and Moxie, are growing by differentiating themselves from the big guys and are emphasizing the healthier ingredients. I’m hopeful that this will pressure Coke and Pepsi to make their soda healthier. Unfortunately, what’s more likely to happen is that they will just acquire the niche manufacturers position them to appeal to the more concious consumer, similar to what’s happening to  craft breweries .
{% include setup %}            I’ve been interested in the Raspberry Pi ever since I first saw it mentioned in the tech news and finally got to play with it over the past few days when my brother (thanks  Simon !) lent me an extra one he had. I’ve been in need of a better media center setup ever since my DisplayPort cable stopped working so I decided to try out Raspbmc, a Raspberry Pi based media center.  I scavenged an SD card from my camera and a microUSB AC adapter from my old Droid phone which I somehow still had lying around. With those two, I was able to install Raspbmc but couldn’t get any farther without a wifi adapter. It took the wifi adapter a few days to get delivered but it worked right out of the box and I had a functional media center. Surprisingly, I didn’t need a keyboard at all and was able to run through the entire setup using SSH and a downloadable iPhone app that acts as a remote. The most time-consuming part was setting up a Samba shared folder under Mountain Lion and adding it to Raspbmc using the onscreen UI.            It works well. It solves my “must-have” problem of using my TV to play videos that are on my computer and also has a bunch of “nice-to-haves”. The two big ones are AirPlay support which allows streaming of audio and video from iOS devices and the ability to use my iPhone as a remote. Only thing left is getting an enclosure so it’s not just lying on the floor.  Here’s the setup:          Raspberry Pi  - $25 or $35 model        Wifi Adapter  - $10 on Newegg       microUSB AC Adapter - I found one but should be around $5       SD Card - I had one but can find one for around $6 on  Amazon        HDMI Cable - $2 and up on  Monoprice
{% include setup %} While reading  The Idea Factory , I came across an interesting passage that explained why cell phones don’t have dialtones:   Meanwhile, Phil Porter, who had worked with [Richard] Frenkiel on the original system, came up with a permanent answer to an interesting question. Should a cellular phone have a dial tone? Porter made a radical suggestion that it shouldn’t. A caller should dial a number and then push “send.” That way, the mobile caller would be less rushed; also, the call would be connected for a shorter time, thus putting less strain on the network. That this idea—dial, then send—would later prove crucial to texting technology was not even considered.    It’s amazing that although this suggestion was made in 1971, we’re leveraging it more than 40 years later with text messaging. How many other technologies and businesses are built on top of SMS that wouldn’t have existed without this decision? I’m sure an SMS-like technology would have come along regardless of this decision but it still makes me wonder how significantly past technological decisions influence us in the present.  An additional meta thought: this is an example of one of those things that gladly lives in the subconscious that has no reason to bubble up to consciousness. I’m sure if someone were to ask me point blank to compare dialtones between landlines and cell phones I’d immediately get it but without a push I never would have thought of it. I wonder how many other connections there are stuck in our heads waiting for a spark to bring them into our consciousness.
{% include setup %} This past weekend I wrote a  small jQuery plugin  that automatically inserts a “Follow this discussion on Hacker News” link on a recently submitted web page. The motivation was to automate the current workflow that consists of first submitting a post to Hacker News, getting the URL of the comment thread, and then updating the original post to link to the thread. I also wanted to see if it could be done entirely in Javascript so that the code could be included on static HTML pages and not require a backend server.  After some research, I settled on the following approach:  1. Use  Firebase  to store a mapping of URL to the Hacker News thread id. I chose Firebase since it provides a way to read/write using Javascript. 2. Use the  HN Droid API  to retrieve recently submitted HN posts for a given user via JSON. 3. If any of the recently submitted posts match the provided url, store that thread id in Firebase and execute the user defined callback function.  It works as expected but has a few limitations:  1. It relies on Firebase and doesn’t use authentication so someone can modify the database to point to another comment thread. 2. It relies on a 3rd party Hacker News API so if that ever goes down it won’t be able to pull recently posted links to Hacker News. 3. The HN API call only pulls the most recent submissions so the plugin will not be able to get the comment thread for older posts. 4. Since Firebase prevents certain characters from being used in a key, I do some string replacement to clean the string which would allow someone to cause a string collision. 5. The HN API isn't real time and uses a cached version so it may take a bit of time for the link to get retrieved. 6. The code hasn’t been thoroughly tested so may have some weird errors. It’s also my first “real” jQuery plugin so it may not follow best practices.  In general, I’m a proponent of offloading as much work as possible to the client side and believe this will become the norm as the technology improves. We’re already using Disqus to handle comments and Firebase as a database and I expect more services to become available via client side Javascript. This will keep pages simple, reduce server costs, and outsource non-core components to specialized vendors.
{% include setup %} A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.         The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday.                       The post ended up being featured in the NY Times  Bits blog  which accounted for ~2,900 of the total visits; the  Gizmodo network  which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.       The way it spread is pretty interesting: I submitted to HN on Friday afternoon, it was picked up by the NY Times Bits Blog that evening and Gizmodo US on Saturday. After that, it expanded to the rest of the Gizmodo network, including the  UK  on Sunday and  France  on Tuesday.  CNET  and the  Guardian  both picked it up on Monday.       Gizmodo added an Amazon affiliate link to the book I quoted, The Idea Factory, but did me the favor of linking to my startup, Makers Alley. I suppose that makes us even.       Only 80 people ended up visiting the  Makers Alley  site, which is 1/3rd of one percent of the total visitors. These visits were pretty evenly split between the link in the Gizmodo article and the link from my blog.       I have no idea why it took off and don’t consider it one of my better posts. I basically quoted a passage from a book and added a bit of my own commentary. I suspect the topic was appealing due to nostalgia and a bit of geek lore.       It’s surprisingly hard to get on to the Hacker News home page these days but it does drive a significant amount of traffic. I joined HN five years ago and it was orders of magnitude easier to end up making it to the main page.       If you write, do it for yourself and not for the recognition. And if you don’t write, start writing. Nathan Marz has a  great post  that everyone who's interested in blogging should read.       It’s great having my blog hosted on Github pages. It’s free and I don’t have to worry about server load.
{% include setup %} Last week, three isolated events gave me a glimpse of how powerful mobile can be. Tech pundits have been saying that for a while now but experiencing it firsthand is definitely more convincing.         I went for a run with only my phone to keep me company. After my run was done, I wanted to grab a cup of iced coffee and realized that Starbucks gave me a free drink on my birthday. Downloading the app on my phone allowed me to get a drink without having cash or a wallet.       While checking out at a grocery store, a friend showed me CardStar which allowed him to store all his loyalty cards on his phone. Since then, I’ve imported all my loyalty cards that have just been sitting in a drawer into my phone.       After getting a Raspberry Pi and installing Raspbmc, I was able to use my phone as a remote control just by downloading an app.     These behaviors are different and yet they’re all converging on the phone. What they have in common is that  software is replacing hardware . Hardware doesn’t need to become smarter, it just needs to be able to sync with our phones which can do the heavy lifting. The functionality then becomes limited by software which can be updated more cheaply and quickly than the hardware. It also solves the problem of hardware companies trying to develop software that results in a terrible user experience. Do cars really need the ability to  read a Twitter  feed? And if they do, why not just do it via a simple Bluetooth connection and an audio streaming app on a phone?
{% include setup %} In the early 90s, being a kid new to the US and new to computers I developed an addiction to computer games. I’d play everything that I got my hands on and remember sharing floppy disks with school friends. Unfortunately, I was plagued by two issues that had pretty clever approaches: age verification and piracy protection.            The first manifested itself in  Leisure Suit Larry . I was as giddy as only a kid can be when I got my hands on it. Unfortunately, that went away when I was required to take an “age quiz” as soon as the game loaded. The age quiz consisted of a series of multiple questions that only an adult would be able to answer. These ranged from factual ones such as “Who recorded ‘Let it be’?” to comical ones such as “Do girls really have cooties?” I do have memories of playing it so I must have figured out some way around the verification. I must have either guesses correctly some of the time or took notes of the answers that allowed me to play.  Another game I enjoyed was a basketball game that I suspect was  Lakers vs Celtics and the NBA Playoffs . Unfortunately, I got the disk from a friend and it had a nifty way of dealing with piracy. When starting the game, it would ask to provide information that could only be found in the game manual, for example asking for the 7th word on the 15th page.  Sadly, both of these approaches disappeared as task switching became standard in the newer operating systems and internet access became common.
{% include setup %} Recently, I’ve been receiving many startups sending out “personal emails” from the CEO or cofounder around 30 minutes after signing up. The idea is to engage the new user by showing them that there’s a real person behind the service that cares and to offer any help that they may need. There’s a great  article  on Segment.io about this tactic as well as a few other emails that can be sent to improve retention. This technique is called “drip marketing” and there are a bunch of companies offering it as a service - the ones I can immediately think of are  Vero  and  Intercom ; and Mixpanel is moving into this space as well with their  Notifications  product. There are also a variety of open source packages available, I’m familiar with  django-drip  for Django and Dan Shipper’s  Faucet  for RoR.  The twist is that you actually need to respond to the people who reply to the email. There have been numerous times where I’d reply to this email without ever receiving a response. At least I understand that the email was most likely automated; I suspect most users wouldn’t be so understanding. I’m not sure why I need to point this out but if you do decide to send out these personal emails, make sure you’re actually going to respond to each reply. Otherwise you’re better off not sending that email in the first place.
{% include setup %} Something that’s been stuck in my head is the relationship between Netflix and bundling. On one hand, we’ve been wishing that cable came unbundled so we’d be able to just pay for the shows we want to watch. On the other hand, we have Netflix which is striving to let us stream every TV show and movie whenever and wherever we want. Why don’t we care that Netflix is actually a bundled product?  I'm sure the major reason is that it’s just not worth worrying about since Netflix is only $7.99 a month; especially when cable TV bills can easily go past $100. Maybe we like the new shows that are exclusive to Netflix (House of Cards, Hemlock Grove, and Arrested Development) and are happy to pay for them; the rest of the content available on Netflix is just an added benefit. Maybe we just don’t view Netflix as being a bundled service at all: the reason I have Netflix is to be able to watch anything I want when I want.  I wonder about the reasons because it helps me think about the future:     Does Netflix want to be the central repository of all video content that can be accessed at any time? What happens when the existing content producers keep raising licensing fees to extract as much as they can?   Does Netflix want to focus on producing its own content? Is it just a TV channel with a unique distribution channel and monetization approach? Does this mean that we’ll start seeing competing TV show/movie producers creating their own Netflix like service? How easy will it be for consumers to find this content if it’s heavily fragmented?   Will the future consist of niche shows and movies? Kickstarter has been used to raise money for the  Veronica Mars  movie as well as Zach Braff’s  “Wish I was here. ” Will we just have thousands of shows that are just supported by small groups of passionate fans?     I suspect we’ll see a combined approach. Mass market won’t be going away since we all want to stand around the water cooler and chat about the latest episodes but we will start having more and more shows and movies that are catered to our interests and passions. This specialization has been happening throughout the 20th century to our physical products and it’s going to extend to the emotional ones. I don’t know whether it’ll be Netflix, Kickstarter, or some unknown company that’ll make it happen but I do believe it’s inevitable.  Disclosure: I own Netflix stock.
{% include setup %} I’ve discovered that every new project lets me correct mistakes from my earlier attempts by allowing me to start from scratch. This is especially true with a web framework such as Django that has a ton of little nooks and crannies that take a while to explore and understand. It’s usually not worth it to go back and fix something that’s not broken on a functional product but starting a new project lets me do it right from the beginning. Now that I’ve developed and launched (with  Sandy  and  Marc ) two serious Django-based products as well as bunch of smaller ones, I wanted to document some personal best practices I’ve picked up. Obviously, I'm still learning and I may be completely wrong with them so let me know if you disagree. If you’re interested in a deeper look at some of the topics let me know and I can write up another post going into detail about a particular topic.         Use  virtualenv : Virtualenv lets you create a virtual environment for each project you’re working on with its own version of Python and its own libraries. I’ve also created alias commands for my major projects that make moving to and activating the virtualenv of that project a single command. Note that using a virtualenv does make a few things more difficult (such as installing  MySQL-python , setting up nginx, configuring  fabric , getting supervisor running) but they’re all surmountable via Stackoverflow and Google.        Use  South : A simpler way of handling database migrations in Django. It’s natural to be updating your database models as the app grows and South makes the migration a little bit easier. It’s not perfect and every once in a while I’ll need to revert some of the migrations and craft them by hand but it’s still better than the alternative.        Use  Fabric : Fabric gives you the ability to set up your own set of commands that can interact with a remote server. This lets you do git pulls, deployments, and run any other command on a server without needing to manually SSH. This becomes especially useful when you have your app served by multiple machines with each one having a different role.        Use  Supervisor : Supervisor monitors the running processes and can restart any that go down.        Nginx/Gunicorn vs Apache: I’ve used both and don’t have strong feelings about either one. I think there’s more information online about getting Apache running but I’ve found Nginx/Gunicorn a bit easier to configure and debug. The other benefits I’ve gotten from Nginx/Gunciron is that it’s less memory intensive out of the box than Apache and I was able to get it to play nicely with Supervisor. In full disclosure, I haven’t really tried to do the same with Apache and it may very well be possible.        Use S3 for static files: Hosting your static files as well as user-uploaded files on S3 is a nice win. You don’t have to worry about serving static content and you can also move the static elements away from your web server. Another benefit I’ve found is that once you move to multiple web servers, it’s nice having all static content on a 3rd party on S3 since that allows all web servers to remain stateless and insync. Otherwise you have to worry about a user uploading a file to one web server and then having to copy it over to the other one to make it accessible.        MySQL/PostgreSQL vs RDS: Unless you plan on monetizing immediately, I suggest using MySQL/PostgreSQL. RDS ends up getting pretty expensive and configuring it isn’t as straightforward as modifying a local installation of MySQL or PostgreSQL. If you end up running into scaling issues you can make the move to RDS relatively easily (especially with MySQL) by dumping and reimporting your database and updating your production settings file.        On Django packages: Install new packages using pip instead of just downloading them into your project folder unless you know you’ll be modifying them. Even then, the well written packages let you customize their behavior by writing your own views, templates, and middleware that can exist outside the installed package. This will keep your project much simpler and better organized, and will force you focus on your app rather than trying to hack someone else’s.     After writing this, I realize I need do another post about the Django packages I’ve found to be useful. I'll put that together in a future post.  Edit: Here's the  follow-up post  where I cover useful packages.
{% include setup %} On Tuesday, I shared some  best practices  I picked up while using Django. This is a follow up post to share the packages that I found useful as well as various hiccups I encountered when using them.               django-registration  and  django-social-auth : Combined, these packages let you handle the basic user registration and activation. Most likely, you will end up having to customize them a bit to do what you want. For example, allowing a user to register using an email address instead of a username or requiring an email address for a user who signs up using Twitter. A small issue that annoyed me is that the signals generated by these two packages occur at different points: django-registration generates signals that includes the request while django-social-auth generates signals that contain the response from the OAuth provider. Depending on your use-case, it may be worth it to use the  simple backend  for django-registration, it automatically activates and logs-in the newly registered users, making your app a bit easy to get into.        django-storages  and  boto : If you plan on using S3 to host static content, definitely take a look at these. They provide backends to make it easy to save and access your static content to S3 without having to deal with the AWS API. I ran into some issues using this along with Cloudfront and django-compressor but I was able to fix them by looking at  Stackoverflow .        django-compressor : This is a neat library that will compress and minify your JS and CSS, check if anything’s been updated, generate an upload the result to static files location, and update the HTML to point to the new location. This makes sure that users never end up with older, cached versions of your static files. One thing to note is that you need to make sure that your Javascript are properly formatted and all end in a semi-colon; otherwise you run the risk of the compression failing. I know that there are other Django compressors  out there  but I’ve been happy with django-compressor.        sorl-thumbnail  and  PIL : If you allow users to upload images this is a must have. It provides a standard way of resizing the images and caching the result. The library comes built in with support for cropping and a variety of other processing options so you don’t have to worry about it. One thing to note is that if a user is loading a page where none of the images have been generated yet, it will delay the page load until all of the images are generated. As long as you know the required sizes of all images, you can run a task on the  backend to generate  each of the images. You may have trouble installing PIL in a virtualenv but doing some Googling it should be easy to figure out.        django-extensions : Just a neat library that comes with additional management commands to make developing Django easier.        django-debug-toolbar : This intercepts every Django request and provides some debug information to help you optimize your code. The most useful piece to me is being able to see the SQL queries that are being executed and helps me figure out what needs tweaking/caching.        django-crispy-forms : If you’re using Twitter Bootstrap, this is a library that lets you generate Bootstrap forms in Django.        django-celery  and  celery : This is a way to run tasks in the background. With Pressi, we initially started with some management commands behind some cron jobs but we ended up switching to Celery when we wanted to distribute it across multiple machines and have built in support for threading and error handling. One thing to note is that we used RabbitMQ as the backend but it takes a bit of time to setup and I’m still struggling to understand the ways to manage it. A lot of people have been using Redis as the backend successfully and I think I’ll give that a go in future projects.        mongoengine  and  pymongo : If you’re using Mongo, take a look at mongoengine, which serves as an ORM for Mongo, and is built on top of PyMongo, a Mongo API. Mongoengine makes it very easy to change your models from a relational database to an documented-based one by keeping the field types and model definitions similar. Be aware that document-based databases are significantly different from relational ones and that although cosmetically your models look similar, the interaction with the backend is very different. You shouldn’t switch to MongoDB just because you can - make sure you’re switching for the right reasons. For Pressi, we use a hybrid approach where we use MongoDB to store a user’s social media content with everything else stored in MySQL. Something to be cautious of is that both of these libraries have been evolving pretty quickly and we ran into an issue where we weren't able to consistently connect to a MongoDB instance until we stumbled unto the right versions of the libraries (in our case, 0.6.20 for mongoengine and 2.4.1 for pymongo).        django-haystack : When you’re ready to graduate from implementing a search using QuerySet filters to an indexing backend, take a look at Haystack. It provides a pretty simple search interface that integrates pretty well with Django and supports a few different backends. We ended up settling on the  Xapian  backend because it was supposedly simpler but ran into some trouble installing it inside a virtualenv until I found  this post . Note that although Haystack supports multiple backends, not all features are supported by every backend so make sure the backend you choose supports everything you need. I believe Solr has the most functionality out of the box but we wanted to keep it simple for Makers Alley.        django-postman : We just implemented this for Makers Alley but it’s a very simple way of doing user to user messaging. It comes with the standard messaging features (inbox, reply, archive, delete) but one thing I wish it had was a way to include attachments.        Fabric : I mentioned this in the previous post but wanted to reiterate it since it makes building and deploying your code easy. It also forces you to think about your environment and you end up with a better structured project as a result.        South : Another package I mentioned earlier that makes it significantly easier to deal with database migrations in Django. The only time we've run into issues using South is when two of us were making changes to the same model in parallel branches. Even then it's easy to replace the two flawed migrations with a functional one.        Unidecode : This isn’t a Django specific library but we found it useful when cleaning up unicode data. If you ever get random unicode exceptions in your code, Unidecode should be able to help.        BeautifulSoup  and  PyQuery : If you need to do some HTML scraping in Python, take a look at BeautifulSoup. It turns HTML code into an object that’s easy to navigate and search. After getting more and more familiar with jQuery, I found a python alternative in PyQuery but am still getting comfortable with it. If you come from the jQuery world I’d try using PyQuery first; otherwise I’d try BeautifulSoup.        requests : Just a nice and simple replacement of urllib and urllib2 that makes it much simpler to make HTTP requests. Your code becomes cleaner, more readable, and more expressive.     I tried to highlight the libraries that have made developing in Django easier but I’m sure there are tons more. I’d love to hear about them so do share.
{% include setup %} Since we’re using Google Apps for Business for our startup, we’ve been getting a bunch of emails trying to get us to sign up for Adwords. The latest promotion is offering a credit of $300 if we spend $100. It’s a pretty common marketing tactic and tons of companies have similar promotions. What’s special about Google is that they’re running an auction for every single click and by giving some businesses free money, they’re driving the prices up for the entire market.  Without being at Google, it’s impossible to know what effect this has but I tried to do some estimates. According to the  NY Times , Google had 4M businesses on Google Apps at the end of 2011 and by June of 2012 it was up to 5M. So over the first 6 months of 2012, Google Apps gained 1M businesses. If each of those businesses was offered $300 a credit and took Google up on it, there would be an extra $300M in the Adwords market provided by Google, in addition to that business’s Adwords contribution. If we compare this against Google’s advertising revenue during the first  two quarters  of 2012, $20,750M, we get 1.4%. That may not seem that big but think of every bid on Google Adwords increasing by 1%. Of course, not every business took Google up on the offer so the true number is going to be lower but I’d bet even then it’s still enough to affect the market.  I was trying to think of an analogy but had trouble. I initially thought this is similar to a casino giving some of the poker players free money but then realized that the majority of that money will still end up in the hands of players. I then thought it would be akin to ebay offering free credit to some buyers, but once again, most of the value would still captured by the marketplace participants; either by sellers getting higher prices or by buyers being able to buy more or better items. I finally settled on the idea of an amusement park offering free passes to random people. Those people end up making the park more crowded for the visitors who paid yet still buy concessions and the amusement park profits.  I suppose this is what happens when the dominant company in a market full of near-zero marginal cost products runs a promotion. I don’t know whether Google has a responsibility to their existing businesses to keep the auction fair but I know if I were advertising on Google I wouldn’t be too happy with Google giving free credits to my competitors.
{% include setup %} After doing a round of customer development for  Makers Alley , we discovered that customers really wanted to communicate with makers about their pieces. In true MVP fashion, we got the first iteration out in a day by using  django-postman  to handle the user to user communication. Within a few days, we quickly discovered that text messages weren't enough and we needed to support file attachments, otherwise makers can’t easily show their designs and customers can’t share what they like. Unfortunately, django-postman does not support attachments and we didn’t want to have to incorporate another messaging library. Another constraint was that we were already using the awesome  jQuery File Upload  library (in truth, a modified  Django version by Sigurd Gartmann ) to allow makers to upload images when managing their storefronts.  We wanted to leverage our existing file upload system but also incorporate it with the django-postman messaging library without having to modify any of the code in django-postman. We weren’t able to find  anything on StackOverflow  that dealt with this issue so we were left with writing our own. Here’s the approach we ended up taking that might come in handy for anyone else running into the same problem. The code needs some cleaning and I need to add some error checking but I’m sharing it with the excuse of “perfect is the enemy of good.”  We built a new app, postman_attachments, that would serve as the intermediary between the file upload piece and django-postman.                  models.py: Attachment that would map the django-postman Message model to an uploaded file         {% highlight python %} class Attachment(models.Model):     message = models.ForeignKey(postman_models.Message)     attachment = models.ForeignKey(fileupload_models.GenericFile)      def __unicode__(self):         return str(self.message) + self.attachment.__unicode__(){% endhighlight %}                     api.py: Versions of pm_write and pm_broadcast that would do the same work as the original but would also map the attachments between         {% highlight python %} def pma_write(sender, recipient, subject, file_ids=[], body='', skip_notification=False,         auto_archive=False, auto_delete=False, auto_moderators=None):          ### Same code as in pm_write          for file_id in file_ids:             f = GenericFile.objects.get(id=file_id)             a = Attachment(message=message,attachment=f)             a.save(){% endhighlight %}                     forms.py: In our case, we needed to tweak the FullReplyForm and created our own version that included a new “file_ids” field to hold the ids of the uploaded files. The full solution would need to make versions of the other forms included in django-postman.         {% highlight python %} allow_copies = not getattr(settings, 'POSTMAN_DISALLOW_COPIES_ON_REPLY', False) class FullReplyImageForm(BaseReplyForm):     """The complete reply form."""     if allow_copies:         recipients = CommaSeparatedUserField(label=(_("Additional recipients"), _("Additional recipient")), required=False)      file_ids = forms.CharField(required=False,widget=forms.HiddenInput())      class Meta(BaseReplyForm.Meta):         fields = (['recipients'] if allow_copies else []) + ['subject', 'body', 'file_ids']      @transaction.commit_on_success     def save(self, recipient=None, parent=None, auto_moderators=[]):         ### Bunch of code from original save method in BaseWriteForm from django-postman         file_ids = [x for x in self.cleaned_data.get('file_ids').split(',') if x]         ### Bunch of code from original save method in BaseWriteForm from django-postman         for file_id in file_ids:             f = GenericFile.objects.get(id=file_id)             a = Attachment(message=self.instance,attachment=f)             a.save(){% endhighlight %}          In addition, we needed to override the default django-postman templates to display the attachments for a message as well as include the necessary javascript to deal with the jQuery File Upload piece.         view.html         {% highlight html %}{% raw %}          {{ message.body|linebreaksbr }}        {% if message.attachment_set.all %}                      Attachments                          {% for a in message.attachment_set.all %}                 {{ a.attachment.file.url }}               {% endfor %}                             {% endif %}{% endraw %}{% endhighlight %}             view.js         {% highlight javascript %} $(document).ready(function(){   var upload_ids = [];   $('#fileupload-attachments').bind('fileuploaddone', function (e, data) {     console.log('Done uploading product images');     $(data.result).each(function(){       upload_ids.push(this.id);     });     console.log( upload_ids.join(',') );     $('#id_file_ids').val( upload_ids.join(',') );      if ($('#fileupload-attachments td.preview').length == upload_ids.length) {       console.log('Enabling input');       $('#reply-form button[type="submit"]').removeAttr('disabled');     };   });    $('#fileupload-attachments').bind('fileuploadstart', function (e, data) {     console.log('Disabling input');     $('#reply-form button[type="submit"]').attr('disabled','disabled');   });    $('#fileupload-attachments').bind('fileuploadpreviewdone', function (e, data) {     if ($('#fileupload-attachments td.preview').length == product_image_ids.length) {       $('#fileupload-attachments tbody.files tr').remove();     };   }); });{% endhighlight %}          The last minor thing we needed to do was update our urls.py file to override the standard django-postman urls to have them use our custom form.         urls.py         {% highlight python %} url(r'^messages/reply/(?P [\d]+)/$', 'postman.views.reply',         {'form_class': FullReplyImageForm},         name='postman_reply'), url(r'^messages/view/t/(?P [\d]+)/$', 'postman.views.view_conversation',         {'form_class': FullReplyImageForm},         name='postman_view_conversation'), url(r'^messages/', include('postman.urls')),{% endhighlight %}          I’d love to release this publicly but don’t have much experience creating standalone Django apps. If you have experience in open sourcing Django apps let me know - I’d love to get this out there as a standalone app or somehow incorporated into django-postman.
{% include setup %}            When reading a long form piece, I favor the single-page view. Unfortunately, I usually don’t find out that it’s longer than a page until I’ve finished the first page. At that point, I switch to the single page view which causes the entire page to reload and I have to skim the page to find the spot where I stopped reading.  Why haven’t any of the major content sites dealt with this yet? It’s not a technical problem. All they’d need to do is have an anchor tag on the single page view to indicate the spot that the reader should be shown if they click the “View as Single Page” link after reading the first page. Do they just want to force their readers to look at the ads again? I’m sure this small change would save people hundreds of hours each day.  If you know of any sites that handle this scenario well, let me know; I’d love to check them out and give them a shout out.
{% include setup %} I initially set out to write a post to complain about how difficult it is for an average investor to “hit it big” these days by investing in a tech company at its IPO but ended up changing my thesis after digging into the data. It’s still possible to get the same returns as it was in the 1980s but it’s not possible by a long-term investment in a single company.  To start, I came up with a sample of large tech companies and looked at their performance since their IPO as well as their annualized return.                    Company  IPO Year  Total Return  Annualized Return                         Apple  1980  12,250%  16%             Microsoft  1986  33,800%  24%             Cisco  1990  60,800%  32%             Yahoo  1996  1,808%  19%             Amazon  1997  14,894%  37%             Ebay  1998  2,817%  25%             Netflix  2002  2,564%  34%             Google  2004  708%  24%             LinkedIn  2011  86%  -7%             Facebook  2012  36%  -64%             Tesla  2010  407%  60%            From this limited sample, it looks as if it’s still possible, but more difficult, to get the annualized returns of the 1980s and 1990s. It’s impossible to know whether the total returns will be comparable but I suspect that it’s going to be extremely difficult, if not impossible. To get a Microsoft-like return, Tesla would need to end up with a market cap of $760 trillion, excluding inflation.  In a way, this is obvious. As more and more money pours into the VC industry, companies can afford to stay private longer and just keep on raising more funding. This gives company management and investors more control, keeps the company leaner, and limits public information. Unfortunately, this leads to most of the growth occurring before the IPO with retail investors not able to capture any of the value.  While I’m optimistic that the  JOBS Act  will help, we unaccredited investors still need a way to invest right now. After leaving my finance job, I tried to replicate the traditional investment approach by doing research, analyzing statements, and reading coverage. But over the past few years I’ve been too busy and have just been investing in companies that I use and like. Unsurprisingly, this new approach has led to me invest in tech companies. Surprisingly, I’m doing better than ever before. Over the past 2 years, I’ve bought stock in three companies: Tesla, Netflix, and Yahoo with the lowest gaining 66%. I realize these returns can’t keep on going so the question becomes when to sell and invest in something else. For this, I’ve been looking at market caps compared to other companies in the same industry to estimate their potential. In my case, Tesla has a market cap of $11B while Audi’s is $26.6B and Toyota’s is $191B, and definitely has room to grow. The standard disclosure when giving financial advice is “past performance is not an indication of future results” and it’s definitely true in this case. I’m just glad I found an approach that suits me.
{% include setup %} A while ago I read Bruce Schneier’s Liars and Outliers and came across a neat passage:   There was this kid who came from a poor family. He had no good options in life so he signed up for the military. After a few years he was deployed to a conflict infested, god-forsaken desert outpost. It was the worst tour of duty he could have been assigned. It was going to be hot and dangerous. Everyday he had to live with a hostile populace who hated his presence and the very sight of his uniform. Plus, the place was swarming with insurgents and terrorists.   Anyhow, one morning the soldier goes to work and finds that he's been assigned that day to a detail that is supposed to oversee the execution of three convicted insurgents. The soldier shakes his head. He didn't sign up for this. His life just totally sucks. "They don't pay me enough," he thinks, "for the shit I have to do."   He doesn't know he's going to be executing the Son of God that day. He's just going to work, punching the time clock, keeping his head down. He's just trying to stay alive, get through the day, and send some money back home to Rome.   Bruce mentions that he found this on the internet and  cited it appropriately  in the footnotes. But when I tried Googling for the phrase "There was this kid who came from a poor family" the  top links  were  people citing  Liars and Outliers, including my own  highlight on Readmill . I even came across a  page  that linked to the  original source  that Bruce cited before I found a link to the original source.  I realize this conflict between authority and originality is a challenge for search engines but it seems that they rank authority ahead of originality. This leads to the unfortunate consequence that if any major site cites your personal blog they will appear earlier in the search results. I had this occur with my  post  on the history of why cell phones don’t have dialtones; searching for “cellphones dialtone” shows the Gizmodo link ahead of my blog’s. The nice thing is that Google seems to be getting better - right now searching for “There was this kid who came from a poor family” shows the original source in the third position; a few months ago it was at the bottom of the first page. Let’s hope this trend continues.
{% include setup %}               Photo by  @rafat      On Wednesday, I took my first bike ride using New York City's new  Citibike  program. So far it's been great but one issue I ran into is being able to plan a trip. Google offers cycling directions from place to place but doesn't take into account the Citibike stations. On the other hand, the Citibke app shows the rental stations but doesn't make it easy to find directions from one station to another unless you're already at one of them.  I decided to actually do something about it and wrote a  little web app  that lets you pick two Citibike stations and retrieves the cycling directions between them using the Google API. It's definitely not perfect and the user experience needs to be improved but it does what it was designed to do.  What I find amazing is how simple it was to write the app - it took me less than 90 minutes to go from having an idea to having something that's usable. The list of stations are available in  JSON from the Citibike site  and Google makes it very easy to use their services to show a map and get directions.The best part is that this app is completely static since it's just using client side Javascript and Google's APIs. I've written about this  before  but I'm convinced that more and more services will become available through APIs which will lead to more and more apps and sites being built this way.
{% include setup %} To coincide with the launch of Citibike, I wrote a  simple web app  that provided cycling directions from one Citibike station to another. The biggest piece of feedback I received was that people care about getting from place to place rather than from one Citibike station to another. Based on this feedback, I  updated the app  to provide directions from any New York City address to another by breaking every trip down into three steps: the first is to walk to the nearest Citibike station, the second is to bike from one station to another, and the last is to walk to the destination. A limitation I ran into is that Google’s  Direction Service  doesn’t support different transit methods for multiple waypoints. This, combined with my desire to get it out there, is why the design’s not as good as it should be. I’ll see if I can improve it over the next few weeks. People have also been telling me this needs to be on mobile so I’m going to use this as an excuse to jump into mobile development. I’m excited.
{% include setup %} The  Priceonomics blog  is one of my favorites so when I saw that they had a  programming puzzle  up I decided to have some fun with it. And what’s more fun than hacking around with a quirky, esoteric programming language? I remember having fond memories of playing around with Prolog in middle school so decided to dig it up again in an attempt to solve this puzzle.  Prolog is pretty different than the mainstream programming languages, it belongs to the logic programming language category and relies on defining a variety of relations and then querying these relationships to get results. A simplified way to think about it is you define a set of equations and tell Prolog to "solve for X".  This leads to some interesting behavior. Many functions end up being bidrectional with the Prolog version of a "concat" function being a good example. The first argument is a list, the second is the separator, and the last is the resulting string. Passing in all 3 will return true if the concatenation statement is true. Passing in the list and the separator will tell us what the concatenated string is. Passing in the separator and a concatenated string is equivalent to a "split" function. The only piece it's not able to figure out is the separator given the list and the concatenated string. Unfortunately, I'm not familiar enough with Prolog to explain why.  {% highlight prolog %} ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', 'Prolog is sweet'). true.  ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', 'Prolog is not sweet'). false.  ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', X). X = 'Prolog is sweet'.  ?- atomic_list_concat(L, ' ', 'Prolog is sweet'). L = ['Prolog', is, sweet].  ?- atomic_list_concat(['Prolog', is, sweet], X, 'Prolog is sweet'). ERROR: atomic_list_concat/3: Arguments are not sufficiently instantiated{% endhighlight %}  For the first pass, I decided to ignore the web side and just focus on defining the exchange rate relationships and have Prolog tell me which exchanges would work. The way it works is that we define a profit to be defined in terms of two intermediate currencies. We can then ask Prolog to give us the currency chain that will result in a profit.  {% highlight prolog %} exchange(usd,eur,0.7779). exchange(usd,jpy,102.459). exchange(usd,btc,0.0083). exchange(eur,usd,1.2851). exchange(eur,jpy,131.711). exchange(eur,btc,0.01125). exchange(jpy,usd,0.0098). exchange(jpy,eur,0.0075). exchange(jpy,btc,0.0000811). exchange(btc,usd,115.65). exchange(btc,eur,88.8499). exchange(btc,jpy,12325.44).  % Calculate profit for a usd->x->y->usd currency chain profit(First, Second, Profit) :-     exchange(usd,First,P1),     exchange(First,Second,P2),     exchange(Second,usd,P3),     Profit is P1 * P2 * P3.  arb :-     profit(First, Second, Profit),     Profit > 1.0,     write('usd '),     write(First), write(' '),     write(Second), write(' usd '),     write(Profit), nl, fail.  :- arb.  % Results: usd eur jpy usd 1.0040882716200001 usd eur btc usd 1.0120965187500002 usd btc jpy usd 1.0025512896{% endhighlight %}  The next step was to get it to retrieve and parse the JSON from the Priceonomics server. After doing a ton of searches and reading a ton of documentation I was able to get it to work. As a next step I'll try to see if I can get it to return currency chains of arbitrary length.  {% highlight prolog %} :- use_module(library('http/json')). :- use_module(library('http/json_convert')). :- use_module(library('http/http_json')). :- use_module(library('http/http_client')). :- use_module(library('http/http_open')).  parse(I) :-     test(CP=S) = test(I),     atomic_list_concat(L,'_', CP),     [A, B] = L,     atom_number(S,R),     assert(exchange(A,B,R)).  % Calculate profit for a usd->x->y->usd currency chain profit(First, Second, Profit) :-     exchange('USD',First,P1),     exchange(First,Second,P2),     exchange(Second,'USD',P3),     Profit is P1 * P2 * P3.  arb :-     http_get('http://fx.priceonomics.com/v1/rates/', JsonIn, []),     json_to_prolog(JsonIn,PrologIn),     PrologIn = json(L),     maplist(parse, L),     profit(First, Second, Profit),     Profit > 1.0,     not(First = Second),     not(First = 'USD'),     not(Second = 'USD'),     write('USD '),     write(First), write(' '),     write(Second), write(' USD '),     write(Profit), nl, fail.  :- arb.  % Results (Might change each run): USD JPY EUR USD 1.0071833283714342 USD EUR JPY USD 1.007164983424893{% endhighlight %}  I'm sure a Prolog pro would have been able to do this much quicker and better but I had a surprisingly fun time doing it. I got a bit frustrated trying to translate the JSON into Prolog relationships but actually getting it to work made it worth it. Trying a whole new programming category is a great way to get more creative and forces us to think about problems differently. Prolog may not be the most practical language but exposing us to new concepts and approaches makes it valuable.
{% include setup %} The real time news cycle bothers me. Every time theres some news there are countless reactions on Twitter and quick, shoddy write ups on various "news" sites. Unfortunately, by the time someone does the research and writes a thoughtful response, we've moved on to the next piece of news. We're reaching the point where writing something stupid quickly is becoming more valuable than writing something thoughtful but late.  Twitter’s strength is its weakness. The 140 character limit makes it very easy for anyone to share an opinion but that also leads to everyone sharing an opinion. Of course, its ability to break and spread news is invaluable. I just wish that the more thoughtful, well-researched pieces could get past the noise. This week, I would have preferred to see a few insightful pieces about WWDC rather than the same exact WWDC coverage from dozens of sites.  This wouldn't be a problem if it were isolated to Twitter but it's becoming the norm. The  Lincoln-Douglas debates  lasted hours and candidates had an hour for a rebuttal. These days, we’re lucky to get a rebuttal longer than a few minutes. The average shot length for movies  decreased  from over 6 minutes in the 1930s to close to 4 minutes now. Even investors are getting in on the trend and want entrepreneurs to have 30 second elevator pitches instead of real conversations.  Clearly I’m simplifying and there are countless other reasons for these changes. At the same time, this trend towards constant stimulation and gratification is dangerous. It reminds me of a  study  that showed that kids who had more patience and self control ended up with higher SAT scores more than a decade later. If we get addicted to constant entertainment, how are we going to tackle on the challenging problems that require focus?  As a kid I used to lie in bed and read a book for hours but now find myself taking a break every 15 minutes to check up on my digital life. This bothers the hell out of me. I can’t imagine the effect it’s having on kids who’ve never even had a chance to be left alone with a good book.
{% include setup %} The past few years have seen the rise of the share economy with companies such as AirBnB, Sidecar, Lyft, and TaskRabbit seeing massive growth. Unfortunately, they’re getting significant opposition from government and the entrenched special interest groups. Most of the pushback is under the guise of consumer safety and that regulations exist to protect the consumer.  Regulation is necessary when there’s an information asymmetry between a service provider and a consumer. In such cases, regulations help bridge that information gap and make the consumer more comfortable making the transaction. But the internet has been chipping away at this gap by building communities where people can share reviews and experiences. Yelp, Angie’s List, and Google are the largest of these traditional review sites but reviews are starting to appear everywhere that money is changing hands. Ecommerce sites offer reviews and ratings of the products they’re selling. The share economy companies self-regulate by offering communities with well thought out rating systems. Without well functioning communities they wouldn’t survive.  Consumer safety regulations are making way for ratings and reviews. We’re replacing centralized regulatory agencies with crowdsourced, self-regulating communities. Some regulation will always be necessary, especially in places with large information asymmetries, but these places are constantly shrinking. Of course the entrenched companies are fighting these trends but they should be focused on innovating themselves rather than battling the inevitable.
{% include setup %} Startups need to use everything in their arsenal to grow. A big part of it is playing in the grey area between moral and immoral. Do you create fake users and comments to portray an active community? Do you reply to posts on Craigslist trying to get visitors to your site? It’s also much easier to play in this area when you’re a startup - you’re most likely too small to be noticed and even if you are the press won’t spend much time on it. Google already gets a ton of flak every time someone complains about losing business due to a search engine update, imagine what would happen if a Google employee was caught spamming Craigslist.  It’s important for all companies, and especially startups, to test these moral boundaries but there’s no clear answer of what the boundaries actually are, just shades of gray which will vary from company to company and from team to team. I believe that until you get some resistance you need to keep on pushing otherwise you never know that you’re doing enough.  At  Makers Alley , our lesson came when we wanted to increase the amount of makers signing up. We decided to create pages for all makers in an area and then email each of them a link to “claim” their page. In order to make the pages look appealing we took images and descriptions from their individual sites. The results were mixed: as expected most emails didn’t even get a response but the ones that did had a wide range of reactions. Some of the makers gladly claimed their page and loved that their content was automatically pulled. Yet others were pissed that we used their copyright images on our own site without their permission. We weren’t comfortable knowingly upsetting some users and quickly removed the images from the unclaimed pages. We continued running a few other tests to try to maximize the “sign up rate per email” and ended up settling on a simple email that asks the makers to sign up and getting rid of the claim functionality. But without making the misstep in the beginning we wouldn’t have been able to settle on this approach. I do worry that less scrupulous companies have a higher chance of success but I can only take actions I’m comfortable with.
{% include setup %} Something I’ve been thinking about is the variety of sales approaches. On one extreme, you have pharmaceutical companies sending sales reps to visit doctors offices to try to get them to prescribe their drugs. On the other you have companies such as MixPanel and Dropbox which rely on a self serve approach. And in between you have companies such as NewRelic which offer a self-serve trial and try to upsell you with emails from a sales rep.  Depending on a product’s complexity and its cost structure your sales approach may be limited but it’s always worth seeing the other approaches available and if any of them may fit. It’s likely that an approach that didn’t work a year ago may work right now. A simple way to check is to look at newly launched competitors in your space and see how they’re acquiring customers.  After trying to come up with an exhaustive list of approaches I figured out it's easier to just rank them across two dimensions:          Proximity : This is both physical proximity as well as familiarity with your customer. It’s much easier to sell when you’re in the same room as them and know their story than when you’re sending out a generic email.        Inbound vs outbound : A customer already having an interest in your product is much better than trying to interest him from scratch.     Here’s my attempt at coming up with matrix showing where different companies would lie based on their sales approach.      It’s possible to be profitable by being in any spot; a higher acquisition cost will just lead to a higher price. That’s why an Oracle installation can  cost millions  of dollars a year and why the enterprise Dropbox product is around  $125/user/year . I believe that current trends favor businesses in the inbound/self-serve quadrant. This is due to people becoming more comfortable with technology as software gets better and easier to use and the ability for companies to offer free-trials with near-zero marginal cost. A corollary is that there’s an opportunity to compete with businesses outside the quadrant by creating simpler, cheaper versions of their product. The first version will suck compared to the existing products but as long as it’s cheaper and still solves a problem you should be able to get some customers and revenue (a la  Lean Startup ). Over time, you can continue to grow and keep on building our product until you’re competing with the existing companies (a la  Innovator’s Dilemma ).
{% include setup %} Tom Tunguz wrote a  great post  yesterday sharing the frameworks he uses to evaluate and analyze startups. For this post, I’m not interested in the content (which is great for anyone building a company) but I am interested in the concept of business frameworks and their application. When I was younger and came across a “business” framework I would dismiss it as obvious and move on. Now, I’m aware of how valuable a good framework can be. A good framework imposes structure that leads to a clearer though process with better results. At the same time, it needs to be simple to apply but be expressive enough to describe the complexity of a business. Being human, we also don’t want to think about our own fallibility and weaknesses which makes it difficult to critique our businesses. We also want to solve problems on our own rather than share our uncertainties with others. A framework serves as an impartial third party where you go through and fill in the blanks until you discover you aren’t in as good of a shape as you thought. Now you can work on growing your company instead of avoiding self-criticism.  I recall struggling to fill out the Lean Canvas for Pressi after reading one of the lean startup books. It took me a few hours with multiple breaks and online searches but I ended up with a much better understanding of our business. It became easier to see where the risks were and gave us tons of ideas around acquiring new users and generating revenue. None of these ideas were groundbreaking and I’m sure we would have gotten to them eventually but it was valuable getting to them earlier since we were able to take them into account when building the product.  I’m not sure why investors don’t require pitching startups to share these instead of a pitch deck, they seem much more useful.
{% include setup %}            Last week, I had a morning flight out of Laguardia Airport and being into all things tech decided to grab a coffee at a place called Biergarten since they had iPads at every seat. Turned out that the only way to order and pay was by using the provided iPad with the attached credit card reader. I had 30 minutes to kill before my flight and decided to spend it observing the interactions others had with this ordering system.  During the 30 minutes, I saw 6 people approach the bartender and every single one tried to order directly from the bartender without paying any attention to the iPads. Surprisingly, none of them gave up after being told they had to order using the iPad although two couldn’t figure out how to use the iPad and needed help. The major points of frustration were finding the app and then realizing that you needed to “check out” before submitting the order.  I suspect no one actually benefits from this sort of setup. The supposed benefits to Biergarten are that they’re able to hire fewer people and collect payments upfront but I’m not sure it’s worth it given the high usability cost to the consumer. The staff is now kept busy explaining how to operate the iPads and customers are significantly slower at ordering than a trained waitstaff would be. In addition, if an iPad is the only way to order then they can’t have more customers than there are iPads - no one can get anything to go or have a drink standing up unless there are iPads available. The only way this is a good idea is if the iPads are able to attract more customers.  Essentially, the company is trying to externalize the cost of serving customers to the customers without taking into account their experience and frustration. You want to make it as easy as possible for people to give you their money and forcing technology down your customers’ throats isn’t always the answer. As optimistic as I am about technology making things easier, it’s going to be difficult for brick and mortar places to move to a self serve model. It’s simply easier to give someone cash or a credit card and have them do the work than doing it yourself.
{% include setup %} This is a bit of a first-world problem but it’s possible for a design to be too good. A great design may lead to an increase in your vanity metrics but that won’t necessarily translate into a successful business. In fact, it’s likely that these low-value users will increase your costs.  When we redesigned the landing page for Pressi (formerly Glossi) we saw the signup rate from our landing page shoot up to to close to 34% from below 5%. Unfortunately, our retention rates were abysmal and we were stuck supporting thousands of Pressi pages that were not seeing any engagement. This led to a massive increase in our AWS costs that we had to scramble to contain. The solution was to be smarter about the frequency of our data pulling as well as minimizing the amount of data we were storing for our users. In hindsight, we should have solved our retention problem before trying to grow our users but we were too obsessed with our user growth numbers to do the right thing.  I realize that user growth is a problem that many startups would love to have and that it’s foolish to choose a crappy design over a great one. At the same time, if you’re not tracking the metrics that align with what you’re trying to accomplish, a surge in growth will be more damaging than beneficial. The corollary to this is that if your design sucks and yet you’re still getting signups then you must be onto something.  For those interested, here’s the signup flow that had the ~34% signup rate. The awesome design work was done by  Marc .                                   The Pressi landing page                                           Pressi signup step two                                           Pressi signup step three
{% include setup %} I struggle with this one. Some days I feel as if I should take every meeting since it’s impossible to know where it can lead. One meeting can completely change a business, generate some consulting work, or lead to new friendships. At the same time, taking every meeting would eat up a chunk of time and most meetings end up fading from memory.  I’m still figuring out my approach but do believe that having fewer, more meaningful relationships is more valuable than having many fleeting ones. Unfortunately, it’s not clear what will end up being meaningful before the meeting. Currently, I try to take every first meeting or at least have a phone call but have been scheduling them all on a single day, early in the morning, or late in the evening to avoid disruption. I’m also trying to make every meeting valuable by taking follow up notes in order to reach out later if I come across anything relevant or if I need to send an introduction. Probably the most important thing I’ve learned is that it’s easier to rejuvenate an older relationship than to create a brand new one so I’ve been making an effort to catch up with at least two former acquaintances each month.  There’s no single approach to meetings that will work for everyone but relationships are important regardless of what you do and it’s essential to maintain and grow them. I’d love to hear how others deal with meeting overload.
{% include setup %}      Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.  At first, I’d just dump the data into Excel to generate various charts but moved on to using Perl and Python to generate charts when I learned the value of reusable code. While at  Yodle , I picked up R which gave me more power than what I knew to do with and introduced me to a whole new set of visualizations and models. Recently, I’ve been having a blast using  D3  and  Vega . The biggest appeal is that they’re in Javascript so they can run in all modern browsers and make it very easy to support interactive behavior. The best analyses always tell a story and allowing users to interact with the data is a great way for them to craft their own story. I’m hopeful that such tools will improve data accessibility and get people excited about gleaning their own insights.
{% include setup %} There are countless posts discussing the business and marketing challenges when building a marketplace but I wanted to discuss the issues on the tech side. While we ran into technical challenges building  Pressi  they were mostly issues with scaling and dealing with the various social network APIs. With  Makers Alley , we didn't run into scaling or API issues but had to deal with a ton of functionality in order to be seen as a credible marketplace. Individually, the features are simple for an intermediate developer to build but there are a lot of them with varying degrees of nuance and logic that need to be worked out.  Note that some of these issues are only applicable to "maker" marketplaces where the merchants make the pieces to order. In those cases, I refer to them as makers rather than merchants.  In no particular order:    	  Payments :  Stripe  and  Balanced  have made this significantly simpler but one thing to watch out for is that you will need to have a merchant signup process to collect the required regulatory information if you want to automate disbursements.  	  Shipping and Tracking : Makers take a different amount of time to make each piece. Buyers should know this information before placing an order and makers need to be able to change it depending on their schedule and order load. Merchants need the ability to mark an order as shipped and possibly provide a tracking number to the buyer. You should notify buyers when their order has shipped.  	  Logistics : While you're not holding inventory, you're charging customers and expect the merchants to fulfill their end of the agreement. How do you deal with a merchant sending an order late or not being able to fulfill an order? Do you want to have merchants approve every order they receive or do you assume that they'll be able to fulfill it? How about a single order containing items from many merchants?  	  Returns : No matter how good the products are there will always be someone who's unhappy with an item and you need to have a return/refund policy. For small items it's simple to figure out the logistics but how do you deal with someone wanting to return a dining table to a merchant a few states away? Where should the item be sent and who's responsible for paying the shipping and handling cost? What happens to the returned item?  	  Messaging : We discovered this a bit late but customers really want a way to talk to the merchant. Some buyers will want to customize an order and may want to both send exchange photos with the merchant to make sure they're getting what they want.  	  Shopping Carts : There are a bunch of existing solutions out there but we weren't able to find one that fit the needs of a two sided marketplace that supported customizable product options. there are a lot of things we take for granted when using a full fledged site like Amazon - making changes to your shopping cart, buying from multiple merchants, applying rebates and discounts, and getting recommendations. A possible edge case is merchants running out of inventory while someone is going through the checkout process.  	  Orders : Both merchants and buyers need to see a history of their orders. The implication is that once an order is placed it needs to be immutable and timestamped so that changes to the items are only reflected going forward. In addition, orders can get messy since a single order may be spread out across multiple merchants and items. What happens if one merchant can fulfill their half of the order while the one can't? Do you issue a refund for part of the order? What if the customer only wanted the items as a package deal?  	  Taxes : At some point you need to start dealing with taxes with each state having their own regulations and rates. We haven't implemented the details here yet but I suspect interstate commerce can get complicated quickly.  	  Reviews : Before a purchase, buyers want to see reviews of an item. After a purchase, buyers may want to rate and review the items. Should merchants have the ability to respond or challenge a review?  	  Search : This is a big one. Buyers need to be able to quickly find what they're looking for or they'll give up and go somewhere else. What criteria can users search for? Are you going to deal with typos and misspellings? Should you support faceting? How should you tier your prices? Do you need to support geospatial search? This is probably the biggest piece that requires understanding your audience and tailoring the search experience to them.  	  Images : It's rare that someone will buy a physical item without at least seeing a picture of it first. Makers need a simple way to upload multiple images and change the order in which they are displayed. The code should also be smart about generating thumbnails that can be used on different pages - search/listing, product view, shopping cart, etc.  	  Changing Inventory : Makers will need to be able to modify and remove what they're selling. At the same time, you need to have a record of the history so that buyers and makers can look at prior sales.     Most of these issues can (and should) be handled manually at the beginning either through the backend or through email but this approach won't scale. The goal is to be able to support the various use cases even if they have to be done manually. This will make you look credible to your customers and also give you a sense of which cases are the costliest and need to be automated. Technology shouldn't be the primary focus for a marketplace business and you will most likely fail due to a lack of users on one of the sides. At the same time, the technology behind a marketplace isn't simple since you're basically smashing together an ecommerce site with a social network. If you have any additional thoughts or questions definitely let me know and I'll try to help.
{% include setup %} Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the  Scrapy  framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.  The full project is  available on GitHub  but I wanted to highlight a few of the components:    	  parse method : This submits a form POST to the Yahoo login page which authenticates the session. The key point here is to specify a callback function which will continue the existing session. {% highlight python %} def parse(self, response):     return [FormRequest.from_response(response,                 formdata={'login': self.settings['YAHOO_USERNAME'],                 		  'passwd': self.settings['YAHOO_PASSWORD']},                 callback=self.after_login)]{% endhighlight python %} 	  	  parse_stats method : In previous projects, I struggled with separating the crawling from the parsing since the page would have information that would relevant to both - for example I would want to extract information from a page as well as find the next page to scrape. Scrapy offers a nice solution by letting you return different types from the same method. Returing a Request will lead to another page being crawled but one can also returned the scraped structured data via an Item. In the case of the scraper, I return the fantasy football stats on each page via Items but also return a Request when I want to navigate to the next page of stats. {% highlight python %} def parse_stats(self, response):     hxs = HtmlXPathSelector(response)      # Parse the next url     next_page = hxs.select('//ul[@class="pagingnavlist"]/li[contains(@class,"last")]/a/@href')     next_page_url = 'http://football.fantasysports.yahoo.com' + next_page.extract()[0]     count = int(RE_CNT.findall(next_page_url)[0]) # Don't go past a certain threshold of players     current_week = int(RE_WEEK.findall(next_page_url)[0])      self.log('Next url is at count {} with week {}'.format(count, current_week))      if current_week   self.settings['MAX_STATS_PER_WEEK']:             yield Request(self.base_url.format(self.settings['YAHOO_LEAGUEID'], current_week + 1), callback=self.parse_stats)         else:             yield Request(next_page_url, callback=self.parse_stats){% endhighlight python %}} 	  	  XPath expressions : In the past, I'd use either BeautifulSoup or PyQuery to traverse the DOM but found XPath expressions to be simpler. There’s less code to write and the expressions are easier to understand and have a higher information density. {% highlight python %} stat_rows = hxs.select('//table[@id="statTable0"]/tbody/tr') xpath_map = {     'name': 'td[contains(@class,"player")]/div[contains(@class,"ysf-player-name")]/a/text()',     'position': 'td[contains(@class,"player")]/div[contains(@class,"ysf-player-detail")]/ul/li[contains(@class,"ysf-player-team-pos")]/span/text()',     'opp': 'td[contains(@class,"opp")]/text()',     'passing_yds': 'td[@class="stat"][1]/text()',     'passing_tds': 'td[@class="stat"][2]/text()',     'passing_int': 'td[@class="stat"][3]/text()',     'rushing_yds': 'td[@class="stat"][4]/text()',     'rushing_tds': 'td[@class="stat"][5]/text()',     'receiving_recs': 'td[@class="stat"][6]/text()',     'receiving_yds': 'td[@class="stat"][7]/text()',     'receiving_tds': 'td[@class="stat"][8]/text()',     'return_tds': 'td[@class="stat"][9]/text()',     'misc_twopt': 'td[@class="stat"][10]/text()',     'fumbles': 'td[@class="stat"][11]/text()',     'points': 'td[contains(@class,"pts")]/text()', }{% endhighlight python %} 	     This also got me thinking about the evolution of my approach to scraping. In 2006, I was into Perl and scraped using the LWP::Simple, WWW::Mechanize and the HTML::TreeBuilder libraries. After I moved on to Python I switched to using urllib and BeautifulSoup. Most recently, I’ve started using the wonderful requests library along with PyQuery. Conceptually, these approaches are the same: first retrieve a web page and then extract the data you want by traversing the DOM. Scrapy does the same thing internally but by removing a ton of the boilerplate, it lets you focus on the key problems in scraping - figuring out what page to scrape next and figuring out how to extract the content. The rest is handled by Scrapy itself - including file storage, retries, throttling, and probably a ton more that I haven’t gotten a chance to explore yet.  This also gives me some time to work on the actual draft algorithm. My goal is to create a strategy that’s using a value based approach combined with my schedule. The idea is that I shouldn’t pick the players that will have the highest point total over the season but the ones that will have more points during my tough matchups. Of course, it’s almost all luck but I’m still looking forward to attempting this approach.
{% include setup %}  I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.    I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies. Imagine trying to write language translation software without having access to Google’s data? Or trying to do audio transcription by relying on publicly available data? It’s going to be impossible to compete by relying on publicly available data source while large companies build out their internal data monopolies - especially by using their existing products to  subsidize the cost  of collecting this data. Data also begets more data. By giving us great experiences, we’re willing to provide more and more information that is then used to launch new products which have us surrendering more and more data.    No matter how good an algorithm is it still needs data to be useful and I hope we’re not shooting ourselves in the foot by volunteering our data so easily. I’d love to see companies that collect user-contributed information be required to have it shared with their users so that they can have it used by other services. It’s not going to solve everything but it’s a step in the right direction.    Successful startups have always had to overcome challenges so the data monopoly problem will just be more of the same and should hopefully lead to some new approaches. An example that comes to mind is how  Duolingo  is able to generate revenue by selling document translations that are transformed into language lessons that are then done freely by the community. I’m excited to see new business models that are able to innovate past this data gap.
{% include setup %} A month ago I needed to duplicate a set of keys. In the past, I’d just go to the cheapest looking hardware store and they’d easily replicate my keys for around $2 each. This time, I tried the same approach but was told that they weren’t authorized to handle the keys I had and directed me to another locksmith. That locksmith told me that they wouldn’t be able to duplicate it without approval from my management company and also charged $18 for a duplicate. Amazingly enough, they were only able to duplicate one of the keys and I had to go to yet another locksmith (and get another approval) to get the last key duplicated.  I’m not sure if they’re trying to increase the security or whether they’re just trying to create an artificial monopoly for the locksmiths but it’s definitely a pain in the ass for the consumer. This is a standard approach for many industries: add needless complexity under the guise of security and then use that to justify higher prices. This is also why additional regulation isn’t always the answer - it leads to complicated, entrenched systems that can’t be easily innovated upon.
{% include setup %} There are only a few tabs I consistently keep open all day on my computer - Gmail, Google Calendar, Hacker News, and New Relic. Out of these, Gmail is the most important with my entire day running through it. The value of having a presence in the inbox hasn't been lost on companies and there are a ton of third party apps that make Gmail more useful -  Rapportive ,  YesWare ,  ToutApp , and  Boomerang . Even Google itself has been providing "Lab features" to augment the default inbox behavior.  One thing that bothers me is that this additional functionality is only provided via browser extensions. The only time I recall seeing an interactive behavior is when a form is embedded in an email and even then it's at the mercy of the  email client's implementation . I'd love to see a new inbox standard adopted that allowed email messages to be richer and more interactive rather than having to rely on a separate browser extension. Imagine being able to send out surveys that can be completed without leaving an email, emails that are able to show whether a product is available in real time (I know this can be done via server side rendered images but it's a hack), or being able to change the content text based on the time the email is viewed. These are trivial examples but this would open up potential for uses that we can’t even imagine. Of course, we’d have to be even more wary of smarter spam and inbox trickery but the potential value is worth that cost.
{% include setup %} After yet another attempt to deploy a  Django  application I decided to document the steps required to get everything up and running. The tutorials I’ve seen tend to focus on individual pieces rather than on the way all these packages work together which always led to me a lot of dead ends and StackOverflow so this will hopefully address some of those issues.  In particular, I want to focus on the configuration rather than the installation of the various packages since that’s covered in the package documentation.  I don't know if this is the best way to deploy Django but it's the approach I've been able to come up with by stumbling around and getting help from the docs, Google, and StackOverflow. If there are better ways out there please let me know.    	  		  Gunicorn - /home/ubuntu/project/scripts/start.sh  		 The nice thing here is that we define the port to serve our application on so we can serve multiple projects on a single server with each one using a different port. Note that the settings approach used here is from  Two Scoops of Django . 		  {% highlight bash %} #!/bin/bash set -e DJANGODIR=/home/ubuntu/project DJANGO_SETTINGS_MODULE=project.settings.prod  LOGFILE=/var/log/gunicorn/guni-project.log LOGDIR=$(dirname $LOGFILE) NUM_WORKERS=3 # user/group to run as USER=ubuntu GROUP=ubuntu cd /home/ubuntu/project source /home/ubuntu/project/venv/bin/activate  export DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE export PYTHONPATH=$DJANGODIR:$PYTHONPATH  test -d $LOGDIR || mkdir -p $LOGDIR exec /home/ubuntu/project/venv/bin/gunicorn_django -w $NUM_WORKERS \   --user=$USER --group=$GROUP --log-level=debug \   --log-file=$LOGFILE -b 0.0.0.0:8001 2>>$LOGFILE{% endhighlight %}      	  Nginx  - /etc/nginx/sites-enabled/project  	 The key parts here are that we're redirecting all www.project.com requests to project.com, serving the static files using Nginx rather than rely on Gunicorn, and passing other requests to the Gunicorn server running on the port defined in the Gunicorn start script above. 	  {% highlight nginx %} server {     # Redirect all www.project.com requests to project.com     listen 80;     server_name www.project.com;     return 301 http://project.com$request_uri; }  server {     listen   80;     server_name project.com;     # no security problem here, since / is alway passed to upstream     root /home/ubuntu/project/;     # serve directly - analogous for static/staticfiles     location /media/ {         # if asset versioning is used         if ($query_string) {             expires max;         }     }     location /admin/media/ {         # this changes depending on your python version         root /home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib;     }     location /static/admin {         autoindex on;         root   /home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib/admin/;     }     location /static/ {         autoindex on;         alias   /home/ubuntu/project/assets/;     }     location / {     # This section is to redirect all http traffic to https if desired     # if ($http_x_forwarded_proto != 'https') {     #   rewrite ^ https://$host$request_uri? permanent;     # }          client_max_body_size 5M;         client_body_buffer_size 128k;         proxy_pass_header Server;         proxy_set_header Host $http_host;         proxy_redirect off;         proxy_set_header X-Real-IP $remote_addr;         proxy_set_header X-Scheme $scheme;         proxy_connect_timeout 300;         proxy_read_timeout 300;         proxy_pass http://127.0.0.1:8001/;     }     # what to serve if upstream is not available or crashes     error_page 500 502 503 504 /media/50x.html;{% endhighlight %}      	  Supervisord  - /etc/supervisord/gunicorn-project.conf  	 Here we just specify the location of the Gunicorn start script so Supervisor can manage it.  {% highlight ini %} [program:gunicorn-project] directory = /home/ubuntu/project user = ubuntu command = /home/ubuntu/project/scripts/start.sh stdout_logfile = /var/log/gunicorn/project-std.log stderr_logfile = /var/log/gunicorn/project-err.log{% endhighlight %}
{% include setup %} Working on  Makers Alley , I've spent a fair amount thinking about the evolution of manufacturing and wanted to share an extremely condensed history.  For most of human history, people either made what they needed on their own or traded with a local craftsman. Over time, this led to a specialization in skills and also the rise of the apprenticeship model. Since trade was mostly local, it was difficult to build a large business and most businesses were family run with parents passing down skills to their children.  This practice remained consistent until a couple of hundred years ago when water and steam power starting taking hold. For the first time, work could be done independently of human labor and started the trend of specialized machines replacing specialized people. The increase in machine efficiency and the reduced skill of workers led to drops in the cost of labor and cheaper products.  The next major shift occurred when electricity became prevalent. This allowed factories to be built anywhere power was available and the locations were now chosen based on the price of labor and the cost of shopping. Thus, many factories ended up being built near cities with harbors and railroads.  At this point, globalization was still in its infancy since the transportation costs were extremely high due to lack of automation and standardization. Only when containerized shipping took off in the second half of the 20th century did shipping costs plunge and allowed companies to move their factories to locations with even lower labor costs. On a side note, read Marc Levinson's  The Box  to understand the impact of the shipping container.  This is the current situation with the majority of manufacturing being done abroad using materials that are sourced from across the world and then shipped and sold worldwide as final products. It’s impossible to predict what will happen over the coming decades but the combination of rising labor costs, demand for customizable products, and 3D printing suggest that manufacturing is going to start moving back towards local, agile methods. At first, it will probably be a hybrid approach with the bulk of the components still being mass made but then customized in our homes from 3D printed parts. Over time, as the quality and cost of 3D printing improves, more and more of the components will be customized, printed, and assembled at home. We’ll see the creation of a new profession - a combination of industrial designer, modeler, and tastemaker who’ll need to help us navigate this new manufacturing world. I’m excited.
{% include setup %} I don’t understand why websites try to compete on having the cleverest 404 page. The fact that someone ended up on a 404 page is a sign that something is broken but instead of trying to fix the problem they try to distract their visitors by making them laugh. It’s equivalent to getting to a restaurant and seeing an amazing menu only to discover that it’s closed.  We can’t always control which URLs our visitors will type in or click on but we can control what they see when they get there. Instead of trying to distract them with humor why not offer suggestions for what they may have wanted to see? The majority of 404 visits are the result of typos which could be fixed with a simple spell check and the remainder are due to moved pages which can be solved by notifying the linker or providing a redirect.  I’ve had some free time over the past month and put together the basics of a simple tool to help sites improve their 404 pages. Appropriately, it’s called  Better 404  and I’m currently running a beta period to get feedback and work out any kinks. If you manage a site and are interested in trying this out, let me know and I’ll help you get started.
{% include setup %} I don’t know whether it’s due to the upcoming version of iOS or Windows 8 but it feels as if flat design is getting more and more common. In the past couple of weeks, I’ve noticed two “mainstream” sites,  Thesaurus.com  and  Optimum , adopt a flat design which I suspect is the first design change they’ve made in years. Many companies are updating their iOS apps in time for the fall release and I understand the motivation to want to fit the style but it’s interesting to see websites doing the same. I wonder whether we’ll see more sites adopting this flat design in the next couple of months.
{% include setup %}               Netflix  recently reintroduced  profiles  so now each household member can get their own recommendations, recently watched items, and instant queue rather than being forced to share the same polluted profile. This is an awesome win for Netflix customers but it’s been bugging me that they didn’t do this sooner; it’s such an obvious feature that it should have been built as soon as Netflix realized that multiple family members would be sharing their account.    The cynic in me says they waited 5 years to bring profiles back in order to force family members to sign up for additional Netflix accounts if they wanted to keep their account history private and the only reason they enabled this functionality now is that they’ve either already captured the bulk of these additional accounts or that customers have been asking for this for so long that ignoring the requests makes Netflix look callous. Even now, the profile functionality remains open with all users of a Netflix account being able to access any of the profiles, forcing household members who value their privacy to sign up for additional accounts.  I may be completely wrong and Netflix is correct that making profiles more complicated will cause significantly more usability problems for the majority of users while only benefiting a tiny percentage of customers that care about private profiles. Simplicity is critical for the success of a consumer product but I’m skeptical when user experience and simplicity are used as excuses to avoid a simple feature change. In this case it could be as simple as adopting a model similar to  parental controls  which are set via the web interface for each profile.
{% include setup %} This year, I started volunteering at a program called  TEALS . The long term goal is to improve computer science education in the United States by having tech professionals volunteer their time to teach computer science classes in schools that want to offer computer science classes but don’t have the necessary teachers. Over time, the goal is to have the in-service teachers in each class learn the material so that they will be able to teach it in the future. Currently, the program exists in 65 high schools across 12 states and offers both Intro to Computer Science and AP Computer Science but I’m looking forward to seeing it expand nationwide and into middle and elementary schools.  Throughout the year, I plan on documenting my experiences remote teaching at a high school in Kentucky as well as sharing the lessons I’ve learned. So far, it’s been a little more than a week and I already developed a much bigger appreciation for teachers and the effort required. The most time consuming piece so far has been preparing daily lessons that balance the requirements of the AP test and everyone’s skills and interests while still being engaging enough when delivered via a video chat. I also discovered the difference between lecturing and teaching: my initial approach was to just go through the prepared slides but am now spending a lot more time thinking about tricky concepts and the exercises that will both get the point across and keep students engaged. The best approach so far has been starting a class with a brief explanation of the concepts, going through some tricky exercises, and then diving into code to put it all together.  If you have computer science experience this is a awesome program to volunteer for so please feel free to reach out to me if you have any questions.
{% include setup %} When we launched  Pressi , I had it set up under my personal AWS account. Recently, we needed to move it into a separate AWS account and I wanted to share the steps to help others running into the same issue. Unsurprisingly, most of the effort went into planning and figuring out the migration steps and order in which they should be done. We weren’t able to eliminate downtime entirely but we reduced it as much as we could.  The services migrated included Route 53, an EC2 instance, ELB, S3, and Cloudfront. At the high level, we copy every service we can (EC2, ELB, S3, Route 53) to the destination account before redirecting client traffic to the new account. After that, we migrate the remaining services (Cloudfront) and make updates to existing ones (Route 53, EC2) to point to the destination account.  Migrating EC2 and ELB:    	 Create the destination AWS account  	 Create an AMI of the instance on the original account  	 Share this newly created AMI with the destination AWS account  	 Launch the AMI in the destination account  	 Set up the load balancer in the destination account to mirror the original     Migrating S3/Cloudfront:    	 Create an S3 bucket in the destination account and copy the files over from the original bucket to the destination bucket. We used  Bucket Explorer  for this piece but needed to change the file permissions in the destination bucket manually to mirror those in the original account. One thing to watch out for is that S3 bucket names need to be unique so your code will need to be updated to reference the new name.  	 Update the Cloudfront record in Route 53 to point to the destination account. Note that after the migration runs you can also update the Cloudfront record in the original account to point to the Cloudfront CNAME of the destination account.  	 Cloudfront requires unique CNAME records so we give it a temporary name until you kick off the migration. As soon as you do, you will need to remove the CNAME record from the original account and add it to the destination Cloudfront account.     Migrating Route 53:    	 Copy the records from the original account to the destination account.  	 Make sure to update the Start of Authority (SOA) and Name Server (NS) records in the original account to have the same values as the ones in the destination account to speed up the DNS propagation.     Migrating the code:    	 This will entirely depend on the application but the goal is to update your code to reference the services on the destination account.  	 Due to the non-immediate nature of DNS propagation, you will most likely need to run two code bases - one on the original account pointing to some of the original services and one on the new account pointing to the destination services. Depending on the statelessness of your code, this may lead to a variety of sync issues and will require some intricate code to handle properly. In our case, we had MySQL running on the EC2 instance so while the app was running simultaneously under two AWS accounts the database would get out of sync with some users hitting the original setup and others hitting the destination. Luckily for us only a few tables were affected and we had to run a few manual SQL queries to deal with the issue but it could have been a lot worse.     The last step is to update your domain registrar NS records to point to the destination account and wait for the migration to occur. Note that the migration will happen gradually so you should look at the server logs on both accounts to make sure there’s no traffic hitting the server in the original account.  The lesson here is that migration becomes a whole lot easier if you keep your architecture as stateless and modular as possible. This way the services are loosely coupled and can be migrated one at a time rather than having to do everything at once. Your app also becomes significantly easier to scale since additional EC2 instances can be provisioned without having to worry about them getting out of sync. The non-instantaneous nature of DNS complicates the migration but a stateless architecture helps address most of the issues. Our migration didn’t go 100% smoothly but having mostly stateless services definitely helped us avoid major problems.
{% include setup %} How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.  In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend. Unfortunately, copying and pasting didn’t work so I decided to jump into JavaScript. Hope these steps give a sense of how to approach a simple scraping problem. The idea is to use the browser’s inspect element feature to find the pattern that the element we’re interested in have in common. Then, we use JavaScript to find the elements matching that pattern and extract the information we want.                   1. The page we want to parse - please ignore the quality of my fantasy team.                                    	 2. Use the Chrome "Inspect Element" feature to figure out the HTML/CSS of the element we're interested in. In this case, the element containing player name has the class value “name playernote”.                                    	 3. Run a JavaScript command to get all the HTML elements that have those classes.       	{% highlight javascript %}document.getElementsByClassName('name playernote'){% endhighlight %}                                           	 4. Store those HTML elements in a variable so we can quickly iterate through the list. 		{% highlight javascript %}players = document.getElementsByClassName('name playernote'){% endhighlight %}                                           	 5. Use JavaScript to go through the previous list and extract the player name. Then we can just copy and paste the list of names without having to deal with the formatting issues.       	{% highlight javascript %}for (var i = 0; i                         In addition to extracting information, JavaScript can be used to interact with a web page. This comes in handy when you want to automate a certain action on a site that would take too long to do manually. For example, I was able to code up some quick JavaScript that would go through a list of my Facebook friends and invite them to like my startup’s new page. Hope this little JavaScript hack comes in handy and let me know if you have any questions.
{% include setup %} Although I come from a backend background, I’ve been spending more and more time on the UX side of things and have been picking up quite a bit - a combination of using Twitter Bootstrap on my projects, subscribing to the Hack Design lectures, and following a ton of designers on Twitter.  Something that’s been bothering me is this obsession with trying to make every product as intuitive and approachable as possible. That’s the right approach when focusing on mass market consumer products but if you’re building internal tools or targeting power users a simple, approachable product might be antithetical to what you actually need.  The tradeoff is between a product that people can immediately start using versus a product that takes time to learn but becomes significantly more powerful when mastered. The developer equivalent would be using a basic text editor vs vi or emacs. The text editor is easy to start using but you hit a productivity ceiling quickly; vi or emacs, on the other hand, take a while to learn but you become significantly more productive than if you were using a text editor.  The challenge is knowing your audience and building the product that will solve their problems. Sometimes it will need to be simple and other times it will need to be complex. This applies at multiple levels - the product may for the most part be simple but certain features will need to be complex in order to be useful.  Many websites and apps have adopted the approach of where it’s extremely easy to get started but provide advanced features for the users that desire and discover them. Excel provides shortcuts for the power user that make it possible to do anything without touching the mouse. Gmail, in addition to shortcuts, provides a “labs” feature that lets users enable more advanced features.  I’m interested in what happens as companies grow and try to increase their market. Some may have started with a complex product that solved a niche problem that they want to simplify in order to appeal to a bigger audience. Others may have started with a simple product that they now want to position to power users. In both cases the challenge is being able to support both use cases without negatively impacting either one. Maybe the right approach is to launch the new product under a different name but I’m curious to see creative solutions that aren’t about adding shortcuts or a settings page.
{% include setup %} When I was making the leap into the startup world I read every post I came across that talked about people’s experiences and guides in running a startup. The goal was to learn as much as I could form others and apply these hard-fought lessons my own startup. Now that I’ve been working on a startup for almost two years I realize how much startups differ from one another and how black and white these guides tend to be. You can read two posts that will promote contradictory approaches. Should you focus on revenue or growth? Should you raise money or bootstrap? Should you go with a freemium model or paid only? Should you go solo or get a cofounder? Should you focus on consumers or the enterprise?  None of these questions have a universally right answer. What worked for one startup will not necessarily work for another one. There are just too many differences; the product, market, teams are all different. Time plays a huge factor as well. In a field as quickly moving, and novelty loving, as technology what worked 6 months ago may not have a chance right now. Startups are tough. If it were as simple as just following a how-to guide the success rate of startups would be an order of magnitude higher than what it actually is.  The best we can do is be aware of the available options and try to understand why certain strategies worked for others. We shouldn’t ignore what we read but we also shouldn’t emulate an approach just because others succeeded with it. We need to be the experts of our markets and imitating others only undermines that knowledge.
{% include setup %}          	    		        		         	               I’m not sure why this needs to be said but if your site offers infinite scroll make sure you don’t have anything clickable in the footer. I’d expect the occasional site to succumb to this but I was surprised to see it happening on LinkedIn. All I wanted to do was read the developer docs but unfortunately the link is located in the footer which provides a nice challenge of clicking the link before new content is loaded. I wasn’t quite able to get it and ended up just searching Google for the LinkedIn documentation link. If your site’s content is only accessible via a Google search you have a problem.
{% include setup %} A trend I’ve been seeing lately is companies boosting their sales by focusing on customer education. The successful companies don’t just focus on the results their product will deliver but also spend time explaining why those results are important and how the product works and how it can be used.  This approach seems obvious to me. Borrowing some terminology from  Crossing the Chasm , the early adopters will use your product as long as it solves an existing problem but education will help the remaining, slower adopting customer segments discover that they even have a problem and look to you for a solution. In my opinion, the major benefits of customer education are to reduce acquisition costs and improve retention. Acquisition costs will drop as you start relying more on inbound interest rather than on outbound sales. Retention will increase since customers that sign up willingly will stick around longer than customers who needed to be coaxed into it by a sales rep. These “self serve” customers will also be more likely to blame themselves when encountering problems rather than whoever got them to sign up. In addition, by developing original and useful content you’ll help your SEO score which will drive more potential customers to look at your products. Your trustworthiness will also improve since you’ll be offering free and useful knowledge.  In many ways, freemium and free trial products are pursuing this strategy by allowing their products to be used with the hope that these users will turn into customers after they discover that they’re able to solve real problems using the product. A sales rep can then work with the customer to understand his needs and pick the right product offering to solve his problem.  I believe we’ll be seeing much more of these self serve, education products in the coming years and it’s important to get involved in this shift now. In my mind, the two companies that have been doing a great job with it are  New Relic  which is providing a great way to try their monitoring service and Mixpanel, which has a  dedicated portal  focusing on analytics education.
{% include setup %}            Although I’ve been meaning to visit the  Museum of Math  ever since it opened in December, I only got the chance to do it this Labor Day. I wanted to share my thoughts and encourage everyone who can to visit.  I love the mission. Math should not be taught in a vacuum and having various activities that each showcase different mathematical properties is a great approach to get kids (and adults) engaged while learning some math. Some of the activities that stood out to me were bikes with differently sized square wheels that can only go around a certain diameter track; a "helix" shape that explains multiplication by lighting up a fiber between the numbers and highlighting the resulting value; and a fractal tree generator that would use your body to create the trunk and branches. I enjoyed these since they had an interactive physical component that provided immediate feedback.  There were also a bunch of activities that were primarily software based. Two examples are a "kaleidoscope" drawing tool and an app that explores 3D shapes and functions by letting you tweak the parameters. These weren't very engaging and most were abandoned quickly. In addition, some of the tools either had broken sensors or were buggy which made them less fun than they should have been.  A museum should not be able to replaced with iPad apps and MoMath places too much emphasis on the software. They should move away from these apps and focus on the physical exploratory activities that cannot be recreated at home. I found that there was very little continuity between the exhibits and wish they did a better job curating so that lessons from one could be applied to another. This would limit the breadth but would make the experience more valuable. Similar to an art museum, they could have a base collection that never changes as well as exhibits that rotate every couple of months.  The museum has been open for less than a year and I’m optimistic it will only improve. I only wish there were more math museums opening up.
{% include setup %} Since becoming active in the startup scene, I've been meeting a ton of founders and am annoyed by how much easier it is to offer suggestions than to apply them to myself. My most common suggestion, in true lean startup fashion, is to advocate a quicker or cheaper way to validate the market before building a product and yet it’s extremely difficult to take my own advice. I’ve been working on  Better404  on and off for two months now and know I should get it in front of potential customers and yet I keep on making minor tweaks and updates to the product.  I'm not sure if this is due to how much easier it is to say something versus executing it or just a fear of failure but it’s something I’m becoming more and more aware of. At least now I’m aware of this bias and and can work on correcting it. What’s been working well so far is coming up with a framework that can be used to evaluate a startup in a similar space and then using it to evaluate my own. The challenge is coming up with the framework without biasing it knowing that you will be using it to evaluate your own project. Nonetheless, this additional indirect step does help.  I’ve been looking at other SaaS businesses and listing what they could be doing better and what they’re doing well as well as what I’d do if I were in their situation. Having a concrete target for this sort of analysis is a great way to come up with a large number of specific suggestions and although many of these aren’t relevant a few can be applied to what I’m working on.  To succeed, it’s critical to be able to look at your startup from an unbiased, external perspective. This is even more true as a solo founder since there’s no one else bringing their own ideas and experiences. I’d love to know how others are overcoming this.
{% include setup %} The tech world is conflicted about how much math a developer needs. Engineers working on quantitative systems or data science clearly require advanced math and there are also countless engineering roles where math is unnecessary. My experience is that even if you don’t use math, having a mathematical mindset makes you significantly more productive. You’re able to quickly estimate the complexity of various tasks and hone your intuition. You’re also able to quickly recognize patterns when refactoring, especially when working in a functional language. A basic understanding of probability and statistics is a great way to analyze the performance of your code as well as help you model and understand your application behavior. I wanted to share a quick story of how a mathematical approach came in handy when working on Pressi.  First, a little bit of background.  Pressi  is a social media mashup page that takes the content a user posted across a variety of social media networks and creates a “Flipboard” style web page to showcase it. At launch, we had a simple cron job that would run every hour and pull new data for each of our users. Over time, we migrated to a task system that let us run these retrieval tasks in parallel and split across multiple machines. Using this approach, we were able to scale well and handle the increased volume but our hosting costs saw a big jump so we went looking for a solution.  Luckily for us, we tracked the history of each social network data pull (containing user, network, datetime, and # of items pulled) and doing a quick query told us that close to 92% of our requests resulted in no data being retrieved. The intuition behind this is that most people will not be posting on every social network every hour. By eliminating these calls we’d be able to drastically cut our hosting costs.  This analysis got us thinking about the ideal case which is for us to pull a moment immediately after it’s posted. One way to achieve it was to leverage the push updates that some of the social networks supported but we wanted to find a more general way that could tell us when we should pull the data for a particular user/network pair.  To figure this out, we looked at another distribution: the average number of moments shared by a user on a network per day. This let us look at the number of users who were extremely active on social media down to the users that pretty much only had accounts. We then dumped this data into Excel in order to come up with ranges that we’d use to segment our user/network pairs in order to see how often we should attempt to pull their data. For example, a user that on average posted 20 updates a day on Facebook would have their Facebook data pulled every 4 hours but a user who posted on Instagram less than once a day would have their data pulled once a day. This also gave us a way to estimate how many fewer calls we'd need to make compared to what we were currently doing and therefore approximate the cost savings. The result of this update was that we dropped the number of useless requests from ~92% to just over 40%. This was by no means perfect but gave us improvement we needed. An additional update we modeled out but but didn't get a chance to implement was to look at day of week and hourly patterns in order to identify when users were actually posting rather than treat every day and hour the same way. The data clearly showed that users had well defined schedules which would have led to another nice improvement.  The key lesson here was that we started by leveraging the data we collected to identify the major cause of our cost increase and then identified the metric we wanted to optimize. In our case it was to reduce the volume of empty requests we were making while making sure that we did not significantly increase the average number of moments that were retrieved for non empty calls. Otherwise, we could make 1 request a week for each user/network which would pretty much drop the number of useless requests to zero but blow up the number of average moments retrieved per call. We could have chosen a variety of other metrics but went with this one since it was intuitive, easy to model, and easy to test. The other neat property is that it’s self correcting so if a user changes their behavior on a particular network we’d shift them into another bucket.  None of the math we used was very complicated and although we tried playing around with a few statistical distributions to model out the user posting behavior we ended up quickly abandoning those when we saw the impact we’d get from a simple approach. I’d bet that almost every code base has something that can be improved with a little bit of mathematical analysis.
{% include setup %}            Excel has developed a reputation of being bloated, slow, error prone and used primarily by "business people" who don't have real quantitative skills. Just like anything else, Excel is a tool that can be misused but is significantly more useful than people give it credit for.  The most important benefit Excel provides is making data approachable and fun. By making it approachable Excel opens up data analysis to a ton of new people that come into it with their own experience and knowledge. Sure they may not have data scientist skills but they're still able to run some neat analyses and derive useful insights.  The fun makes it very easy to experiment and try a lot of different ideas by making the cost of failure so low by providing quick feedback and visuals. The value of writing a formula and then dragging it down, quickly seeing the calculations is massive. This gives the quick feedback that encourages people to keep on driving their analysis. And although Excel's visualizations are simple, they provide a fast way to visualize the data and hopefully lead to more analysis. Similar to the way we use Python for a quick project instead of Java, it's much easier to run a quick analysis in Excel than in a "real" language such as R.  My typical approach to quantitative problem is to write a query to retrieve the data I want and then immediately dump it into Excel for a quick analysis. This lets me apply some pretty basic formulae and visualizations to to see if there's anything worth pursuing in more depth. Only then will I move to R or Python to do a deeper analysis. Even then, I most likely rewrite the code to make it ready for production. This approach forces me to focus on the data and dimensions I want to analyze. Excel only serves as a way to quickly explore the data before deciding whether there’s anything worth pursuing.  The only tool I can think of that comes close is  Tableau  but my experience has been that it has a somewhat steeper learning curve and doesn’t support the flexibility to quickly add and adjust various calculations. Replacing Excel is tough. I use Google Docs for working with documents and yet for my data I use Excel rather than Google Spreadsheets.
{% include setup %} I've been doing some consulting work over the few months and wanted to share a pricing model that’s been working well for smaller projects. I’ll sit down with the client to understand the scope of the project and work with them to break it down into smaller, more manageable components. Based on this break down, I’ll estimate the time required for each piece and come up with an estimated total time. I charge my usual hourly rate for the work that falls within the estimated time but will charge a steeply discounted rate for every hour that goes over.  This aligns incentives since it gives me an incentive to complete the work within the estimated time and helps the client feel more comfortable that the project won’t run past its estimate for the sake of me working more hours. The other major benefit is that it forces us to scope out the project together so we’re both on the same page and avoid any future surprises.  I don’t think this approach would work well for larger projects since those are significantly more difficult to estimate but any feature can be broken down this way. For larger projects, I think trying to bill weekly as  advocated by Patrick  is the right approach if you’re able to agree with the client on it. In my case, I have multiple projects going on simultaneously so it's been difficult getting a weekly rate worked out.
{% include setup %}   	  		  A couple of months ago I started noticing popups on various ecommerce sites offering a first purchase discounts in exchange for entering an email address. Every time I noticed this happening I took a screenshot to track the offer and compile a list of the retailers using this approach. I’m still collecting examples and would more of them but so far the going rate seems to be anywhere from 10 to 25% off the first order. The pitch is pretty compelling and I think most people would gladly give up their email for the possibility of a discount. I’d also love to know what impact the magnitude of the discount has on the sign up rates; I suspect it’s minimal but definitely better than gaining entry to a sweepstakes or a lottery. 		  	  	      Company   Offer      Ann Taylor  Lottery    Blue Nile  Lottery    Bonobos  20%    Gap  25%    Wayfair  10%    West Elm  10%    Williams-Sonoma  10%     	             	                  Ann Taylor offers the chance to win a vacation                  	                  Blue Nile offers the chance to win a diamond                  	                  Bonobos offers 20% off your first purchase                  	                  Gap offers 25% off your first purchase                  	                  Wayfair offers 10% off your first purchase                  	                  West Elm offers 10% off your first purchase                  	                  Williams-Sonoma offers 10% off your first purchase
{% include setup %}  Many people  expected  the iPhone 5C to be priced low in order to compete with the cheaper Android phones in countries without carrier subsidies. The news that the 5C’s starting price was $549 left many in the tech community  surprised and concerned  with many believing that the price needed to be lower than $400 in order to compete worldwide.    I'm definitely speculating but I believe the reason for such a high price for the 5C was to avoid cannibalizing the sales of the 5S while also framing the comparison to be iPhone vs iPhone instead of iPhone vs Android. When people go smartphone shopping they see that the 5S is “only” $100 more than the 5C and pay the difference for the more premium product. If the 5C were significantly cheaper people would be comparing it to a similarly priced Android phone which may encourage them to go with the Android or they'd compare it against the 5S which would get many to purchase the much cheaper 5C instead.     Localytics  put together a  great chart  showing the share difference between the 5S and the 5C which supports this view. The 5S is outselling the 5C in every country by a large margin. And although Apple could get higher sales volume with a cheaper 5C, I believe that the cannibalization of the 5S and the reduction in margins would have made Apple less profitable overall. At the same time., charging a lower price for the 5C to get more market share at the cost of profit may have been the right long-term decision.     	  		   	      Pricing products that are sold worldwide is a tough problem since every country has its own economic environment with unique shopping behaviors. This challenge is magnified for products that can easily be bought in one country and resold in another. The opportunities also vary significantly: the US is a mature smartphone market where carrier subsidies exist and make the buyer less price sensitive while in China many people are getting smartphones for the first time and lack carrier subsidies that will help reduce the initial sale price. We’ll just have to wait to see whether this was the right pricing decision.              Note: This reminded me a pricing table the Economist had a while back that looked like an error. There were three options: 1) a web-only subscription for $59, 2) a print subscription for $125, and 3) a print and web subscription for $125. In this case, the last two options were the same price but option 3 offered more value so it was clearly better than option 2. Yet by having option 2, the Economist got people to compare option 3 against option 2 rather than option 3 against option 1. This phenomenon is called the “ Decoy effect ” and worth understanding, especially if you’re a marketer.
{% include setup %} Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.  To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &lt;td&gt; tag and then wrap each row in a &lt;tr&gt;tag. I’d then copy and paste the result into the text editor to add the header row and finish up the styling. To generate a growth-share matrix, I’d just use Google Drawing or Keynote to draw the axes and labels before taking a screenshot and cropping it into a square.  The solution to these was a bit of JavaScript with some help from  StackOverflow . These tools are hosted on  GitHub  and accessible via  http://dangoldin.com/js-tools/  and are under the MIT License. As I run into more of these I'll keep on adding various tools to this list. If you have any suggestions or want to add your own let me know.
{% include setup %} As I've gotten older and most likely more mature, I've become far less cynical. I used to be dismissive of people trying to improve things and believed that they were just wasting their time and nothing would change. Yet as a I've gotten older I've come to appreciate this effort even if it doesn't lead to noticeable progress.  The fact that someone is working for their beliefs should be applauded. The waste is dismissing others’ work while sitting in front of a computer or a TV. We all want to see progress and yet we exert effort belittling others that are actually committed to making things better. If we applied this effort into our own passions we’d be all be much better off.  Teddy Roosevelt said this better than I ever could:   It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.
{% include setup %}      I'm not entirely sure why, but I started off 2013 with the goal of running 1000 miles which breaks down into a little more than 19 miles a week. Remarkably, I stuck with it and am somehow at 822 miles for the year and need to average 16/week for the remainder of 2013 to hit the goal. Yet it took me a surprisingly long time to work up to a weekly distance of 19 miles and even longer to consistently run more than 19 miles a week. My first run was less than 1.5 miles and I only started consistently eat into my deficit in June. It took me until the end of August to actually reach the 19 mile cumulative weekly average I need to maintain until the end of the year. It’s been an awesome adventure and I’ve even managed to run three half marathons and improved my time from 1:58:11 on an easy course to 1:56:11 on a challenging one. Here’s a table I put together showcasing my running progress over the course of the year. I’ll update it at the end of the year after hopefully achieving the 1000 mile goal.      Week #  Start of Week  Cum Dist Needed  Dist Ran  Cum Dist Ran  Cum Run Avg      1  1/1/2013  19.23  5.43  5.43  5.43    2  1/8/2013  38.46  11.94  17.37  8.69    3  1/15/2013  57.69  13.65  31.02  10.34    4  1/22/2013  76.92  11.88  42.90  10.73    5  1/29/2013  96.15  12.64  55.54  11.11    6  2/5/2013  115.38  19.16  74.70  12.45    7  2/12/2013  134.62  15.64  90.34  12.91    8  2/19/2013  153.85  0.00  90.34  11.29    9  2/26/2013  173.08  25.24  115.58  12.84    10  3/5/2013  192.31  24.27  139.85  13.99    11  3/12/2013  211.54  3.31  143.16  13.01    12  3/19/2013  230.77  7.09  150.25  12.52    13  3/26/2013  250.00  12.60  162.85  12.53    14  4/2/2013  269.23  7.89  170.74  12.20    15  4/9/2013  288.46  18.54  189.28  12.62    16  4/16/2013  307.69  12.80  202.08  12.63    17  4/23/2013  326.92  11.62  213.70  12.57    18  4/30/2013  346.15  21.60  235.30  13.07    19  5/7/2013  365.38  26.67  261.97  13.79    20  5/14/2013  384.62  25.10  287.07  14.35    21  5/21/2013  403.85  12.03  299.10  14.24    22  5/28/2013  423.08  13.73  312.83  14.22    23  6/4/2013  442.31  26.14  338.97  14.74    24  6/11/2013  461.54  23.71  362.68  15.11    25  6/18/2013  480.77  32.66  395.34  15.81    26  6/25/2013  500.00  19.94  415.28  15.97    27  7/2/2013  519.23  23.51  438.79  16.25    28  7/9/2013  538.46  40.54  479.33  17.12    29  7/16/2013  557.69  35.68  515.01  17.76    30  7/23/2013  576.92  29.05  544.06  18.14    31  7/30/2013  596.15  37.82  581.88  18.77    32  8/6/2013  615.38  11.37  593.25  18.54    33  8/13/2013  634.62  33.23  626.48  18.98    34  8/20/2013  653.85  16.83  643.31  18.92    35  8/27/2013  673.08  34.41  677.72  19.36    36  9/3/2013  692.31  27.70  705.42  19.60    37  9/10/2013  711.54  34.76  740.18  20.00    38  9/17/2013  730.77  9.19  749.37  19.72    39  9/24/2013  750.00  32.75  782.12  20.05    40  10/1/2013  769.23  23.66  805.78  20.14    41  10/8/2013  788.46  16.40  822.18  20.05    42  10/15/2013  807.69  0.00  822.18  19.58    43  10/22/2013  826.92  0.00  822.18  19.12    44  10/29/2013  846.15  0.00  822.18  18.69    45  11/5/2013  865.38  0.00  822.18  18.27    46  11/12/2013  884.62  0.00  822.18  17.87    47  11/19/2013  903.85  0.00  822.18  17.49    48  11/26/2013  923.08  0.00  822.18  17.13    49  12/3/2013  942.31  0.00  822.18  16.78    50  12/10/2013  961.54  0.00  822.18  16.44    51  12/17/2013  980.77  0.00  822.18  16.12    52  12/24/2013  1,000.00  0.00  822.18  15.81
{% include setup %} I'm frustrated by the expression "I don't have time". As my friends and I have gotten older, I’ve been hearing it more and more frequently. I’ve even caught myself using when trying to come up with an excuse when coordinating evening or weekend plans.  The reason I dislike the phrase is that it’s equivalent to saying "it's not a priority" and yet we phrase it such that we convince ourselves it’s something outside our control rather than due to the choices we make. I could go out until 3 AM if I make that a priority over running 6 miles in the morning before heading to work just like I could go catch a movie  instead of working on a side project. If we expressed our choices in terms of priorities rather than time we’d be more likely to deal with them.  Life is full of constraints and it’s impossible to do everything we want. This will only get worse as we get older and deal with more “grown-up” things. Better to develop the right mindset now rather than realize it later.
{% include setup %} My mom owns a small local business in suburban NJ,  The Do Re Mi School , that’s akin to an after-school program where music, dance, art, language and math is taught. Being surrounded by a family of engineers, we’ve been helping her on the tech side and my brother created the web site she’s been using it for the past couple of years. It’s based on Drupal and allows her to make changes without having to dive into the tech details. This approach has been working well she’s recently started using YouTube, Facebook, and Twitter to help with her marketing and social efforts.  Earlier this week, she sent an email saying there was a new website, bestnewjerseyartsschool.com, that was completely ripping off her site. They claim to be Do Re Mi and have copied various parts of the content, including paragraphs of text and various images. They’ve even created a YouTube video, www.youtube.com/watch?v=lvE6tBl8xU4 that's embedded on the site’s home page.  Looking at the whois and dns info doesn’t reveal much since they’re using a privacy protection service. All I know is that the domain was registered in April using WildWestDomains, is protected by DomainsByProxy, and that the name servers are under  hbuse.com  which claims to be "Hosting by unbelievably sweet elepehants" and yet doesn’t contain any real content and is not in Google's index. The interesting thing is that looking at the whois info for hbuse.com indicates it was registered by someone with a PO Box in Nevada with an email address that also seems to be anonymyzed. I found a site,  yougetsignal , that allowed me to search for other sites that were all hosted on “hbuse.com” and came up with a list of ~270 sites that all look to be site rip offs. They all seem to follow a pattern of having the location, the service, and possibly an adjective (best being the most common). Below's a screenshot of some of the sites that I found sharing the host.      I don’t know what the motivation behind these. They are all running adsense so I suspect part of it just a way to generate easy ad revenue but the more cynical part of me thinks it’s a way to blackmail the existing sites into buying these fake domains to avoid SEO penalties.  It’s terrible that honest business owners have to deal with these things. They don’t have the background to know what to do and many are not even aware that their brands are being manipulated, damaged, and monetized. There’s also an SEO risk that these rip-off sites will start dominating the search results and hurting business even more. And since these sites are using AdSense, Google is able to generate more revenue.  I’d love to know what I should do next but my current thinking is that I should send a DMCA takedown notice for my mom’s site and report this entire list of rip-off sites to Google and hope that they stop AdSense from running on the sites and remove them from search results.  I worry that as it becomes easier and easier to generate written content using software we’ll see more and more of these scenarios where it’s going to become increasingly difficult to find the source of the original content and real site owners are hurt.  Update: So the way my mom discovered this other site is because she got a notice from Getty images that she was using one of their images without licensing it. When she asked which image they sent her a link to that other site that has her contact info on it. I guess that answers the question as to why that other site still uses the actual contact information.
{% include setup %} I don’t know why, but I’ve become more aware of the UI/UX of various sites and apps that I encounter. Whereas before I might have gotten frustrated about some behavior, I’m now starting to get annoyed whenever I encounter something that’s obviously crummy. Here’s a few of the more recent design anti-patterns I’ve been noticing.       Submitting a login form with the wrong password removes the entered email address. Especially on mobile, where it both takes longer to type and typos are more common, it’s crappy having to type both my email and password again if I made a simple typo in my password or just don’t know which of my passwords I used. A quick hack I saw that makes this a bit easier is to add a keyboard shortcut to your phone to replace “@@” (or any other character set) with your email address.                                Confusing placement of sign in and register. I forget which app I saw this in but as you can screen from the screenshot I’m on the sign in screen and yet the button under the form is to register, which causes the app to load the registration screen. The sign in button is up top which is a confusing flow since the user goes down the page first before having to go back to the top. The fact that the app uses a flat UI makes this worse since there’s not a lot of differentiation between the sign in and register actions.                                   Create new versus add to existing contact. This is probably the most “first-world” one here but without knowing who is currently in your address book it’s impossible to know whether you want to create a new contact or update an existing and contact. My current approach is to choose add to existing, realize that I actually don’t have that contact in my address book, and then go back a few screens and choose create new. A common database operation is “insert or update” - insert if it doesn’t already exist and update if it does. I’d love to have something like that to manage my address book.
{% include setup %} To supplement my income while working on a startup, I took on a few consulting projects and wanted to share some lessons learned. It seems that everyone’s consulting experience is different so consider mine experience as just another data point.       I was able to get more work from my existing network than anything else I tried. As soon as I told people I was looking to take on some consulting projects I was able to get interest and referrals. If I didn’t have that I’m not sure how I would have gotten my first few projects.     It took longer than I expected to agree on a project’s scope and get the contract signed. My approach was to do a call or meeting to understand the goals of the project and then break it down into components with an estimated time and cost for each piece. I liked this approach since we were able to discuss the priorities of various pieces and talk about the risks associated with each.     It took longer to get paid than I expected. I was confident that I’d get paid but it took a few emails and meetings to get the payments made. The part that helped was getting an initial deposit before starting the work.     The biggest benefit was the flexibility to choose when and how to work on the projects.  Unfortunately, this flexibility is better in the abstract. I didn’t find the flexibility that valuable since almost everyone I know is working at a full time job which causes me to also follow a pretty standard schedule.     Most of the knowledge I gained was on the business/marketing side rather than on the tech side. I wasn’t doing challenging work and for the most part didn’t get a chance to work closely with others. The projects I did were also pretty independent so I had to resort to Google and Stack Overflow to help me deal with various questions that came up.     The projects I had were not critical to the company and were mostly “nice to haves.” This had the effect of me not feeling very aligned with the company vision which made the projects less interesting than they should have been. I’m not sure if this was due to the way I positioned myself for consulting work or due to the companies not wanting to outsource their critical projects.     A shared GitHub account worked amazingly well. The client was able to track the progress and provide feedback at various stages. This required me to commit well documented, working code but it definitely made communication easier. I also had a staging environment set up for my projects which let the clients see the code in action.     I wrote a  post  last month on pricing smaller consulting projects and wanted to highlight that again. I would come up a time estimate for a project that would be billed at my usual rate. Any work that spilled over would be billed at a discounted rate. This gave clients confidence that my estimate was reasonable and gave them a sense of the total project cost.     This was my first time doing serious consulting work and it’s a mixed bag. I enjoyed the flexibility but didn’t find it being a huge deal. I was also taking on projects that paid the bills but weren’t the most exciting. My biggest gripe was that I felt I wasn’t learning as much as I would have had I been working as part of a team. This gave the illusion that I was falling behind on my skills and not improving as much as others were.If I were to do it again, I’d want to specialize in a particular field and only do projects that fit in with my passions and interests. I’d also want to get it to the state where I’d be working alongside others rather than being entirely independent.
{% include setup %} Last week I  posted  about a site my mom discovered that had copied the content from her site and has been positioning as their business. I had no clue what the motivation behind it was other than thinking it was just a sketchy way to either blackmail the real business owner or use black hat SEO tactics to increase traffic and monetize using AdSense.  After sharing my problem and getting a ton of advice, I sent a DMCA request to the host, Colocation America, and received a surprisingly quick reply. Within a few days I was on the phone with a representative from the site claiming that my mom had signed up for a service that was offering free mobile websites and that’s why my mom’s business information and content had been appearing on the other site. My mom doesn’t recall signing up for any site and I believe her - she’s been sending me nearly every offer she receives asking whether it’s legitimate and worth doing and I don’t recall ever seeing this one.  I suspect they use this as a cover for if they get caught and until then they make some money from their ads. In any case this particular issue was resolved pretty quickly and I wanted to thank everyone who helped. In particular,  Rob Adler  did a ton of research on the infringing site and  Matt Cutts  helped send this to the relevant teams at Google. The web’s not always getting worse!
{% include setup %} In honor of today’s NYC marathon, I finally finished up this post that’s been sitting in my drafts folder the past few weeks.  I’ve never been into running until the beginning of this year when I decided to run 1000 miles. This led to me to three half marathons and is actually making me consider doing a full one. It’s amazing where a habit and a bit of effort can take you. Initially, I ran just to hit my goal and only signed up for races in order to keep myself motivated and on track. Now, it’s become significantly more than that. There are so many things outside my control yet running is solely about my effort and willpower. If I fail it’s my defeat and if I succeed it’s my victory. I can easily skip a run on a cold, rainy morning and yet I know I’m just deceiving myself and I’ll have to make it up later. Running is one of the simplest things to do and that’s a huge appeal. The human bodies have evolved to run and kids start running as soon as they learn to walk. As our world becomes increasingly complex it’s nice being able to escape with a quick run. Whether it’s running or something else, it’s important to have an outlet that rewards us based on our efforts. The further away this activity is from our day jobs the better.
{% include setup %} Over the past few days my inbox has been filled with security alert emails caused by the  MongoHQ  database hack. I’m impressed by the number of customers MongoHQ was able to sign up - they spanned the gamut from sites that I don’t even recall signing up for to startups that have been getting significant buzz.  If a database as a service company is able to get hacked it doesn’t leave me optimistic about the way other companies are securing our data. As much as these “as a service” products make our lives easier they bring an increased risk to our business and more importantly our customers. Sure their security will be better than someone who’s setting up a MongoDB instance for the first time but that has to be balanced against the fact that a hosting site offers a much higher reward for a hacking attempt. Access to the infrastructure provides a lot more information than hacking an individual site.  I used to believe that doing security internally was dumb but now I’m not so sure. No one will care about hacking a small site and if it turns out that the site is becoming successful you can dedicate the resources to properly secure it. At the same time, with so many people sharing passwords across multiple accounts it only takes one careless site to undermine the efforts of all the others.  Some of the security alerts I’ve received have mentioned that they plan on managing their database internally rather than relying on a third party; I wonder if this is the start of a trend.
{% include setup %} The longer I’ve been involved in tech the fewer Windows laptops I’ve been seeing. It seems that to even be considered a startup you need to be giving your employees MacBooks. My conversion came years ago when I made the move from Linux in order to be able to run Excel since neither OpenOffice nor Google Spreadsheet were cutting it. Unfortunately, even after years of effort, I still can’t get to the same level of productivity as I had when using Windows during my consulting days. It’s entirely due to the shortcuts. Some of the shortcuts just changed while others simply disappeared.  The difference is most likely due to a different keyboard layout on a Mac but the cynic in me can’t help but think that it’s also a way to keep the Excel power users on Windows. No one in the finance or consulting industry will switch to Macs until the actual workflow of using applications is the same between Mac and Windows. The more power user focused an app is the more difficult it is to convince its users to switch from one OS to another. People already get annoyed when a minor change is introduced by a new version; I can’t imagine the reaction a new workflow would produce.  This is why the web apps are so intriguing, they’re able to maintain their look, feel, and functionality no matter where they’re accessed giving users the ability to choose the hardware that fits them. I’m already replacing Word with Google Docs but it’s going to take quite a bit of effort to get spreadsheet apps to the same level. In the mean time, I’m trying to replace Excel with R. I’m not there yet but improving every day.
{% include setup %} In my quest to replace Excel with R I’ve been spending the past week trying to do everything in R. It hasn’t been that easy with many things taking longer due to me having to reference the R docs but one thing that’s been great so far is being able to quickly run a query on Amazon’s RDS and pull data into a data frame for quick analysis. Being able to wrap this into a reusable function makes things even better. The one thing that makes it tricky was not being able to connect to RDS directly but having to tunnel through an EC2 instance. Below are the steps to replicate the setup.  In your shell, run the following command to set up the SSH tunnel: {% highlight bash %} ssh -L  : :  @  {% endhighlight %}  Now in R: {% highlight r %} install.packages(‘RMySQL’) # Install the R MySQL library library(RMySQL) # Load the library m<-dbDriver("MySQL") # Load the driver con<-dbConnect(m,user='username',password='pass',host='127.0.0.1',dbname='db',port=3307) # Connect to the local instance res<-dbSendQuery(con, 'select * from table') # Execute the query data <- fetch(res, n = -1) # Load the retrieved data into a data frame dbClearResult(dbListResults(con)[[1]]) # Use this to free the connection {% endhighlight %}
{% include setup %}      I was going through my drafts and stumbled onto one that was going to criticize Twitter’s API changes that they  announced last August . In light of last week’s IPO I thought I’d finally finish it up.  The blog post described the changes Twitter planned on making with the intent of taking control of the developer ecosystem. The changes included being a lot more strict with their API by limiting the number of users a client could authenticate, reducing the volume of API calls, and requiring all Twitter content to be displayed the same way. The post also included a matrix indicating that Twitter did not want anyone developing on the consumer/engagement side but the rest being open.  At the time, many developers (me included) felt betrayed. We made Twitter successful and now we were limited in what we could do. In hindsight, these moves have been obvious. Since then, Twitter’s been rolling out a ton of changes that wouldn’t have been possible without controlling the entire experience. It also gave an indication of their monetization model. At the time, some thought that Twitter would try to monetize by selling access to the data feed or by offering premium features that large brands could use to control their pages. Turns out it was just advertising. By controlling the way every tweet is seen they’ve been able to roll out sponsored tweets and now, images.  I still think they took the easy way out by choosing an obvious business model and yet I can’t really fault them given last week’s result. It does make me wonder whether advertising is the business model of choice for an IPO.
{% include setup %} Just a quick update on my professional life. I recently joined  TripleLift ’s engineering team. I met the founders while at  ERA  and liked the problem they were solving. It was also time for me to move on from my other projects so when I found out they were growing it was a pretty easy decision. Being a startup, it’s hard to pinpoint exactly what everyone’s responsibilities are since everyone becomes a generalist but I’ve been focused on the data side. This entails developing our various data pipelines, leveraging the data we have to improve performance and unlock new opportunities, and doing some light data science to help model and understand the native advertising space.  In 2012, US marketers spent close to $15B on internet display ads and this number’s only growing. The old banner ad model is a huge chunk of it but it’s being supplanted by better, smarter technologies that provide significantly higher engagement rates. Social networks have embraced native advertising that blends into their products rather than being delegated to a sidebar or a banner that’s immediately ignored. Native advertising is a much better approach and I believe it’s going to take up the majority of display budgets within the next couple of years. TripleLift is doing some incredibly awesome things by allowing publishers to design sites around their content and then easily integrate ads that enhance the user experience rather than disrupt it while advertisers are able to get their ads in front of users that actually enjoy seeing them. It’s an evolution of display advertising that’s better for everyone involved that I can’t see it failing. Clearly I’m excited.  PS. TripleLift is hiring so let me know if you want to talk about the various roles and what we’re doing.
{% include setup %} Every time I launch a new website one of the first things I do is add  Google Analytics  to start gathering data. This blog was no different but I’ve recently been wondering whether Google Analytics is the right way to measure a blog. It’s great for tracking the total number of visitors, where they’re coming from, and how long they’re staying but I wish there was something that was optimized for blogs rather than something that was designed as a general solution.  My ideal blog analytics tool would help me understand both how readers are finding my content as well as how they’re engaging with it. It would also automatically segment my blog’s readers so I’d be able to quickly tell the different types of readers I have and what each of the groups is interested in seeing. Many of my posts end up being shared but I only discover that when I look at the referrals in Google Analytics. I’d also love to get a notification whenever one of my posts is shared on another site or social networks so I can participate in the discussion as it’s happening rather than days later.  I’m not sure about the available tools but I have some experience with  MixPanel  and am going to see if I can jerry rig it to do what I want. It’s great for doing simple funnel and segmentation analysis but it should also be flexible enough to do a ton more stuff. One thing I’ve been thinking about is generating additional meta data for each of my posts (topics, number of words, number of images) and then feeding that into MixPanel to see what impact they have. I’ll share both the source code and results once I have it up and running.
{% include setup %} Over Thanksgiving break I was going through some old GitHub repos and found an interesting one I wanted to share. It’s a  Connect 4 bot  that’s evolved through a genetic program. The goal of the strategy is to choose a column to move to that will give the highest probability of a win given a board position. To figure out the move column, the genetic program simulates play of strategy against strategy and gives the most successful ones a greater chance of reproducing into the next generation. The idea is that over time the resulting strategy will be the most fit.  The way a typical genetic program works is represented is through a tree structure with the leaf nodes (terminals) containing the various features of the input and the non-leaf nodes containing functions to evaluate the values in the leaf nodes. This way, the program can evaluate any input and we can create new functions by taking subbranches from one tree and combining them with another.  I used the  PyEvolve framework  which took care of all the simulation code so the bulk of my work was spent in figuring out which features and functions to use as well as a way of tracking the intermediate strategies so I could store the resulting strategy for later use. The features I ended up using where the number of own and opponent’s pieces adjacent to the move, the number of own and opponent’s 3 piece segments that would be created with the move, and the height of the column. I experimented with a few functions but ended up keeping a simple set of four - add, subtract, multiply, and an “is greater than” function.  In the end, the best I could get was a genetic program that was able to beat a random move strategy a little over 70% of the time. Unfortunately, this “optimal” strategy failed to win against a real strategy, such as  minimax . I suspect the strategy would have done a bit better had I trained it against a smarter set of strategies but I doubt it would have ever been able to compete with the minimax approach. I’m mostly amazed that by starting with a few features and some simple functions it’s possible to evolve a strategy that’s actually better than random. I doubt I can use this approach in a professional project but it’s still great being exposed to it.
{% include setup %} I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.  To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries. Every single publisher used DoubleClick and yet only a quarter of them used Google Analytics while 80% of the social networks I looked at used Google Analytics and only 40% used DoubleClick. The other interesting piece was how many more libraries an average publisher uses compared to a social network or ecommerce site. Five of the 13 publishers I looked at included at least 20 JavaScript libraries while the most libraries included by a social network was 4, which was Pinterest. The bulk of these additional libraries tend to be advertising specific so it’s not that surprising that publishers have more of them but the difference in volume was shocking. I’ve included the data at the bottom of this post in case someone wants to take a stab at it but something on my todo list is to automate the process of gathering this info rather than relying on Ghostery and copy and paste. Once I get get it done I’ll follow up with another post analyzing the larger set of data.  Even if these libraries get cached in the browser it’s still quite a lot of JavaScript that’s executed every time a site is loaded. It’s insane that Forbes is loading 39 libraries every time a page is seen. I suspect most people use AdBlock not because of ads but because of the degraded performance of a site having to load these libraries and the associated images. I’m aware that publishers are in trouble but I don’t think adding more and more libraries to eke out additional revenue is a sustainable model.     # of Libraries by Site      Site  Site Type  # of Libraries      Forbes  Publisher  39    BBC  Publisher  33    The Guardian  Publisher  25    Washington Post  Publisher  24    DailyKos  Publisher  23    NY Times  Publisher  19    USA Today  Publisher  18    Huff Po  Publisher  17    ABC  Publisher  15    Fox News  Publisher  15    Amazon  E Commerce  12    CNN  Publisher  10    ESPN  Publisher  9    Ebay  E Commerce  8    Yahoo  Publisher  6    Pinterest  Social Network  4    Facebook  Social Network  3    Tumblr  Social Network  3    Reddit  Social Network  2    Twitter  Social Network  1           # of Sites by Library      Library  # of Sites      DoubleClick  17    ScoreCard Research Beacon  14    Google Adsense  10    ChartBeat  8    NetRatings SiteCensus  8    Facebook Connect  8    Quantcast  8    Google Analytics  7    Datalogix  7    Right Media  7    Omniture (Adobe Analytics)  6    AppNexus  6    Moat  6    Audience Science  6    Evidon Notice  5    MediaMath  5    TrackingSoft  5    Adobe Test &amp; Target  4    Visual Revenue  4    Aggregate Knowledge  4    Acxiom  4    Google AdWords Conversion  3    AdRoll  3    Criteo  3    DoubleClick Spotlight  3    Facebook Social Plugins  3    Twitter Button  3    Outbrain  3    Quigo AdSonar  3    Atlas  3    Rubicon  3    Advertising.com  3    Krux Digital  3    BrightRoll  3    eXelate  3    BuzzFeed  2    Optimizely  2    Typekit by Adobe  2    AdXpose  2    Casale Media  2    Media Optimizer (Adobe)  2    VoiceFive  2    AdMeld  2    Amazon Associates  2    Facebook Exchange (FBX)  2    OpenX  2    PubMatic  2    TRUSTe Notice  2    Facebook Social Graph  2    MediaMind  2    [x+1]  2    BlueKai  2    Brilig  2    Media Innovation Group  2    Neustar AdAdvisor  2    SpotXchange  2    24/7 Media Ad Network  2    Dynamic Logic  1    Gravity Insights  1    Crazy Egg  1    DoubleClick Floodlight  1    FreeWheel  1    Gigya Socialize  1    MixPanel  1    Specific Media  1    Twitter Badge  1    ValueClick Mediaplex  1    Janrain  1    Parse.ly  1    Yahoo Analytics  1    Burst Media  1    PulsePoint  1    eBay Stats  1    Genome  1    ADTECH  1    Google +1  1    DoubleClick DART  1    Adzerk  1    Effective Measure  1    Mindset Media  1    Rocket Fuel  1    Brightcove  1    New York Times  1    WebTrends  1    ForeSee  1    Google AJAX Search API  1    Integral Ad Science  1    Media6Degrees  1    Adap.tv  1    AddThis  1    AMP Platform  1    DoubleClick Bid Manager  1    i-Behavior  1    Intent Media  1    Lotame  1    Martini Media  1    Media.net  1    Tacoda  1    Tapad  1    TidalTV  1    TradeDesk  1    Turn  1    Undertone  1    SimpleReach  1    Tealium  1    Bizo  1    New Relic  1    Trove  1           Avg # of Libraries by Site Type      Site Type  # of Sites  Avg # Libraries      Publisher  13  19.46    Social Network  5  2.60    E Commerce  2  10.00           Raw Data      Site  Library  Site Type      ESPN  Adobe Test &amp; Target  Publisher    ESPN  ChartBeat  Publisher    ESPN  DoubleClick  Publisher    ESPN  Dynamic Logic  Publisher    ESPN  Evidon Notice  Publisher    ESPN  Google Adsense  Publisher    ESPN  Gravity Insights  Publisher    ESPN  NetRatings SiteCensus  Publisher    ESPN  ScoreCard Research Beacon  Publisher    ABC  BuzzFeed  Publisher    ABC  ChartBeat  Publisher    ABC  Crazy Egg  Publisher    ABC  DoubleClick  Publisher    ABC  DoubleClick Floodlight  Publisher    ABC  Facebook Connect  Publisher    ABC  FreeWheel  Publisher    ABC  Gigya Socialize  Publisher    ABC  Google AdWords Conversion  Publisher    ABC  NetRatings SiteCensus  Publisher    ABC  Omniture (Adobe Analytics)  Publisher    ABC  Optimizely  Publisher    ABC  Quantcast  Publisher    ABC  ScoreCard Research Beacon  Publisher    ABC  Typekit by Adobe  Publisher    DailyKos  AdRoll  Publisher    DailyKos  AdXpose  Publisher    DailyKos  AppNexus  Publisher    DailyKos  Casale Media  Publisher    DailyKos  ChartBeat  Publisher    DailyKos  Criteo  Publisher    DailyKos  DoubleClick  Publisher    DailyKos  DoubleClick Spotlight  Publisher    DailyKos  Evidon Notice  Publisher    DailyKos  Facebook Connect  Publisher    DailyKos  Facebook Social Plugins  Publisher    DailyKos  Google Adsense  Publisher    DailyKos  Google AdWords Conversion  Publisher    DailyKos  Google Analytics  Publisher    DailyKos  MediaMath  Publisher    DailyKos  MixPanel  Publisher    DailyKos  Quantcast  Publisher    DailyKos  ScoreCard Research Beacon  Publisher    DailyKos  Specific Media  Publisher    DailyKos  TrackingSoft  Publisher    DailyKos  Twitter Badge  Publisher    DailyKos  Twitter Button  Publisher    DailyKos  ValueClick Mediaplex  Publisher    Fox News  Adobe Test &amp; Target  Publisher    Fox News  AdRoll  Publisher    Fox News  DoubleClick  Publisher    Fox News  Evidon Notice  Publisher    Fox News  Google Adsense  Publisher    Fox News  Janrain  Publisher    Fox News  Media Optimizer (Adobe)  Publisher    Fox News  Moat  Publisher    Fox News  NetRatings SiteCensus  Publisher    Fox News  Outbrain  Publisher    Fox News  Parse.ly  Publisher    Fox News  Quigo AdSonar  Publisher    Fox News  ScoreCard Research Beacon  Publisher    Fox News  TrackingSoft  Publisher    Fox News  Visual Revenue  Publisher    Facebook  Aggregate Knowledge  Social Network    Facebook  Atlas  Social Network    Facebook  DoubleClick  Social Network    Twitter  Google Analytics  Social Network    Yahoo  Datalogix  Publisher    Yahoo  DoubleClick  Publisher    Yahoo  Right Media  Publisher    Yahoo  ScoreCard Research Beacon  Publisher    Yahoo  VoiceFive  Publisher    Yahoo  Yahoo Analytics  Publisher    Amazon  AdMeld  E Commerce    Amazon  Amazon Associates  E Commerce    Amazon  AppNexus  E Commerce    Amazon  Burst Media  E Commerce    Amazon  DoubleClick  E Commerce    Amazon  Facebook Exchange (FBX)  E Commerce    Amazon  Google Adsense  E Commerce    Amazon  OpenX  E Commerce    Amazon  PubMatic  E Commerce    Amazon  PulsePoint  E Commerce    Amazon  Right Media  E Commerce    Amazon  Rubicon  E Commerce    Ebay  Aggregate Knowledge  E Commerce    Ebay  Datalogix  E Commerce    Ebay  DoubleClick  E Commerce    Ebay  eBay Stats  E Commerce    Ebay  Genome  E Commerce    Ebay  MediaMath  E Commerce    Ebay  Right Media  E Commerce    Ebay  TRUSTe Notice  E Commerce    Tumblr  Google Analytics  Social Network    Tumblr  Quantcast  Social Network    Tumblr  ScoreCard Research Beacon  Social Network    Pinterest  DoubleClick  Social Network    Pinterest  Facebook Connect  Social Network    Pinterest  Facebook Social Graph  Social Network    Pinterest  Google Analytics  Social Network    Huff Po  Adobe Test &amp; Target  Publisher    Huff Po  ADTECH  Publisher    Huff Po  Advertising.com  Publisher    Huff Po  DoubleClick  Publisher    Huff Po  Facebook Connect  Publisher    Huff Po  Facebook Social Plugins  Publisher    Huff Po  Google +1  Publisher    Huff Po  Google Analytics  Publisher    Huff Po  MediaMind  Publisher    Huff Po  Moat  Publisher    Huff Po  NetRatings SiteCensus  Publisher    Huff Po  Omniture (Adobe Analytics)  Publisher    Huff Po  Quantcast  Publisher    Huff Po  Quigo AdSonar  Publisher    Huff Po  ScoreCard Research Beacon  Publisher    Huff Po  TrackingSoft  Publisher    Huff Po  Twitter Button  Publisher    CNN  ChartBeat  Publisher    CNN  DoubleClick  Publisher    CNN  DoubleClick DART  Publisher    CNN  Facebook Connect  Publisher    CNN  Facebook Social Plugins  Publisher    CNN  Krux Digital  Publisher    CNN  NetRatings SiteCensus  Publisher    CNN  ScoreCard Research Beacon  Publisher    CNN  Twitter Button  Publisher    CNN  Visual Revenue  Publisher    Reddit  Adzerk  Social Network    Reddit  Google Analytics  Social Network    BBC  [x+1]  Publisher    BBC  Acxiom  Publisher    BBC  AdMeld  Publisher    BBC  Advertising.com  Publisher    BBC  AdXpose  Publisher    BBC  Aggregate Knowledge  Publisher    BBC  AppNexus  Publisher    BBC  Atlas  Publisher    BBC  Audience Science  Publisher    BBC  BlueKai  Publisher    BBC  BrightRoll  Publisher    BBC  Brilig  Publisher    BBC  Casale Media  Publisher    BBC  Datalogix  Publisher    BBC  DoubleClick  Publisher    BBC  DoubleClick Spotlight  Publisher    BBC  Effective Measure  Publisher    BBC  Facebook Exchange (FBX)  Publisher    BBC  Google Adsense  Publisher    BBC  Media Innovation Group  Publisher    BBC  MediaMath  Publisher    BBC  Mindset Media  Publisher    BBC  NetRatings SiteCensus  Publisher    BBC  Neustar AdAdvisor  Publisher    BBC  Omniture (Adobe Analytics)  Publisher    BBC  OpenX  Publisher    BBC  PubMatic  Publisher    BBC  Right Media  Publisher    BBC  Rocket Fuel  Publisher    BBC  Rubicon  Publisher    BBC  ScoreCard Research Beacon  Publisher    BBC  SpotXchange  Publisher    BBC  TrackingSoft  Publisher    NY Times  Acxiom  Publisher    NY Times  AppNexus  Publisher    NY Times  Atlas  Publisher    NY Times  Audience Science  Publisher    NY Times  Brightcove  Publisher    NY Times  ChartBeat  Publisher    NY Times  Datalogix  Publisher    NY Times  DoubleClick  Publisher    NY Times  eXelate  Publisher    NY Times  Facebook Connect  Publisher    NY Times  Google Adsense  Publisher    NY Times  Krux Digital  Publisher    NY Times  Moat  Publisher    NY Times  NetRatings SiteCensus  Publisher    NY Times  New York Times  Publisher    NY Times  ScoreCard Research Beacon  Publisher    NY Times  Typekit by Adobe  Publisher    NY Times  VoiceFive  Publisher    NY Times  WebTrends  Publisher    The Guardian  24/7 Media Ad Network  Publisher    The Guardian  AppNexus  Publisher    The Guardian  Audience Science  Publisher    The Guardian  BrightRoll  Publisher    The Guardian  ChartBeat  Publisher    The Guardian  Criteo  Publisher    The Guardian  DoubleClick  Publisher    The Guardian  Evidon Notice  Publisher    The Guardian  Facebook Connect  Publisher    The Guardian  Facebook Social Graph  Publisher    The Guardian  ForeSee  Publisher    The Guardian  Google Adsense  Publisher    The Guardian  Google AdWords Conversion  Publisher    The Guardian  Google AJAX Search API  Publisher    The Guardian  Integral Ad Science  Publisher    The Guardian  Media6Degrees  Publisher    The Guardian  MediaMath  Publisher    The Guardian  NetRatings SiteCensus  Publisher    The Guardian  Omniture (Adobe Analytics)  Publisher    The Guardian  Optimizely  Publisher    The Guardian  Outbrain  Publisher    The Guardian  Quantcast  Publisher    The Guardian  Right Media  Publisher    The Guardian  Rubicon  Publisher    The Guardian  ScoreCard Research Beacon  Publisher    Forbes  24/7 Media Ad Network  Publisher    Forbes  [x+1]  Publisher    Forbes  Acxiom  Publisher    Forbes  Adap.tv  Publisher    Forbes  AddThis  Publisher    Forbes  Advertising.com  Publisher    Forbes  Aggregate Knowledge  Publisher    Forbes  AMP Platform  Publisher    Forbes  AppNexus  Publisher    Forbes  Audience Science  Publisher    Forbes  BlueKai  Publisher    Forbes  BrightRoll  Publisher    Forbes  Brilig  Publisher    Forbes  Datalogix  Publisher    Forbes  DoubleClick  Publisher    Forbes  DoubleClick Bid Manager  Publisher    Forbes  DoubleClick Spotlight  Publisher    Forbes  eXelate  Publisher    Forbes  Google Adsense  Publisher    Forbes  Google Analytics  Publisher    Forbes  i-Behavior  Publisher    Forbes  Intent Media  Publisher    Forbes  Lotame  Publisher    Forbes  Martini Media  Publisher    Forbes  Media Innovation Group  Publisher    Forbes  Media.net  Publisher    Forbes  Moat  Publisher    Forbes  Omniture (Adobe Analytics)  Publisher    Forbes  Quantcast  Publisher    Forbes  Right Media  Publisher    Forbes  ScoreCard Research Beacon  Publisher    Forbes  Tacoda  Publisher    Forbes  Tapad  Publisher    Forbes  TidalTV  Publisher    Forbes  TradeDesk  Publisher    Forbes  TRUSTe Notice  Publisher    Forbes  Turn  Publisher    Forbes  Undertone  Publisher    Forbes  Visual Revenue  Publisher    USA Today  AdRoll  Publisher    USA Today  Audience Science  Publisher    USA Today  BuzzFeed  Publisher    USA Today  ChartBeat  Publisher    USA Today  Datalogix  Publisher    USA Today  DoubleClick  Publisher    USA Today  Evidon Notice  Publisher    USA Today  Facebook Connect  Publisher    USA Today  Google Adsense  Publisher    USA Today  Media Optimizer (Adobe)  Publisher    USA Today  Moat  Publisher    USA Today  Quantcast  Publisher    USA Today  Right Media  Publisher    USA Today  ScoreCard Research Beacon  Publisher    USA Today  SimpleReach  Publisher    USA Today  Tealium  Publisher    USA Today  TrackingSoft  Publisher    USA Today  Visual Revenue  Publisher    Washington Post  Acxiom  Publisher    Washington Post  Adobe Test &amp; Target  Publisher    Washington Post  Amazon Associates  Publisher    Washington Post  Audience Science  Publisher    Washington Post  Bizo  Publisher    Washington Post  ChartBeat  Publisher    Washington Post  Criteo  Publisher    Washington Post  Datalogix  Publisher    Washington Post  DoubleClick  Publisher    Washington Post  eXelate  Publisher    Washington Post  Google Adsense  Publisher    Washington Post  Krux Digital  Publisher    Washington Post  MediaMath  Publisher    Washington Post  MediaMind  Publisher    Washington Post  Moat  Publisher    Washington Post  Neustar AdAdvisor  Publisher    Washington Post  New Relic  Publisher    Washington Post  Omniture (Adobe Analytics)  Publisher    Washington Post  Outbrain  Publisher    Washington Post  Quantcast  Publisher    Washington Post  Quigo AdSonar  Publisher    Washington Post  ScoreCard Research Beacon  Publisher    Washington Post  SpotXchange  Publisher    Washington Post  Trove  Publisher          You can also grab the entire set of data  here .
{% include setup %} The recent rise of marketplace startups is great and benefits all except the incumbent. They provide much needed liquidity and transparency to markets that helpfully reduce costs to the consumer and increase volume to the provider.  Yet I’m surprised by the number of home cleaning service startups out there. I’m aware of  HomeJoy ,  Exec ,  GetMaid ,  MyClean , and  HandyBook , but am sure there are countless other copycats. The Uber approach works because it’s for an immediate service with a one time transaction where the value provided is somewhat of a commodity. This is not the case with home cleaning services. The range of quality among cleaners varies significantly more than the quality among drivers and I’d be willing to have a good cleaner come in at a slightly inconvenient time rather than a poor cleaner at the perfect time. And once I find a cleaner I like I’d want to book them directly rather than go through the company again. This way I can get a lower rate while also giving a cleaner more than they’d otherwise make from using the service. This would violate the company’s terms but I don’t see how they can be enforced.  I’m sure there’s still a large market for such cleaning services, I’m just not as  optimistic as others  seem to be. I see it being great for one off events - cleaning after a party, getting your apartment ready a visit from the parents, or preparing your home for sale. I just don’t see how this is a huge market that warrants all these startups. I understand it’s just the beachhead but it doesn’t seem to be a very strong one. Have the better ones already been done? Uber is dominating car service. Food delivery is full of one-time transactions that need to be met quickly but it’s already a mature market. Laundry is another one and there are two startups I’m aware of working on it - Washio and Prim. HandyBook also offers handyman services although they seem to be more focused on home cleaning. I really don’t know why home cleaning startups are so popular.
{% include setup %} Something that’s been really helpful to me in understanding a MySQL database is the built in  information_schema.columns  table. It provides information on every column in the database and is queryable just like any other table. This makes it easy to quickly find all tables that have a particular column name or all columns that are the same data type. There have been countless times where I knew the data existed somewhere but couldn’t recall which table it was in. Querying the information_schema.columns table for the foreign key helped me quickly figure it out. Below are some sample queries that retrieve data from the information_schema.columns table:  {% highlight sql %}select table_schema, table_name from information_schema.columns where column_name like '%user_id%’;  select * from information_schema.columns where column_name like '%time%’;  select * from information_schema.columns where data_type = 'datetime'; {% endhighlight %}  Since it’s just like any other table, except for being read-only, you can write jobs that access the data. Something I had to recently do was write a quick script to generate fake data. All a user has to do is specify the table name to populate and the script would look up the columns and their types from information_schema.columns and dynamically generate the INSERT statements. For example, if a column was of type varchar it would generate a random text string less than or equal to the length constraint and if it were an int it would generate a random number. It wasn’t perfect and only handled foreign keys that were specified by the user but it was great given the effort. A later version could use the information provided by two other information_schema tables, table_constraints and key_column_usage, to get rid of this manual step. If you’re a frequent MySQL user, familiarizing yourself with the tables in information_schema will make you significantly more efficient.
{% include setup %} Before leaving for a trip to India, I wanted to make sure that I’d be allowed to access the ATM so I decided to contact my bank. Surprisingly, Bank of America was modern enough to allow me to do this online. Unsurprisingly, the UX was lacking.  Instead of just asking which country I was traveling to using a simple autocomplete or dropdown they have a three step process. First, I get to choose whether I’m traveling domestic or international. If internationally, I get presented with four options that are just the first letter of each country name. After choosing a country range bucket, I can finally pick the actual country.              I understand when an inferior UX decision is made because it’s cheaper to implement but in this case it must have actually been more difficult. Instead of having a single dropdown or autocomplete they have three different input elements. Even if the first selection is necessary, there’s no need to split 206 countries into 4 separate lists.  I’m not sure what I was expecting but it’s still frustrating seeing such decisions being made. I’d love to know the reasons.
{% include setup %} Something that I've been thinking about ever since I worked as a product manager focused on internal tools is being able to run a product entirely through CSV file uploads. Instead of building a UX designed to handle bulk operations and complicated workflows you build support for file uploads and handle the business logic entirely on the backend. The motivation is that it’s extremely difficult to build a UI that’s going to be as powerful and flexible as a simple CSV file, especially when outside tools, such as Excel, can help generate these files.  This approach also has the nice property of decoupling the input from the core business logic. Over time, tools and interfaces can be built that are optimized for specific use cases without having to modify any of the backend. Effort can be spent on improving workflows that are already being done rather than building support for workflows that may or may not be common. Permissions and controls can also be added that make the application accessibility to a wider range of users.  Many companies spend tons of time building advanced tools that will never be as powerful as Excel paired with a simple file upload. Workflows vary significantly across users and most products impose a single approach. Why not build more general tools that take advantage of the unique work styles?
{% include setup %} I’ve heard about the wonders of an unlocked phone and decided to try it out during my recent trip to India. The idea was to get a cheap unlocked Android phone that I’d be able to use on this and future trips. I was able to get a relatively cheap Samsung phone but it took me a surprisingly long time to get a working SIM card. This post is a description of the steps I took as well as some advice for others trying to do the same.  First off, to get a SIM card as a foreigner in India, you need to have a copy of your passport and visa, a passport sized photo, and a local to act as a reference. After giving this information to vendor they will do the necessary paperwork, call the reference, and if everything goes well they will activate your SIM card within 24 hours after which you will need to call them to verify and start using the service.  My first attempt was in New Delhi where I went to an Airtel shop based the advice of my uncle. Unfortunately, I didn’t know I needed to have a passport sized photo but was referred to a nearby computer shop that was able to print them out at the cost of 10 rupees (~17 cents) a piece. I was able to buy a regular sized SIM for 300 rupees (~$5) but was told it would take around 24 hours to activate and would only be cut after that. Unfortunately, I had to leave Delhi for a wedding so didn’t get a chance to get it cut to a micro SIM until I had already arrived in Mumbai. By that point, I was in a different city and no longer able to activate a Delhi SIM card although it took me multiple days to figure that out.  After going back and forth to the Airtel shop in Mumbai a few times, and discovering a new hoop I had to jump through every time, I was about to give up until I shared my problem with someone at my hotel. He took me to a nearby stand which was able to take care of everything for me within a few hours. This went smoothly since I had a few of the passport photos left and he was willing to act as my local reference. Total cost was 600 rupees (~$10) and included 250 rupees of credit.  Now that I had a functional phone, it worked great. It took me a little bit of time to understand the prepaid model but once I did I actually preferred it more than the postpaid one I have in the US. You can go to the dozens of mobile vendors around cities which will glady load some money unto your account. You can then activate various services either by using these vendors, doing it online, or via text messaging. At any time you can text various numbers and codes in order to get the balance you have left on your plans as well as add new ones. With my 250 rupee balance (~$4) I was able to buy 150 MB of 3G for 44 rupees (~75 cents) and load the rest into a national dialing plan.  Having a phone that works wherever you go is immensely convenient. Traversing and exploring Mumbai became significantly easier and more fun when we were able to get the phone working. We were able to explore the city without having to worry about getting lost and were able to discover and research local gems. It wasn’t as serendipitous as just walking around but we hopefully struck the right balance.  The challenge was in getting the SIM card working and I’m sure the process will vary in every country. My advice is to do research on how to get a prepaid SIM card before travelling and come prepared with everything you need so you can get the process started soon after arriving. If you know you’ll only be spending time in a city for a single day it may not make sense to get the SIM card there since it may not be possible to activate it in another city - I’m not sure if this is due to my experience or just the way things are done in India but it’s something to be aware of. You can also try contacting the hotel you’re staying at since they should have had experience helping their guests get SIM cards.  T-Mobile recently launched a  global plan  and I’m sure more and more carriers will follow suit. Until then we’re stuck with SIM cards and the unique challenges of obtaining one in different countries.
{% include setup %} The trip to India got me thinking about “global products” that work the same wherever they are. It’s surprisingly difficult to find tech products that fit this description. Cell phones will almost always work internationally but roaming charges make it impractical. Having an unlocked phone helps but you still need to get a SIM card which is a  hassle  in many countries.  Even something as standard as a laptop isn’t as easy to use as it should be. Wifi connectivity varies depending where you are with most cities being great fickle elsewhere. Dealing with voltage conversion and plug adapters is something that always comes up. I’ve learned to travel with an adapter kit that includes enough combinations to be able to charge my laptop wherever I go.  The one product that actually worked as expected was the GPS running watch my wife got me for our anniversary. No matter where I was it was able to lock on to a satellite within a few minutes and accurately track my run. The only issue was charging which I was able to do via a USB cable connected to my laptop. Even that shows a weakness since if I didn’t have a laptop I wouldn’t have been able to charge the watch.  The funny thing is, each of these products was designed to work globally - it’s just that the infrastructure differences prevent that from being a reality. Whether it’s having a different set of of plugs or a particular way of getting a SIM card it’s not the product that’s the problem. As powerful as our products are they’re still operating within an infrastructure. And since products evolve faster than infrastructure we’ll continue to see this inconsistent product behavior around the world. Maybe by the time we colonize space we’ll have a consistent global infrastructure.
{% include setup %}                              Source:  Business Insider                In 2012 global smartphone ownership surpassed PC ownership and smartphones are still seeing massive growth. The obvious consequence is that many people who’ve never owned a computer are starting to own smartphones and that’s having a huge impact on the world. Almost everything will be affected - not just technology but also business, politics, and general culture. As these smartphones get more powerful and pervasive we’ll see applications that we can’t even imagine right now.  What’s not being discussed is the impact this will have on the world’s literacy rate. In 2010, the  global literacy rate  was estimated to be 84% but increasing smartphone ownership should drive it higher. Having something in your pocket that is both a business and entertainment device will encourage people to learn all its features. Sure one can just familiarize oneself with the various icons and key combinations to achieve certain results but I suspect being exposed to a smartphone’s potential will also serve as motivation to learn more.  Of course, this is just hopeful speculation on my part but I think we tend to view technological change through a tech filter. There’s a whole other world that’s difficult for us to imagine so we tend to not think about it too much. I had a professor,  Prof Levent Orman , discuss the impact that the car had on the world. The direct effect was the replacement of horses but the long term effects were the rise of highways and suburbs and a change in American culture. Smartphones are one of the technologies that will have such an impact, it’s just impossible to know what it will be.
{% include setup %} Something else that struck me during my trip to India was the difference in taxi fare between  New York City  and  Mumbai . I expected them to be different but the magnitude of the difference was shocking. In NYC, the base fare is $2.50 and increases 50 cents for each additional 1/5th of a mile or 60 seconds of not moving. In Mumbai, the rate starts at 19 rupees (~32 cents) and includes the first 1.5 km. After that it’s 12.35 rupees (21 cents) for each additional km and 30 rupees (50 cents) for an hour of not moving.      Distance (mi)  Wait Time (min)  Total NYC Fare ($)  Total Mumbai Fare ($)  Est NYC Gas Cost ($)  Est Mumbai Gas Cost ($)  Est NYC Driver Profit  Est Mumbai Driver Profit      1  1  5.50  0.39  0.18  0.24  97%  38%    1  2  6.00  0.44  0.18  0.24  97%  45%    1  5  7.50  0.59  0.18  0.24  98%  59%    2  1  8.00  0.72  0.35  0.48  96%  33%    2  2  8.50  0.77  0.35  0.48  96%  38%    2  5  10.00  0.92  0.35  0.48  97%  48%    5  1  15.50  1.71  0.88  1.20  94%  30%    5  5  17.50  1.91  0.88  1.20  95%  37%    5  10  20.00  2.16  0.88  1.20  96%  45%    5  20  25.00  2.66  0.88  1.20  97%  55%    10  5  30.00  3.57  1.75  2.40  94%  33%    10  10  32.50  3.82  1.75  2.40  95%  37%    10  20  37.50  4.32  1.75  2.40  95%  44%    10  30  42.50  4.82  1.75  2.40  96%  50%    20  10  57.50  7.14  3.50  4.80  94%  33%    20  20  62.50  7.64  3.50  4.80  94%  37%    20  30  67.50  8.14  3.50  4.80  95%  41%    50  0  127.50  16.58  8.75  12.00  93%  28%    100  0  252.50  33.15  17.50  24.00  93%  28%    1000  0  2502.50  331.40  175.00  240.00  93%  28%      The differences are crazy. A short ride will cost $5 in NYC but only 40 cents in Mumbai. Even if we look at the limit where we’re always moving and there’s no stopping, a NYC fare will cost 7.55 times   1   that of one in Mumbai. Given these differences, I was surprised to discover that gas is 40% more expensive   2   in Mumbai. If we assume an average car gets 20 miles a gallon, it works out that in NYC the profit to the driver is over 90% of the total fare whereas in Mumbai it’s closer to 30%. The fare pricing echoes this: standing still for an hour costs 50 cents in Mumbai but $30 in NYC. This is simplified since there are many other factors at play, ie the  NYC medallion system , but it’s still a massive difference in labor costs.  This reminds me of something I read about the pricing of soda in grocery stores. In the US, a 12 pack of Coke is only slightly more expensive than a 20 oz bottle whereas in countries with lower labor costs they’re much closer to the actual unit costs. The reason is the same - the cost of labor in US contributes the most to the cost of an item whereas in countries with lower labor costs it’s the item cost that’s the bulk of the final item price.   1  7.55 = 2.5/(1.61 &times; 12.35/60)  2  $3.50/gallon in NYC vs 78 Rupees/gallon ($4.80) in Mumbai
{% include setup %} Now that I actually have over 100 posts for the year I can actually follow the trend and highlight the most popular ones as well as share some data from my Google Analytics account. This is the first year I’ve seriously committed to blogging and didn’t think I’d enjoy it as much as I did. I will continue to write at least twice a week in 2014 so it will be interesting to see how next year’s data compares against the data from 2013. Thanks for reading and definitely let me know if you have any topics you want me to write about.                                                A general overview of 2013 traffic. I'm honestly surprised by the number of visitors I've had but it's mostly come from a few posts that ended up getting signifcant traffic from Hacker News and Twitter. Note that the bounce rate dropped near the beginning of the year since I added a  Google Analytics event  to consider 15 seconds of being on the site as a "read" event.                                                         The obligatory top posts. One thing I discovered is that I am terribly poor at predicting which posts will be the most successful. We'll see if I get better at this in 2014. Here are the links in clickable form:                                                   Why don't cell phones have a dialtone?                                                                   Drowning in JavaScript                                                                   Eighteeen months of Django                                                                   What the SEO?                                                                   Fun with Prolog: Priceonomics Puzzle                                                                   In defense of Excel                                                                   Getting a SIM card in India                                                                   Eighteeen months of Django: Part 2                                                                   The power inbox                                                                   Run Django under Nginx, Virtualenv and Supervisor                                                                   Coke, Pepsi and Passover                                                                   Scraping Yahoo fantasy football stats with Scrapy                                                                                                    Definitely surprised by how significant mobile and tablet traffic was. I imagine these will only increase in the coming years.