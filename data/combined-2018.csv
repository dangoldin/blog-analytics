num_text_description,num_images,description,tags,text,num_links,num_tags,date,num_keywords,keywords,num_text_words,title
26,0,Every once in a while I receive a class action settlement email which comes with something small but free. Last week it came from Optimum.,#meta,"{% include setup %} I enjoy receiving the occasional class action emails. Other than being a customer during a particular period I didn’t do anything to encourage this and it’s always a surprise that usually comes with a free gift. Sometimes it’s a few dollars, other times it’s a free service, while other times it’s something I’m not qualified nor interested in.  Last week I received an email from Optimum around a class action settlement. Optimum was my ISP years ago and I never had any problems so this email was a pleasant surprise. At the same time I can’t recall the last time I needed a wifi pass when traveling since so many shops and public spaces are already offering it for free.  What I find interesting is that the email was sent on July 11 but can only be acted on August 1st. On one hand it’s nice that they gave 3 weeks notice but on the other hand it’s very likely that this email will be lost and very few people will follow up on it. This is a form of [breakage](https://en.wikipedia.org/wiki/Breakage) and I’m sure was something they thought about when sending the notice. At the same time giving a few customers free wifi for two days puts very little stress on their network and it’s unlikely those same customers would have paid Optimum for the day passes so this feels like a very cheap settlement. At the same time it is free so who am I to complain?",1,1,2018-07-21,3,"class action settlement, optimum, breakage",255,Class action settlement emails
10,0,I found a pretty potent cardboard poster near Time Square,#photo,,0,1,2012-07-09,1,startups,13,Photo taken near Times Square
30,0,I've recently noticed more mainstream sites adopting flat design. I wonder if this is due to the upcoming update of iOS and whether other sites will start adopting it.,#design,"{% include setup %} I don’t know whether it’s due to the upcoming version of iOS or Windows 8 but it feels as if flat design is getting more and more common. In the past couple of weeks, I’ve noticed two “mainstream” sites,  Thesaurus.com  and  Optimum , adopt a flat design which I suspect is the first design change they’ve made in years. Many companies are updating their iOS apps in time for the fall release and I understand the motivation to want to fit the style but it’s interesting to see websites doing the same. I wonder whether we’ll see more sites adopting this flat design in the next couple of months.",2,1,2013-08-11,2,"design, flat ui",163,Rise of flat design
23,0,My frontend coding experience ends at jQuery so over the past few weeks I've been getting into the modern frontend development world.,#code,"{% include setup %} Until a few weeks ago my frontend programming experience ended with jQuery so I decided to do something about it and start getting up to speed with modern frontend development. This ranged from starting to mess around with React, to using ES6, to integrating webpack and Babel in these projects. I’ve been using Sublime Text for the past 6 years but am switching to [Visual Code Studio](https://code.visualstudio.com/) as my primary editor.  So far I have two projects in different stages of completion. The first is a turning my “[JavaScript Tools](https://dangoldin.github.io/js-tools/)” page into a standalone app using React and [Electron](http://electron.atom.io/). The idea here is that I have a set of tools that live online but I have a soft spot for desktop apps and want one of my own that I can just keep open and quickly switch to whenever I need to get anything done. Over time I see this evolving with tools that I end up using on a frequent basis as well as shortcuts to make them fit for a “power user.”  The other project was inspired by the [Sleeping Giants twitter account](https://twitter.com/slpng_giants) which calls out major brands for advertiser on hate-based sites. It’s still a work in progress but the intent is to automate the capture of the ad on the site and the identification of the advertiser’s twitter account. The next step is to broadcast this information back on Twitter - either automatically or surfacing the content so people can do it themselves.  If you’re interested in helping out or just want to follow along both projects are on GitHub: [https://github.com/dangoldin/dan-tools](https://github.com/dangoldin/dan-tools) and [https://github.com/dangoldin/ad-detection](https://github.com/dangoldin/ad-detection).",6,1,2017-02-01,3,"frontend development, react, jquery",289,Learning modern frontend development
27,0,Amazon is challenging craigslist core use case by building out a P2P marketplace. Starting in India but interesting to see how and if it will expand.,#product,"{% include setup %}             It looks as if Amazon is entering yet another market - [the local peer to peer marketplace](https://www.amazon.in/p2p). They just launched a peer to peer marketplace in Bangalore. The pitch is incredibly simple - you create a listing and it’s listed on Amazon. As soon as it’s sold Amazon will pick up the item and deliver it to the buyer. I’ve used Craigslist a ton in the past and coordinating the delivery and handoff was always a frustrating experience. There doesn’t seem to be much information on how quality is measured - what happens if the time you get isn’t in the shape it was listed or just doesn’t work?  This is a perfect example of Amazon applying their standard approach of leveraging an existing infrastructure in order to launch a new service. They have the logistics and relationships necessary for delivery figured out and are somehow able to drive the cost to something incredibly low, in this case starting at 10 rupees (~15 US cents). The only way this will work is if there’s enough scale in order to handle the pick ups and drop offs in bulk - and I’m sure is also the reason Amazon is launching it in one city for now so they can work out the kinks.            Andrew Parker's  spawn of craigslist .     In 2010, Andrew Parker [put together a visualization](http://thegongshow.tumblr.com/post/345941486/the-spawn-of-craigslist-like-most-vcs-that-focus) of startups that were competing with the different craigslist categories but there hasn’t been a ton of competition in P2P selling. Craigslist’s network effects were just too massive but it looks as if Amazon is taking some serious steps to improve the experience by leveraging their existing strengths. Given this cost structure I can’t imagine it expanding to a country with higher delivery costs - at least until drone pickup and delivery arrives.",3,1,2016-12-31,4,"amazon, peer to peer marketplace, p2p marketplace, craigslist",356,Amazon's peer to peer marketplace
17,0,"Technology is causing us to move away from central regulatory agencies to crowdsourced, self-regulated communities.",#meta,"{% include setup %} The past few years have seen the rise of the share economy with companies such as AirBnB, Sidecar, Lyft, and TaskRabbit seeing massive growth. Unfortunately, they’re getting significant opposition from government and the entrenched special interest groups. Most of the pushback is under the guise of consumer safety and that regulations exist to protect the consumer.  Regulation is necessary when there’s an information asymmetry between a service provider and a consumer. In such cases, regulations help bridge that information gap and make the consumer more comfortable making the transaction. But the internet has been chipping away at this gap by building communities where people can share reviews and experiences. Yelp, Angie’s List, and Google are the largest of these traditional review sites but reviews are starting to appear everywhere that money is changing hands. Ecommerce sites offer reviews and ratings of the products they’re selling. The share economy companies self-regulate by offering communities with well thought out rating systems. Without well functioning communities they wouldn’t survive.  Consumer safety regulations are making way for ratings and reviews. We’re replacing centralized regulatory agencies with crowdsourced, self-regulating communities. Some regulation will always be necessary, especially in places with large information asymmetries, but these places are constantly shrinking. Of course the entrenched companies are fighting these trends but they should be focused on innovating themselves rather than battling the inevitable.",0,1,2013-06-16,6,"regulation, share economy, airbnb, sidecar, lyft, taskrabbit",233,Regulation and the share economy
34,0,If you think about it SQL is the perfect interface. It's so simple on the surface while allowing a variety of different databases to be created that are optimized for different use cases.,#meta,"{% include setup %} The more I code the more I’m exposed to SQL. It started with the usual relational suspects - MySQL, PostgreSQL, and even SQL Server (back in the finance days), but has since then expanded to columnar database such as Redshift, Vertica, and MonetDB. And now I’m starting to use SQL to query data on S3 using SparkSQL, Athena, and Hive.  SQL was [introduced in the 70s](https://en.wikipedia.org/wiki/SQL) and became an official standard in 1986 and it’s just incredible to see how dominant and dispersed it’s become. It’s still used for the original RDBMS use cases but it’s expanded significantly since then for a whole slew of new use cases. At the same time the underlying query syntax remained surprisingly similar. We talk about designing good interfaces that allow us to write reusable and clean code but SQL is an interface that’s existed, evolved, and expanded for more than 40 years.  It’s amazing that a single query can run across a variety of databases (or distributed files) and it’s up to you to pick the engine you want to power your use case. If you’re doing a massive volume of selects and updates pick an OLTP database such as MySQL or PostgreSQL. If you’re analyzing large datasets use an OLAP such as Vertica or Redshift. If you have even more data that’s on a distributed file system use Hive or SparkSQL. And if those aren’t good enough there’s an ever-growing list of SQL-based database products optimized for different use cases - the two that immediately come to mind are [VoltDB](https://www.voltdb.com/) for super quick and accurate transactions and [TimescaleDB](http://www.timescale.com/), a recently launched database built on top of PostgreSQL that’s optimized for time series data.  Every experienced developer has some familiarity with SQL which makes new relational databases feel approachable. In addition, we have so many tools and libraries built for relational databases that it becomes straightforward to extend them to the new entrants. It actually feels as if the pace of new SQL-based databases is growing and it’s because of this creativity granted due to constraints. By committing to a fixed SQL standard database developers are able to focus on designing the perfect engine for a specific use case knowing that if they’re able to hit their performance goals developers will feel comfortable integrating it into their code.  Edit: Remarkably, [Softdroid](http://softdroid.net) found this interesting enough to translate into Russian which can be found at [http://softdroid.net/sql-idealnyy-interfeys](http://softdroid.net/sql-idealnyy-interfeys).",5,1,2017-04-11,5,"sql, database, olap, oltp, rdbms",421,SQL is the perfect interface
9,0,Which will hit zero first? RIM or IE?,#product,"I read an  article  earlier today about how companies are preparing for a possible demise of RIM and couldn’t help but compare RIM’s decline over only a few years compared to how long it’s taking IE to disappear.  To confirm that there is in fact a difference in behavior, we can compare the RIM share among smartphones and IE share among browsers. Turns out that they are noticeably different: IE is on a linear decline with close to 70% in Q3&#160;2008 but around 36% in Q1&#160;2012 while RIM starts at 16% in Q3&#160;2008, goes up to a high of 21% in 2009 and then drops to 7% in Q1&#160;2012. Plotting their % decline since the data starting point highlights this further. If we calculate the average decline per quarter from their highest levels and try to see how long it will take to hit 0% share, IE will take almost 4 years while RIM will take less than 5 quarters.          Why are they so different? If they’re both in the enterprise why don’t we see a similar decline in both? I was able to think of a few reasons but would love to hear what others think.       RIM’s competition has been much stronger - both Apple and Android have been eating up the share at a massive rate while the browser market has been relatively stable. This is compounded by smartphones being a new, quickly evolving industry where people are upgrading phones as frequently as they can.     Guy Kawasaki says that companies should focus on making their product  10 times better  than the existing competition in order to get adoption. This may be a lot easier to accomplish with smartphones than with browsers.     Browsers are an older industry and there’s no point in even doing this comparison. We should do this analysis when the smartphone market is more mature and we can normalize the two time frames.     I tried digging in a bit further but it’s unfortunate how difficult it is to find browser market share data. I’d love to dive in and look at the trend in the browser market since the 1990s and see how that compares to the trend in smartphones. If anyone has this data please let me know.  Here's the  Google spreadsheet  if you want to play around with the data.",3,1,2012-07-10,2,"research in motion, internet explorer",442,Race to 0 - RIM vs IE
31,0,AIM will be shut down in December but there's a lot of sentimental value in the buddy list so use the following code to download it before it's lost forever.,#code,"{% include setup %} While writing the [most recent post](/2017/10/07/farewell-aim/) about the impending AIM shut down I became curious and logged in to see what what I’ve been missing. The application felt worse but seeing my buddy list made me nostalgic and going through the usernames brought back some fond memories as I tried to remember who each screen name belonged to.  I’m a bit of a hoarded, across both the physical and digital worlds, so didn’t want to lose my buddy list after the shut down. Unfortunately, AIM doesn’t make it very easy to download a copy of your buddy list so I came up with a crude but effective approach. Normally I’d look at the source code, identify the HTML elements containing what I wanted, and write a little bit of code in the JavaScript console to extract what I needed. I tried this approach in the AIM web client but it turns out that they update the HTML code to only show the screen names that are actually in view and it wasn’t obvious where the full list of screen names was being stored.  My extremely simple approach was to open the console and write some code to capture the screen names visible on the screen, add them to a running list, and then scroll further down the page and repeat the process. The efficiency here is limited by the number of screen names visible at any one time so probably the cleverest part of the entire script was just zooming way out to get the list to be as long as possible. Then it only took a few scrolls to get the list of screen names which could then be printed and copied over from the JavaScript console. I didn’t even bother with the deduplication step since I could easily remove duplicates by piping the results into the uniq shell command and then piping it back to my clipboard.  Make sure to download your buddy list before the scheduled shutdown on December 15.  {% highlight javascript %} // Start by logging in to www.aim.com and zoom way out  // Create the initial array that we will usernames = []  // Run this a bunch of times after scrolling a bit to get the next set of screen names els = document.getElementsByClassName('user-item'); for (var i = 0; i < els.length; i++) {     console.log(els[i].textContent);     usernames.push(els[i].textContent) }  // Just write to the screen but there will likely be duplicates unless you did your scrolls perfectly. console.log( usernames.join(""\n"") ); {% endhighlight %}",1,1,2017-10-09,3,"AIM, buddy list, screen names",434,Downloading your AIM buddy list
30,0,Just a thought experiment I've been having on what the world would look like if there was a programming language that truly had a single way of doing things.,#meta,"{% include setup %} A thought experiment I’ve had on my mind is this idea of a programming language that only has a single way of solving every programming problem. Regardless of the problem, multiple people writing code independently would end up with the same exact code. No such language actually exists (yet) but it’s fun to think about extreme cases in order to understand where we stand now. With this programming language the only differentiation between developers would be time since the end result would be the same. Beyond that, if there was always a unique solution to every problem this language would be able to write the code itself.  On the other extreme you have current languages which provide a ton of flexibility with dozens of ways to solve a simple problem. In this world developer skills are paramount. You want to make sure you find the approach that solves the current problem but is also written in a way that’s flexible enough to be easily modified for whatever the future brings. Enforcing a structure that’s based on best practices makes it easy to write code that grows with the team.  A great example of this is the rise of JavaScript frameworks. JavaScript is extremely flexible and gives the developer a wide range of paradigms to choose from. This leads to the same problem being solved hundreds of different ways depending on the style and mood of the author. The fact that it actually has a book dedicated to the “ good parts ” highlights how flexible the language is and how easy it is to go off track. Despite being close to twenty years old, only now are we seeing frameworks being developed that take a very opinionated view of how JavaScript should be written. There’s nothing in the language itself to enforce a standard so each framework takes on the responsibility. This allows large teams to collaborate on large projects without having to worry as much about individual styles and decisions.  Software engineering is a new industry and I suspect we’ll see more and more standardization as it evolves. The current approach is to use general languages for a wide range of problem domains but I think we’ll start seeing more and more languages that are specialized by problem domain. This won’t get us to the language with a single way of doing things but it will it a lot simpler to solve problems in a well defined and standard way.",1,1,2015-05-02,3,"programming, software engineering, javascript",422,A unique solution to every problem
37,0,I read a book explaining that the reason Apple didn't use Intel chips since the very beginning was due to Steve Jobs not paying Steve Wozniak fairly. Turns out it's not true but an interesting hypothetical.,#meta,"{% include setup %} I recently finished  The Intel Trinity  which detailed the history of Intel and its rise from a small memory manufacturer to the leader in microprocessors. The entire book is worth a read if you’re interested in startups and the rise of Silicon Valley but one anecdote that immediately stood out was about the reason Apple didn’t use Intel chips  until 2005 . Before then Macs relied on  MOS Technology, Motorola and PowerPC  chips. The Intel Trinity makes the case that the reason Apple waited so long to adopt Intel chips was due to the fact that Steve Wozniak didn’t have enough money to build the Apple I prototype using Intel and had to resort to the cheaper option - a MOS 6502/Motorola 6800. And the reason Steve Wozniak didn’t have enough money was because Steve Jobs didn’t split the Atari payment fairly between them and took the lion’s share without even telling Steve Wozniak about it.  After a tiny bit of digging around the  concensus seems  to be that this is most likely a fabrication and even if Wozniak had the money he would have still gone with the Motorola chip - he was more familiar with the technology and the end goal was to make an affordable personal computer which would have been impossible with the Intel chip. It does make one wonder what the history of Apple and the computer industry would look like had they adopted Intel at the very beginning rather than in 2005.",4,1,2015-07-08,5,"apple, intel, steve jobs, steve wozniak, microprocessors",283,Apple and Intel
14,0,"A recent experience with locksmiths got me thinking about regulation, entrenchment and security.",#meta,"{% include setup %} A month ago I needed to duplicate a set of keys. In the past, I’d just go to the cheapest looking hardware store and they’d easily replicate my keys for around $2 each. This time, I tried the same approach but was told that they weren’t authorized to handle the keys I had and directed me to another locksmith. That locksmith told me that they wouldn’t be able to duplicate it without approval from my management company and also charged $18 for a duplicate. Amazingly enough, they were only able to duplicate one of the keys and I had to go to yet another locksmith (and get another approval) to get the last key duplicated.  I’m not sure if they’re trying to increase the security or whether they’re just trying to create an artificial monopoly for the locksmiths but it’s definitely a pain in the ass for the consumer. This is a standard approach for many industries: add needless complexity under the guise of security and then use that to justify higher prices. This is also why additional regulation isn’t always the answer - it leads to complicated, entrenched systems that can’t be easily innovated upon.",0,1,2013-07-22,5,"locksmiths, security, business, startups, regulation",200,Security through monopoly
15,0,My thoughts on when to leave a full time job to do a startup.,#meta,A common question I get when telling my friends that I’m leaving my full time job to work on Glossi is “Why now? Why not continue working on it nights and weekends?” It’s a fair question - we’ve been working on Glossi for 6 months of nights and weekends and made significant progress. Why not keep doing that and have the best of both worlds?    Everyone has their own reasons but for me it was more of a gut feeling that it was the right time. I had the following thoughts in mind but I didn’t sit down to make a list of pros and cons:          We’re at the stage where the next steps cannot be done on nights and weekends. The coding is no longer the highest priority work and we now have to worry about business development and trying to raise funding. Neither of those can be done easily or efficiently on nights and weekends.        We’re close to finding our product-market fit and we want to get there as quickly as we can.        We felt as if the market is getting more competitive and we need to focus or be left behind. This may be especially true in the tech space where everything moves so quickly.        We realized that our todo list is growing much quicker than our progress through it. I don’t expect this to change but we want to make as much progress as we can.        I was spending more and more time thinking about Glossi that I was not as committed or focused at work. This was both a disservice to me and the company and I wanted to get out of the way as soon as I stopped being as productive as I should have been.      Please share your thoughts  - I’d love to hear other reasons.,0,1,2012-03-21,2,"startups, entrepreneurship",314,When is it time to leave the full time job?
15,0,TV commercials feel as if they're a lot longer than they used to be.,#meta,"It may be my memory but it seems that TV commericals have been getting longer and longer as compared to a decade ago. It would be very interesting to see a plot of the length of the average commercial break over the past few decades - I think we'll see that the length of the average commercial break has drastically increased.    In addition, it seems as if there are no commerical breaks between consecutive TV shows anymore. Clearly this is a way to keep us from turning off the TV and doing something productive with our lives.    Does anyone have any thoughts?",0,1,2008-09-03,3,"tv, commercials, advertising",104,On TV commercials
25,0,I recently went to the library for the first time in nearly a decade to check out a book and felt pangs of nostalgia.,#meta,{% include setup %}      A.D White Library @ Cornell University by eflon    For the first time in almost a decade I checked out a book from a library. I don’t know why I ever stopped - the experience is extremely simple and you’re able to read a book for free. I had a book on my Amazon wishlist for a couple of weeks that I held off on buying but was able to read it over the past week after a quick visit to the library.  As a kid I used to go the library all the time and would go through multiple books a week and this brought back all those memories. I feel guilty for abandoning libraries in favor of Amazon and my iPad and think many people are missing out by consuming everything digitally. It’s not just the physical element but also the nostalgia and the knowledge that there were dozens of people who have read the exact same book you’re reading now. Reading a library book makes you feel that you’re a step in the book’s journey as it grows from home to home and person to person. That’s completely lost in a digital world and it will only get worse as libraries change to stay relevant. Until then I look forward to going back and checking out another book.,1,1,2014-10-28,2,"libraries, books",249,Libraries
24,0,All calendars are are event streams and I'd love to be able to interact with them in a way taking advantage of that.,#meta,"{% include setup %} I like calendars. They appeal to both my sense of organization and efficiency but also to the actual depth of the technical implementation. It’s not easy creating a good calendar application - you have to think about all sorts of edge cases - for example rescheduling an event that was part of a recurring series or the usual time zone nonsense. A thought I keep going back to is treating calendars as event streams. Right now I have a personal calendar as well as a few others that are shared with different groups. Using code it’s possible to collect the events across these calendars and then apply various operations on top of them - find events that overlap, find people that overlap, analyze the event frequencies, etc but this has to be done through very specific logic. I love the idea of calendars just consistent of event streams. That way you can apply transformations on top of the streams to answer any questions you have. I imagine this to be a form of lazy list expression or even something like a SQL query that would allow me to take all the events I have going on and apply a map/reduce like operation to get what I need. This still feels a bit abstract and I need to flesh out the idea a bit more but I feel there’s something here - just being able to treat your calendar as a single stream and provide a simple way to act on top of them.",0,1,2018-12-24,2,"calendars, event streams",254,Calendars as streaming events
31,0,I'm a much worse speller than I used to be due to modern technology. To rectify this I'm forcing myself to retype every word correctly rather than rely on autocorrect.,#meta,{% include setup %} I recently discovered that I’m a much worse speller than I used to be. The culprit is obvious - computers make it too easy to correct our mistakes. In school when making mistakes we’d have to rewrite each word until it became ingrained but these days all we do is just click on the suggested fix without a second thought. There’s nothing there to help me retain the mistake so I continue making it.  This realization made me uncomfortable so I’ve adopted the hybrid strategy of retyping every word that I make a spelling mistake on. It’s never going to be as good as using pen and paper but it’s much better than picking a word from a dropdown menu. There are tons of behaviors that technology has made obsolete but that should not be the reason to abandon those skills. For the same reason that having math skills helps us in daily lives it’s important to hold on to the basic skills we learned as children.,0,1,2015-03-15,3,"spellchecking, education, technology",171,Learning to spell again
21,0,I love strongly typed languages and would love to see dependent types make their way up to the database level.,"#meta,#data","{% include setup %} I’m a huge proponent of strong types when it comes to coding. Unless it’s a throwaway project it’s always worth spending the extra time to define your objects and the way they will be exposed in the code. This investment makes it more likely that you’ve thought through the way the code will need to evolve and the various edge cases you need to handle.  This philosophy is even more important when thinking about your database structure since that’s going to be even more difficult to change than your code. Changing the code requires a deploy while changing a database schema will require a migration and a series of corresponding code changes.  Just last week I came across a language called [Idris](https://en.wikipedia.org/wiki/Idris_(programming_language)) that supports a concept called [dependent types](https://en.wikipedia.org/wiki/Dependent_type). With dependent types one is able to make type definitions that depend on the values themselves - thus a type can be a list where the numbers add up to a particular sum or a an array that requires its elements to be in increasing order. I definitely don’t have a lot of practice or experience with these but I find the concept incredibly powerful. The more constraints we can shift to the type definitions the simpler and more declarative the code gets.  I would love to see this concept extended to databases. I already spend a fair amount of time thinking through database schemas and being able to impose more rules that are enforced at the database level would make databases much more powerful. Right now we spend a ton of time validating our data through APIs and code but imagine being able to have this done at the database level. Databases already provide simple rules around nulls, uniqueness constraints, and foreign keys but nothing about the relationship between fields within a single row. Then all the rules that are implemented in code can be moved upstream to the database, saving a ton of time, improving performance, and simplifying the code.",2,2,2017-05-23,3,"databases, idris, type dependence",339,Type dependent databases
33,0,A simple way to automate AWS deployments is to use the AWS CLI to retrieve information about every instance and then use the instancedata endpoint to retrieve information about the current instance.,#devops,"{% include setup %} A little known feature in AWS is an endpoint that allows you to retrieve various information about about the requesting instance. If you log in to one of your EC2 instance and make a simple request to http://169.254.169.254/latest/meta-data/instance-id you will get back the id of that instance. Similarly, you can get all sorts of  other instance information , including the public hostname, the instance region, and instance type.  This can be useful when you want to automate a simple deployment where you have a few instance with a variety of roles. A lightweight approach would be to use the  AWS CLI  to retrieve a list of all running instances along with their tag names, make a request to the meta-data/instance-id endpoint to get the id of the current instance, and then look up that id in the instance list in order to figure out what the role of this instance should be. Then execute the appropriate set of scripts to configure the instance properly.  More advanced solutions would involve using  Chef ,  Puppet , or  Opswork  but they come with a steep learning curve and overkill for a simple application. If it turns out your application is growing you can always upgrade to a more robust deployment solution.",5,1,2014-07-16,4,"aws, deployment, automation, devops",240,A simple way to automate AWS deployments
48,0,During my trip to India I bought an unlocked phone and spent an absurdly long time trying to get a SIM card. This is a guide on what you need to do in order to get a SIM card in India and the challenges I ran into.,#meta,"{% include setup %} I’ve heard about the wonders of an unlocked phone and decided to try it out during my recent trip to India. The idea was to get a cheap unlocked Android phone that I’d be able to use on this and future trips. I was able to get a relatively cheap Samsung phone but it took me a surprisingly long time to get a working SIM card. This post is a description of the steps I took as well as some advice for others trying to do the same.  First off, to get a SIM card as a foreigner in India, you need to have a copy of your passport and visa, a passport sized photo, and a local to act as a reference. After giving this information to vendor they will do the necessary paperwork, call the reference, and if everything goes well they will activate your SIM card within 24 hours after which you will need to call them to verify and start using the service.  My first attempt was in New Delhi where I went to an Airtel shop based the advice of my uncle. Unfortunately, I didn’t know I needed to have a passport sized photo but was referred to a nearby computer shop that was able to print them out at the cost of 10 rupees (~17 cents) a piece. I was able to buy a regular sized SIM for 300 rupees (~$5) but was told it would take around 24 hours to activate and would only be cut after that. Unfortunately, I had to leave Delhi for a wedding so didn’t get a chance to get it cut to a micro SIM until I had already arrived in Mumbai. By that point, I was in a different city and no longer able to activate a Delhi SIM card although it took me multiple days to figure that out.  After going back and forth to the Airtel shop in Mumbai a few times, and discovering a new hoop I had to jump through every time, I was about to give up until I shared my problem with someone at my hotel. He took me to a nearby stand which was able to take care of everything for me within a few hours. This went smoothly since I had a few of the passport photos left and he was willing to act as my local reference. Total cost was 600 rupees (~$10) and included 250 rupees of credit.  Now that I had a functional phone, it worked great. It took me a little bit of time to understand the prepaid model but once I did I actually preferred it more than the postpaid one I have in the US. You can go to the dozens of mobile vendors around cities which will glady load some money unto your account. You can then activate various services either by using these vendors, doing it online, or via text messaging. At any time you can text various numbers and codes in order to get the balance you have left on your plans as well as add new ones. With my 250 rupee balance (~$4) I was able to buy 150 MB of 3G for 44 rupees (~75 cents) and load the rest into a national dialing plan.  Having a phone that works wherever you go is immensely convenient. Traversing and exploring Mumbai became significantly easier and more fun when we were able to get the phone working. We were able to explore the city without having to worry about getting lost and were able to discover and research local gems. It wasn’t as serendipitous as just walking around but we hopefully struck the right balance.  The challenge was in getting the SIM card working and I’m sure the process will vary in every country. My advice is to do research on how to get a prepaid SIM card before travelling and come prepared with everything you need so you can get the process started soon after arriving. If you know you’ll only be spending time in a city for a single day it may not make sense to get the SIM card there since it may not be possible to activate it in another city - I’m not sure if this is due to my experience or just the way things are done in India but it’s something to be aware of. You can also try contacting the hotel you’re staying at since they should have had experience helping their guests get SIM cards.  T-Mobile recently launched a  global plan  and I’m sure more and more carriers will follow suit. Until then we’re stuck with SIM cards and the unique challenges of obtaining one in different countries.",1,1,2013-12-23,4,"SIM card, travel, unlocked phone, 3G",798,Getting a SIM card in India
23,0,Remove unused code is important to keeping your code clean and maintainable. To do it properly you need to aggressively deprecate code.,#code,{% include setup %} Part of writing high quality code quickly is deprecating no longer used features and functionality. It sounds simple but more often than not there’s an abundance of references throughout - some tightly coupled and others loosely coupled - that make a full deprecation difficult. In some cases it’s is as simple as an isolated code change while in other cases it’s removing code along with some database migrations and in the extreme case it may be removing an entire service. It’s crucial to be exhaustive in your deprecation or you’ll end up in a situation months or years later where the team has changed enough that no one can tell what the code is meant to do and whether it’s still used.  Ideally you have enough tests and code coverage to take care of the obvious changes but even that won’t catch everything. I found the following checklist to be helpful in aggressively deprecating code:  - **Actual code deprecation**. All references to the deprecated should be removed across all relevant repos. It’s straightforward to remove it from the primary code base but are there references to it in other applications? You may have a field you’re deprecating in the API but you need to make sure you remove its references in the UI code as well. As a side note this is a reason why you should make your applications as loosely coupled as possible. With the previous example was there a way to write the UI code so it doesn’t make any assumptions about the API and instead makes a request to the API to get the list of available fields? That way not every API change will warrant a UI change. - **Database migrations**. If you removed a field in the code make sure to remove it from the database along with all foreign key references. If necessary make a dump of the table(s) as a backup but remember to set a reminder to delete the backup in the future. If it’s a risky change rename the column first and make sure nothing breaks - then if something does break you can quickly revert the name change. - **Package requirements**. Did you install a particular package just to get your code working? If so you should review your package requirements and remove the ones that are no longer being used. It’s unlikely that you installed an open source library just to use it in a single place but you never know. The fewer external packages you have the more maintainable your code since you don’t have to worrying about version incompatibilities or library deprecation. - **Configuration options**. Was there any configuration that referenced the code? What about a configuration option that was only used by the deprecated code? Having additional fields in your configuration files won’t break anything but it makes future deprecation more difficult since someone will need to confirm these configuration options are no longer used and just introduces bloat. - **DevOps**. Unless you’re removing an entire application this isn’t common but something to definitely keep in mind is to reduce DevOps bloat. The goal here is to remove all resources that were created for your application. If you were using Terraform or another “infrastructure as code” system this is easier since you may be able to just remove files and let Terraform do the rest. Otherwise you should take a look at all the DevOps resources your application needed and remove them. These range from the DNS entries to the relevant security groups to the instance images to the cloud-stored files. The cloud makes it easy to spin things up but it’s on us to fully spin things down.  The work to aggressively deprecate isn’t easy but thoroughness is essential. It’s much better to take the time to do a deep cleanup while the code is still fresh in mind instead of having a new set of people figuring out what’s still used years later. It’s also a strong investment in keeping your code clean and light which improves future development speed.,0,1,2018-11-25,3,"code deprecation, software engineering, tech debt",675,Aggressive code deprecation
28,0,"I've been working on a simple project, jsonify.me, that lets users create a JSON object of whatever they want and host it under their own domain.",#product,"{% include setup %} Over the past few weeks I’ve been experimenting with Node.js and wanted to share the project I’ve been working on,  jsonify.me . It’s an “API only” product without an interface other than a  documentation page . The idea is to allow anyone to have a publicly accessible URL endpoint that can contain whatever information they want as long as it can be stored as a JSON object. In my case, I have all sorts of semi-structured data that I want to make accessible and keeping it under my domain ( json.dangoldin.com ) makes it easy to access for whoever is savvy enough to figure it out.  The code is still in the early stages and needs to be cleaned up but the core functionality is there. It works by uploading each user’s JSON object to S3 and then doing a redirect whenever a GET request is made to that user’s subdomain. In my case I registered an account with jsonify.me, got an authentication token, set json.dangoldin.com as my subdomain, updated the CNAME record of json.dangoldin.com to point to domains.jsonify.me, and then made a POST request to json.dangoldin.com with my JSON object which uploaded it to S3 under my username.  I love the idea of a semi-structured format evolving over time as more and more people standardize around common field names as well as a set of third party apps that will migrate data from other services into these JSON objects. Imagine being able to have a service that will authenticate with LinkedIn and then extract the relevant data and put it as JSON into your object or a service that fetches new posts from your blog and adds them to your JSON object. There’s still a long way to go and the code right now is very minimal but I’d love to see people give it a shot and suggest improvements.",3,1,2015-01-04,3,"jsonify.me, quantified self, node",337,Introducing jsonify.me
41,0,The more I code the more I believe that code should be minimized and ideally never even written. Good engineers are able to do a ton with well thought out and abstracted code which is easier to manage and improve.,#management,"{% include setup %} The best code isn’t code that’s elegant or code that’s brilliant it’s code that doesn’t need to be written. One of the best feelings is when you can take a new problem and turn it into an existing problem that already has a solution. Sometimes that requires making a few tweaks and compromises to the problem or the code but the time and effort saved can be massive. This requires a deep understanding of the problem being solved as well as the existing code. Someone knowing the code but not the problem won’t be able to transform the problem into something applicable. And someone having a deep knowledge of the problem but not the code won’t be able to see how the code can be adapted to solve this scenario. The optimal result comes from someone who can strip away the cruft from both of them while still maintaining the spirit of both in order to combine them.  To make this work you need code that’s clean, well architected, and accessible. Such code is a pleasure to work with and is transparent enough that a decent programmer can see how it can be tweaked to solve new problems that arise. This requires massive amounts of discipline to go back and refactor your code when necessary to keep it in a pristine state so it can be easily transformed when needed. And that transformation with introduce wrinkles that will need to be ironed out to set it up for the next wave of changes.  While writing this post I was reminded of a joke that emphasises this idea of minimizing work by focusing on what you’ve already done:     A mathematician was interviewing for a job. The interviewer asks him - ""You are walking towards your office and running late for a very important meeting and you glimpse a building on fire with people screaming for help. What will you do?"". The mathematician thinks for a while and replies : ""People's lives are more important than an office meeting. I would immediately call for a fire brigade and help the trapped to the best of my abilities"". The interviewer seems to be impressed with the mathematician's answer and moves on to the last question. Just to check his sanity, she asks: ""And what if the building is not on fire?""  After a moment of thought, the mathematician replies with confidence: ""I will set the building on fire. Now, I have reduced it to a problem that I have already solved before!""    -  ScottElliot  on  reddit",2,1,2016-04-05,3,"code, productivity, management",440,The best code is no code
9,0,What does Shakespeare have to do with startups?,#meta,"I was reading Ben Yagoda&#8217;s book,  When You Catch An Adjective; Kill It , when I came across the following passage: &#8220;In Shakespeare&#8217;s day, there were no fancy props, so the text had to do the work of stage settings.&#8221; Although it was referring to starting sentences with conjunctions, it got me thinking about constraints and the way they foster innovation. Startups operate the same way: you don&#8217;t always have the resources to do what you want and are forced to innovate a way out.    Here are some examples:     Google realized that scaling vertically was significantly more expensive than scaling horizontally using commodity hardware. To achieve this, they had to create the  Google File System  to deal with the thousands of computers and the frequent hardware failures.    Many computer science algorithms and data structures were created when the CPU speeds were low and memory was lacking. Low CPU speeds led to more efficient graph search algorithms like Djikstra and A*. Limited memory led to the creation of probabilistic data structures like Bloom filters and Skip lists.     The Oakland Athletics baseball team could not compete on salary. But under Billy Beane, they were able to compete by adopting an analytical approach to identify talented, undiscovered baseball players (via  Moneyball ).    Even the Lean Startup methodology, which emphasizes rapid prototyping to quickly test market hypotheses, is a way to deal with the limited financial and time constraints that plague startups.       We often complain about obstacles while ignoring their impact on innovation. If you have any other examples of innovations caused by constraints, please share.",3,1,2012-01-29,2,"shakespeare, startups",288,Shakespeare and Startups
21,0,I don't understand why live sports seem to be untouchable. Why can't people wait a few hours to watch something?,#meta,"{% include setup %} Yesterday the world was so abuzz with the Mayweather vs McGregor boxing match that even I couldn’t escape from hearing about it. One small tidbit I discovered was that it would cost you $100 just to watch it in the comfort of your own home. And while I definitely don’t consider myself a sports fan that still feels ridiculous.  Everyone believes that the live nature of sports makes them completely different than other media and yet that doesn’t strike a chord with me. I’ve been to some live events and when you’re with a group the real time element and environment is critical. This also goes for the case where you’re a huge fan and need to know everything as soon as possible. But for the casual fan having a couple of hour delay doesn’t seem that critical. Sure you have to avoid hearing or reading about the outcome but without that knowledge the two situations seem equivalent - at least rationally, if not emotionally.",0,1,2017-08-27,3,"sports, live, delay",169,Why live sports?
23,0,"The new Gmail compose is a step backwards in usability. We need to use data to help understand users, not replace them.",#design,"{% include setup %}            A few days ago, Google made the new compose default on Gmail. It went from a separate page to a popup that’s accessible from anywhere in Gmail. And for the vast majority of the time, it’s better: it’s quicker to get to and makes it easy to reference other emails while writing a new one. Unfortunately, for attaching an image (not embedding it inline) or doing some heavy formatting, it’s a huge step backwards and makes me want the old compose back.  I’m sure the data backed up the decision. Only a tiny fraction of all messages needed this additional functionality so why worry about it? The problem with this approach is that even taking into account the infrequency, the cost of the workaround is large enough to cause a usability problem for the power users. It’s akin to the  old version  of the iOS App Store that would close itself every time you downloaded a new app. Sure that was great when you only wanted to download a single app but it made every other scenario significantly worse.  In the rush to be data driven, we shouldn’t forget the actual users and what they’re trying to do. A data driven approach should be used to improve our understanding, not replace it. Otherwise, we run the risk of “nice to have” features replacing the “must have” ones.",1,1,2013-04-02,4,"gmail, ux, analytics, metrics",254,The new Gmail compose
17,0,I wish clicking a link on an email would automatically log me in to the site.,#product,{% include setup %} This is somewhat heretical but whenever I receive an email that has a link behind a login I wish clicking the link would automatically log me in. I’ve spent more time than I’d like to admit typing my username and password on various sites and too often have had to do it multiple times to get the password right. It would be amazing if clicking a link on an email would automatically log me in and navigate to the linked page.  I know why no one does this: people forward emails all the time and it would be incredibly foolish to log people in without any authentication. That’s a perfectly valid reason and I get it but there are still some interesting things that can be done to improve the experience. One idea is to provide a read-only mode. It won’t work on sites where the content is private or interaction-heavy sites but it might make sense on others. Another option is to track the click and follow up with another email that contains an actual login link. The idea here is that you’re sending it to the user’s registered email so it would have to be clicked by the true owner. On the flip side this is a crappy experience if the person did have a successful login since they’d now have an extra email that added no value. Using a sophisticated AI/ML approach here might work as well - maybe looking at the IP address to see if it’s consistent with previous visits or leveraging browser fingerprinting.  There’s no obvious solution here and this is more of a personal venting sessions but the entire email to site experience can be improved. I value my privacy and use multiple browsers throughout the day. I’ll also purge cookies and history every once in a while and rebuilding that is a slow experience. My use case is rare but it’s often the extreme cases that drive a new insight or workflow and I’d love to see more innovation happening with this sort of “smart login.”,0,1,2018-12-08,4,"login, email, efficiency, user experience",350,Automatic login
22,0,I'm late to the party but finally got a chance to Dockerize a script and learned a ton along the way.,#code,"{% include setup %} I’m a bit embarrassed to admit this but I’ve been a bit behind the Docker craze. Sure I’ve done the tutorials when it came out but never really applied it to any of my actual projects. Given that nearly everyone is using Docker in some shape or form, I decided it was finally time to give it an honest effort.  I had a small Python script that I’ve been running weekly off of my laptop and wanted to come up with a better solution. My old approach would have been to just run it as a cronjob on a VPS but the problem was that it had a variety of third party libraries that needed to be installed via pip and if I ever needed to move it elsewhere I’d have to set it up again. After speaking with a few people it seemed that this could be a use case for Docker and then running it either within a build server (Jenkins) or on top of a distributed system (DC/OS).  From my ancient experiments the Docker tutorial I figured I’d be in for a world of pain but wrapping my script into a Docker container was surprisingly simple. I don’t think it took longer than 10 minutes to get it working and then a bit more time to figure out and modify the code to properly handle the various tokens and secrets.  The next step was getting it working in Jenkins but being able to base it on an already functional job built and deployed a container made it much easier. Once again the majority of the work here was figuring out how to securely handle tokens and secrets as part of the build process but I had a bunch of help.  Now that I had a working end-to-end system I decided to revisit the other scripts in my repo and see if I could Dockerize them as well. This is where I had a few false steps. I tried moving each script into its own folder with its own Dockerfile but that ended up failing when they needed to access shared libraries that I didn’t want to split out or duplicate. The final approach I settled on is to build one container that’s able to run any of the scripts but then trigger the exact script through the run command. I’m not 100% sold that this is the best way of doing it since it feels a bit hacky but I’m happy that I decided to give Docker a shot and the progress I’ve been made. There’s still a lot to learn but I’m much more comfortable diving right in.",0,1,2018-01-03,4,"docker, containerization, jenkins, dcos",449,Learning Docker
26,0,"Craigslist is still king when it comes to selling locally but the future is in NextDoor's hands given their emphasis on community, trust, and quality.",#product,"{% include setup %} Last weekend my wife and decided that we needed to upgrade our couch and in our eagerness decided that we wanted it delivered as soon as possible which happened to be Friday (yesterday). This led us to the question of what to do with our existing couch. The pragmatist in me decided that we should list it on [Craigslist](http://craigslist.org) and [NextDoor](https://nextdoor.com/) for $300 and see who would bite but after a few days without any response it became clear that if we wanted it gone soon it would need to be free. So this past Wednesday I relisted as a free and started receiving bites - just under a dozen on Craigslist and one via NextDoor.  Surprisingly enough the couch ended up being picked up through the response on NextDoor. Very few on Craigslist followed up to my question about timing and it seems no one there actually had any incentive to come. NextDoor, on the other hand, has that community element built in which adds a bit of trust to both sides in the transaction. It also makes it easier to follow through on a commitment versus an anonymous email. In this case I got a chance to help a neighbor and we plan on meeting up for a beer over the next few weeks.  Given its prevalence Craigslist is here to stay for a while but NextDoor is doing something special. At lower volumes you have quantity beating quality but once there’s some volume the momentum quickly shifts to quality. NextDoor seems to have gotten to this point and I look forward to seeing where they take it.",2,1,2017-04-08,5,"nextdoor, craigslist, community, local, commerce",276,Quality over quantity: NextDoor vs Craigslist
28,0,Bank of America has a really crappy way of notifying them of travel plans. No idea why they decided to implement it in such an odd way.,#design,"{% include setup %} Before leaving for a trip to India, I wanted to make sure that I’d be allowed to access the ATM so I decided to contact my bank. Surprisingly, Bank of America was modern enough to allow me to do this online. Unsurprisingly, the UX was lacking.  Instead of just asking which country I was traveling to using a simple autocomplete or dropdown they have a three step process. First, I get to choose whether I’m traveling domestic or international. If internationally, I get presented with four options that are just the first letter of each country name. After choosing a country range bucket, I can finally pick the actual country.              I understand when an inferior UX decision is made because it’s cheaper to implement but in this case it must have actually been more difficult. Instead of having a single dropdown or autocomplete they have three different input elements. Even if the first selection is necessary, there’s no need to split 206 countries into 4 separate lists.  I’m not sure what I was expecting but it’s still frustrating seeing such decisions being made. I’d love to know the reasons.",0,1,2013-12-20,4,"bank of america, ux, design, ui",239,"Why Bank of America, why?"
20,0,Combining RelayRides data with the Edmunds API provides an interesting look into the best car to list on RelayRides.,"#data,#dataviz","{% include setup %} After discovering and browsing [RelayRides](https://relayrides.com/) I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.  There were only a couple of factors at play here: the initial cost of the car, the price the car will rent at, and how often the car is rented. By combining these values we can come up with a ratio of car price to expected revenue per day. The challenge was in getting this data but it turned out to be surprisingly easy.  My first attempt was to simply scrape RelayRides but I ran into a variety of issues with the authentication process and not being able to evaluate JavaScript via a Python script so I switched gears. My second attempt was a lot simpler but got me what I wanted - after doing a search I opened the network tab in Google Chrome and examined the HTTP requests being made. One of these was to the /search endpoint which gave me a JSON feed of the 200 most relevant cars as well as their make, model, year, the daily rate, the listing time, as well as the total number of trips taken. All I needed to do was dump it into a file and start writing a quick script to start parsing and analyzing the data.            The next step was actually getting the price of a car. Once again I thought this would be a problem but it turns out the  Edmunds API  was perfect for this. It’s entirely free and amazingly worked in nearly all cases with the data I was able to get from RelayRides. The API was smart enough to take the make, model, and year from the RelayRides and provide an estimated price without any sort of data cleaning of transformation. The only issue I ran into was a few rate limiting errors when I decided to parallelize my script but the fix was to just introduce a delay between consecutive requests and retry if I ever encountered an issue.            Combining the RelayRides data with the Edmunds API and doing some simple math gave me the answer I was looking for - a 2008 Toyota Prius. There were a few cars that had a better expected return but they also had very few trips taken and weren’t listed for long which leads me to believe that their return won’t last. For the most part, the rental rate ends up being highly correlated with the price of the car - the most expensive in my dataset was a 2011 Mercedes G-Class which is listed at a $550/day and has an estimated cost of $80k while the cheapest was a 2003 Ford Taurus that’s listed at $32/day and has an estimated cost of $4k. In general, the market seems pretty balanced in terms of price but there’s a wide variance in how often different cars get rented out - it’s clearly proportional to price but there’s definitely something else there. Unfortunately, this approach only lets us examine the cars that are actually listed and won’t let us predict how a random car would do. In the future I might take a stab at running a regression to generalize this approach but the challenge will be in figuring out the relevant factors.            As usual, the code’s up on [GitHub](https://github.com/dangoldin/relay-rides-analysis) and I’d love to hear ideas or thoughts on how to improve the code or the analysis.",3,2,2015-06-07,4,"relayrides, edmunds, maximizing return, data analysis",693,Finding the optimal car to list on RelayRides
30,0,Sometimes the best thing to do when working on a tough problem is take a break and think about something else - this allows the subconscious to do the work.,#meta,{% include setup %} Something I’ve encountered is being stuck on a difficult problem but then taking a break until an “aha moment” just materializes. This happened throughout college on difficult problem sets as well as countless engineering projects at work. Sometimes instead of getting hung up on a tough problem the best thing to do is to forget about it and go for a walk and let the subconscious take over. I don’t know why this works but it seems to be common with others as well.  Bill Gates takes an  annual reading vacation  where he reads books across a variety of disciplines in order to have their themes cross pollinate and spawn new thoughts and ideas. This isn’t a conscious process and he relies on his subconscious to do the organization and connect the different ideas together. This is akin to what happens when we stop thinking about a tough problem and focus on something else: the mind is free to wander and may combine them into something that’s useful - or at least inspire another thought that may be relevant.,1,1,2014-12-14,3,"thinking, coding, subconscious",187,Waiting for the aha moment
13,0,I wrote two blog posts via phone and it went surprisingly well.,#meta,"{% include setup %} A few weeks ago I had to run some errands at the mall and ended up having some free time. I was also a few blog posts behind so decided to see how much I could actually do via phone. Surprisingly, I got a fair amount done. The posts still required a fair amount of editing when I was back on my computer but for getting the bulk of the content and structure down on my phone was nearly as good as via a real keyboard. What it lacked in speed it made up for by not having real multitasking which made it more difficult to get distracted. It wouldn’t work for posts that require search or significant research but for quick blurbs or jotting down thoughts it works remarkably well and I suspect it will only improve with time. Years ago I viewed phones and tablets as being purely designed for consumption rather than creation so this has been a pleasant surprise and I’m coming around to the idea that one can be productive without an actual computer. Next is to try attempting to write a blog post via voice dictation.",0,1,2016-06-19,4,"blogging, phone, smartphone, productivity",197,Blogging from my phone
19,0,"Did a few more data visualizations on my blog again, including word clouds and a text similarity analysis.","#code,#dataviz",{% include setup %} I’m a sucker for data visualizations so when I came across a simple word cloud-generating [Python script](https://github.com/amueller/word_cloud) I knew I had to give it a shot. Lucky for me I’ve been blogging fairly consistently since the beginning of 2013 and have a large text set to visualize. The first step was generating a word cloud for every single post I wrote and the second was to break it down by year. This didn’t reveal too much but got me thinking about how my writing has changed over the years. This led my discovery of a [script on StackOverflow](http://stackoverflow.com/questions/8897593/similarity-between-two-text-documents) that works by translating each block of text into an tf-idf (term frequency - inverse document frequency) vector and then calculating the cosine distance between them. This intuitively makes sense. The tf-idf vector is used to highlight and quantify the unique words in a given document as a vector and the cosine distance is used to compare the similarities between them - if they vectors are equivalent the angle between them is 0 which has a cosine of 1. Turns out that high school math is incredibly useful.                                                A word cloud across every blog post.                                                               A word cloud for 2008 blog posts.                                                               A word cloud for 2009 blog posts.                                                               A word cloud for 2010 blog posts.                                                               A word cloud for 2011 blog posts.                                                               A word cloud for 2012 blog posts.                                                               A word cloud for 2013 blog posts.                                                               A word cloud for 2014 blog posts.                                                               A word cloud for 2015 blog posts.                                                               A word cloud for 2016 blog posts.                                                               Blog post similariy between 2008 and 2016. It's shocking to see how similar my blog posts have been since 2014. Based on the word clouds it seems all I write about is code and data.,2,2,2016-12-10,4,"blogging, data visualization, word clouds,",547,Word clouds and text similarity
28,0,Website loading times are drastically different between NYC and Beijing - as expected the sites in China load faster in Beijing but slower in NY and vice versa.,"#datascience,#dataviz","{% include setup %} Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and  compare them  against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from  rahimnathwani  who ran the script in Beijing, using home ADSL, and wanted to share his dataset.  I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.  Below are some visualizations highlighting the differences in a few different ways. I’d love to get my hands on more data so if fee free to submit a  pull request  with your data and I’ll rerun the analysis. I’ve also included the R code that generate the plots below for those curious to see how they were done.                                                          Parallel plot.  The idea here is to see whether the lines are mostly horizontal or if they're steep. Horizontal lines would indicate that sites are universally slow (and fast) while steep lines indicate that some sites load much faster in one city compared to the other.                                                                          Load time differences.  Here we sort the sites by the difference in average load time, NYC minus Beijing. Most of the sites loaded faster in NYC but I suspect the biggest reason was due to internet speed differences. The sites that loaded faster in Beijing are for the most part in China.                                                                          Scatter plot.  A different perspective than the parallel plot but trying to answer the same question. We do notice a few outliers here which we can investigate by adding text labels.                                                                          Labeled scatter plot.  This provides a nice look at the outliers but makes it impossible to look at the sites that loaded quickly in both NYC and Beijing.                      {% highlight r %}times <- read.csv(""out-times-beijing.csv"", sep=""\t"", col.names=c(""url"", ""time"")) times$url <- as.character(times$url) final <- ddply(times,~url,summarise,mean_time_beijing=mean(time),sd_time_beijing=sd(time))  times2 <- read.csv(""out-times.csv"", sep=""\t"", col.names=c(""url"", ""time"")) times2$url <- as.character(times2$url) final2 <- ddply(times2,~url,summarise,mean_time_nyc=mean(time),sd_time_nyc=sd(time))  combined <- merge(final,final2,by=""url"") combined$time_diff <- combined$mean_time_nyc - combined$mean_time_beijing combined.m <- melt(combined, id.vars=c('url'), measure.vars=c('mean_time_beijing', 'mean_time_nyc'))  png('crawl-stats-comparison-parallel.png', width=600, height=600) ggplot(combined.m) +   geom_line(aes(x = variable, y = value, group = url)) +   theme_tufte() +   ylab(""Load Time (ms)"") + xlab("""") dev.off()  png('crawl-stats-comparison-time-diff-bar.png', width=600, height=600) ggplot(combined, aes(x=reorder(url, -time_diff), y=time_diff)) +   geom_bar() +   theme_tufte() +   coord_flip() +   xlab(""Load Time Diff (ms)"") +   ylab(""URL"") dev.off()  png('crawl-stats-comparison-scatter.png', width=600, height=600) ggplot(combined, aes(x=mean_time_beijing, y=mean_time_nyc)) +   geom_point() +   theme_tufte() +   xlab(""Beijing Load Time (ms)"") +   ylab(""NYC Load Time (ms)"") dev.off()  png('crawl-stats-comparison-scatter-text.png', width=600, height=600) ggplot(combined, aes(x=mean_time_beijing, y=mean_time_nyc)) +   geom_text(aes(label=url), size=3) +   theme_tufte() +   xlab(""Beijing Load Time (ms)"") +   ylab(""NYC Load Time (ms)"") dev.off() {% endhighlight %}",7,2,2014-03-11,5,"site speed, web performance, nyc, beijing, data analysis",724,Website load times: NYC vs Beijing
21,0,One of my favorite programs that had a huge impact on the way I approach code was Peter Norvig's spellchecker.,"#code,#meta",{% include setup %} While working on a [small programming puzzle](/2016/07/17/coding-puzzle-word-transformation-through-valid-words/) I remembered Peter Norvig’s [spell checker](http://norvig.com/spell-correct.html) and how blown away I was after seeing it for the first. It’s one of my favorite examples of code that’s clean and elegant while being extremely expressive and powerful. If you haven’t seen it yet I encourage you take a look and step through it since he does a much better job of explaining both the code and theory than I ever could.  I don’t want to attribute my improvement as a coder to a single program but this program forced me to think much deeper about the code I write and provided a glimpse of good code. It serves as a goal and pushes me to be more aware of the code I write and whether it’s as simple and expressive as it can be. It’s not easy but approaching development through a lens of self improvement has been instrumental in helping me become a better coder. Good programmers are never happy with the code they wrote a year ago which is a sure sign that they’ve improved over the past year. Dissatisfaction is what drives people to improve and code is no different. It’s rare to find code that’s shocking in its brilliance and I’d love to see more examples so please share.,2,2,2016-08-06,1,code,233,One of my favorite programs
18,0,First pass at a summary of the personal stats I've been collecting over the course of 2015.,#stats,{% include setup %} Over the past year I’ve been collecting personal stats nearly every day in order to see if I can spot any patterns and just understand myself better. These ranged from the time I spent sleeping to my mood (both physical and mental) to what I ate and drank. Over the weekend I hope to dive deeper into them and work out some relationships and patterns but for now I wanted to share just some basic summary stats. The script to analyze the data is up on [GitHub](https://github.com/dangoldin/annual-stats-analysis) but note that it’s designed for my file format.         Avg  Min  Max  Std Dev      Sleep (Hours)  7.35  3  11.5  0.96    Alcohol (Drinks)  1.79  0  14  1.97    Coffee (Cups)  1.38  0  2.5  0.66      I also have data for my physical and mental moods three times each day. I haven't gotten the chance to get anything meaningful out of it yet but for the most part I'm a pretty happy person! I've had a few colds and headaches but I categorized over 90% of the days as being in a good mood but just under 80% where I'm a good physical mood due to some congestion or allergies which end up improving by the end of the day.  These summaries are interesting despite being simple and I can't wait to see what I discover when I take a deeper look. The goal is also to use this exercise to tweak the what and how of what I'm going to collect in 2016. Definitely let me know if you have ideas.,1,1,2016-01-12,3,"stats, quantified self, personal tracking",255,2015 Stats: Part 1
36,0,An interesting behavior I've adopted is to use apps when I'm on LTE and mobile web when on wifi. My gut tells me apps are faster and use less data but are also less flexible.,#meta,"{% include setup %} Lately, I’ve noticed an interesting trend with my smartphone usage. When I’m on wifi I’m much more likely to use the mobile web, click links, and read various articles whereas if I’m on LTE I’ll stick to dedicated apps. I noticed this at my apartment which has a narrow layout with my living room having wifi and my bedroom stuck on LTE. Despite me being in the same mindset regardless of which room I’m in my behavior changes dramatically.  I don’t recall making a conscious decision to change my behavior so I suspect this behavior evolved to deal with the increase in my data usage. I’m still not sure that apps are more data efficient than the mobile web but it definitely feels that way due to the improved speed and responsiveness. If you’re concerned about data usage it’s obvious that you’ll want to do as much as you can on wifi rather than go through your data plan but what’s interesting is that in my mind I’ve concluded that apps and mobile web are differentiated by my data plan. I’d love to know if others do something similar or I’m the anomaly.",0,1,2015-11-09,5,"apps, mobile web, data usage, lte, wifi",198,"Apps on LTE, mobile web on wifi"
21,0,Procrastinating is terrible and I've picked up a few tricks over the years that encourage me to get stuff done.,#productivity,"{% include setup %} I often find myself wanting to postpone something but after forcing myself to actually do it I discover that most of the difficulty was getting over the postponement hump. Especially these days it’s very easy to get distracted - whether it’s checking your email, responding to a few tweets, or clearing notifications on Slack - but it’s important to just focus on the most important task at hand. After starting you discover that the task wasn’t worth delaying and get an energy boost from actually finishing something.  There are a ton of tricks and tools to discourage procrastination and I wanted to share some of my favorites. At the end of the day if you’re not serious about doing the work they won’t help but they do help in turning a healthy process into a valuable habit.  - **Plan tomorrow out today**: It’s easy to go home at the end of the day and then spend the following morning figuring out what you should work on. Instead take the time at the end of today to figure out your priorities for tomorrow. This allows you to get a jump start on the next day since you don’t have to spend the effort figuring out what you should be doing. - **Do the difficult stuff first**: Difficult tasks take time and energy and you’re better off doing it when you’re at your peak. For me it’s in the morning when I’m distraction free and I try to get as much done during that time as I can. Similarly, if you have a project with a ton of components you want to derisk it by working through the riskiest pieces first rather than delaying the uncertainty and ending up with an unpleasant surprise. - **Schedule your tasks**: Many people use a todo list but I find a much stronger form of this to be actually putting down tasks as calendar events. This forces you to dedicate time to the task and you have no excuses for not working on what you committed to doing. Of course your estimates will be wrong but all that means is you dedicate some future time to finishing it up. A side benefit of this is that you end up becoming much better at estimated how long various tasks will take. - **Avoid distractions**: This is an obvious one and arguably should not even be on this list but the more distractions there are the more distracted you will be. Extreme versions of this are to close every program you’re not using and even install apps that won’t let you do anything but the task at hand. Instead of resorting to those it’s more important to train your mind to focus. - **Try the [Pomodoro technique](https://en.wikipedia.org/wiki/Pomodoro_Technique)**: I used this in the past with moderate success but the idea is to break your day into chunks: 25 minutes to work on a task, followed by a 5 minute break, repeated until you’ve done this 4 times and earned yourself a longer break. I’ve found it a bit too structured for me since when I get in the zone I want to remain in the zone instead of having to think about the next break. Many people swear by it though.",1,1,2017-03-09,3,"productivity, procrastinating, postponing",542,Stop procrastinating
27,0,We've made a ton of progress in self driving cars but we haven't really considered what happens when malicious actors purposefully try to trick the code.,#meta,"{% include setup %} While catching up on the latest self driving car news and digging into the way neural networks work I started thinking of the ways self driving cars would navigate optical illusions or seemingly impossible physical scenarios. I’m by no means an expert but current machine learning and artificial techniques don’t build a relational representation of the world but instead focus on statistical ways of classifying the information they see and then making decisions off of that information.             Artist sets a trap  for a self driving car by surrounding it in a white border.     This approach works in the vast majority of cases but one can imagine a world where a malicious third party can easily mess with the data going on. About a month ago an artist “trapped” a self driving car by surrounding it in a white circle. That was just an art piece but imagine such actions by malicious actors. Some examples can be someone painting a realistic picture of a pothole, a street sign, or even a person on the road and seeing how self driving cars would react. A proper self driving car would likely be cautious under uncertainty and wouldn’t know how to navigate these situations. Even worse one can probably develop a whole set of these optical illusions that are easy for a human to dismiss as entertaining yet fake but will cause significant problems for self driving cars.  Over time they’ll adapt with the addition of new sensors and a slew of data but it is a real concern I haven’t seen discussed much. Even something as simple as driving in rain or snow is a problem when you’re a computer that learned to drive in sunny weather on perfect road conditions. People, on the other hand, have an innate understanding of physics and how the world works which allows them to easily adapt to new environments. I can learn to drive on a sunny road but within a few minutes feel comfortable driving in the rain and snow. Existing self driving cars don’t work that way and would need to have thousands of hours of training on these new conditions to be comfortable enough to drive on them. It’s impossible to predict the future but I suspect the real advances will come from a combined model - one that combines the existing machine learning techniques with one that’s able to model the relationship and physics of the world.",1,1,2017-04-09,4,"self driving cars, optical illusions, machine learning, artificial intelligence",426,Optical illusions and self driving cars
21,0,Has social media made us less likely to cause real change in the world since we're liking and sharing instead?,#meta,"{% include setup %} The recent revolt around Instagram’s TOS changes got me thinking about the revolt against SOPA/PIPA and the impact social media is having on cultural participation. We’re wired to want to improve things and when we come across what we feel is an injustice we want to change it. Unfortunately, social media has made us lazy. Sharing something on Twitter or Facebook gives us the nice, warm feeling that we’re actively contributing to a cause. Instead of going out and demonstrating in public, snail mailing our representatives, or providing financial support, we’re clicking a link and think we’re making a difference. PIPA/SOPA wasn’t stopped due to internet outrage but from people calling their representatives and doing more than just mentioning their opposition. Wikipedia and Reddit didn’t just put a message up saying they oppose PIPA/SOPA but blacked out their site. Would the result be the same if they just had a message stating they opposed it?  Social media is great at raising awareness, it’s just not very useful until someone down the line acts on it. Does the increase in awareness lead to actual change? This simplicity and reach also leads to a massive number of causes being championed. I can’t login to Facebook without seeing some cause being shared and promoted. Causes now need to market themselves as much as a consumer product. Are we really better off?  I can’t help but think of the pen and sword metaphor. The pen is mightier than the sword because the pen is able to get many swords. Does social media just give everyone pens or does it lead to more swords?  My way of dealing with this is to take a real action every time I share something on social media. If I share a Kickstarter project then I will donate to it. If I share opposition or support of a bill, I will call my representative. We’d be in much better shape if everyone did the same.",0,1,2013-01-11,1,social media Instagram SOPA PIPA,331,Is the pen mightier than the sword in a social world?
29,0,"In order to get more sales and increase market size, companies need to focus on educating their customers. They'll be able to decrease acquisition costs and increase retention.",#sales,"{% include setup %} A trend I’ve been seeing lately is companies boosting their sales by focusing on customer education. The successful companies don’t just focus on the results their product will deliver but also spend time explaining why those results are important and how the product works and how it can be used.  This approach seems obvious to me. Borrowing some terminology from  Crossing the Chasm , the early adopters will use your product as long as it solves an existing problem but education will help the remaining, slower adopting customer segments discover that they even have a problem and look to you for a solution. In my opinion, the major benefits of customer education are to reduce acquisition costs and improve retention. Acquisition costs will drop as you start relying more on inbound interest rather than on outbound sales. Retention will increase since customers that sign up willingly will stick around longer than customers who needed to be coaxed into it by a sales rep. These “self serve” customers will also be more likely to blame themselves when encountering problems rather than whoever got them to sign up. In addition, by developing original and useful content you’ll help your SEO score which will drive more potential customers to look at your products. Your trustworthiness will also improve since you’ll be offering free and useful knowledge.  In many ways, freemium and free trial products are pursuing this strategy by allowing their products to be used with the hope that these users will turn into customers after they discover that they’re able to solve real problems using the product. A sales rep can then work with the customer to understand his needs and pick the right product offering to solve his problem.  I believe we’ll be seeing much more of these self serve, education products in the coming years and it’s important to get involved in this shift now. In my mind, the two companies that have been doing a great job with it are  New Relic  which is providing a great way to try their monitoring service and Mixpanel, which has a  dedicated portal  focusing on analytics education.",3,1,2013-09-06,4,"product, marketing, sales, startups",369,Want more sales? Start teaching
17,0,We're in the golden age of browsers despite the top 5 browsers having 95% market share.,#meta,"{% include setup %}      Browser market share June 2018    In June 2018 the top 5 browsers had an estimated 94.98% share of the browser market. This makes sense since most people either use their OS’s default browser or find a mainstream alternative, such as Chrome or Firefox. Yet there’s a lot of interesting and novel browser work happening on the fringes. [Brave](https://brave.com), started by Brendan Eich (creator of JavaScript and cofounder of the Mozilla project), is designed for privacy and comes with built-in adblocking; [Vivaldi](https://vivaldi.com) is all about customization and tab management; [UC Browser](http://www.ucweb.com) is all about mobile performance - and is, in fact, the third most popular mobile browser.  The fact that there’s this much browser diversity is great. These alternative browsers are able to experiment with their approach and appeal to a passionate enough user base that keeps them going. And if these features do work, the mainstream browsers end up adopting them in future versions.  Nearly all, if not all, browsers are free. This is all happening due to the wonders of the modern web. The mainstream browsers have huge corporations supporting their development. The others support themselves through advertising by selling the search bar to search engines. I don’t know the how much revenue the average user makes or how many users it would take for a browser to stay alive but the more we experiment with these alternative browsers and try them out the healthier the browser ecosystem.",4,1,2018-12-05,4,"browsers, brave, vivaldi, uc browser",267,The golden age of browsers
22,0,It doesn't bother me when apps launch on iOS before Android. What bothers me is that I can't reserve my username.,#product,"{% include setup %} Both Meerkat and Periscope launched on iOS first. That doesn’t bother me despite have an Android phone. They’re running a business and it’s up to them to decide where they want to invest the time they have. What bothers me is that I’ve been using a specific username across the various services, dangoldin, and now run the risk of losing it on these newer networks. A simple fix would be to at least allow me to preregister my username without requiring an iOS device. This would also encourage me, and a lot of other Android users, to download the app when it finally does make its way to Android.  This is a first world problem but as more and more people start building their brands it’s useful to have a single name that spans across services and networks. It makes it easier for your audience to find you and lets you transfer audiences from one to another. There’s always going to be a way to login with Twitter or Facebook but I like having my networks divorced from one another with my username serving as the only link.",0,1,2015-03-29,3,"iOS first, Android, usernames",193,iOS first and username claiming
13,0,"I found an odd, unexpected behavior in Perl that drove me crazy.","#code,#perl","I ran into this problem a while back and wanted to share it. It was a bit unintuitive but documentd so I guess I shouldn't be surprised by the results. Hopefully this will help someone else avoid this pitfall.    It looks as if declaring a variable with the ""my"" statement but then guarded with an ""if"" statement causes the scope of the variable to be global - note that the ""use strict 'vars';"" pragma does not give an error in this case.   {% highlight perl %} #!/usr/bin/perl -w use strict  'vars';  sub foo{     my $val = 0 if (0);     $val = 1 unless defined($val);     print ""Val: $val\n"";     $val = 2; }  foo(); foo();{% endhighlight %}   The output of this call gives:  Val: 1  Val: 2     Although the expected result would seem to be:  Val: 1  Val: 1     Using Google, I found the following nugget from perlsyn:    NOTE: The behaviour of a my statement modified with a statement modifier conditional or loop construct (e.g. my $x if ... ) is undefined. The value of the my variable may be undef, any previously assigned value, or possibly anything else. Don't rely on it. Future versions of perl might do something different from the version of perl you try it out on. Here be dragons.  http://perldoc.perl.org/perlsyn.html#Statement-Modifiers",1,2,2008-05-30,2,"perl, coding",226,Interesting Perl behavior
22,0,Being productive on the command line is crucial for a developer and zsh with Oh My Zsh makes it significantly easier.,#devops,{% include setup %} I spend a fair amount of time in the command line and one of my biggest wins in productivity has come from adopting Z shell along with the wonderful  oh-my-zsh  framework. I initially installed it when looking for better git integration but have been discovering tons of new tricks and features since. In addition to the standard autocompletion for both paths as well as commands there are various plugins to support a variety of other scripts. Just a few days ago I enabled a plugin to allow for autocompletion for Python’s fabric commands. The advantage for a single command is tiny if you’re quick on the keyboard but when you’re running hundreds of commands each day it’s nice to get your typing speed to be as quick as your thought process. Zsh comes close.  If your bash environment is optimized for your flow after years of tweaking zsh is not for you. Otherwise you should give it a shot - it’s a superset of what you get in bash and you can easily migrate your bash configuration to zsh and can easily switch back. I have dozens of apps installed right now that I’m sure I’ll forget to reinstall when I get a new computer - until I realize I need them. Zsh on the other hand will be one of the first things I install - it’s that critical to my productivity.,1,1,2015-07-06,5,"zshell, zsh, oh my zsh, bash, shell",242,Zsh and Oh My Zsh
16,0,In order to keep my coding skills sharp I've been messing around on HackerRank problems.,#meta,{% include setup %} These days I rarely code during work since there’s other work to be done but I miss it. Lately I’ve started working on some [HackerRank](https://www.hackerrank.com) problems. They send me a problem a day and nearly every time I’ve been able to knock out a solution in a few minutes. It feels as if I’m keeping my skills sharp and gives me that quick win. Every once in a while I’ll pick up an old project to hack on but that’s a very different style of coding. That’s more focused on understanding modern frameworks and web development while HackerRank problems are entirely standalone and tend to be logic based that often depend on finding the insight before coding up the solution:: they’re basically a poor man’s version of [Project Euler](https://projecteuler.net). I know most people use them to prepare for interviews but I like using them for the challenge and the win.,2,1,2018-12-22,2,"hackerrank, project euler",158,HackerRank
31,0,I had an old GitHub repo that visualized Twitter activity so I thought it would be fun to dig it up and run it again to see how it compared.,"#dataviz,#code","{% include setup %} While going through my old GitHub repos I discovered that the most starred repo was [twitter-archive-analysis](https://github.com/dangoldin/twitter-archive-analysis), a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working. Compared to three years ago, the results are surprisingly not that different - I definitely tweet less frequently than I used to and my activity has shifted into being more about replies rather than general tweets.                              No tweets while I'm asleep but tend to be the most active in the evenings.                                       Pretty even distribution but more active on the weekends than the weekdays.                                       Hit my peak in 2013 and have been declining since.                                       I tried to get at the idea of how much I tweet over time and by day - the weekends have remianed steady but my weekday tweeting has dropped off.                                       Definitely not taking advantage of the full 140 characters.                                       The next visualization provides a much better idea of my tweet type distribution.                                       A clear trend to being more about replies and engagement rather than just posting thoughts and ideas.",1,2,2016-10-19,2,"twitter, data visualization",417,Revisiting my Twitter activity
14,0,A quick JavaScript that allows you to specify your own Sierpinksi generation strategy.,"#dataviz,#javascript,#code","{% include setup %}  As a follow up to my previous  post , I modified my Sierpinski generation  code  to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it's difficult to confirm given a fixed screen resolution. I'd love to know what's going on here and whether there's a relationship between the number of sides and the distance ratio.                                       N = 4, ratio = 0.4                                                      N = 10, ratio = 0.2                                 # of sides                                    Distance ratio                                            Generate!                    {% include D3 %}  {% include custom_js %}",2,3,2014-02-21,3,"Sierpinksi, gasket, triangle",290,More Sierpinski fun
31,0,Similar to sports teams that have a limited budget for a limited number of roster spots startups should approach hiring the same way and find arbitrage opportunities where they can.,#meta,{% include setup %} With the football playoffs upon us I’ve been thinking of why some teams are more successful than others. They all have approximately the same budget to allocate across a 53 person roster yet some teams are consistently dominant while others predictably fail. What’s so special about the teams that succeed? There’s obviously the coaching but I believe the bulk of the credit should go to the players. Imagine if a single team had a budget that was an order of magnitude more than the next best team: that team would be stacked with the best players and would dominate in competitions.  The best teams are the ones that are able to find arbitrage opportunities by getting players that are much better than their salary dictates. This may involve drafting players with high potential or trading for players that are undervalued and underutilized. If everyone knows that a player is great that will be baked into the price and while that player will improve the overall quality of the team it would reduce the overall budget and make getting additional star players more difficult.  Hiring can be the same thing. Netflix has a well known [culture deck](http://www.slideshare.net/reed2001/culture-1798664) indicating that they want to view themselves a sports team with a limited number of roster spots and they’re looking to maximize the contribution of each one. While a bit extreme this is a healthy attitude for a company to have. We all have limited budgets and making each dollar go as far is it can is what separates success from failure. This is especially important in a startup which may not be profitable and is going through a limited amount of funding.,1,1,2017-01-14,3,"recruiting, hiring, startups",287,Find the arbitrage opportunities in hiring
16,0,I show the differences between the ages of actors vs actresses over the past few decades,"#dataviz,#datascience,#code","I recently watched  Miss Representation  which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data. The goal was to replicate the results as well as provide some tools for others to do similar analyses.  I took a quick look at the IMDB site and realized that they did not have an API available. I looked at a few open source alternatives but they all seemed like overkill for what I wanted to do so I decided to just write a quick Python script to scrape the pages I needed. I started by pulling the top 50 movies for each decade (via    http://www.imdb.com/chart/1910s    -    http://www.imdb.com/chart/2010s )   and then pulling the top 5 cast members for each movie (via    http://www.imdb.com/title/tt1375666/fullcredits#cast )  . I had to actually look at the actor/actress pages as well in order to pull the birth dates as well as the sex. After loading this data into a database it was a very simple query to run the analysis and then  Google Spreadsheets  to clean it up.   Not surprisingly, it turns out that over the past 11 decades, the average actor is 41 while the average actress is 32. Interestingly, during the 1980s they were almost the same but the gap has been widening since then.        I may be a bit late to the “ Don’t learn to code ” debate but I think this illustrates that coding is a pretty useful skill to have. It’s not about being able to develop enterprise applications but more about automating some work and being able to scratch a curiosity itch. If this data were publicly available and everyone had the tools and abilities to do these types of analyses I believe we’d be in much better shape. Maybe someone can take the work I started and leverage it to discover something new.      Note: The code to scrape IMDB is posted on  github  but note that it’s definitely crude and hackish at times. My goal was to get the analysis done as quickly as possible so I didn’t spend too much time refactoring.",11,3,2012-05-23,3,"actors, actresses, hollywood",481,Trend of actor vs actress age differences
17,0,I have two ways I avoid being tracked in email - disabling images and not clicking any links,#meta,"{% include setup %} It’s tough to protect your digital privacy with the modern web but I try my best. One of the tricks I’ve been using is to disable all images and avoid clicking any links that don’t go directly to the desired location. The reason for disabling images is that it prevents your opening of the email from being tracked since the image request (which contains some unique identifier) does not get made. The reason for not clicking on any of the links is similar - they’re rarely the final page you’re trying to get to but instead go through an intermediary that’s able to track your click. You can identify these by hovering over the link and seeing what the destination actually shows in the status bar. If it’s the page you want, great, otherwise I’ll go to the desired page directly or do an incognito search to get where I need to go.  The flow is slower but reducing tracking is important to me and I’m willing to take a bit of friction to give myself the illusion of privacy. It’s not perfect and I’m still being tracked in a variety of ways but it’s one small way of fighting back.",0,1,2018-11-15,2,"privacy, email tracking",205,Limiting tracking in email
10,0,Target the consumer in order to sell to the enterprise,"#product,#sales","A trend I’ve been noticing more and more is enterprise sales being done bottoms up. The typical approach is to offer a free trials or have some sort of freemium product. Each sign up is then treated as an inbound lead that is assigned an account manager. Within two weeks of signing up for New Relic I was contacted by an account manager who helped answer my questions and helped me get New Relic set up for Glossi. Working with him, we were able to get a longer trial period and a discounted price for when we’re ready to upgrade.  HubSpot found  that inbound leads cost 61% less than outbound leads. If having a strong SEO and Social Media presence drops acquisition costs that much imagine the drop caused by having a usable product. Although we’re a small, scrappy startup that’s quick to try new products and services, I believe this approach will become the standard way of selling SAAS in the enterprise. It’s much easier to get a person to try something new and if you can turn him into a fan, you’re one step closer to getting the company signed up.    An extreme case of this would be to initially build a product that’s focused on the consumer and only building out enterprise features when there’s a clear demand for them. A great example would be Dropbox, they initially focused exclusively on making a kick-ass experience for the consumer and only after nailing that down did they release the “ Dropbox for Teams ” plans. I don’t recall the history of  GitHub  but they may have done something similar - initially focusing on public and private repositories and then growing into the more enterprise friendly plans. This is a great approach for a product driven startup since you can focus on building your product without getting stuck in the twisted path of custom client work. But when your product and team are more fleshed out, you can focus on the additional revenue opportunities created by going after the enterprise.",3,2,2012-05-22,4,"selling, startups, enterprise, marketing",361,Selling to the enterprise? Target the consumer
27,0,AIs will dominate more and more of society but rather than being a sudden shift we'll see humans and AI cooperating while AIs continue to mature.,#society,"{% include setup %} There’s been a lot written about the rise of AI and the impending automation which will lead to a mass reduction in industries requiring a human touch. The counterargument is that every time there has been a technological improvement new, higher value jobs have been created that more than offset the loss. Farming used to grossly inefficient when done by man but now massive harvesters are able to replace dozens of human workers. Similarly, technology has allowed entirely new industries to be created that just wouldn’t have been possible before - every company is embracing technology to succeed and with that comes improved efficiency and lower costs.  I forwarded an [article](https://www.theguardian.com/technology/2016/dec/22/bridgewater-associates-ai-artificial-intelligence-management) about Bridgewater, a hedge fund, trying to replace managers with AI, to a [friend](https://www.linkedin.com/in/jacob-mazour-prm-cfa-b2668b11) and he replied with the astute observation that this is already happening at Uber. The drivers are depending on an algorithm to tell them where to go and how to get there. Sure someone wrote the code but there’s no person actually telling the drivers what to do - that’s left to the algorithms. It’s only a small example now but I have no reservation that these sorts of change will sweep all industries over the next few decades.  I’m not as optimistic nor as bearish as many and I expect that at least in the short term we’ll find a sweet spot that’s able to leverage the strengths of AI with those of humans. A great example is to look at chess. The best chess players consistently lose to AIs but a [human paired with an AI](https://en.wikipedia.org/wiki/Advanced_Chess) is consistently better than an AI alone. It’s unlikely that machines will all of a sudden take over and more likely it will be a fusion of the two with AI growing larger and larger in share as it improves. By then we’ve hopefully figured out how the world should look.",3,1,2016-12-30,7,"human, artifical intelligence, ai, cooperation, chess, automation, society",332,Human-AI cooperation
20,0,Using AWS's Lambda and API Gateway we can write recursive functions that work by sending state through HTTP redirects.,#code,"{% include setup %} Two years ago I [toyed around](http://dangoldin.com/2014/12/31/redirect-recursion/) with an odd idea of implementing recursion over HTTP redirects. The idea is that the state is managed through the query string arguments and at each recursive step we just redirect to the URL for the next one. I still can’t think of a legitimate use case for this approach but have been on an AWS [Lambda](https://aws.amazon.com/lambda/) binge lately and wanted to see whether I can get this “redirect recursion” working under Lambda. Turns out it’s incredibly easy.  The only question was exposing the Lambda function to the outside world but AWS offers the [API Gateway](https://aws.amazon.com/api-gateway/) service to make this happen. This also gave me a chance to mess around with the API Gateway for the first time and definitely has me thinking about entire tools and applications that can be done in a “serverless” way.  {% highlight javascript %} # A simple Lambda function to calculate the factorial 'use strict';  exports.handler = (event, context, callback) => {      const done = (err, res) => callback(null, {         statusCode: err ? '400' : '200',         body: err ? err.message : JSON.stringify(res),         headers: {             'Content-Type': 'application/json',         },     });      switch (event.httpMethod) {         // Calculate the factorial         case 'GET':             var n = parseInt(event.queryStringParameters.n,10) || 1;             var a = parseInt(event.queryStringParameters.a,10) || 1;             if (n   20) {                 done(null, {'status': 'try a smaller number'});             } else {                 var url = 'https://rrouzys2ra.execute-api.us-east-1.amazonaws.com/prod/redirect-recursion?';                 var args = 'n=' + (n-1) + '&a=' + (a*n);                 callback(null, {                     statusCode: 302,                     headers: {                         'Location': url + args                     }                 });             }             break;         default:             done(new Error(`Unsupported method ""${event.httpMethod}""`));     } }; {% endhighlight %}        This connects any request to the /redirect-recursion endpoint to the Lambda function.         This shows the URL that needs to be invoked to run the recursion.",5,1,2016-11-13,6,"AWS, lambda, api gateway, redirects, recursion, http",368,Recursive redirects with AWS Lambda
27,0,When things work we don't give it a second thought which only gives us a superficial understanding of a systen. Failure is how we truly learn.,#meta,"{% include setup %} Maybe it’s due to all the Sherlock Holmes I read as a kid but I enjoy the process of debugging and exploring technical outages and failures. The more cryptic and challenging the problem the more fun it is to sink my teeth into and the more rewarding it is when I finally get to the root cause. The real benefit of going through failures is that it is one of the best ways to get familiar with a tool or a technology. When things work we often only get a superficial understanding of the technology since we have no reason to go beyond the surface. When things fail, on the other hand, we’re forced to dig deep until we understand the cause of the failure and can make the necessary modifications to prevent the same issues in the future.  Digging through a problem makes us understand how sensitive many of our systems are and what sorts of edge cases will cause them to break. Rather than thinking about failures we often get tunnel vision and build something to work in a perfect world where everything is going to come in as expected. Outages break us out of this optimism and get us to actually think about and build for the real world.  Anna Karenina starts with the sentence “All happy families are alike; each unhappy family is unhappy in its own way” and the equivalent version for software engineering is “All functional code is alike, each dysfunctional code is dysfunctional in its own way.” Rather than getting frustrated at broken systems we should use them as an opportunity to learn since we’re not going to get that from a perfect system.",0,1,2017-05-30,3,"software engineering, coding, software development",287,"Learn from failure, not success"
8,0,We shouldn't just writing by the author.,#meta,"A common idiom is ""Don't judge a book by its cover"" but I think that in this modern age this needs to rehashed into ""Don't judge words by their author.""    How often do we look at the author before we read an article or blog post? And how does this impact the way we absorb it? Studies have been done[1] to show that the same words coming from two different people, one a professor and one an average Joe, are interpreted differently: the professor is trusted while the average Joe is not. This can be expanded to any source of information, anything from a book to a YouTube video. In the past, these sources of information were concentrated - not everyone could write a book, but now anyone can start a blog to spread their thoughts and opinions.    In such a world, it's becoming increasingly important to come up with our own opinions and facts and applying a ""trusted"" filter may just be the shortcut we developed to not actually have to think about what we read. We need to be aware that knowing who the author is exposes the author's biases but it also creates biases in the reader.    Try reading something before looking at who wrote it and see if changes how you read. If you can control yourself, don't even look for the author after reading the piece.    [1] I'll try to look these up and update the post.",0,1,2008-05-28,1,writing,246,Don't judge words by their author
31,0,It does seem as if the big tech companies are going all in on a few key industries. I wonder if this is a sign of a new Gilded Age.,,"{% include setup %} I just saw that Apple has [acquired](http://techcrunch.com/2015/11/24/apple-faceshift/) Faceshift, a VR based startup that makes it easier to create realistic animated characters. Two years ago Facebook [acquired](https://www.facebook.com/zuck/posts/10101319050523971) Oculus VR and Google soon followed by an [investment](http://venturebeat.com/2014/10/13/google-counters-facebooks-oculus-buy-with-500m-investment-in-vr-startup-magic-leap/) in Magic Leap. Apple is rumored to working on a self driving car and we all know Google is doing the same thing. And around the time that Facebook acquired Oculus Uber was [poaching](http://www.nytimes.com/2015/09/13/magazine/uber-would-like-to-buy-your-robotics-department.html?_r=0) a good chunk of the robotics department at Carnegie Mellon.  VR and self driving cars have been hailed as the next big thing but it’s still shocking how so many of these large tech companies that started off in different industries are converging and out spending each other on these new technologies. It seems as if in their desire to own a new market they’re all joining the fray hoping to become monopolies in these new industries.  Coupled with the news of how [dominant](http://www.wsj.com/articles/the-only-six-stocks-that-matter-1437942926) the larger tech companies have been in the stock market compared to the other players it’s hard not to think that we’re moving into a world dominated by a few big companies that can outspend others and take over brand new industries. We love the idea that a small group of people in a garage can become the next Google but I wonder whether that’s likely to remain true. I hope so but maybe we really are just setting us up for another Gilded Age.",5,0,2016-01-29,6,"technology, Apple, Google, Facebook, Uber, market",279,Unification in tech and a new Gilded Age
35,0,It's very handy being able to analyze ELB access logs in Redshift but unfortunately it's not clear what the schema should be. The post shares the schema necessary to load the access log data.,#code,"{% include setup %} Back in February I [wrote](/2018/02/20/analyzing-aws-elb-logs/) about using Redshift to quickly analyze ELB access logs. This worked great until we switched from using ELBs to using ALBs. Unsurprisingly in hindsight but frustrating at the time the ALBs have a different log schema. Both the [Classic](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html) and [Application](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html) Load Balancer logs are well documented on the AWS site but unfortunately the code to create the appropriate Redshift schema is not. In the hope of helping others and passing it forward I wanted to share the Redshift schemas for both types of access logs.  {% highlight sql %} -- ELB Logs create table elb_logs ( 	requesttime timestamp, 	elbname varchar(100), 	requestip_port varchar(22), 	backendip_port varchar(22), 	requestprocessingtime double precision encode bytedict, 	backendprocessingtime double precision encode bytedict, 	clientresponsetime double precision encode bytedict, 	elbresponsecode varchar(3), 	backendresponsecode varchar(3), 	receivedbytes bigint, 	sentbytes bigint, 	httprequest varchar(5083), 	useragent varchar(500), 	ssl_cipher varchar(40), 	ssl_protocol varchar(40) ) sortkey(requesttime);  -- ALB Logs CREATE TABLE alb_logs (  type varchar(10),  requesttime timestamp,  elb varchar(100),  client_and_port varchar(24),  target_and_port varchar(24),  request_processing_time double precision encode bytedict,  target_processing_time double precision encode bytedict,  response_processing_time double precision encode bytedict,  elb_status_code varchar(3),  target_status_code varchar(3),  received_bytes bigint,  sent_bytes bigint,  request varchar(5000),  user_agent varchar(500),  ssl_cipher varchar(500),  ssl_protocol varchar(100),  target_group_arn varchar(100),  trace_id varchar(100),  domain_name varchar(100),  chosen_cert_arn varchar(100),  matched_rule_priority varchar(10),  request_creation_time timestamp,  actions_executed varchar(100)  ) sortkey(requesttime);  {% endhighlight %}",3,1,2018-06-05,7,"redshift, aws, load balancer, elb, alb, elastic, application",228,ALB and ELB access log schemas for Redshift
27,0,The ultimate efficiency would come from a driver being able to plug into as many food delivery networks as possible in order to optimize their route.,#product,"{% include setup %} Full disclosure: I know very little about the food delivery space so this is more speculative than anything rooted in reality.  Given how many companies are doing food delivery it seems there should be a single delivery aggregation service that combines all of them to act as the perfect dispatcher for drivers. If I’m delivering food on behalf of Uber Eats, DoorDash, Postmates, Grubhub, and the countless others I missed it would make sense for me to be able to pick up from as many nearby restaurants as possible and then deliver to as many colocated locations as possible. The ideal situation would be that I’m able to pick up all my deliveries from a single restaurant and then deliver them all to the same floor of an apartment building.  My understanding is that the dispatch is managed by the food delivery companies and not the restaurant which makes this impossible. Instead, I have to follow each company’s dispatching service which prevents me from potentially getting more deliveries on the same route. This would be slightly better if the restaurant were responsible for the delivery since they would be able to batch but unfortunately they would then also be responsible for keeping drivers busy at all times. The ideal situation would be to have the driver have the ultimate responsibility which would incentivize them to integrate into every service in order to tap into as many aligned routes as possible. This is similar to the Uber vs Lyft situation - they both offer enough incentives to keep drivers sticky but without the incentives drivers would be better off playing both sides and doing what’s best at any single time.",0,1,2018-11-29,4,"food delivery, uber eats, grubhub, doordash",283,A delivery aggregation service
45,0,"There's no way to avoid unsecure devices, especially as we move to the internet of things, and we need to figure a way to make our infrastructure more resilient. One way is to make our routes to the internet - routers and ISPs - more intelligent.",#society,"{% include setup %} After Friday’s DNS DDOS attack I’ve been thinking of approaches that could prevent this from happening in the future. In a perfect world every device would be up to date with the latest updates and it would be difficult to compromise anything that’s connected to the internet. Unfortunately, this is not the case and there’s an ever growing number of devices that are quickly hacked together and sold without any focus placed on security. Akamai did a [study that shows](https://www.wired.com/2016/10/akamai-finds-longtime-security-flaw-2-million-devices/) over 2 million internet connected devices have been compromised which allows them to be used to run DDOS attacks, very similar to the one that took down a big chunk of the internet on Friday. The challenge is that most owners both don’t know and don’t bother to do any security audits when setting up these devices and very likely never upgrade the firmware nor the software to make them more secure.  The solution is either to have much stronger regulation on what’s able to be sold to force manufacturers to secure their devices but I suspect this is a non starter - it’s tough to control the global world and there will always be incentives to deviate. A better solution would be one that assumes the internet will be filled with these malicious devices but can still handle them.  One idea is to make our routers smarter. They’re our homes’ gateway to the internet and improving the way they handle outbound traffic can reduce the impact these faulty devices have. Imagine them being smart enough to know the typical pattern of every connected device and throttle atypical traffic. Or have them serve as a both a cache and a throttler of DNS requests. The risk here is that the router itself becomes compromised or ends up accidentally rejecting valid traffic. I suspect most people have a router that was given to them by their ISP and ISPs have a strong incentive to keep their routers secure. And even if the router does get compromised we can push this sort of “smart throttling” unto the ISPs. In the case of the accidental throttling we’ll either need to deal with a small delay or provide the ability for a human to override the throttling - something that they would not unknowingly do to support a random device.  The solution here is to accept that we will always have bad actors and that we’ll never have total security. In that sort of world the network itself needs to be robust and resilient enough to handle whatever is thrown it’s way. Making the network more intelligent is one way but other ways include building in more resiliency into the protocol itself or making more and more of the internet distributed. This problem will only get worse as our entire homes connect to the internet and we need to find a solution before then.",1,1,2016-10-23,5,"DDOS, ISPs, router, interent of things, connected home",492,Preventing future DDOS attacks
31,0,Engineers love getting the latest and greatest code but it's important to read the release notes to make sure the new versions don't have any unintended consequences on your application.,#devops,"{% include setup %} I often find myself upgrading an open source to a newer version but I have a bad habit to only skim the release notes. More often than not an upgrade will work out of the box and you’ll get the immediate benefits of the newer version but every once in a while things blow up and you need to revert or scramble to get a fix out. Reading documentation tends to be dry with only a few relevant parts but when working on large systems it’s paramount to go through and understand the nuances of every upgrade. During my career I’ve run into a variety of issues that could have been avoided by a thorough reading of the release notes. There’s still a chance you’ll miss something and that’s why you should always have a sandbox environment and try to containerize as much as you can. Below are a few examples of issues I’ve run into upgrading various applications over the past few months:  - **Kafka 0.8 to 0.10**. This wasn’t a true upgrade but we wanted to spin up a parallel Kafka cluster that was a significant departure from our previous version. Kafka is a complicated application and we assumed that our code was backwards compatible. This was half-true. The code worked but it took a major performance hit that was [clearly document](http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact) in the release notes. - **Mongo/PHP**: This was a simple case of not reading the compatibility chart between the different versions of Mongo, PHP, and the associated drivers. If you’re running an old version of PHP you are limited to a subset of drivers that don’t support the latest version of Mongo. Once again this was discovered when messing around on our sand environment. - **Sentry**: Sentry’s a wonderful product that collects errors from every application you’re running and consolidates them into a single, slick UI. We wanted to get the benefit of a few additional plugins and decided to upgrade it to the latest version. Lucky for us there were some significant changes that required us to install a variety of build tools, including the C compiler. Unexpected but quickly remedied.  In our desire to get the latest and greatest we should be taking a step back to weight the benefits and the risks and looking at the release notes is a great way of understanding the potential impact. Even then it’s critical to have a separate environment to test different versions and a plan to roll back since it’s impossible to know what may actually happen on your unique system and configuration.",1,1,2016-12-03,4,"software engineering, upgrading versions, open source, release notes",436,Read the release notes
40,0,There are a few services and products that analyze your AWS usage in order to provide some recommendations. They are all just going off of the detailed billing report so I wrote a quick script to provide similar visualizations.,"#code,#dataviz,#devops","{% include setup %} There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, [Trusted Advisor](https://aws.amazon.com/premiumsupport/trustedadvisor/), that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month. External vendors are cheaper but can still get expensive depending on your bill: [Cloudability](https://www.cloudability.com) starts at 1% of your AWS costs which compared to Trusted Advisor is significantly cheaper is still 1% of your AWS bill.  One option is to sign up for a single month and use that to take the necessary steps to improve your cloud configuration. If your infrastructure is stable month to month this is a simple and cheap way to revamp your setup. But if your infrastructure is constantly evolving you need a way to revisit your environment when necessary.  I spent some time looking at our AWS infrastructure last week and it turns out AWS provides an option to export a [detailed billing report](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html) to S3 which is what the external vendors use to provide their recommendations. AWS offers a few reporting options but the most detailed one contains every resource and tag and in my case was over a million rows and nearly 500MB. There’s a wealth of information here and I wrote a small script to slice, dice, and visualize the data along a few dimensions to help provide some transparency into what is the biggest cost. The code is extremely simple since it’s just grouping and visualizing the data by different dimensions. The only real enhancement I made was to translate the OpsWorks tags into a layer dimension to make the visualization more useful. The two big things I still need to do are provide recommendations around reserved instance usage and do a better job of grouping the usage types since they’re too specific. As usual the [code is up](https://github.com/dangoldin/aws-billing-details-analysis) on GitHub and I’d love to hear any suggestions or feedback. Below are some graphs the script generates but note that I removed the axes labels to avoid revealing our costs and configuration.                                                          By product name.  A simple summary of cost by AWS product/service.                                                                          By product name.  This shows every type of usage AWS has in the billing report. To deal with the long tail the script also generates a plot for the top 25 but one thing I need to do is a better job of grouping these - for example data transfer has different values depending on region and type and I want to consolidate them into a one in order to see total costs due to data transfer.                                                                          By layer and usage type.  To me this is the most interesting one since it's looking at data for multiple dimensions - in this case layer and usage type. The goal here was to see which application/usage pairs result in the largest costs and allow me to prioritize investigation effort. Once again this will be more useful when I do a better job of grouping the usage types.",7,3,2016-11-27,5,"aws, cloud cost management, cloud management, trusted advisor, cloudability",654,Visualizing your AWS costs
14,0,I just finished tagging my posts. It's not perfect but it's a start.,"#blog,#meta",{% include setup %} Turns out that tagging and categorizing blog posts is more difficult than I thought. I start with one set of tags but as I go through my posts I realize that my initial set of tags no longer make sense and I need to restart. The challenge is finding the set of tags that are specific enough to categorize a single post yet general enough that they can be applied to other ones. I haven’t found the perfect set of tags yet but did manage to go through and tag each of  my posts . Over time I hope to improve the tag taxonomy and update the existing posts. I’d love to hear suggestions on how to effectively organize my posts and examples of other blogs that are doing this well.,1,2,2014-02-26,2,"blog, tags",140,Blog posts are now tagged
37,0,Toyota is starting to replace the robots in their factories because only by dealing with the details of a process can it be improved. I think this is a great approach and applies well to coding.,#product,"{% include setup %} Last week I read an interesting article about  humans replacing robots  in Toyota's factory. The thesis being that only humans are creative enough (right now at least) to develop new skills and processes to deal with production inefficiencies. This rings true - in order to improve a manufacturing system you need to understand the entire process, from the raw ingredients up to the way consumers end up using the product. It's more difficult to do these days as products become more complicated with an increasing number of specialized components and I'm glad to see companies taking a longer term view and focusing on the value of human creativity rather than short term cost cutting.  Writing code is similar, it's easy to take off the shelf libraries and use them in an application. This is a perfectly reasonable approach when starting out and you need to get something built quickly; in fact this is the prefered approach so you don’t fall into the trap of premature optimization. But as you scale the big improvements will only come when you understand both the high level goals of what you’re writing and the low level details of how they are achieved. Then you can focus on removing and rewriting extraneous code and improve the components that are the bottlenecks. Without this understanding it’s very easy to waste time optimizing the wrong components rather than figuring out where the big wins will come from.",1,1,2014-04-28,4,"toyota, coding, product, manufacturing",259,System knowledge and human creativity
32,0,I've been hesitant to use the MySQL enum field after reading some stories but it turns out to be pretty useful and stable. Not for everything but has it's use cases.,"#sql,#code","{% include setup %} The MySQL enum field provides a nice compromise - the space efficiency of using an integer, the human readability of text, and basic type safety. Yet I had this vague recollection of reading something that made it seem enums carried a ton of risks when changing the column definition so wanted to see if I could “break” it. Turns out it’s a lot more resilient than I thought. I went through a series of combinations - ranging from changing the order of the enums in the definition to trying to insert values that didn’t exist but in every case it handled it as expected. Doing a bit of research I discovered how MySQL represents the enum type. Rather than storing the values in a specific order MySQL supposedly creates a map-like structure to relate the integer values with their enum counterparts. This allows you to change the order of the enum definition without changing the underlying map or any of the stored values. I still wouldn’t use enums for anything that would require a join but for storing small and simple sets of data it works great.  {% highlight sql %} drop table if exists test;  -- Create the toy table create table test (   id int auto_increment primary key,   e enum('a','b','c') );  -- Populate it with some sample values insert into test (e) values ('a'), ('b'), ('c');  -- Confirm they look good select * from test;  -- Now let's add another possible enum value alter table test modify column e enum('a','b','c','d');  -- Looks good select * from test;  -- Add some more values insert into test (e) values ('d'),('a'), ('b'), ('c');  -- Looks good select * from test;  -- Change the order around alter table test modify column e enum('a','b','e','c','d');  -- Looks the same select * from test;  -- Change it again alter table test modify column e enum('a','b','c','d','e');  -- Looks the same select * from test;  -- Add and change the order alter table test modify column e enum('b','c','d','e','f','a');  -- Looks the same select * from test;  -- Fails since 'g' is not a valid value insert into test (e) values ('g');  -- Replace 'a' with 'f' update test set e = 'f' where e = 'a';  -- Now get rid of 'a' alter table test modify column e enum('b','c','d','e','f','g');  -- Now add 'a' back in alter table test modify column e enum('a', 'b','c','d','e','f','g');  -- Now swap 'f' back with 'a' update test set e = 'a' where e = 'f';  -- Looks just like before select * from test; {% endhighlight %}",0,2,2016-03-10,4,"mysql, code, enum type, enum data field",409,The MySQL enum type
21,0,I used to have an old GeoCities site but am struggling to find it in an archive. I remain hopeful.,#meta,"{% include setup %} One of my first exposures to the web was geocities and I recall creating a site on GeoCities under the “Cape Canaveral” space. And I only remembered that when I stumbled across [OoCities.org](http://www.oocities.org/) and saw the name. Being a kid I was really into space and astronomy and created my GeoCities within that space. I keep trying to find my page but am constantly disappointed when every attempt ends in failure - although you do run into some amazing sites from the early years of the web.  It’s amazing how our memories work. I barely remember anything about my site other than that I had an animated gif genie on the homepage, an off-yellow-orange background, and the fact that I took full advantage of [frames](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/frame) (not iframes) and [framesets](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/frameset). I have no idea what i wrote about or any of the text which makes existing search tools nearly useless. And so I go on getting a bout of nostalgia every couple of days that gives me the motivation to keep exploring the ancient corners of the web.  I’m not quite there yet but I suspect the last step will be just downloading the full 652 GB archive and attempting a bunch of different shell commands and scripts to try to find my site. I would have already done this if I were able to download the archive but unfortunately just don’t have the space. In this case using AWS to spin up an instance just to download and extract the Cape Canaveral data might be worth it.",3,1,2018-03-03,3,"geocities, nostalgia, frameset",272,Hunting for my old GeoCities site
20,0,The NFL is losing customers in its core market yet is trying to grow with an inferior offering abroad.,#product,"{% include setup %} NFL viewership is [down 10%](http://www.wsj.com/articles/ratings-fumble-for-nfl-surprises-networks-advertisers-1475764108) this season and I understand the desire to grow the brand and the sport [abroad](https://en.wikipedia.org/wiki/NFL_International_Series#Long-term_deals_and_the_NFL.27s_return_to_Mexico:_2016.E2.80.93present). It seems misguided to take a product that’s declining in popularity and rather than fixing the core problems to try to grow it as is. This is akin to a tech startup marketing the hell out of a product that’s unable to retain its existing customers. The proper approach is to nail the product before trying to push it into the market.   In this case it’s actually worse since the NFL is not something isolated and the experience of one fan will influence the experience of another. By having some games played internationally the NFL runs the risk of alienating some fans that have to wake up much earlier to watch the game. The London games have been airing at 9:30 AM EST, which is 1:30 PM in London and 6:30 AM in California. This requires the California fan to be up at dawn to watch the game. And on the flipside, imagine a London fan actually adopting an NFL team: an 8:30 PM EST game would start at 12:30 in London and go on for a few hours.   Given the time difference it’s tough to imagine the NFL expanding internationally in its current form.  The only events with this sort of mass appeal are international competitions - the Olympics and the World Cup come to mind - and they occur every 4 years. They should instead focus on attaining the perfect experience for their existing users - that includes making the league more balanced, ditching the blackout rules and embracing full digital distribution, and actually dealing with the concussion and violence issues. Instead it feels as if the NFL is diluting themselves for a small change at some short-lived growth.",2,1,2016-10-30,2,"nfl, digital sports",320,The NFL abroad
16,0,"While impressive, Duplex should also simply integrate with APIs to further improve the booking experience.",#meta,"{% include setup %} Google’s Duplex is clearly impressive but what I actually want to see is the frontend of Duplex hooked up to the various booking APIs. For example, instead of actually making a call to a restaurant on my behalf it would just use the various booking sites already out there, such as OpenTable, to make the reservation. From the caller’s experience the two are indistinguishable since you’re just using your voice but on the restaurant side there’s no need to talk to someone to do something that can be done through a series of API calls. The calling approach is incredibly impressive and makes sense when a business doesn’t offer any online booking functionality but many do and involving a person introduces overhead.  Imagine combining the speech recognition power of an Echo or a Google Home with the investment that’s already been made in the various APIs. Google is the leader in voice recognition and should be able to find a way to map speech to intent that can be executed without actually needing to interact with a person.",0,1,2018-12-07,3,"speech recognition, google duplex, booking",183,Speech recognition and a bunch of APIs
19,0,I have no idea why IVR systems haven't moved to SMS. It's a massive win for everyone involved.,"#meta,#product","{% include setup %} Seems it’s all about messaging now and yet we somehow still have the ridiculous automated call systems which the industry calls interactive voice response. Every single person I know hates having to call a customer support number only to hear a robotic voice that requires a series of frustrated enunciations to get what you wanted. I understand that it’s a huge win in terms of cost savings since you’re replacing people with software but moving over to SMS and asynchronous messaging would make it a better experience at an even lower cost.  The voice systems would no longer need to maintain a synchronous phone call connection and would be able to respond to messages asynchronously. In addition, translating and acting on text is much easier than having to do voice transcription and would lead to more powerful functionality rather than the usual yes/no questions. The end-to-end speed would also be improved since reading is quicker than listening. This can lead to better information density and provide more expansive options than what you’d be limited to with an automated voice system. Being asynchronous also allows people to take their time and find whatever they need without being pressured by the time constraint.  I’m honestly shocked that this hasn’t taken over the world yet. I did a Google search and it seems there are a few companies in the space but it’s surprising that companies aren’t scrambling to move their IVR systems to SMS.",0,2,2017-05-25,4,"ivr, sms, automated call systems, interactive voice response",249,IVR to SMS
21,0,"I fear that the trend to be constantly entertained is dangerous and going to cause problems, especially for newer generations.",#meta,"{% include setup %} The real time news cycle bothers me. Every time theres some news there are countless reactions on Twitter and quick, shoddy write ups on various ""news"" sites. Unfortunately, by the time someone does the research and writes a thoughtful response, we've moved on to the next piece of news. We're reaching the point where writing something stupid quickly is becoming more valuable than writing something thoughtful but late.  Twitter’s strength is its weakness. The 140 character limit makes it very easy for anyone to share an opinion but that also leads to everyone sharing an opinion. Of course, its ability to break and spread news is invaluable. I just wish that the more thoughtful, well-researched pieces could get past the noise. This week, I would have preferred to see a few insightful pieces about WWDC rather than the same exact WWDC coverage from dozens of sites.  This wouldn't be a problem if it were isolated to Twitter but it's becoming the norm. The  Lincoln-Douglas debates  lasted hours and candidates had an hour for a rebuttal. These days, we’re lucky to get a rebuttal longer than a few minutes. The average shot length for movies  decreased  from over 6 minutes in the 1930s to close to 4 minutes now. Even investors are getting in on the trend and want entrepreneurs to have 30 second elevator pitches instead of real conversations.  Clearly I’m simplifying and there are countless other reasons for these changes. At the same time, this trend towards constant stimulation and gratification is dangerous. It reminds me of a  study  that showed that kids who had more patience and self control ended up with higher SAT scores more than a decade later. If we get addicted to constant entertainment, how are we going to tackle on the challenging problems that require focus?  As a kid I used to lie in bed and read a book for hours but now find myself taking a break every 15 minutes to check up on my digital life. This bothers the hell out of me. I can’t imagine the effect it’s having on kids who’ve never even had a chance to be left alone with a good book.",3,1,2013-06-12,3,"constantly entertained, twitter, creativity",384,Constantly entertained
31,0,Amazon rewrote the boto library to take advantage of their domain specific models that represents the AWS API. It's wonderfully clever and I'd love to see standardization around API definition.,"#meta,#code","{% include setup %} Yesterday, Amazon  announced  a major update to their Python client, boto3. The core functionality is unchanged but they used a clever solution to make it easier to add, modify, and remove endpoints. By coming up with a  standardized representation  for each of the endpoints they’re able to write wrappers in different languages that generate the API calls programmatically. For example, I've included a subset of the  EC2 definition  below. It contains the information necessary to programatically generate the API wrapper to hit the appropriate EC2 endpoints.  {% highlight json %} {   ""service"": {     ""actions"": {       ""CreateDhcpOptions"": {         ""request"": { ""operation"": ""CreateDhcpOptions"" },         ""resource"": {           ""type"": ""DhcpOptions"",           ""identifiers"": [             { ""target"": ""Id"", ""source"": ""response"", ""path"": ""DhcpOptions.DhcpOptionsId"" }           ],           ""path"": ""DhcpOptions""         }       },       ""CreateInstances"": {         ""request"": { ""operation"": ""RunInstances"" },         ""resource"": {           ""type"": ""Instance"",           ""identifiers"": [             { ""target"": ""Id"", ""source"": ""response"", ""path"": ""Instances[].InstanceId"" }           ],           ""path"": ""Instances[]""         }       },       ...     }   } } {% endhighlight %}  This domain specific approach is great when working with APIs and I’m surprised more libraries don’t adopt it. The benefits include being able to keep the actual code the same and only updating the definitions as well as having definitions shared across various language implementations. An additional benefit that can be gotten is actually downloading the latest definitions at runtime. This way you’re always running against the latest version of the API and don’t have to worry about upgrading versions.  I’d love to see more companies adopt this approach and even come up with a standard API declaration language. Then a single set of scripts can be used to wrap any API. Imagine how much simpler it would be to integrate with third party APIs when all you need to do is read the docs and have everything else wired. In fact the docs themselves can be generated from the base definitions.",3,2,2015-06-23,5,"apis, amazon, aws, boto3, domain specific languages",333,Domain specific API definitions
33,0,I found Nathan Yau's R script that plots GPS data and made a few changes to it to add a map overlay and the ability to focus in on a particular area.,"#dataviz,#code,#R","{% include setup %} Earlier today I read Nathan Yau’s  post  that had a quick  R script  to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were. I made a two quick changes to the script to make it more useful for my data: a map overlay to see where in the world I ran and an ability to view a zoomed in area of the map. I’ve included the updated script and the resulting plots below.  {% highlight r %} # From Nathan's script library(plotKML) library(ggplot2) library(maps)  # GPX files downloaded from Runkeeper files   lat_south & routes$long > long_west & routes$long                                                        A few specks here and there - clearly visible runs in the NY/NJ area as well as some in Virginia, New Orleans, and San Francisco. Can also see a few runs in India.                                                                         Zoom in on my runs in the Hoboken/NYC area. I don't have the lat/long coordinates here but if I had them it would be pretty easy to generate a map overlay.",4,3,2014-02-05,6,"running, visualization, analysis, gps, gpx, runkeeper",513,Visualizing GPS data in R
22,0,We built a small app that let us add attachments to django-postman by leveraging the the jquery-file-upload library.,"#python,#code","{% include setup %} After doing a round of customer development for  Makers Alley , we discovered that customers really wanted to communicate with makers about their pieces. In true MVP fashion, we got the first iteration out in a day by using  django-postman  to handle the user to user communication. Within a few days, we quickly discovered that text messages weren't enough and we needed to support file attachments, otherwise makers can’t easily show their designs and customers can’t share what they like. Unfortunately, django-postman does not support attachments and we didn’t want to have to incorporate another messaging library. Another constraint was that we were already using the awesome  jQuery File Upload  library (in truth, a modified  Django version by Sigurd Gartmann ) to allow makers to upload images when managing their storefronts.  We wanted to leverage our existing file upload system but also incorporate it with the django-postman messaging library without having to modify any of the code in django-postman. We weren’t able to find  anything on StackOverflow  that dealt with this issue so we were left with writing our own. Here’s the approach we ended up taking that might come in handy for anyone else running into the same problem. The code needs some cleaning and I need to add some error checking but I’m sharing it with the excuse of “perfect is the enemy of good.”  We built a new app, postman_attachments, that would serve as the intermediary between the file upload piece and django-postman.                  models.py: Attachment that would map the django-postman Message model to an uploaded file         {% highlight python %} class Attachment(models.Model):     message = models.ForeignKey(postman_models.Message)     attachment = models.ForeignKey(fileupload_models.GenericFile)      def __unicode__(self):         return str(self.message) + self.attachment.__unicode__(){% endhighlight %}                     api.py: Versions of pm_write and pm_broadcast that would do the same work as the original but would also map the attachments between         {% highlight python %} def pma_write(sender, recipient, subject, file_ids=[], body='', skip_notification=False,         auto_archive=False, auto_delete=False, auto_moderators=None):          ### Same code as in pm_write          for file_id in file_ids:             f = GenericFile.objects.get(id=file_id)             a = Attachment(message=message,attachment=f)             a.save(){% endhighlight %}                     forms.py: In our case, we needed to tweak the FullReplyForm and created our own version that included a new “file_ids” field to hold the ids of the uploaded files. The full solution would need to make versions of the other forms included in django-postman.         {% highlight python %} allow_copies = not getattr(settings, 'POSTMAN_DISALLOW_COPIES_ON_REPLY', False) class FullReplyImageForm(BaseReplyForm):     """"""The complete reply form.""""""     if allow_copies:         recipients = CommaSeparatedUserField(label=(_(""Additional recipients""), _(""Additional recipient"")), required=False)      file_ids = forms.CharField(required=False,widget=forms.HiddenInput())      class Meta(BaseReplyForm.Meta):         fields = (['recipients'] if allow_copies else []) + ['subject', 'body', 'file_ids']      @transaction.commit_on_success     def save(self, recipient=None, parent=None, auto_moderators=[]):         ### Bunch of code from original save method in BaseWriteForm from django-postman         file_ids = [x for x in self.cleaned_data.get('file_ids').split(',') if x]         ### Bunch of code from original save method in BaseWriteForm from django-postman         for file_id in file_ids:             f = GenericFile.objects.get(id=file_id)             a = Attachment(message=self.instance,attachment=f)             a.save(){% endhighlight %}          In addition, we needed to override the default django-postman templates to display the attachments for a message as well as include the necessary javascript to deal with the jQuery File Upload piece.         view.html         {% highlight html %}{% raw %}          {{ message.body|linebreaksbr }}        {% if message.attachment_set.all %}                      Attachments                          {% for a in message.attachment_set.all %}                 {{ a.attachment.file.url }}               {% endfor %}                             {% endif %}{% endraw %}{% endhighlight %}             view.js         {% highlight javascript %} $(document).ready(function(){   var upload_ids = [];   $('#fileupload-attachments').bind('fileuploaddone', function (e, data) {     console.log('Done uploading product images');     $(data.result).each(function(){       upload_ids.push(this.id);     });     console.log( upload_ids.join(',') );     $('#id_file_ids').val( upload_ids.join(',') );      if ($('#fileupload-attachments td.preview').length == upload_ids.length) {       console.log('Enabling input');       $('#reply-form button[type=""submit""]').removeAttr('disabled');     };   });    $('#fileupload-attachments').bind('fileuploadstart', function (e, data) {     console.log('Disabling input');     $('#reply-form button[type=""submit""]').attr('disabled','disabled');   });    $('#fileupload-attachments').bind('fileuploadpreviewdone', function (e, data) {     if ($('#fileupload-attachments td.preview').length == product_image_ids.length) {       $('#fileupload-attachments tbody.files tr').remove();     };   }); });{% endhighlight %}          The last minor thing we needed to do was update our urls.py file to override the standard django-postman urls to have them use our custom form.         urls.py         {% highlight python %} url(r'^messages/reply/(?P [\d]+)/$', 'postman.views.reply',         {'form_class': FullReplyImageForm},         name='postman_reply'), url(r'^messages/view/t/(?P [\d]+)/$', 'postman.views.view_conversation',         {'form_class': FullReplyImageForm},         name='postman_view_conversation'), url(r'^messages/', include('postman.urls')),{% endhighlight %}          I’d love to release this publicly but don’t have much experience creating standalone Django apps. If you have experience in open sourcing Django apps let me know - I’d love to get this out there as a standalone app or somehow incorporated into django-postman.",6,2,2013-05-17,5,"Django, python, django-postman, attachments, jquery file upload",848,Adding attachments to django-postman
21,0,I finally got the chance to mess aroun with Python 3's async functionality and had fun using the aiohttp library.,#code,"{% include setup %} A few months back I read about [aiohttp](https://aiohttp.readthedocs.io/en/stable/) and asyncio and finally got the chance to play around with it a few weeks back. The project was a quick one-off scrape of a few thousand domains to see what percentage had implemented the [pubvendors.json](https://github.com/InteractiveAdvertisingBureau/GDPR-Transparency-and-Consent-Framework/blob/master/pubvendors.json%20v1.0%20Draft%20for%20Public%20Comment.md) spec, an extension of GDPR that allows publishers to specify the vendors they’re working with.  My initial reaction was to do it in the way I’ve done it countless times before: the requests library in a for loop. Instead I decided to actually try something new and use the aiohttp library, a new asynchronous library for Python 3. It took me a little bit of time to figure out how to structure the code and use Python’s new async functionality (which by the way is very similar to modern JavaScript) but the end result is simple for what it does and runs incredibly quickly.  The code below is my attempt at a functional solution - it’s inspired by imitating the aiohttp examples as well as a few helpful blog posts. I’m sure it’s far from perfect and I don’t do much other than print the status but it got me what I needed. If you’re doing any sort of scraping or network heavy work take the time to learn the new async functionality provided in Python 3 - it’s a massive performance improvement.  {% highlight python %} #! /usr/bin/env python  import aiohttp import asyncio import json  timeout = aiohttp.ClientTimeout(total=10)  URLS = ""example1.com example2.com example3.com"".split("" "")  async def fetch(session, url):     try:         async with session.get(url) as response:             data = await response.text()             valid_pv = data.startswith('{')             print(url, valid_pv)             return valid_pv     except:         print(url, 'Error')         return 'Error'  async def bound_fetch(sem, session, url):     async with sem:         await fetch(url, session)  async def run(urls):     tasks = []     sem = asyncio.Semaphore(100)      async with aiohttp.ClientSession() as session:         for url in urls:             # Blame the spec for the path, don't worry about HTTPS for now             full_url = 'http://' + url + '/.well-known/pubvendors.json'             task = asyncio.ensure_future(bound_fetch(sem, full_url, session))             tasks.append(task)          responses = asyncio.gather(*tasks)         await responses  URLS = [url.lower() for url in URLS if not url.lower().endswith('.app')]  print(URLS)  loop = asyncio.get_event_loop() future = asyncio.ensure_future(run(URLS)) loop.run_until_complete(future) {% endhighlight python %}",2,1,2018-11-16,4,"python3, scraping, aiohttp, asyncio",394,Python 3 and aiohttp
15,0,I compare my most frequently used shell commands between now and those in 2014.,#code,{% include setup %}  A few years ago I [wrote](http://dangoldin.com/2014/05/12/most-commonly-used-shell-commands/) a simple script to analyze my shell history in order to examine my most frequently run shell commands. Being in dire need of a new blog post and suffering from a pretty heavy bout of writer’s block I thought it would be interesting to rerun the analysis and see how it compared to results from over 3 years ago.  It’s tough to say whether my usage has changed significantly. My adoption of zsh with the [oh-my-zsh plugin](https://github.com/robbyrussell/oh-my-zsh) have made my usage a bit more efficient - especially when using git. The other obvious change is that I’m running python code half as much as I used to and have also reduced my usage of text editors. I used to use fabric a ton to automate some deploys but have moved completely off of that. What has remained consistent is my blogging - my little alias (cdblog) to move to my blog directory and jekyll have stayed roughly the same as well as some other administrative commands.       Command  2014 Count  2017 Count  2014 Pct  2017 Pct      gst  0  795  0.0%  9.6%    cd  49  608  5.1%  7.3%    git  347  581  36.0%  7.0%    gl  0  525  0.0%  6.3%    pwd  12  438  1.2%  5.3%    gd  0  421  0.0%  5.1%    ls  103  415  10.7%  5.0%    gp  0  370  0.0%  4.4%    python  89  354  9.2%  4.3%    aws  0  325  0.0%  3.9%    gbda  0  304  0.0%  3.7%    code  0  297  0.0%  3.6%    connectec2  11  251  1.1%  3.0%    npm  0  241  0.0%  2.9%    curl  0  139  0.0%  1.7%    jekyll  12  112  1.2%  1.3%    emacs  22  101  2.3%  1.2%    grep  4  87  0.4%  1.0%    less  0  85  0.0%  1.0%    find  3  80  0.3%  1.0%    gco  0  75  0.0%  0.9%    cat  28  74  2.9%  0.9%    pip  14  68  1.5%  0.8%    docker-compose  0  65  0.0%  0.8%    ssh  28  62  2.9%  0.7%    cdblog  14  62  1.5%  0.7%    rm  15  61  1.6%  0.7%    sudo  9  61  0.9%  0.7%    gcm  0  59  0.0%  0.7%    workon  7  54  0.7%  0.6%    rake  15  52  1.6%  0.6%    brew  5  49  0.5%  0.6%    mv  4  49  0.4%  0.6%    to_temp  0  49  0.0%  0.6%    go  0  47  0.0%  0.6%    gradle  0  46  0.0%  0.6%    from_temp  0  46  0.0%  0.6%    open  3  42  0.3%  0.5%    hub  0  42  0.0%  0.5%    mediumify  0  41  0.0%  0.5%    grunt  0  38  0.0%  0.5%    cp  2  37  0.2%  0.4%    history  5  35  0.5%  0.4%    mkdir  2  35  0.2%  0.4%    gb  0  32  0.0%  0.4%    pbpaste  0  26  0.0%  0.3%    node  0  25  0.0%  0.3%    bower  0  24  0.0%  0.3%    g_pass  0  22  0.0%  0.3%    du  0  18  0.0%  0.2%    connect_ec2  0  17  0.0%  0.2%    diff  0  17  0.0%  0.2%    ping  23  16  2.4%  0.2%    mkvirtualenv  3  16  0.3%  0.2%    alias  0  16  0.0%  0.2%    echo  2  15  0.2%  0.2%    zip  0  15  0.0%  0.2%    mail  0  14  0.0%  0.2%    scp  0  13  0.0%  0.2%    n  0  13  0.0%  0.2%    man  0  12  0.0%  0.1%    touch  0  11  0.0%  0.1%    which  0  11  0.0%  0.1%    gt  0  10  0.0%  0.1%    \n  0  10  0.0%  0.1%    g  0  10  0.0%  0.1%    GEN_PASSWORD  0  10  0.0%  0.1%    wc  7  9  0.7%  0.1%    webpack  0  8  0.0%  0.1%    ruby  0  8  0.0%  0.1%    join  0  7  0.0%  0.1%    gem  0  7  0.0%  0.1%    crontab  0  7  0.0%  0.1%    cut  0  7  0.0%  0.1%    xmllint  0  7  0.0%  0.1%    protoc  0  7  0.0%  0.1%    ps  0  7  0.0%  0.1%    ffmpeg  0  7  0.0%  0.1%    unzip  0  6  0.0%  0.1%    sendEmail  0  6  0.0%  0.1%    airflow  0  5  0.0%  0.1%    chmod  0  5  0.0%  0.1%    java  0  5  0.0%  0.1%    conn  0  5  0.0%  0.1%    phantomjs  6  4  0.6%  0.0%    export  0  4  0.0%  0.0%    traceroute  0  4  0.0%  0.0%    sdk  0  4  0.0%  0.0%    textutil  0  4  0.0%  0.0%    ggl  0  3  0.0%  0.0%    top  0  3  0.0%  0.0%    pd  0  3  0.0%  0.0%    sort  0  3  0.0%  0.0%    cdjsonifyme  0  3  0.0%  0.0%    ./grailsw  0  3  0.0%  0.0%    tunnel_prod  0  3  0.0%  0.0%    wpd  0  3  0.0%  0.0%    php  0  3  0.0%  0.0%    jupyter  0  3  0.0%  0.0%    code.  0  3  0.0%  0.0%    fab  70  0  7.3%  0.0%    stash  15  0  1.6%  0.0%    head  5  0  0.5%  0.0%    c_do  5  0  0.5%  0.0%    sh  4  0  0.4%  0.0%    make  4  0  0.4%  0.0%    sass  3  0  0.3%  0.0%    redis-cli  3  0  0.3%  0.0%    celery  3  0  0.3%  0.0%    source  2  0  0.2%  0.0%    sed  2  0  0.2%  0.0%    redis-server  2  0  0.2%  0.0%    dig  2  0  0.2%  0.0%,2,1,2017-09-21,5,"shell, history, commands, bash, zsh",421,Examining my shell command history
19,0,Slack notifies the entire channel when you leave which discourages people from leaving channels. This is an antipattern.,#design,{% include setup %} Slack has grown incredibly quickly and solves a difficult problem but I can’t help but notice that some design antipatterns that increase Slack usage but don’t benefit the user. I came across one of these designs on Friday when I saw someone leave a channel. They no longer found the channel useful or were only there to answer a few questions but as soon as they left everyone in the channel saw the message “so and so has left the channel.”  While helpful to let others know they’re no longer in the channel I suspect it may send a negative signal to the remaining people. Imagine if you’re in a channel and see an executive or manager leave - sure you understand that they don’t want to be overwhelmed and want to focus but I can’t help but think that it may still cause some discomfort. This forces people to stick around in channels they may not be interested in and unless they’ve gone the extra step of muting the channel they end up seeing a notification every time a new message is posted.  Now imagine this happening hundreds of times a day. A ton of people end up in channels they don’t necessarily want to be in but aren’t comfortable leaving. This may make Slack’s engagement numbers better but hurt overall productivity. The nice thing is that I reached out to Slack suggesting a “silent exit” and it does look as if they’re aware of the problem and working on a few addition channel notification options.,0,1,2017-04-02,3,"slack, anti pattern, behavior psychology",262,Slack's channel exit anti pattern
46,0,"As great as the various blog platforms, such as Medium and Svbtle, are, I manage my own blog. Managing my own blog gives me the control I want and prevents me from having to rely on the provided marketing that I have no control over.","#blog,#meta,#product","{% include setup %} Given the recent news of Medium  raising $25M  and Svbtle  opening up  to the public I thought it would be an appropriate time to explain why I’m not using either of them. They’re both simple, clean products that allow writers to concentrate on their writing rather than configuring the dozens of options available in other blogging platforms. They’ve also done a great job with the typography that makes the content enjoyable to read. Compared to the other content websites out there, they’re incredible fast - they have a minimal structure and don’t load a ton of external content - especially when compared to the major publishers out there now such as the news sites and the social networks.  Yet I’m not writing on either of them, nor on Tumblr, Wordpress, or Quora even though I tried each one. For me, writing is about personal expression and being able to control the entire experience - both from the content generation up to the consumption - is important to me. I realize my design will never be as elegant as theirs but at least I can change it whenever I want. A year ago I wanted to include the D3 library for a  post  - this would have been impossible with Svbtle or Medium and I would have had to use static images. Recently I wanted to share  some charts  that I generated but on the first pass I realized they were too large for the content - with a few small tweaks to my theme I was able to incorporate them into my blog. I’ve also been thinking about using Mixpanel to track various events - something I’d never be able to do without full control.  The custom design is part of it but the other value lies in decoupling. I want to be able to decouple the creation of content from the presentation of content from the spreading of content. As an engineer, I like the fact that I’m not tied down to any platform - I know I can get additional pageviews by leveraging the built-in marketing networks these platforms provide but having marketing integrated into a creation tool feels dirty. I’d rather rely on Twitter and Hacker News to share my content. Sure it’s more difficult but it’s a more lasting way to get followers and readers for your content rather than the platform itself.  By being independent, I never have to think about how these platforms evolve and what the impact will be. They’ll have to monetize at some point and I don’t want to worry about that outcome. We’re already seeing massive changes in the way content is produced and consumed and being able to experiment with various approaches and technologies is important - especially for someone in technology. Relying on a third party that’s trying to do too much betrays that.  PS - I just realized I never mentioned how I host my blog. It’s currently hosted on Github pages using the  Jekyll-Bootstrap  plugin. At the moment, it gives me the control I want, deals with usage spikes, and is free. If anything ever changes, I can quickly pull everything down and host it on my own.",5,3,2014-02-02,6,"medium, svbtle, blogging, github pages, content, creation",562,Why I manage my own blog
19,0,I pulled my meeting history over the past 4 years and visualized it to look at the trends.,"#dataviz,#code","{% include setup %} As part of never ending goal to improve my efficiency I was curious to understand how my meeting habits have evolved over time. I had an old script that would [identify meeting room hogs](http://dangoldin.com/2016/10/01/shaming-meeting-room-hogs/) and [repurposed it](https://github.com/dangoldin/gcal-shaming/blob/master/meeting_duration_growth.py) to just download every one of my calendar events from when I joined TripleLift and another small script to [analyze](https://github.com/dangoldin/gcal-shaming/blob/master/analyze.py) this data. Two things I had to filter out were multi day events which were tended to be vacations and events with me as the only attendee which were my reminders and todos. Unsurprisingly, there was a pretty large increase over time as we grew from a scrappy startup of 15 people to one with over 150 and as my role evolved from an individual contributor to a manager and then to the head of the engineering team.                              Clear growth in the number of meetings I have on my calendar with a pleasant dip in the holiday season.                                       Sure enough the increase in meetings also led to an increase in the number of meeting hours.                                       I was curious to see whether the number of people in my meetings changed and this shows a worrying trend upwards.",3,2,2017-07-28,2,"meetings, data visualization",275,Visualizing my meetings over time
27,0,These days apartment buildings are filled with package deliveries which makes it difficult to find your package. What if every apartment had their own unique packaging?,#society,"{% include setup %} ‘Tis the season where mailrooms get filled with delivery boxes and it’s getting busier every year.  Mary Meeker mentioned the trend of building lobbies becoming [ecommerce storage](/assets/static/images/mm-landlord-storage.png) facilities and it’s more true than ever. My building has a small package room which has been filled solid this week after Black Friday and Cyber Monday.  It’s so inaccessible that it’s easier for me to just wait for others to get their packages and free some space before I go in to search for my own. Unfortunately, if everyone feels that way then no one gets their package and it becomes a true [tragedy of the commons](https://en.wikipedia.org/wiki/Tragedy_of_the_commons). Instead of helping each other out we’re waiting for others to improve the situation while it constantly gets worse with newly arriving packages.  This is clearly all tongue in cheek and everyone wants to get their package as soon as they can but there is some truth here. Mailrooms are getting more and more packed and as more and more of our daily items are delivered at ever increasing frequency it really does become more difficult to manage the situation.  The primary problem is that all packages look the same. What if each apartment had their own unique style that was somehow reflected on the boxes? Maybe it’s a uniquely designed label. Or maybe it’s unique packing tape. Similar to an airport where people have their own unique luggage tags what if ecommerce companies started giving each box a unique look to make it easier for it to be found? It’s definitely more expensive but would be a nice touch and make package delivery more efficient for us apartment dwellers.",2,1,2018-11-30,4,"ecommerce, mary meeker, apartment buildings, delivery",285,Tragedy of the commons: Apartment edition
26,0,Link bait titles are great at getting initial page views but cheapen your content and annoy your audience. This leads content to become a commodity.,"#meta,#product",{% include setup %} A couple of days ago I saw a mic.com article with the title “A European country is now offering free college education to Americans” but the only way to find out which country this was (Germany) was by clicking through to the actual page.      I understand that content sites make the bulk of their revenue through advertising but resorting to a link-bait approach seems like a terrible idea. It’s a shortsighted attempt that increases page views at the cost of insulting your audience and cheapening your effort that will not work as a sustainable strategy. Relying on headlines to generate traffic without any meaningful content is a great way to get to become a commodity. I hope that there are enough people out there that care about the content they’re producing and have a passionate audience that can be monetized based on quality of engagement rather than on quantity of page views. Otherwise we’ll all end up in a race to zero.,0,2,2014-10-18,3,"content, advertising, publishing",185,Link bait titles are a race to zero
8,0,A few interesting and amusing technology anecdotes.,#meta,"{% include setup %} I’ve always been interested in hacker lore and have recently started compiling a list of tech-related stories and anecdotes that I found amusing. My ideal story includes an odd, somewhat ridiculous, situation that required a bit of technical ingenuity to solve while highlighting an arcane corner case and providing some glee.  So far, I’ve only been able to recall and find two such anecdotes but will add more as I discover them. Depending on how many I gather I may put together a permament list page. If you have any to contribute let me know and I’ll add them to this post.  The two so far:        Print this file, your printer will jam       The case of the 500-mile email",2,1,2014-03-28,2,"hacker lore, stories",137,Hacker lore
26,0,Both Uber and AirBnB asked to give their gig workers equity. It's a long awaited move and sets a strong foundation for the gig economy.,#society,{% include setup %} Recently both [AirBnB](https://techcrunch.com/2018/09/23/airbnb-wants-to-give-its-hosts-equity-in-its-business/) and [Uber](https://gizmodo.com/uber-just-asked-the-sec-permission-to-give-its-drivers-1829708257 ) have asked the SEC to allow them to give equity to their contractors - hosts in the case of AirBnB and drivers in the case of Uber. The gig economy is clearly here to stay and it’s encouraging that we’re seeing companies trying to adapt to the changes. I’m sure it’s partially for the marketing spin - both AirBnB and Uber have been facing significant criticism lately - but even then this is a good example to set for future companies. We are moving towards the gig economy and being able to give equity to participants is a great way of sharing the wealth.  The amounts are not going to be life changing but there are benefits beyond the purely economic. If and when the companies go public these shares should have some voting rights attached and give the workers a chance to backup their concerns. This has been going on for centuries (since the 17th century according to [Wikipedia](https://en.wikipedia.org/wiki/Activist_shareholder )) and is nothing new. What is new is giving people who have historically not had any financial say a concrete way to vote with their wallets.  The idea of giving equity to the participants in the marketplace echoes what’s happening in the blockchain world - participants are able to hold stakes in the network and as the network becomes more valuable so do their stakes. Both of these ideas are novel and it’s going to be interesting to see how they play out.,1,1,2018-10-13,4,"gig economy, airbnb, uber, equity",274,Equity in the gig economy
21,0,Surge pricing is a huge deal with the on demand economy and I wanted to share some of my thoughts.,"#product,#pricing","{% include setup %} Every time there’s a big event or terrible weather, there’s a slew of complaints about Uber’s surge pricing. By now, you’d think that Uber customers would expect this to happen and yet they’re surprised when a $10 cab ride turns into a $100 Uber ride. I suspect Uber’s already done as much as it can on the messaging side; psychologically it’s just tough for someone to take a $10 ride one day and then a day later pay an order of magnitude more.  Every Uber transaction involves three parties - the customer, the driver, and Uber. In every case it’s up to Uber to set the prices in order to get the supply (drivers) equal to the demand (customers). In most cases, these are in alignment since people are willing to pay more for an Uber than a cab for the convenience. the problem occurs when the demand side gets too large and Uber needs to drastically increase prices in order to encourage more supply. Uber should have enough data by now to be able to determine the prices that will lead to supply being equal to demand for every demand level but that doesn’t solve the perception problem.  One  suggestion  I liked was having Uber drop their margin on these high demand days in order to maintain goodwill. Uber  supposedly takes ~20%  of each fare and that could remain the case for low fares but Uber can drop that on surge days in order to reduce the customer cost. This way a $10 drip gives Uber $2 but a $100 trip no longer needs to bring Uber $20 and can be set closer to $80. The issue is that people won’t care that a $100 trip now costs $80. Instead they’ll hear that a $10 trip now costs $80. The only way to make consumers happy would be for Uber to have a deep negative margin for the surge days.  Another option is to move to an auction model. Each customer would specify where they are, where they want to go, and what they want to pay and it would be up to a driver to either accept or ignore that offer. This way Uber can achieve  perfect price discrimination  with both drivers and customers getting what they want.  Uber must have considered both of these approaches. The latter one would require a significantly different product with more complicated logistics and a more difficult pitch but it would keep the various parties aligned to their reserve price. The former approach, on the other hand, would be much easier and cheaper to achieve. I believe Uber would be still be profitable if they took a negative margin on the rare surge day and they could offset it with a small increase in the margin on a normal day. The only thing I can think of is that they’re taking the long term view and are hoping to change the customer perception of what’s fair. I don’t know if they’ll succeed but if they do I expect many more services start adopting these “purer price” models.",3,2,2014-01-05,4,"surge pricing, product, uber, marketing",549,Surge pricing ideas
32,0,It's easy to focus on what problems we're solving but it's equally important to focus on what our code won't do. This ensures we're able to ship a high quality product.,#management,{% include setup %} When starting to spec out a new feature a good habit is to think about what it won’t do. This forces you to focus on the problems that aren’t being solved and makes you aware of the tradeoffs you’re making. Rather than focusing on the problems being solved it’s equally important to know what you’re not doing as well as what your implementation will preclude you from doing in the future. To be effective we need to make tradeoffs or we’d never be able to launch anything but we shouldn’t make them blindly. We need to be aware of the tradeoffs we’re making and understand the paths that will be closed off by a given implementation. By thinking of the negatives of a particular approach we’re able to surface many of these dormant issues. This helps avoids surprises later on and ensures the code has been dissected and thought through in a variety of ways.  Another great thing to do is to share this list with the end users of the product. We’re known for having a variety of biases and a common one is risk aversion. In this case if we just list the problems we’re solving everyone’s glad to endorse it but as soon as we highlight the negatives people will start speaking up. We’re never going to ship perfect code but it’s something that we should strive for and getting actionable feedback early in the process is one of the best ways to get closer to that goal.  Everyone picks up this skill naturally through experience after being bit too many times by a crappy implementation but imagine how much better we’d be if we understood the tradeoffs we’re making with every decision. Focusing on the cases our solution doesn’t work for and prevents us from handling is a great way to get this experience earlier.,0,1,2016-01-31,2,"software quality, engineering management",314,Describe what your code won't do
19,0,I wrote up an extremely short and simplified history of manufacturing and some quick predictions about the future.,"#meta,#history","{% include setup %} Working on  Makers Alley , I've spent a fair amount thinking about the evolution of manufacturing and wanted to share an extremely condensed history.  For most of human history, people either made what they needed on their own or traded with a local craftsman. Over time, this led to a specialization in skills and also the rise of the apprenticeship model. Since trade was mostly local, it was difficult to build a large business and most businesses were family run with parents passing down skills to their children.  This practice remained consistent until a couple of hundred years ago when water and steam power starting taking hold. For the first time, work could be done independently of human labor and started the trend of specialized machines replacing specialized people. The increase in machine efficiency and the reduced skill of workers led to drops in the cost of labor and cheaper products.  The next major shift occurred when electricity became prevalent. This allowed factories to be built anywhere power was available and the locations were now chosen based on the price of labor and the cost of shopping. Thus, many factories ended up being built near cities with harbors and railroads.  At this point, globalization was still in its infancy since the transportation costs were extremely high due to lack of automation and standardization. Only when containerized shipping took off in the second half of the 20th century did shipping costs plunge and allowed companies to move their factories to locations with even lower labor costs. On a side note, read Marc Levinson's  The Box  to understand the impact of the shipping container.  This is the current situation with the majority of manufacturing being done abroad using materials that are sourced from across the world and then shipped and sold worldwide as final products. It’s impossible to predict what will happen over the coming decades but the combination of rising labor costs, demand for customizable products, and 3D printing suggest that manufacturing is going to start moving back towards local, agile methods. At first, it will probably be a hybrid approach with the bulk of the components still being mass made but then customized in our homes from 3D printed parts. Over time, as the quality and cost of 3D printing improves, more and more of the components will be customized, printed, and assembled at home. We’ll see the creation of a new profession - a combination of industrial designer, modeler, and tastemaker who’ll need to help us navigate this new manufacturing world. I’m excited.",2,2,2013-08-02,6,"manufacturing, history, 3d printing, factories, production, customization",438,A brief history of manufacturing
21,0,Finally played around with the export of my Twitter analytics in Excel. Nothing too surprising but still an interesting exercise.,#data,"{% include setup %} I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.                              Has hashtag         No hashtag                             Has mention         171         251                     No mention         237         181               - Avg impressions vs hashtag and mention (excluding @replies): Idea was to investigate whether tweets with hastags or mentions end up being due to the fact that they are more likely to appear in search results. The results are a bit weird since it seems as if having a hashtag only helps if there wasn't also an @ mention. Otherwise it hurts.                     @mention         # tweets         Total engagments         Total impressions         Avg engagement rate         Engagements / Impressions         Avg impressions                             No         446         2192         89714         2.7%         2.4%         201                     Yes         385         914         58112         2.9%         1.6%         151               - I suspected that sending someone an @reply would reduce total impressions but increase the engagement rate since it's directed at someone. It does reduce the average impressions and only leads to a slight increase in engagement rate - and only when looking at the average of rates, not the total engagements over total impressions. I suspect most people don't differentiate between an @reply and an @mention which doesn't lead to a significant difference in actual engagement rates.                     Has mention         # tweets         Total engagments         Total impressions         Avg engagement rate         Engagements / Impressions         Avg impressions                             No         331         1524         62850         2.6%         2.4%         190                     Yes         115         668         26864         2.8%         2.5%         234               - If we exclude @replies and compare tweets with and without mentions, the tweets with mentions have both a higher average number of impressions and a higher engagement. Nothing surprising here - @mentions are good since they draw attention to a tweet while @replies hurt since they limit total reach. Still nice to have some data to confirm.                     Has Link         # Tweets         Total engagements         Total impressions         Engagement rate         Avg impressions                             No         426         990         59385         1.7%         139.4                     Yes         405         2116         88441         2.4%         218.4               - Tweets with links have higher engagement - most likely since there's a stronger call to action. Again this isn't surprising but nice to see it backed up by a bit of data.",0,1,2015-01-06,3,"twitter, analysis, excel",477,Some quick Twitter analytics analysis
39,0,It's possible for a design to be too good that leads to a boost in vanity metrics. We saw this happening with Pressi which caused us to do a bunch of adhoc work to deal with the growth.,#design,"{% include setup %} This is a bit of a first-world problem but it’s possible for a design to be too good. A great design may lead to an increase in your vanity metrics but that won’t necessarily translate into a successful business. In fact, it’s likely that these low-value users will increase your costs.  When we redesigned the landing page for Pressi (formerly Glossi) we saw the signup rate from our landing page shoot up to to close to 34% from below 5%. Unfortunately, our retention rates were abysmal and we were stuck supporting thousands of Pressi pages that were not seeing any engagement. This led to a massive increase in our AWS costs that we had to scramble to contain. The solution was to be smarter about the frequency of our data pulling as well as minimizing the amount of data we were storing for our users. In hindsight, we should have solved our retention problem before trying to grow our users but we were too obsessed with our user growth numbers to do the right thing.  I realize that user growth is a problem that many startups would love to have and that it’s foolish to choose a crappy design over a great one. At the same time, if you’re not tracking the metrics that align with what you’re trying to accomplish, a surge in growth will be more damaging than beneficial. The corollary to this is that if your design sucks and yet you’re still getting signups then you must be onto something.  For those interested, here’s the signup flow that had the ~34% signup rate. The awesome design work was done by  Marc .                                   The Pressi landing page                                           Pressi signup step two                                           Pressi signup step three",1,1,2013-07-03,4,"design, startups, growth, pressi",365,A design that's too good?
5,1,Some thoughts on GoDaddy/SOPA situation,#meta,"I'm not entirely sure what to think about the GoDaddy/SOPA situation. On one hand, it's great that the online community was able to get GoDaddy to completely reverse their position on SOPA. On the other, it's disappointing that a web company would support it in the first place.    Should we still be penalizing GoDaddy for their initial SOPA support or move on the same way they did? What type of example does this set for other companies? That they will be judged based on their original position and nothing after? Why even reverse your position if the community will behave as before?    In the meantime, there are  many companies  that support SOPA that the online community is not rallying against, including the majority of television networks (ABC, CBS, Disney, ESPN, Time Warner), and yet we still continue to watch tv. Our expectations for media companies are different than web companies, but do they have to be?",1,1,2011-12-30,2,"godaddy, sopa",171,"Screwed if you do, screwed if you don't"
31,0,After reading Rob Ewaschuk's post I spent some time going through our alerts and figuring out how they could be simplified. This led to a discovery of latency based alerting.,"#devops,#aws","{% include setup %} A month or so ago I read Rob Ewaschuk’s  philosophy on alerting  and since then I’ve been trying to be more aware of the alerts we have and whether any can be improved. The most actionable insight was to start thinking in terms of “symptom-based monitoring” where the alerts should reflect what the users are experiencing rather than various issues along the tech stack. This aligns your alerts with user expectations and can also simplify alerting since they will all be running at a high level. It may take longer to diagnose what the underlying problem is but it will reduce the total number of alerts required.  One of our alerts checks for faulty instances that are attached to a load balancer. We're notified whenever one goes down with the goal of investigating the cause and getting it back up and running. While serious, it's not critical since there's a fair amount of redundancy and the user won't notice any impact unless a large enough number of instances fail. Using the symptom-based monitoring approach we were able to tweak the alert to monitor the  latency of requests  made by the load balancer to the instance and trigger a warning if it gets too high for too long. This reduces the number of non-critical alerts while making critical alerts more in line with customer expectations.  The larger an application gets the more essential it is to have a firm overview of the system with a solid set of alerts. Too many and you end up either missing important ones or wasting too much team dealing with false positives. Too few and you discover problems too late. Alerts are typically, and rightly, ignored on smaller projects but when you have multiple applications distributed across dozens of instances it's increasingly important, and simultaneously more difficult, to understand what's happening. Something I've started doing to identify potential improvements is tracking every single alert and tracking it's false-positive rate with the goal of finding alerts that aren't meaningful and either get rid of them or replace them with something more actionable. Over time we'll hopefully get the right balance.",2,2,2014-12-01,4,"aws, monitoring, alerting, devops",374,Symptom based monitoring
32,0,What if we made all laws temporary with an expiration date? Then only the most valuable of laws would stick around and we would have a much more experimentation-friendly culture.,#society,"{% include setup %} I see the debate around net neutrality going on everywhere around me and I can’t help but think of the law making process and how it can be improved. I’m a huge proponent of net neutrality but at the same time I realize that maybe I don’t know the whole story and maybe we would all really be better if the FCC rolls it back. I don’t believe it’s likely but I’m not 100% convinced that it’s not the case either. Unlike a video game, you can’t save life at any point and restart it if something goes wrong or you want to try a different approach: we’re stuck with the decisions we’ve made. At the same time it would be nice to bring some more experimentation into the world and our laws so we can keep iterating to a more ideal state.  One idea that I’ve been toying with lately is to make every law temporarily and be rolled back automatically within a set of number of years. The only way to extend it would be with another vote. And in order to make sure they’re still valuable the vote would require a higher margin than the previous vote. This way only the truly valuable laws that have widespread belief end up standing the test of time and weaker laws end up naturally dropping off. The exact way and times it would work can be worked out but the key idea is to assume every law passed will be temporary and keep increasing the bar that keeps it active.  This would change the entire way we approach laws and make it much more experimental. If you know something is temporary you’re more likely to treat it as an experiment which would make it easier to pass laws but more difficult to sustain them. Of course we would still need to consider the risks and consequences of every law passed but it’s impossible to plan for everything and trying something that can be rolled back is a much better situation than committing to something that will stick around forever.",0,1,2017-11-25,3,"laws, government, society",354,Make all laws temporary
30,0,It turns out there's a series of algorithms called phonetic algorithms that allow you to compare two words but instead of looking at their spelling it examines their sound.,#code,"{% include setup %} Last year I [wrote](/2017/03/04/automating-admin-work-spreadsheets-to-slack/) a simple script to automate posting our On-Call schedule. It worked by reading the schedule from a Google Spreadsheet, looking up the names in Slack, and then sharing these usernames on Slack. A tiny problem I ran into was the fact that since I was using an exact match the names in the spreadsheet had to match the names in Slack. This is a trivial problem to solve since we have a finite number of engineers but it still felt a bit too sensitive. While lying in bed last night I got to thinking of ways to measure similarity between the names in order to make it a bit more fuzzy. I've used the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) in the past but it felt a bit too clinical for what I was trying to do and I wondered whether it was possible to do a phonetic match.  After during some research I discovered, unsurprisingly, that there's a whole category of [phonetic algorithms](https://en.wikipedia.org/wiki/Phonetic_algorithm) designed to solve these problems. The original was Soundex but has been superseded by the [Metaphone](http://www.amorphics.com/index.html) family. What's interesting is that their implementation is more heuristic than anything else. They were primarily designed for the English language and have a series of rules to simplify words or names into much simpler forms that avoid confusion. For example, one of the rules says to treat the letter V the same as the letter F while another says to treat the letter Q the same as K. Using hundreds or thousands of these transformations with a litany of exceptions leads to a canonical word which can then be compared against others.  I find this approach fascinating since it's so antithetical to the modern approach of collecting a ton of data and pumping it through some machine learning algorithms. Instead this feels like a finely tweaked series of if-else statements designed for a single purpose.",4,1,2018-01-16,4,"phonetic distance, levenshtein, name similarity, word similarity",335,Phonetic distance
34,0,I've been collecting all sorts of stats for 2014 and wrote a quick Python script to get some quick insights. The goal is to help me identify patterns and strive to live better.,"#data,#code","{% include setup %} At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year. I tracked the data via a Google spreadsheet and exported it as a CSV in order to analyze it via a  simple Python script . For now I’ve only pulled some summary stats but will take a deeper look in the next couple of days to examine the distributions and identify any patterns.  - After removing days without data, I’m left with 354 days for the year. - Out of these days, I was in a good mood for the vast majority (89%) with the remaining days a combination of being sick, getting sick, having a sore throat, or being congested. - I slept an average of 7.15 hours each night with the most being 11 and the least being 3. - The most popular breakfast items where cheese (53 days), eggs (45), smoothies (32), bagels (25), with the rest being a various combination of fruits and various dinner leftovers. - Sandwiches were the most popular lunch item (63 days), followed by soup (42), Chipotle burrito bowls (32), salads (28), and Sophie’s (23), a nearby Cuban restaurant. The remainder were food places that hovered around the teens. - Dinner was a lot more basic. I had salad at almost a third of my meals with the main dishes being dominated by protein: chicken (43 days), fish (33), turkey (20), salmon (16). My favorite starches were potatoes (34), pasta (27), and rice (16). - I drink too much coffee averaging 1.3 cups a day. The most coffee I’ve had on a single day was 3 cups which happened on 6 days. The goal is to drop this below 1 a day in 2015. - I drink too much alcohol also averaging 1.25 drinks a day. Beer (259) and wine (143) made up 91% of my drinking with the rest being a combination of mixed drinks, cocktails, and hard liquors. Similar to coffee, I’ll try to drop the average to below 1 a day in 2015.  Despite being a simple exercise there’s a lot of interesting information that can help me improve in 2015. By looking at the data more and examining relationships between various fields (mood and drinking, drinking and sleep, etc) I’ll hopefully find even more ways to live healthier and better.  On a related note, I’m repeating the same exercise in 2015 but am modifying the template a bit in order to collect more structure data. I’m going to be tracking the times I sleep and wake up rather than the duration and decided to put all the foods under a single column rather than per meal to make it easier to include snacks. The other big change was splitting my mood columns into a physical and a mood component to measure them separately. Looking forward to seeing what I discover in 2015.",1,2,2015-01-11,3,"quantified self, tracking stats, 2014 in review",556,2014 stats
16,0,Copying relationship data from one database to another is a surprisingly common yet difficult task.,#code,"{% include setup %} I suspect most developers have encountered this problem at least once: how do I copy some production data to my test or development environment? This can stem from needing to fix a bug that only manifests in production or just getting a more complicated, real-world dataset that doesn’t yet exist in the test environment. In an ideal world we’d have everything we need in fixtures and properly tested but in the cases we don’t it seems simpler to just copy the data over from the production environment.  This turns out to be surprisingly difficult. The simple cases are easy but it spirals into a world of pain very quickly. To do a proper copy you also need to copy all the dependents and dependencies since they will influence the behavior. Then for each of those you need to do the same thing and keep repeating until you’ve copied over the entire chain. Many ORMs allow you to specify the foreign keys which can be used to figure out the dependency chain; otherwise you need to manually specify the table relationships.  That was the easy part, now you have to handle the fact that the primary keys between the two databases will be different. On one hand you may copy over a row with some values that will violate the constraints of the destination table. On the other hand the data may copy over without any issues but you’ll end up with existing objects referencing this new data. At this point you’re squeezed on both sides.  Now imagine you have references to the data in external systems. This can be additional data that’s kept in another database mapped by ids or content stored on S3. At this point you have to decide what the minimum amount of data it is you need to migrate to do what you need and whether it’s easier to just recreate the production conditions on your environment. More often than not you’re better off skipping the migration and just doing the dirty work of replicating the same configuration on your environment. It ensures your database stays internally consistent and actually gives you a proper scenario to test against outside of production that can be part of your test suite.  As an exercise, I took a [stab at writing a script](https://github.com/dangoldin/db-tools/blob/master/migrate.py) to do this sort of recursive migration. The initial parts were pretty straightforward and I was able to write a simple script that did a recursive migration using an explicitly specified table to table relationship mapping. At the moment the script is unable to handle foreign key constraint violations but in theory it should be possible to resolve this with an exhaustive definition of the table relationships and designing the appropriate execution order. It was a fun, little exercise that confirmed it’s much easier to recreate the objects than to try migrating the data.",1,1,2017-05-20,6,"database migrations, data copy, production, sand, development, test",487,Copying production SQL data to other environments
13,0,Finally finished the first pass at visualizing the Jersey City parking zones.,"#code,#dataviz,#python,#javascript","{% include setup %}                                    I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at [https://dangoldin.github.io/jersey-city-open-data/](https://dangoldin.github.io/jersey-city-open-data/) and includes the zones that had enough geocodeable addresses to generate a valid polygon.  As expected, most of the work was going from the PDF to a set of valid geocoded addresses. The biggest challenge was extracting the text from the PDF and transforming them into addresses that could be accurately geocoded. Once I had that it was simply modifying the Google Maps  polygon example  to generate a list of polygon and finding a library to overlay the zone labels.  The two things I want to change are to modify the visualization to also include a street level visualization rather than relying on a polygon since it’ll make the information bit more useful as well as incorporate the street cleaning hours. If anyone has that data I’d love to get it.  As usual, the code is up on  GitHub  and pull requests are welcome.",3,4,2015-09-24,4,"jersey city, parking zones, 07302, open data",243,Mapping the Jersey City parking zones II
6,0,Welcome to the new blog.,"#meta,#blog",So I am porting this blog over from wordpress.com to my own local hosting. Please bear with me and I'll hopefully have more things to read soon.    Edit: I did a rough job changing the dates in the database so the posts should all have the actual post date now.,0,2,2008-05-21,1,welcome,53,Welcome
39,0,Piketty's Capital in the Twenty-First Century is a wonderful book but the biggest impact is showing how useful it is to have publicly accessible data. I'm hopeful we see more and more of this in the future.,"#meta,#data","{% include setup %} I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions. This allows the reader to get a feel for the data and makes the ideas much more tangible than an abstract formula. Piketty couples this with the economic data from the past two centuries to craft a persuasive argument for the causes of wealth accumulation.  Countless others have looked through the data, identified issues, and provided counterarguments so I don't want to get into that but I do want to highlight how important having data is for all types of research. If we're serious about these topics we should strive to collect as much data as possible while making it as accessible as possible. Piketty spent numerous hours collecting and transcribing the data from various paper sources and it's amazing what came out; I can only imagine how much other valuable research would come out if there was more publicly available data.  Governments should be responsible for collecting data and releasing it publicly. Many are starting to do this already although it still tends to be obfuscated behind a navigational maze and hidden in esoterically formatted PDFs. Over time we should see it become more transparent as the data formats standardize and we develop better tools to dig through the existing data.  Another issue we need to address is data correctness. On one hand it's great that people are going through Piketty's data and making sure it's valid but on the other if it's extremely confrontational and used to invalidate his work it serves as a warning to others that plan on releasing their data. Why would a researcher spend thousands of hours collecting data and making it accessible and then have to deal with the critics who find a few issues? Much easier to keep the data hidden and only provide the high level numbers that can't easily be challenged without doing the hard work. This perverse incentive needs to be resolved if we expect to see high quality researched being produced with open sourced data.  I'm hopeful that these larger scope theories with potential societal-impact become more common as we move into the 21st century. We have an increasing variety of tools to start making sense of this data with both individuals and institutions being more involved in organizing the world's data. No theory will ever be perfect or explain every case but having more data will serve as a guide for governments to hopefully improve life for their citizens. And if data is collected along the way it will fuel more analysis with actionable insights.",0,2,2014-12-09,4,"piketty, capital in the twenty-first century, economics, data",510,Piketty and the power of data
37,0,I realized that our brains work just like Bloom filters. They don't always know the details and they have some false positives but they're great for at least telling us that we ought to know something.,#meta,"{% include setup %} As many at TripleLift will tell you I have a fondness for  Bloom filters  but only recently did I realize that our brains work in a similar way. We don’t always know every particular detail or have perfect recall but what we do have is the ability to realize that something is familiar and that we might have encountered it before. This triggers enough additional thoughts that we’re able to dig up the actual thought or reference. For example I can’t always recall the exact Java library I need to use for a particular problem but I know that I’ve solved similar problems before and can quickly rediscover my previous solution, whether through an online search with the appropriate keywords or even by going through some old code.  I’d even argue that it’s more important to have awareness of everything you’ve done and seen in the past than to have a perfect recollection of a smaller subset of items. Knowing that you’ve seen something before takes care of the fear of the unknown - very similar to how  George Dantzig  was able to solve an “unsolvable problem” as a student since he didn’t know it was considered unsolvable.  Unknowingly I’ve even developed an approach to take advantage of this mental model. I dump interesting notes and links into text files that I “tag” with a bunch of additional thoughts or keywords I think of at the time. Then whenever I run into an issue and realize that one of my notes might be useful it’s a simple text search to find exactly what I’m looking for. Rather than rely on a structured approach such as Evernote I rely on my own adhoc system and am rarely unable to find what I was looking for. In the extreme cases I can even resort to a regex search and some piping to deal with too many results or a very scattered document. Every once in a while I’ll even write a quick Python script to provide a semblance of order although almost always I just resort to a text search.",2,1,2015-07-16,2,"bloom filter, memory",360,A Bloom filter in my head
35,0,We're collecting so much data at a low enough price point that there's a massive proliferation in open source tools to help get it under control. It's the golden age of big data tools.,#data,"{% include setup %} I really dislike using the phrase “big data” but it is catchy so I’m going with it. It really does feel we’re in the golden age of big data tools. The rise of cloud computing, distributed storage, and the proliferation of open source have led to multiple orders of magnitude more data generated now than a decade ago. It’s an impossible number to calculate but some project that between 2010 and 2020 there will be a 50 fold increase in the amount of data collected. And the rate is only increasing as more and more people around the world get smartphones and the internet of things starts becoming a part of daily life.     Source:  IDC, EMC    But having this data without making it accessible isn’t very helpful and it’s exciting to see the variety of tools we have our disposal to process and analyze it. This ranges from massively parallel systems that make it possible for us to run queries across terabytes of data that run in less than a minute to real time systems that are able to handle millions of events each second. In the world of real time stream processing there’s Kafka Streams, Storm, Spark Streaming, Samza, and Flink in addition to the  slew of proprietary technologies that will hopefully get open sourced. In the world of batch job processing there’s Hadoop, Spark, Impala, Presto, and Hive. The fact that there are multiple open source projects with their own unique spin is a great sign that we’re innovating and experimenting with different approaches. We have so much more to go and it’s unlikely we’ll ever have a single tool that excels at everything. Instead we’ll likely have a menu of use-case specific options that are each optimized for a unique workflow. By leveraging a combination of such tools we’ll be able to understand and handle the ever increasing data volume.",1,1,2017-04-23,7,"big data, real time stream processing, batch processing, kafka, hadoop, spark, storm",343,The golden age of big data tools
31,0,I'm a huge fan of static sites and think it would be straightforward to implement a simple version of search. I plan on giving it a shot on this blog.,#meta,"{% include setup %} I’ve written  previously  about the appeal of static sites and recently came up with another example of how powerful the setup can be. The gist is that the site’s content is static HTML, CSS, and JavaScript but the relevant underlying content is refreshed on a recurring basis with a separate job. This allows you to host the entire site on S3 and avoid maintaining your own web server.  Normally, implementing a site search requires a backend to accept queries, break them down into the appropriate keywords, and hit a search index to find the matching documents. A simple implementation would index the site using a server side script and store the results in a JSON file that could then be access on the client side. The client side JavaScript would need to be intelligent enough to parse the query string and reference the right index file but a simple solution is easily doable. I’m surprised more simple sites haven’t adopted this approach and I’ll give it a shot with this blog to see what issues I run into.",1,1,2015-08-09,2,"javascript, static sites",189,Static site search
38,0,My old projects are impossible to get running. Not just because the code is worse but because I never thought to document the deployment process or any of the requirements. This is an valuable habit to develop.,"#devops,#meta","{% include setup %} A clear pattern emerged as I was digging through my old projects. Other than the code quality and approach improving over time what stood out was the way I approached deployment. My earliest projects didn’t have a set of requirements and the configuration was all over the place. The more recent projects have a clear set of requirements as well as the command lines needed to get them running. In fact, I’m able to build and run my recent projects within a few minutes by running “pip install -r requirements.txt”, updating the configuration file, creating the database, and running the database migration script. This is a massive improvement when compared to my initial projects where there was no documentation and my setup involved a ton of adhoc, undocumented work directly on the production server that’s now lost.  I’d argue that this is one of the better habits to adopt as a developer. We do a surprising amount of duplicate work over the years and being able to reference a prior solution is immensely useful, especially when it’s easily discoverable. It’s also a great way of identifying patterns and similarities between projects and understand why some approaches worked and why some failed. This retrospective approach is an active way of improving rather than relying on the “osmosis” approach of just waiting for information to get absorbed.",0,2,2015-11-01,4,"code, code styling, devops, deployment",231,Good code is easy to build and deploy
16,0,I needed to convert JSON data to a CSV which is a surprisingly challenging problem.,#code,"{% include setup %} A while back I needed to dump some some EC2 instance information into a CSV file for a quick analysis. Just to get it done I took the immediate approach of using the AWS API to pull the details and then just navigating the massively deep structure. This approach required code designed for that exact structure so it got me thinking of a more generic approach that would be able to extract CSV data from an arbitrary JSON structure. It’s a surprisingly tricky problem since JSON consists of both lists and dictionaries and can have a pretty hairy nesting structure. Just to get the EC2 instances one has to go through a list of reservations each containing a list of instances with the various fields at different hierarchy levels - and some depending on another value within the same structure.  The problem is similar to extracting various elements in HTML and one of my favorite solutions there is to use XPath. It’s straightforward to implement an XPath-like selector for JSON but the challenge is extract a set of fields at once while maintaining some sort of internal consistency - in the EC2 example an example is getting the public ip, the private ip, as well a tag name based on its key. To get this to work we need to take a list of XPath-like expressions that need to be evaluated concurrently in order to extract what’s needed.  The [first pass is up on GitHub](https://github.com/dangoldin/python-tools/blob/master/json_csv.py) but there’s definitely a bit of work that still needs to be done. It’s been tested on a few simple files and examples but definitely doesn’t work for complex hierarchies or extractions.",1,1,2017-08-20,3,"json, csv, ec2 instance list csv",285,JSON to CSV
28,0,I was curious to see my most commonly used shell commands so wrote a quick awk/shell script to figure that out. Let me know what you find.,#code,"{% include setup %} I spend a large chunk of time working in the terminal and was curious to see what my most commonly used shell commands were. This also gave me an opportunity to practice writing one liners and learn a bit of awk.  {% highlight bash %}history | cut -d' ' -f4 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r{% endhighlight %}  The script is simple - look through my command history, extract the first word, and count the number of times that word appears. I was surprised to see git at the top but it makes sense - I tend to run it as a sequence (git status, git commit, git push) so it leads to an inflated count. The rest make sense - they’re a mix of the standard navigation commands as well as command related to my current projects. Next step is to set up a cron job to track this usage over time and see how it changes.      Command  Frequency      git  347    ls  103    python  89    fab  70    cd  49    ssh  28    cat  28    ping  23    emacs  22    stash  15    rm  15    rake  15    pip  14    cdblog  14    pwd  12    jekyll  12    connectec2  11    sudo  9    workon  7    wc  7    phantomjs  6    history  5    head  5    c_do  5    brew  5    sh  4    mv  4    make  4    grep  4    sass  3    redis-cli  3    open  3    mkvirtualenv  3    find  3    celery  3    source  2    sed  2    redis-server  2    mkdir  2    echo  2    dig  2    cp  2",0,1,2014-05-12,4,"shell, bash, scripting, command line",169,Most commonly used shell commands
28,0,I started a project with the hope of analyzing a bunch of Meerkat data but ran into issues dealing with the size and scope of the network.,#data,"{% include setup %} While going through some old repos I came across an old [project](https://github.com/dangoldin/meerkat-crawl) I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections. The challenge became actually analyzing this data to derive something useful. I tried a few tools - including [Gephi](http://gephi.github.io/), [Cytoscape](http://www.cytoscape.org/), and [NetworkX](https://networkx.github.io/) - but was unable to get anything more useful than a few simple summary stats. I was hoping to get a neat visualization of clusters to see the various cliques on the network but visualizing that data either broke the programs or took too long to even complete. I made the most progress when using a simple script to filter out the “tail” of the data which allowed the remaining data to be visualized but I felt that the filtration may have eliminated a bunch of interesting information. If anyone has some experience dealing with the analysis of large networks I’d love to hear some ideas.",4,1,2015-11-26,3,"big data, network visualization, data analysis",229,Analyzing large networks
21,0,Lately I've been suffering from writer's block and suspect it's due to my lack of coding. I need to fix this,#meta,"{% include setup %} The past few months I’ve been finding it more difficult to stick to my two post a week schedule. As I write this I’m 5 posts behind that I need to make up for by the end of the week in order to stick with my commitment. I suspect it’s common for true writers to have periods of writer’s block but I’m a casual blogger so it shouldn’t be that hard for me to just sit down and knock something out. I’ve also developed the habit, or maybe even the skill, of going through my day and being able to identify topics and themes that would be interesting to blog out. I have close to 150 of these ideas and yet lately I’ve found it difficult to even start.  I don’t know why but my suspicion is that it’s related to the fact that lately I’ve been much more focused on managing and haven’t had a chance to code as much. Coding gives me that creative recharge that I can use to write. Without that energy writing is less of a pleasure and feels like more of a chore. Over the next few weeks I’m going to block of some coding time with the hope that it helps me get my writing back.",0,1,2017-10-02,3,"writing, coding, creativity",218,Writer's block and code
14,0,Google's AlphaGo victory is a passing of the torch from IBM to Google.,#meta,"{% include setup %} Last week the big news was that Google’s AlphaGo was able to win 4 of the 5 games against Lee Sedol in Go. As we’ve gotten better and better hardware it’s not surprising that an AI was finally able to win in a well defined environment. AIs will continue to improve and we’ll start seeing more and more of this behavior across a wide range of problems and not just games. The most significant part for me was that this was achieved by Google and not by IBM. IBM had two recent notable achievements in AI - one was building Deep Blue in 1997 which beat Gary Kasparov in chess and the other was building Watson in 2001 which dominated at Jeopardy. Yet just five years later Google has claimed the AI victory with AlphaGo.  It’s tough to not see this is as the passing of the torch. Google has tons of incredible AI that’s used to power the business but building an AI to focus on Go was taking something out of IBM’s playbook. Luckily, we’re seeing a ton of AI work being open sourced - from Google’s [TensorFlow](https://www.tensorflow.org/) to Facebook’s [FAIR](https://research.facebook.com/blog/fair-open-sources-deep-learning-modules-for-torch/) to [OpenAI](https://openai.com/blog/introducing-openai/) - and I’m excited to see what we come up with.",3,1,2016-03-20,6,"AI, AlphaGo, Deep Blue, Watson, IBM, Google",221,Passing the torch: IBM to Google
36,0,We tend to split apps into frontend and backend. It may make more sense to think about our apps in terms of functionality which may lead to a tight coupling between the frontend and backend.,#code,"{% include setup %} Lately, I’ve been thinking about tightly coupled systems and how prevalent JavaScript has become on the web.  Tightly coupled systems scare me. They will undoubtedly break and bring down big chunks of your infrastructure. The solution is to think about your system in terms of various independent services that are responsible for only doing a few things well that won’t bring down the rest of the system if they fail. This approach makes it easier to maintain your code as it grows and also reduces the risk of massive failure. The challenge is figuring out how to break your project down into these services and being sure to revisit that decision as you grow.  JavaScript is pervasive in the modern web. I’ve been using Ghostery for the past couple of months and am constantly amazed by how many external JavaScript libraries are loaded on popular sites. It’s not surprising to see dozens of libraries being included and evaluated. They range from advertising, to tracking, to adding functionality, and it’s incredibly rare to see just one.  On the surface, these two thoughts are different but their intersection is interesting. Similar to the way these sites include additional functionality by loading external scripts, we can compartmentalize various functionality into standalone components and make them available to our applications via simple APIs. In the case of a web app, we’d expose functionality through client side JavaScript libraries that would be coupled with a backend that does the heavy lifting. Rather than slicing horizontally, which is what typical apps do by having a separate UI and a s separate API, we can learn from these external libraries and slice vertically by function.  We integrate tools such as Google Analytics, Stripe, Disqus and MixPanel into our apps without a second thought and we should strive to write our code the same way. This allows us to choose the right tool for the job. If it’s a simple, low volume API that will be used internally, go ahead and do it quickly in a scripting language such as Python, Ruby, or PHP. If, on the other hand, the service will get a ton of requests, you can implement it in Node. In the extreme case of a site that’s using a ton of content, it may make sense to have the content hosted on S3 and just being retrieved by JavaScript called from the client - then the backend can be solely dedicated to providing the dynamic functionality.  This is a pretty extreme approach with it’s own set of challenges. It will definitely require more thought up front on how you want your application to work and will require a different approach than we’re used to but I feel this is the right approach if you’re building for scale. Every application should be broken down into components to see which would benefit from different approaches. If it turns out that two components have drastically different requirements, it might make sense to build them as completely standalone services and only communicate amongst each other via APIs.  This is nothing new, people have been preaching  service oriented architectures  for decades but I think we’ve forgotten it when thinking in terms of “web.” It feels more intuitive to split services in terms of frontend and backend but the right approach is to think in terms of actual functionality. It may turn out to be that tightly coupling the frontend and the backend is the right decision.",1,1,2014-04-16,5,"coding, architecture, development, engineering, coding",587,Vertical integration and web development
13,1,"I finally realized a use case for Twitter - getting real time info""",#product,"I may be a bit late to the party but I was finally able to see the power of  Twitter  this afternoon.    I kept on getting an ""Authentication failed"" message when trying to log in to AIM. A few years ago I would not know what to do except ask my friends if they were having any trouble. Right now, I went to searched for ""AIM"" on Twitter and discovered that other people were having the same problem. Turns out it was a systematic problem and I wasn't the only one affected. Being able to know more about this problem is a great benefit. There has been a lot of talk of the power of real time search and real time news but this was my first real glimpse into the power of Twitter.    My first action wasn't to search for ""AIM log in problem"" on Google but to search for ""AIM"" on Twitter. It's amazing to me that Twitter was able to replace a certain type of search. The majority of my searches will still be done on Google but it seems that for anything with a pulse - Twitter search is the way to go.    Twitter's character limit is a great way to take advantage of the network effect. Having a low character limit encourages a lot more users to tweet thereby making Twitter feel alive and giving everyone else more information.    There have been numerous ideas of Twitter being used for market research and to get an early customer response - I can imagine that happening now and it's mind blowing.",1,1,2009-03-08,1,"twitter""",299,"Power of Twitter"""
33,0,Now that I've committed to writing more than a post a day until the end of the year it's gotten easier to just write. My brain has adapted itself to the task.,#meta,{% include setup %} Now that I’ve forced myself to write every day in order to catch up on my blogging goal it’s been much easier. The challenge was always getting started but as soon as I start the thoughts and sentences come out pretty quickly. They nearly always require a bit of editing and cleanup at the end but the initial dump is usually a pretty good basis for the rest of the post. Knowing that I need to write more than a post a day is enough to motivate me to put the proverbial pen to paper and get over that initial hump.  I’ve also found myself being much more aware of what I encounter throughout the day and whether it would make for an appropriate blog post. I have a running list of a few hundred blog topics that I keep adding to and my constant writing has me thinking of multiple ideas every day. It’s amazing how the mind works - you make something a conscious effort and your brain quickly rewires itself to prioritize it.,0,1,2018-12-03,2,"writing, blogging",180,Overcoming writing rustiness
27,0,JavaScript services are becoming increasingly powerful and allow a whole slew of applications to be built without touching a line of backend code. This is incredible.,#meta,"{% include setup %} I’ve been a fan of GitHub pages ever since I started using them to host my blog a couple of years ago and a thought that’s been constantly popping up is why there haven’t been any products or services that help small businesses host their sites on GitHub. GitHub’s  terms of service  forbid a third party from hosting pages on behalf of customers but it doesn’t seem as if there’s anything stopping someone from building a tool or documenting the set of steps to help someone create a simple site and have it hosted on GitHub. That way the business only has to pay a domain registration fee while still getting fast and robust hosting with a fairly solid CMS.  Going further this site can be made significantly more dynamic by integrating third party services via client side JavaScript. Use Facebook to handle authentication, Google Analytics to provide analytics, Disqus to provide comments, and Firebase to provide a data layer. There’s no backend to maintain and you get to use a set of free and powerful tools. This isn’t going to work in every use case but over time we’ll see more and more applications built using this approach.  The simplest code is the code that you don’t have to write and I think we’ll start seeing more and more of these third party services that provide specialized functionality via JavaScript snippets. Many of them will also be free when starting out since their marginal cost will be virtually zero for small projects and will encourage tons of people to create these pseudo static sites that provide dynamic functionality without a backend. I suspect that as these tools become more popular we’ll see them packaged together by other companies and services that will make web development accessible to a wider audience. One of these days I’ll see what kind of application I can build using free services and client side JavaScript.",1,1,2014-11-18,4,"javascript, github, static sites, client side",333,Backendless applications
31,0,"Oftentimes the best programmaing language doesn't win and it's because tools are what determine the success and adoption of a language. Moreso, the tools are often what drive language improvements.",#meta,"{% include setup %} The more I code the more I’m convinced that the quality of tools determines the value of a programming language. Some languages, such as Haskell, have wonderful and elegant design and are a joy to write but mostly remain hobby languages. Other languages, on the other hand, such as Java, may be rote and uninspired but have a massive amount of supported tools that allow them to keep growing. Unless we’re exploring or playing around the ultimate goal of every line of code we write is for it to be deployed to production and solid tools allow that to happen. Unfortunately, this also leads to a “rich get richer” scenario where a popular language continues to grow with an evolving and more compelling toolset rather than on any individual merits. And while the result is a better experience than it was before these new tools other languages may not have even gotten the chance to succeed.  Java grew alongside the JVM which is the foundation Scala and Clojure built on. They brought a nice, functional touch to the JVM which wouldn’t have happened without Java’s success - and Java 8 itself has embraced some of these functional elements.  JavaScript is another great example. A decade ago jQuery was the way front end development was done. Until it was supplanted by Angular. Until it was replaced by React. One of the major reasons for these shifts was the introduction of tools that improved the build process. Without webpack or browserify it would be possible, but definitely unpleasant, to work in React. In this case the tools evolved alongside the language with each one feeding of the other. And if there are some adjacent benefits introduced - such as CSS preprocessors and various JavaScript supersets - that’s even better. As I write this I’m starting to think that it may even be the tools themselves that drive the growth of a language since they are in fact its truest customer.",0,1,2017-05-07,5,"programming languages, tools, software development, java, javascript",330,Tools over languages
16,0,Coke and Pepsi make healthier soda over Passover. Why don't they do it year round?,"#product,#meta","{% include setup %}           During Passover, Coke and Pepsi sell sugar based versions of their sodas in order to stay kosher for Passover. These high fructose corn syrup (HFCS) free sodas are extremely popular and people stock up while they can. I don’t know whether this is due to the better taste, the nostalgia, or the limited supply but these sugar based versions are definitely more popular. I wonder what would happen if either Coke or Pepsi decided to go “all in” on sugar and launch a marketing campaign against HFCS based food and drinks. I’d love to look at the margins of sugar vs HFCS based sodas and see what the market share increase would need to be in order to offset the switch to sugar. My gut tells me that pursuing this strategy would be a win but the companies are too entrenched in their current process that it’s just not going to happen. Smaller soda manufacturers, such as Boylan, GuS, and Moxie, are growing by differentiating themselves from the big guys and are emphasizing the healthier ingredients. I’m hopeful that this will pressure Coke and Pepsi to make their soda healthier. Unfortunately, what’s more likely to happen is that they will just acquire the niche manufacturers position them to appeal to the more concious consumer, similar to what’s happening to  craft breweries .",1,2,2013-04-05,7,"passover, coke, pepsi, soda, breweries, local, craft",247,"Coke, Pepsi and Passover"
7,0,Photo from a trip to Martha's Vineyard,#photo,A nice change of scenery for the long weekend. It's amazing how quiet the nights are when you are not in the city.,0,1,2008-05-24,1,martha's vineyard,43,Martha's Vineyard Lighthouse
16,0,Just a review of where I am with my goal of running 1000 miles during 2013,#running,"{% include setup %}      I'm not entirely sure why, but I started off 2013 with the goal of running 1000 miles which breaks down into a little more than 19 miles a week. Remarkably, I stuck with it and am somehow at 822 miles for the year and need to average 16/week for the remainder of 2013 to hit the goal. Yet it took me a surprisingly long time to work up to a weekly distance of 19 miles and even longer to consistently run more than 19 miles a week. My first run was less than 1.5 miles and I only started consistently eat into my deficit in June. It took me until the end of August to actually reach the 19 mile cumulative weekly average I need to maintain until the end of the year. It’s been an awesome adventure and I’ve even managed to run three half marathons and improved my time from 1:58:11 on an easy course to 1:56:11 on a challenging one. Here’s a table I put together showcasing my running progress over the course of the year. I’ll update it at the end of the year after hopefully achieving the 1000 mile goal.      Week #  Start of Week  Cum Dist Needed  Dist Ran  Cum Dist Ran  Cum Run Avg      1  1/1/2013  19.23  5.43  5.43  5.43    2  1/8/2013  38.46  11.94  17.37  8.69    3  1/15/2013  57.69  13.65  31.02  10.34    4  1/22/2013  76.92  11.88  42.90  10.73    5  1/29/2013  96.15  12.64  55.54  11.11    6  2/5/2013  115.38  19.16  74.70  12.45    7  2/12/2013  134.62  15.64  90.34  12.91    8  2/19/2013  153.85  0.00  90.34  11.29    9  2/26/2013  173.08  25.24  115.58  12.84    10  3/5/2013  192.31  24.27  139.85  13.99    11  3/12/2013  211.54  3.31  143.16  13.01    12  3/19/2013  230.77  7.09  150.25  12.52    13  3/26/2013  250.00  12.60  162.85  12.53    14  4/2/2013  269.23  7.89  170.74  12.20    15  4/9/2013  288.46  18.54  189.28  12.62    16  4/16/2013  307.69  12.80  202.08  12.63    17  4/23/2013  326.92  11.62  213.70  12.57    18  4/30/2013  346.15  21.60  235.30  13.07    19  5/7/2013  365.38  26.67  261.97  13.79    20  5/14/2013  384.62  25.10  287.07  14.35    21  5/21/2013  403.85  12.03  299.10  14.24    22  5/28/2013  423.08  13.73  312.83  14.22    23  6/4/2013  442.31  26.14  338.97  14.74    24  6/11/2013  461.54  23.71  362.68  15.11    25  6/18/2013  480.77  32.66  395.34  15.81    26  6/25/2013  500.00  19.94  415.28  15.97    27  7/2/2013  519.23  23.51  438.79  16.25    28  7/9/2013  538.46  40.54  479.33  17.12    29  7/16/2013  557.69  35.68  515.01  17.76    30  7/23/2013  576.92  29.05  544.06  18.14    31  7/30/2013  596.15  37.82  581.88  18.77    32  8/6/2013  615.38  11.37  593.25  18.54    33  8/13/2013  634.62  33.23  626.48  18.98    34  8/20/2013  653.85  16.83  643.31  18.92    35  8/27/2013  673.08  34.41  677.72  19.36    36  9/3/2013  692.31  27.70  705.42  19.60    37  9/10/2013  711.54  34.76  740.18  20.00    38  9/17/2013  730.77  9.19  749.37  19.72    39  9/24/2013  750.00  32.75  782.12  20.05    40  10/1/2013  769.23  23.66  805.78  20.14    41  10/8/2013  788.46  16.40  822.18  20.05    42  10/15/2013  807.69  0.00  822.18  19.58    43  10/22/2013  826.92  0.00  822.18  19.12    44  10/29/2013  846.15  0.00  822.18  18.69    45  11/5/2013  865.38  0.00  822.18  18.27    46  11/12/2013  884.62  0.00  822.18  17.87    47  11/19/2013  903.85  0.00  822.18  17.49    48  11/26/2013  923.08  0.00  822.18  17.13    49  12/3/2013  942.31  0.00  822.18  16.78    50  12/10/2013  961.54  0.00  822.18  16.44    51  12/17/2013  980.77  0.00  822.18  16.12    52  12/24/2013  1,000.00  0.00  822.18  15.81",0,1,2013-10-13,1,running,435,My running progress
34,0,While blogging I had to deal with copying an Excel table into HTML and generating a BCG style growth-share matrix. Here are some tools I came up with that make it easier.,"#javascript,#code","{% include setup %} Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.  To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &lt;td&gt; tag and then wrap each row in a &lt;tr&gt;tag. I’d then copy and paste the result into the text editor to add the header row and finish up the styling. To generate a growth-share matrix, I’d just use Google Drawing or Keynote to draw the axes and labels before taking a screenshot and cropping it into a square.  The solution to these was a bit of JavaScript with some help from  StackOverflow . These tools are hosted on  GitHub  and accessible via  https://dangoldin.github.io/js-tools/  and are under the MIT License. As I run into more of these I'll keep on adding various tools to this list. If you have any suggestions or want to add your own let me know.",3,2,2013-10-05,4,"JavaScript, growth-share matrix, bcg matrix, html table",215,Some JavaScript Tools
31,0,I'm less into the various tech services than I used to be and I decided to go through the common ones and see how much I'd care if they disappeared.,#meta,"{% include setup %} A  recent article  on our attachment to social media got me thinking about my most commonly used services and their relative importance. The goal is to answer the question of how I’d feel if various services suddenly disappeared. After going through this process it feels as if these services moved from being necessities to feeling like luxuries. They either have a substitute that will do what they do or only have value due to the network - clearly these are important but I just have no attachment to the product itself.  - Facebook: I don’t care much for it and at a certain level I want it to disappear. The best reason I use it is the only reason I use it - everyone I know is on it.  - Twitter: I enjoy Twitter and use it more frequently than Facebook but don’t think I’d feel the loss terribly. I use it as a content source primarily and in the end the stuff I’m interested in can most likely be found via Hacker News.  - Hacker News: Years ago, I remember participating in the discussions but lately I’ve just been using it as a source of news. My current use case would be taken care of via Reddit or any other tech news aggregator.  - Foursquare: This would be the biggest loss but purely from nostalgia. I started using it when it launched and saw it evolve through the various products. I’m not a fan of the recent split into Swarm and Foursqare but still suspect I’d be most upset if Foursquare disappeared.  It’s sad that I feel so cynical about the apps and I’m trying to understand why. I suspect part of it is that the novelty of these has worn off and part is that there’s so many new tech products out there that it may just be overload. We’ll see how I feel about them after the next five years.",1,1,2014-09-07,1,social media,327,Importance of various tech services
26,0,Every once in a while I'll look at my old software projects and am always impressed when old code successfully executes on the first try.,#meta,"{% include setup %} I have over [50 repositories](https://github.com/dangoldin?tab=repositories) on GitHub with the majority being one time projects that were either me exploring a new technology, writing a small script, or doing a quick data analysis and visualization project. Every once in awhile when I’m a bit nostalgic I’ll go through these old projects and mend some of the code.  What’s surprising is discovering old projects and scripts that work as is without me having to do anything to update the underlying code. I’m used to so working with so many open source libraries and cryptic documentation that it’s rare to find a public library that works exactly as you expect. Of course my projects are much simpler than the typical open source library but I find it remarkable that I can get code up and running within a few minutes of a checkout.  This is something that we should aspire to when writing code - writing it in such a way that if someone were to use it in a few years it would be easy to follow and understand while being able to be run without requiring any modification. There’s an urge to constantly improve and extend everything we write but in the world of software it’s possible that we may end up making it worse. Rather than adding bells and whistles to everything we write we should take a step back and think how we would react if we were to discover it in a few years.",1,1,2016-11-19,3,"code, refactor, software engineering",252,Joy of old code
5,0,Another waste of a post,"#blog,#meta",I will try a new policy - at least one post a week. Harass me if I'm not doing it.,0,2,2010-03-10,1,blogging,20,More posts coming soon
27,0,The title of this post is from Louis Gerstner's book on the revival of IBM and highlights the key factor in getting people to change behavior.,#management,"{% include setup %} While reading [Who Says Elephants Can’t Dance](https://www.amazon.com/Who-Says-Elephants-Cant-Dance/dp/0060523808) about the revival of IBM in the 90s I came across a simple, yet profound statement by Louis Gerstner: “People do what you inspect, not what you expect.” We hear variations of this constantly and it’s true - if you want to drive behavior change you need to make sure that’s what you’re actually measuring and holding people accountable for. Otherwise we all run the risk of preaching what we don’t practice.  The more I work the more I see this pop up in all sorts of situations. The obvious one is to think about compensation plans and how the dollar usually wins if it’s in conflict with anything else. Beyond that it influences the way we tackle any project. If we focus on collecting and sharing metrics those will be the guiding posts for the project’s evolution. This is why it’s critical to think deeply about the goals and metrics for a project and make sure they’re properly collected. Exposing these via a dashboard to the team will do more for motivation than anything else.  If you’re constantly talking about and measuring story points or hours in the office then those become what people optimize their days around. If that’s not what drives the business forward then you’re not focusing on the right levers and should switch to something that’s more aligned with the business.",1,1,2017-12-26,2,"management, leadership",244,"People do what you inspect, not what you expect"
34,0,I read a ton and really enjoy reading about computer history. It's a recent field but it's always awe-inspiring to read about previous generations that laid the foundation for the digital age.,#meta,"{% include setup %} I’m fascinated by computer history and love reading computer history books. We live in such a digital heavy world that it’s difficult to imagine life without it. Yet it really is fairly recent. Personal computers only started becoming popular in the 1980s and the internet was only introduced in the 1990s. And it took a whole decade before the internet started resembling what we currently see. If we take the least restrictive definition of a computer we still get that computers have existed for less than a hundred years. That’s a blink of an eye in our history and it’s incredibly rewarding to read about the origin of the industry I’m a part of. Part of me wishes that I was around in the formative years so this is my way of feeling a little bit of that spark and discovery.  Below are some of the books I’ve read that stood out and I recommend to anyone that’s interested in the history of computers and computing.  - [The Dream Machine](https://www.amazon.com/gp/product/B07GBCX7YC/ref=oh_aui_d_detailpage_o02_?ie=UTF8&psc=1) by M. Mitchell Waldrop. The Dream Machine is a biography of J. C. R Licklider but it’s much more than that. Despite focusing on one person, Licklider had a hand in introducing the computer to the masses and the book does a great job covering the birth of the computer industry and its growth up into the internet era. - [Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age](https://www.amazon.com/gp/product/B0029PBVCA/ref=oh_aui_d_detailpage_o01_?ie=UTF8&psc=1) by Michael A. Hiltzik. If there was any time and place I could experience I would pick Xerox PARC in the 1970s and 80s. There was such an incredible amount of talent doing incredible work and the books makes you feel as if you were there. - [The Soul of A New Machine](https://www.amazon.com/gp/product/B005HG4W9W/ref=oh_aui_d_detailpage_o01_?ie=UTF8&psc=1) by Tracy Kidder. Similar to the above, this is a tale of Data General, a designer and manufacturer of “mini” computers in the 1970s and 80s, and the work that was involved in building a computer. It’s amazing that modern computers work given how complex they are and it’s even more incredible to read about how computers were actually made 40 years ago. - [The Intel Trinity: How Robert Noyce, Gordon Moore, and Andy Grove Built the World's Most Important Company](https://www.amazon.com/gp/product/B00G2A7WL2/ref=oh_aui_d_detailpage_o03_?ie=UTF8&psc=1) by Michael S. Malone. The transistor led to the creation of the microprocessor which reduced the size of the cips, the cost of computers, and in turn led to Moore’s Law, the reason we all have multiple computers. This is more of a business than technology book but is still a worthwhile read given Intel’s impact. - [A Mind at Play: How Claude Shannon Invented the Information Age](https://www.amazon.com/gp/product/B01M5IJN1P/ref=oh_aui_d_detailpage_o08_?ie=UTF8&psc=1) by Jimmy Soni and Rob Goodman. Claude Shannon created information theory and laid the foundation to think about digital information. Before Shannon there was no way to think about digital information and how it could be safely and stored and communicated. Computers are constantly shuffling bits around - whether internally or externally - and without information theory we wouldn’t know where to begin.",5,1,2018-11-19,3,"books, computer history, software",520,Computer history books
34,0,Sometimes it's necessary to reprioritize tasks on a RabbitMQ queue. One way to do it is to get every task on there and use shell scripting to requeue then in the correct order.,#code,"{% include setup %} Earlier today we had a hiccup where we had a bunch of messages piled up on a RabbitMQ queue that were not being consumed. Some of these tasks were very quick data loads while others were more involved jobs that could take multiple minutes to run. Normally these are distributed relatively evenly across the day so it’s not a problem but in this case we had hundreds of tasks in a random order and we wanted to shuffle them around such that the data load tasks executed first so that the data would be quickly accessible to other higher priority jobs.  Luckily, we remembered we had some old shell commands that helped us backup and restore a RabbitMQ queue so it only required a bit of scripting to come up with a sequence of commands to do exactly what we wanted. The script works by dumping the contents of the queue into a file, extracting the message field, filtering the messages into the desired buckets, turning them into queue addition commands, and executing the resulting files.  {% highlight sh %} # Dump the contents of the queue to a file. # To be safe requeue the messages and do a manual purge when # we confirm the data looks right. ./rabbitmqadmin get queue=data_queue requeue=true count=2000 > tasks.log  # 1 - Get the appropriate field (in our case the fifth one) # 2 - Remove the header rows # 3 - Trim the line # 4 - Prepend the publish command and turn the task message string into an argument cut -d'|' -f5 tasks.log | sed '$d' | sed '1,3d' | sed 's/^ *//;s/*$//' | sed -e ""s/^/.\/rabbitmqadmin publish exchange=data_queue.task routing_key=standard payload='/"" | sed -e ""s/$/'/"" > tasks.clean  # Split the tasks into two pieces cat tasks.clean | grep log > tasks.clean1 cat tasks.clean | grep -v log > tasks.clean2  # Queue the tasks in the appropriate order sh tasks.clean1 sh tasks.clean2 {% endhighlight %}",0,1,2015-08-12,3,"rabbitmq, tasks, devops",335,Reprioritizing a non priority RabbitMQ queue
17,0,Modern electronic goods are cheaper than they've ever been. The modern suppyl chain is incredibly impressive.,#meta,{% include setup %} While stumbling across yet another Black Friday deal it hit me that electronics are ridiculously cheap. One can get state of the art computers for about a thousand dollars and huge flat panel TVs for a few hundred dollars. I grew up in the 90s and it never felt that comparable gadgets were that cheap. It may be that I didn’t have a job and nearly everything was out of reach but I suspect electronic goods really are that much cheaper now. Even if a product launches at a higher price it doesn’t stay that way for long and drops much quicker than before. No price has staying power and there are always more efficient competitors catching up.  I haven’t dug into why but the modern supply chain must play a part. There are tons of factories in Asia specializing in the manufacture of sophisticated circuitry that can be reorganized as needed for new designs. Shipping both the raw materials and final products is more efficient than it has ever been and as consumers we’re all benefiting from this efficiency.,0,1,2018-11-23,3,"electronic goods, price, supply chain",185,Electronic goods are cheaper than ever
30,0,The government can play a more active role in speeding up the development of self driving cars by offering prizes for different stages of achievement and mandating data sharing.,#society,{% include setup %} There’s a rush by the world’s largest companies to develop the first fully self driving car. The investment so far has been insane and it’s only the start. It’s one of those problems that get more and more difficult to solve the closer they are to the finish. The upside is so large that it also leads to some perverse behavior in order to get any advantage - the Uber/Waymo lawsuit comes to mind here.  The space is competitive and everyone has an incentive to be first. At the same time the sooner we have self driving cars the more lives are saved. That’s a big reason for the worlds’ governments to encourage self driving technology to improve as quickly as possible. Something that I’ve been pondering is the idea of using [prizes instead of patents](https://en.wikipedia.org/wiki/Prizes_as_an_alternative_to_patents) to encourage innovation. This is an idea attributed to Joseph Stiglitz and focuses on incentivizing research areas where patents don’t offer a good enough incentive. The self driving car space doesn’t have this incentive problem but the playing field isn’t level and it’s big company competing against big company. What if governments offered prize money for various stages of self driving car achievement but in return the winners needed to share their data?  Governments allow these cars to drive on public roads but in return I’d love to see governments mandate that these companies start sharing their data. This would allow lower resourced companies to benefit and get a jump start without the massive initial outlay necessary. There should be an advantage for the first comers so the data release can be delayed but having a shared data reserve will help society as a whole get to self driving cars sooner - a goal worth paying for.,1,1,2018-04-03,4,"self driving cars, uber, waymo, stiglitz",298,Open sourcing self driving car data
41,0,Our apps are getting much smarter due to improvements in AI but I'd love to see AI that is able to work across apps and services - using the information from one with information from another to improve my human experience.,#meta,"{% include setup %} More and more apps are starting to leverage some form of AI to improve the user experience. These range from inferring user preferences when surfacing new information to notifications that come just at the right time to be helpful. Google has been leading the pack here and nearly every product has some AI-based functionality built in: Photos is doing an unbelievable job of identifying faces and objects, Maps is all about pathfinding and is constantly improving as new data is collected, Assistant is probably my favorite new product and serves as a generic catch all for keeping track of my schedule and answering my ad hoc trivia questions.  While these services are constantly improving what I can’t wait for is one that is able to work across services and applications. Apps tend to be optimized for one specific use case but the real world requires tasks that consist of multiple steps. For many seemingly simple tasks I need to open a variety of apps and do a tiny bit in each app but would much prefer this could be done in a single request. An example of this is me figuring out whether I should take Citibike, the subway, or do a regular walk. In an ideal world this would be a function of the weather, bike and station availability, the train schedule, and whether I’m trying to get somewhere by a specific time. It’s possible to get all this information but by the time I dig through each of these sources it would have been better to just pick one and go with it. Yet if I had all the information at once it wouldn’t be too difficult to make an immediate decision.  I suppose what I want is an [ifttt](https://ifttt.com/) that doesn’t require anyone to write integrations between apps but is able to self-develop the appropriate links and API connections. Sure it’s possible for someone to write the code manually for each combination but it’s just not tractable. This doesn’t seem as difficult as true [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) but it’s beyond what we currently have. My gut is that we’re not that far away - so much of our APIs are meticulously documented which should make the integrations easy - it’s just the cross app logic that depends on a variety of human contexts that’s difficult to program.",2,1,2017-09-04,3,"artifical intelligence, automation, efficiency",398,Cross app AI
28,0,Comparing EMR and Databricks costs is not the easiest since Databricks introduced a DBU 'currency' type but with a little bit of math it's easy to compare.,#devops,"{% include setup %} It’s frustrating when vendors introduce their own currency in what seems to be a way to obfuscate pricing. The most recent example is Databricks which offers a slick Spark hosting solution on top of AWS and Azure. Unfortunately, instead of being explicit about the prices they introduced a Databricks Unit (DBU) currency type that then translates into dollars based on the type of usage - ranging from a simple Spark cluster with limited optimizations (Basic Plan) to an interactive one with all sorts of behind the scenes performance tweaks (Data Analytics Plan).  The nice thing is that [Databricks is transparent](https://databricks.com/product/aws-pricing/instance-types) about the amount of DBUs per EC2 instance and the price per DBU so it took a bit of data cleanup to dump everything into a [spreadsheet](https://docs.google.com/spreadsheets/d/1WnHBixRBXw0PhKfA0XidgNQN1sM0TEHRCxFHL6laEHY/edit#gid=0 ) and then do the lookups and math to compare the [EMR](https://aws.amazon.com/emr/pricing/) vs [Databricks](https://databricks.com/product/pricing) pricing.  Turns out that the Databricks Basic plan is comparable to standard EMR - in some cases it’s more expensive and in some cases it’s significantly cheaper. For example an i2.xlarge costs $0.213/hour in AWS EMR but 1.5 DBUs (equivalent to $0.105/hour) in Databricks. At the same time an i3.16xlarge costs $0.270 in AWS EMR but 16 DBUs (equivalent to $1.120/hour) in Databricks. That’s a huge range, the i2.xlarge is less than half the cost in Databricks but the i3.16xlarge is more than 4 times as much in Databricks than in AWS. In general Databricks is more expensive for the larger instance types and cheaper for the smaller ones and I’d be curious to understand the reasoning. Also note that this is just using the Basic plan - Databricks has other plans which are never cheaper than the EMR equivalent.  I’ve included a screenshot of the analysis below but all the data is also available on a shared [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1WnHBixRBXw0PhKfA0XidgNQN1sM0TEHRCxFHL6laEHY/edit#gid=0 ).",3,1,2018-12-10,3,"aws emr, databricks, spark hosting",341,EMR vs Databricks costs
18,0,Meetings are both valuable and a time sink. I share my thoughts and approach to taking meetings.,#meta,"{% include setup %} I struggle with this one. Some days I feel as if I should take every meeting since it’s impossible to know where it can lead. One meeting can completely change a business, generate some consulting work, or lead to new friendships. At the same time, taking every meeting would eat up a chunk of time and most meetings end up fading from memory.  I’m still figuring out my approach but do believe that having fewer, more meaningful relationships is more valuable than having many fleeting ones. Unfortunately, it’s not clear what will end up being meaningful before the meeting. Currently, I try to take every first meeting or at least have a phone call but have been scheduling them all on a single day, early in the morning, or late in the evening to avoid disruption. I’m also trying to make every meeting valuable by taking follow up notes in order to reach out later if I come across anything relevant or if I need to send an introduction. Probably the most important thing I’ve learned is that it’s easier to rejuvenate an older relationship than to create a brand new one so I’ve been making an effort to catch up with at least two former acquaintances each month.  There’s no single approach to meetings that will work for everyone but relationships are important regardless of what you do and it’s essential to maintain and grow them. I’d love to hear how others deal with meeting overload.",0,1,2013-07-07,3,"business, meetings, entrepreneurship",252,Meetings: to take or not to take?
29,0,Coding isn't for everyone but basic computer literacy should be. Knowing how to download an run an open source program can help protect against malware and increase productivity.,#meta,"{% include setup %} Earlier today I came across another reason why basic computer literacy is a necessary skill. It’s not just about knowing to code or understanding how computers work but getting access to a slew of tools that are orders of magnitude better than what you’d find on a sketchy site.            My realization came when I was trying to download some YouTube videos that could be watched without internet access. Doing a quick Google search I found dozens of sites with each one trying to force me to download some additional software to “speed up” my experience. I’m positive most, if not all, of these would fall into the malware category so I decided instead to do a simple search on GitHub for “youtube download.” Lo and behold the first result was the wonderful  youtube-dl  library. Within two minutes I kicked off a script that proceeded to download a dozen videos.  I can only imagine what someone without access to GitHub would have done - either given up or downloaded some ridiculous malware that may have worked but would have left their computer in a sorry state. Learning to code is one thing but having a little bit of familiarity with the command line and reading technical documentation would do wonders in helping people solve their problems, avoid malware, and even encourage them to learn more.  Replacing Google with GitHub for any computer related “how to” search may be overkill but I’m going to start doing it. The side benefit of GitHub is that it even comes with a built in review system due to the star and fork system which hasn’t yet been gamed.",1,1,2015-03-08,2,"computer literacy, malware safety",304,Computer literacy for protection and productivity
20,0,I discovered that GitHub had a series of tools to render maps and decided to play around with them.,#code,"{% include setup %} After discovering  GitHub's map visualization  feature I needed to give it a shot on the only GPS dataset I had available, my runs from RunKeeper. Unfortunately, the RunKeeper files were in GPX while GitHub expects either geoson or topjson. A short  Python script  later and I was able to convert the GPX data into  geojson . The other hiccup I encountered was that the generated geojson file was too large for GitHub to visualize. My 232 runs contained 162,071 latitude/longitude pairs which turned into a 4MB file - not massive but large enough for GitHub to refuse to visualize it. The simplest solution was to generate multiple files but that made it impossible to see all my runs on a single map. The other solution was to see if converting to topojson would reduce the file size. That helped but I wasn't able to find the right balance between compression and quality and ended up with a hybrid approach - two files, one per running year, each in topojson.             -->  The entire process was painless and quick. The geojson format was straightforward to generate and GitHub does a great job rendering it. The entire process took an hour and I had to read the  topojson utility docs  to figure out how simplification worked. One thing I didn't get to do was explore GitHub's map diffs but will try to in the next couple of weeks.",4,1,2015-01-18,3,"github, tools,",283,Fun with GitHub's map tools
16,0,Software can replace hardware if we let it. The smartphone can be that central point.,"#product,#meta","{% include setup %} Last week, three isolated events gave me a glimpse of how powerful mobile can be. Tech pundits have been saying that for a while now but experiencing it firsthand is definitely more convincing.         I went for a run with only my phone to keep me company. After my run was done, I wanted to grab a cup of iced coffee and realized that Starbucks gave me a free drink on my birthday. Downloading the app on my phone allowed me to get a drink without having cash or a wallet.       While checking out at a grocery store, a friend showed me CardStar which allowed him to store all his loyalty cards on his phone. Since then, I’ve imported all my loyalty cards that have just been sitting in a drawer into my phone.       After getting a Raspberry Pi and installing Raspbmc, I was able to use my phone as a remote control just by downloading an app.     These behaviors are different and yet they’re all converging on the phone. What they have in common is that  software is replacing hardware . Hardware doesn’t need to become smarter, it just needs to be able to sync with our phones which can do the heavy lifting. The functionality then becomes limited by software which can be updated more cheaply and quickly than the hardware. It also solves the problem of hardware companies trying to develop software that results in a terrible user experience. Do cars really need the ability to  read a Twitter  feed? And if they do, why not just do it via a simple Bluetooth connection and an audio streaming app on a phone?",2,2,2013-04-24,4,"hardware, software, smartphones, technology",295,Smart software; dumb hardware
8,1,Why is everyone so down on Yahoo?,#product,"Reading recent tech coverage makes you think that each newly startup is more valuable than Yahoo. Yahoo is the 4th most visited site in the world with over 300 million users on Yahoo mail. This is a problem every startup should hope to have.      User acquisition is the most difficult task for a consumer startup. User attrition is an easier problem to solve than user acquisition. Yahoo doesn’t need to build a product that’s 10 times better than the competition, they just need to simplify and improve what they already have. Yahoo also has massive usage among the mass market with millions of people having Yahoo as their home page. These are not the same people that sign up for every startup featured on TechCrunch. Yahoo has challenges but worrying about user acquisition is not one of them. Yahoo will need to develop a vision and relentlessly pursue it. The culture will need to change and vested interests will need to be broken.      It’s easy to criticize Yahoo for ignoring Google and Facebook but impossible to say what Yahoo should be doing now. I look forward to seeing what happens to Yahoo with Marissa Mayer at the helm.",0,1,2012-08-05,1,yahoo,206,In Defense of Yahoo
19,0,I wrote a simple plugin to automatically insert a Follow this discussion on HN link using client side Javascript,#meta,"{% include setup %} This past weekend I wrote a  small jQuery plugin  that automatically inserts a “Follow this discussion on Hacker News” link on a recently submitted web page. The motivation was to automate the current workflow that consists of first submitting a post to Hacker News, getting the URL of the comment thread, and then updating the original post to link to the thread. I also wanted to see if it could be done entirely in Javascript so that the code could be included on static HTML pages and not require a backend server.  After some research, I settled on the following approach:  1. Use  Firebase  to store a mapping of URL to the Hacker News thread id. I chose Firebase since it provides a way to read/write using Javascript. 2. Use the  HN Droid API  to retrieve recently submitted HN posts for a given user via JSON. 3. If any of the recently submitted posts match the provided url, store that thread id in Firebase and execute the user defined callback function.  It works as expected but has a few limitations:  1. It relies on Firebase and doesn’t use authentication so someone can modify the database to point to another comment thread. 2. It relies on a 3rd party Hacker News API so if that ever goes down it won’t be able to pull recently posted links to Hacker News. 3. The HN API call only pulls the most recent submissions so the plugin will not be able to get the comment thread for older posts. 4. Since Firebase prevents certain characters from being used in a key, I do some string replacement to clean the string which would allow someone to cause a string collision. 5. The HN API isn't real time and uses a cached version so it may take a bit of time for the link to get retrieved. 6. The code hasn’t been thoroughly tested so may have some weird errors. It’s also my first “real” jQuery plugin so it may not follow best practices.  In general, I’m a proponent of offloading as much work as possible to the client side and believe this will become the norm as the technology improves. We’re already using Disqus to handle comments and Firebase as a database and I expect more services to become available via client side Javascript. This will keep pages simple, reduce server costs, and outsource non-core components to specialized vendors.",3,1,2013-04-15,5,"coding, hacking, jQuery, javascript, hacker news",423,Automatically add a “Follow on HN” link
34,0,Google has amazing integration between their tools that allow them to do a ton of search inference. I want to see this behavior develop in our operating systems to link various apps together.,"#meta,#product","{% include setup %} It feels as if Google has been getting better and better at what I call search inference. I’ll oftentimes do a search for a particular place on either Google Maps or the general site and see it automatically show up in Google Now. Or I will start with a simple search query that needs to be refined with Google able to offer perfect suggestions. Given how much data they’re collecting it’s not a surprise but it’s an easy way to realize I’m not that unique.  The fact that I can even sense improvement highlights how significant the improvements have been - gradual ones wouldn’t be as noticeable. What I want is for these innovations to become part of the OS. I currently use OS X and while it’s easy to use with a lot of neat utilities and applications it’s not smart. I constantly go back and forth between apps - referencing some notes in Sublime, writing some SQL queries, messing around with Excel - and would love the OS to be smart enough to understand my intent. The simplistic version of this is a smarter auto complete that transcends apps - rather than going back and forth between Sublime and Sequel Pro copying and pasting queries it would be nice for the OS to allow me to autocomplete fields and table names that are being actively used. The more advanced version would detect patterns in my workflows and allow me to skip numerous steps. When working on most tasks we have a mental model of how we’ll proceed - first I’ll write a query to pull this data, then I’ll dump it into Excel and run these calculations to figure out some values, then I’ll use these values to make a few updates in the database. These tasks are abstract with a lot of context locked up in our heads but I can see our computers getting smart enough to help us skip the majority of these steps. Given how often we do these trivial manipulations an intelligent OS can make us strikingly more productive.  Google has a huge advantage given both the massive data they have as well as controlling the entire ecosystem. This gives them the ability to know how apps fit together on a technical and behavioral level. Modern OSes separate themselves from the applications and run in isolation to the rest of the world so they don’t have Google’s key advantages. Despite this, I think it’s inevitable we’ll see these smart OSes develop - especially if we want to be as productive on our smartphones as we are on our desktops.",0,2,2015-09-05,3,"smart os, google, artificial intelligence",435,Smart OSes
36,0,The iPad Pro will have performance similar to the Macbook at a fraction of the price. I wonder if Apple is purposefully taking a margin hit to get more and more people over to iOS.,#meta,"{% include setup %} Supposedly the performance of the recently announced iPad Pro will rival that of the Macbook and encourage tons of people to buy it with productivity in mind rather than just consumption. And at a starting price of $799 it’s significantly cheaper than the Apple laptop options. Apple is known for achieving large product margins but I wonder if the iPad Pro is sold at a lower margin to get people to switch to iOS and get even more tied to Apple’s ecosystem. I currently use a Macbook Pro for my work but can switch to any Unix based environment without any hit to my productivity. I’m not sure if this is me being too cynical but I can definitely see Apple taking a long term view here and taking a much lower profit margin on the iPad Pro in order to get people to actually make the switch to the walled garden of iOS. Among the developer community Safari is already seen as the  reincarnation of IE  given Apple’s lackluster support. These may just be coincidences but for a company as detail oriented as Apple this feels like a strategic decision to shift away from the open web and into the walled app garden.  During the 90s Apple nearly failed due to the dominant Windows ecosystem and Windows’ ability to run on commodity hardware. At least for now, mobile isn’t at the commodity hardware stage and Apple has taken the lead in smartphone hardware. This position allows Apple to impose its ecosystem and software but I wonder what happens if smartphone hardware get commoditized - similar to what happened to PCs in the 80s and 90s.",1,1,2015-09-26,4,"apple, ipad pro, hardware, software",287,The iPad Pro and Apple's walled garden
17,0,I can’t figure out why nearly every website forces you to login after resetting your password.,"#design,#product",{% include setup %} I can’t figure out why nearly every website forces you to login after resetting your password. It’s an extra step that adds nothing to security and introduces friction into the experience. The fact that I just entered my password into a form field should be enough to trigger the authentication flow and get me back into the app. The only reasons I can think of that it’s a way to confirm that the person actually remembers their new password or that the functionality just hasn’t been built. The former case doesn’t make sense - the fact that they forgot their password indicates they rarely use the site and will just forget it again by their next login attempt. It’s easier to just give them the immediate access and have them reset their password later. An even better approach would be to just have them enter the same password twice to make sure they match. The latter reason is just sloth - the engineering effort would be minimal and it would improve the experience and mood of the users who are already frustrated after multiple failed login attempts.,0,2,2015-03-24,6,"design, ux, authentication, password, reset password, forgot password",190,Why login after resetting your password?
22,0,As the immigration ban shows the freedmom to move is a human right and can be the foundation of all others.,#society,"{% include setup %} I've had this thought for a while now but Trump's latest executive order banning Muslim immigration is forcing me to put it in writing. The freedom to move should be a human right and can act as the foundation of all others. If you don't like the policies of your city? Move to another one. Your state? Move to another one. Your country? Same thing.  By giving people the freedom to choose where they live policies will arise that benefit the greatest number of people. If you're keen on having a strong religious state you should be able to move there. If you want to live in a big city you should be able to move to one. If it turns out that a particular location is losing residents it will act as a signal that they need to do something. Over time the best policies will rise to the top and the world will have an rapid pace of policy innovation.  Of course it's not realistic since we all have family, friends, and jobs and aren't able to get up and move whenever something bothers us. Beyond that the value in the world's wealth is not equally distributed which would leave to some immediate shifts. One can argue that that's good for the long term since it will smooth out the wealth distribution but it will be dangerously destructive. Cultures are also significantly different across the world and beyond the conflict we'll have when some collide it's a risk that the world's cultures will blend into one and we'll lose our uniqueness.  This won't happen any time soon but to have a fair world we must allow individuals to choose to live wherever they want. Most won't take the offer since there's always going to be something keeping them around but having the ability to move anywhere is powerful and liberating.",0,1,2017-01-28,5,"freedom, politics, world, democracy, future",317,The freedom to move
36,0,Earlier today I was catching up on Euro Cup and encountered a content not available in your country error message. This is Sloppy and Twitter needs to do a better job with their flagship product.,#product,"{% include setup %}           I’m a huge Twitter fan so it’s especially frustrating when I encounter issues. The latest one was discovering a ""This content is not available in your country"" message when trying to catch up on some Euro Cup highlights in a moment. I understand that in today’s digital rights world there’s always a chance for some content to be unavailable but there’s no reason it should have been included in Twitter’s flagship product that’s supposed to attract and engage new users. The fact that it’s manually curated makes it even worse - how could this have slipped through? One explanation is that the curator was not based in the US and had access to the video. The other is that the video was available initially but was pulled later on. In both cases Twitter should have had the appropriate safeguards to identify this was happening and amend the moment. An even better approach would have been to have different versions of the moment depending on the user's location. The current implementation just feels sloppy and I can’t stand to see it in a product I love using.",0,1,2016-07-10,2,"twitter, product",211,A Twitter Moments fail
21,0,I received a promotion in the mail that eschewed the usual Facebook and Twitter callouts in favor of a Snapcode.,#meta,"{% include setup %}      Earlier today I received one of those typical promotions in the mail from Jet.com - $20 off for a $100 purchase. Nothing special about the offer itself but something I found fascinating was that there was a Snapcode that when scanned via Snapchat took you to a mobile optimized Jet.com page within Snapchat. Most mail I’ve seen has the callouts to encourage people to follow the brand on Facebook and Twitter but this promotion didn’t have either of those - just the Snapcode.  I haven’t seen this before and find it odd. Is it really the case that people are using Snapchat that frequently that they would open it up to scan a QR code that takes them to an ecommerce site? Maybe this was Jet.com categorizing me into the young and hip heavy Snapchat using audience segment. Maybe the fact that I noticed it and am writing this blog post is proof enough that replacing the social follows with a Snapcode was well worth it.  QR codes are extremely popular in Asia but have never taken off in the United States so it’s interesting to see them starting to appear, and potentially even take off, through Snapchat.",0,1,2017-08-12,5,"snapchat, snapcode, jet.com, marketing, mail promotion",219,A Snapcode in the mail
25,0,While vistiing London I couldn't help but compare the way the two cities handle public transit. They're both very similar but also very different.,#society,"{% include setup %} I'm behind on my 2 blog posts per week goal since I'm traveling and want see as much as I can but plan on catching up next week when I'm back. Right now I'm on a high speed train going from Porto to Lisbon and have 2.5 hours to do a bit of writing. And what's more appropriate than comparing the public transit system of New York with that of London, and generally those of the United with those of Europe.  New York City has one of the world's best transit systems with 233 miles of track and 24/7 service. This supports a population of 8.4 million spread out across 304 square miles. London, meanwhile, has 250 miles of track for a population of 8.7M over 607 square miles, and only started running a nighttime service 4 months ago.  The trains themselves are different - London has smaller trains, both in the passenger capacity per car as well as the number of cars per train. This smaller size is offset by more cars running more frequently which makes transfers much quicker. Every station I've been to in London also showed the estimated arrival time of the next train; New York is getting better but only a few stations and lines provide this information. In fact it seemed as if even the London buses had these time estimates available since Google Maps provided accurate bus arrival times.  London also has a more comprehensive payment system. New York is on the Metrocard while London is on the Oyster card. Similar to the Metrocard it can be used across a variety of transit options but also seems to extend to larger train stations in the area - think the equivalent of the Long Island Rail Road in New York. The usage is different as well - the Metrocard works via swipes but the Oyster card is designed around a wireless tap and go system that seems to be extending to paying directly with a phone. Another way to look at the Oyster card is as the next generation of the PATH SmartLink card. The one annoyance with the London system is that you need to tap when leaving a station since the system is divided into differently priced zones.  Generally, it felt that London takes more pride in their transit system and wants to do more than the minimum while New York is just focused on keeping it operational. The London stations are cleaner and better maintained with many having their own unique look. Both systems get the job done and quickly take you from point A to point B but the overall experience just seems a tad more polished designed in London.  The United States is not known for having the best public transit systems. It's significantly less dense than Europe with a car owning majority that wouldn't work well in millenia-old city grids. Europe, on the other hand, has the density and old city streets that make public transit a viable option. Strong public transit systems improve the equality of a city by making it cheap and easy for anyone to go anywhere. This allows the density of a city to rise and provide jobs that can be accessed at a low cost. Without these a city ends up being split into rich and poor neighborhoods with the poor either having to undertake an extraordinarily long or expensive commute to get to work. Europe seems to have figured this out while we discuss income inequality.",0,1,2016-12-24,5,"subway, the tube, public transit, london, new york",590,Comparing public transit systems: New York vs London
18,0,I really like the idea behind AWS Athena but just couldn't get it to handle our workload.,#meta,"{% include setup %} Almost a year ago, AWS [launched Athena](https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/) which allowed you to query data directly off of S3. I loved the idea since it would allow us to simplify our workflow by reducing the need for Spark and Redshift while also cutting our costs. In theory queries that were being run via Spark or Redshift could just be run on top of data stored in S3 without having to load it into any system.  I messed around with a few toy examples and was able to get up and running pretty quickly. Yet I was left disappointed when I threw a real problem at it. We have a variety of jobs that require us scanning a month’s worth of filtered impression data and I wasn’t able to get Athena to successfully complete the query no matter how many different permutations I tried. Every time I thought I had a clever workaround I ended up being disappointed when Athena failed to execute the query.  I’m optimistic that Athena will work through its kinks but at the moment we’re still sticking with our tried and true solutions in Spark and Redshift. Over the next few weeks I’m going to give [Redshift Spectrum](https://aws.amazon.com/redshift/spectrum/) a shot which seems to be the best of both worlds.",2,1,2017-10-14,4,"aws athena, redshift, spectrum, big data",228,Scale challenges with AWS Athena
20,0,I wrote a simple web app that allows you to get cicycling directions from one Citibike station to another.,#code,"{% include setup %}               Photo by  @rafat      On Wednesday, I took my first bike ride using New York City's new  Citibike  program. So far it's been great but one issue I ran into is being able to plan a trip. Google offers cycling directions from place to place but doesn't take into account the Citibike stations. On the other hand, the Citibke app shows the rental stations but doesn't make it easy to find directions from one station to another unless you're already at one of them.  I decided to actually do something about it and wrote a  little web app  that lets you pick two Citibike stations and retrieves the cycling directions between them using the Google API. It's definitely not perfect and the user experience needs to be improved but it does what it was designed to do.  What I find amazing is how simple it was to write the app - it took me less than 90 minutes to go from having an idea to having something that's usable. The list of stations are available in  JSON from the Citibike site  and Google makes it very easy to use their services to show a map and get directions.The best part is that this app is completely static since it's just using client side Javascript and Google's APIs. I've written about this  before  but I'm convinced that more and more services will become available through APIs which will lead to more and more apps and sites being built this way.",5,1,2013-06-01,3,"citibikenyc, bike sharing, google maps api",287,Citibike Station to Station Directions
35,0,Just because your code works on a development environment doesn't mean it will work on production. The scale and infrastructure may be completely different and you nede to keep it mind when writing software.,"#devops,#management","{% include setup %} The biggest development lesson I learned over the years is that production is a completely different beast from development. Code that works perfectly in a development environment can fail catastrophically in production and cause a severe impact on the business. Issues can stem from bits of inefficient codes to database schemas that just don't scale on production. Ideally your development environment  mirrors production and has the same load and hardware but that's rarely the case. For the other cases cases I’d go through the following items to make sure your code is ready for production:  - General code efficiency: Your code may pass unit tests and work fine when you’re running it on development data but you should make sure the code itself can scale to production data. Inefficient code may be fine to push to production if it’s not being hit often or you have the hardware to back it up but you need to make sure this is the case. This also extends to UI applications: if your development environment has a few rows for a customer while in production a customer will have hundreds, you need to make sure that the UI is responsive and that the design actually fits the production use case. - Query performance: This is the most frequent problem I’ve seen when new code is deployed. A query may run fine in development which can have a magnitude less data but as soon as it’s pushed to production queries that used to take milliseconds while developing start taking multiple seconds. The simplest way to deal with this is to just run your queries on production and confirm they work - especially on the datasets and filters you suspect will be problematic. The results may lead to solutions such as adding new indices to a table or generating new summary tables to speed up the code, neither of which would have been easy to discover during development. - Deployment plan: Part of writing code is thinking through the deployment and a big part of deployment is making sure you’re avoiding down time. In addition to making sure your application rolls over gracefully to the new code you should be thinking about the database migrations you’ll need to make and confirming they will run as expected on production. I’ve encountered cases where adding a column took a few seconds on a development database but multiple hours on production. If that’s the case you should rewrite the migration to avoid downtime - for example creating a new table and population it with legacy data and only then renaming it to the original name. - Rollback plan: As much as we like to think our code is perfect mistakes happen and we should write code that’s easy to rollback. Ideally it’s as simple as just pushing the older code but it may need a bit more work if it depends on database changes or other applications.  None of these should be earth shattering and over time they become a habit but until then it’s important to go through each one to ensure a successful production deployment.",0,2,2015-10-25,3,"software, development, coding",518,Production makes fools of us all
34,0,If you're doing a startup you need to pick an area that you're so passionate about that you can work through the trials and tribulations since it's like no other job you'll have.,"#product,#meta","{% include setup %}   Let’s start with a joke:          An American consultant was at a pier in a small coastal Mexican village when a small boat with just one fisherman docked. Inside the small boat were several large tuna. The American complimented the Mexican on the quality of his fish and asked how long it took to catch them.       The Mexican replied ""Only a little while."" The consultant then asked why didn't he stay out longer and catch more fish? The fisherman said he had enough to support his family's immediate needs. Then the American asked how he spent the rest of his time.       The Mexican fisherman said, ""I sleep late, fish a little, play with my children, take a siesta with my wife, Maria, and then stroll into the village each evening where I sip wine and play guitar with my amigos. I have a full and busy life, senor.""       The American consultant scoffed, ""I am a very successful business consultant and could help you. You should spend more time fishing and, with the proceeds, buy a bigger boat. With the proceeds from the bigger boat, you could buy several boats, and eventually you would have a fleet of fishing boats. Instead of selling your catch to a middleman you would sell directly to the processor, eventually opening your own cannery. You would control the product, processing and distribution. You would need to leave this small coastal fishing village and move to Mexico City, then Los Angeles and eventually New York City where you will run your expanding enterprise.""       The Mexican fisherman asked, ""But senor, how long will this all take?""       The consultant replied, ""Probably 15 to 20 years.""       ""But what then, senor?"" asked the fisherman.       The consultant laughed, and said, ""That's the best part! When the time is right, you would announce an IPO and sell your company stock to the public. You'll become very rich, you would make millions!""       ""Millions, senor?"" replied the Mexican. ""Then what?""       The American said, ""Then you would retire. Move to a small coastal fishing village where you would sleep late, fish a little, play with your kids, take siestas with your wife, stroll to the village in the evenings where you could sip wine and play your guitar with your amigos.""    Other than the chuckle, this joke got me thinking about how people view work. The joke suggests that you should only work to support your life outside of work. If you work more than that then the joke’s on you since you’re sacrificing your personal life. This is view that you can’t have if you’re starting a company. Entrepreneurs need to combine their personal and professional lives. If you’re running a startup and aren’t thinking about the market, your product, or your users when you’re in the shower or in bed you’re doing it wrong. More importantly, this should be natural and not forced. If you don’t enjoy thinking about your startup when times are good, how will you be able to do it when times are tough (which they will be)? We need our passion to get over the humps so if you’re not passionate about your startup when you’re starting out, you will abandon it when facing challenges. Your startup will end up consuming you so why not pick something that you care about?",0,2,2012-11-29,1,,621,Entrepreneurship is not a job
24,0,One of the most effective ways to reduce risk is to make sure that your development environments mirror production as closely as possible.,#devops,"{% include setup %} An important lesson I’ve picked up is to have a consistent development environment across your computers. These days it’s common to have a home computer, a work computer, as well as a series of VPSs that we use for development. The more similar they are the easier life gets. Having the same code and libraries reduces the risk of an application working on one machine but not the other and avoid the hassle of upgrading esoteric libraries. I’ve run into numerous issues where small version difference led to weird behaviors that ended up taking a long time to debug. Consistent tools help as well - using emacs on one machine but vim on another slows you down when you have to context switch and figure out which one you’re using. By committing to one you become more efficient as you develop the shortcuts and flows that are possible. Using virtual environments and containers helps get at this point - they’re both ways to ensure that the code you’re writing and testing is going to be the same code that’s running on production. Without this every time you deploy new code you’re risking failure. More often than not it will work as expected but it’s those rare cases that will be problematic and anything that can be done to avoid them should be done. One of the simplest ways is to align your development environments with your production ones.",0,1,2016-01-09,4,"dev ops, production, development, deployment",241,Have consistent development environments
21,0,Thinking about book prices and how they compare to other common items makes you realize how cheap books actually are.,#pricing,"{% include setup %} While the pricing battle between Amazon and Hachette rages on, I’ve been thinking about the relationship between price and value. A typical ebook on Amazon costs $9.99 while a movie in a theater, especially one in New York, can cost more than $10. And yet the book takes longer to experience - a movie is over within 2 hours while a book can be enjoyed for hours. Or how about a beer or coffee, they’re two to three times cheaper than an ebook but are consumed an order of magnitude faster book and only provide immediate gratification.  Amazon released data indicating that dropping the price of an ebook from $14.99 to $9.99 (33%) leads to a 74% increase in number of books sold. I understand that a lower price increases demand but it’s still ridiculous that we’re that concerned about a $5 price difference for a book when we’re spending that much on a coffee.  We’re so used to spending that much on a coffee that we’ll pay it without question and we’re so convinced that ebooks should be less than $10 that we refuse to pay more. It’s amazing what habit and expectations can do. I understand this bias and yet I still have a hard time believing that by skipping two cups of coffee I can buy a book. It definitely makes me appreciate the effort required to change people’s perceptions of what’s a fair price.",0,1,2014-08-11,4,"books, amazon, hatchette, pricing",245,Books are insanely cheap
24,0,I love efficiency and got a nice surprise a few weeks ago when Google Docs introduced a much better way to move documents.,"#design,#product","{% include setup %}            Old Google Docs behavior     I’m an efficiency fiend and love seeing my heavily used products updated with any and all functionality that make them a tad easier to use. A few weeks ago, Google Docs, one of my most heavily used applications - in fact where I’m writing this very post - made a minor change to the “Move to” functionality that created one of these small efficiency wins. Before the change, clicking the small folder icon while in a document would show a menu with the item’s current location and a “Move this item” option. Clicking that behavior would enable move mode and would require the user to navigate to the appropriate folder and then confirming the move.            New Google Docs behavior     Seems simple enough but in order to move a document required clicking that icon, switching to move mode, navigating with a few clicks, and then concluding the move with another click. The new behavior defaults to move mode and allows you to move the item to a folder without opening it up so long as it’s selected in the list.  This is a trivial change but shows how seriously Google takes usability and constantly thinking about the details. Beyond that, I use this a few times a day so that time saved adds up.",0,2,2017-07-24,4,"google docs, ux, efficiency, design",258,A Google Docs efficiency win
10,0,I attempt to solve the Priceonomics Puzzle using Prolog.,"#prolog,#code","{% include setup %} The  Priceonomics blog  is one of my favorites so when I saw that they had a  programming puzzle  up I decided to have some fun with it. And what’s more fun than hacking around with a quirky, esoteric programming language? I remember having fond memories of playing around with Prolog in middle school so decided to dig it up again in an attempt to solve this puzzle.  Prolog is pretty different than the mainstream programming languages, it belongs to the logic programming language category and relies on defining a variety of relations and then querying these relationships to get results. A simplified way to think about it is you define a set of equations and tell Prolog to ""solve for X"".  This leads to some interesting behavior. Many functions end up being bidrectional with the Prolog version of a ""concat"" function being a good example. The first argument is a list, the second is the separator, and the last is the resulting string. Passing in all 3 will return true if the concatenation statement is true. Passing in the list and the separator will tell us what the concatenated string is. Passing in the separator and a concatenated string is equivalent to a ""split"" function. The only piece it's not able to figure out is the separator given the list and the concatenated string. Unfortunately, I'm not familiar enough with Prolog to explain why.  {% highlight prolog %} ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', 'Prolog is sweet'). true.  ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', 'Prolog is not sweet'). false.  ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', X). X = 'Prolog is sweet'.  ?- atomic_list_concat(L, ' ', 'Prolog is sweet'). L = ['Prolog', is, sweet].  ?- atomic_list_concat(['Prolog', is, sweet], X, 'Prolog is sweet'). ERROR: atomic_list_concat/3: Arguments are not sufficiently instantiated{% endhighlight %}  For the first pass, I decided to ignore the web side and just focus on defining the exchange rate relationships and have Prolog tell me which exchanges would work. The way it works is that we define a profit to be defined in terms of two intermediate currencies. We can then ask Prolog to give us the currency chain that will result in a profit.  {% highlight prolog %} exchange(usd,eur,0.7779). exchange(usd,jpy,102.459). exchange(usd,btc,0.0083). exchange(eur,usd,1.2851). exchange(eur,jpy,131.711). exchange(eur,btc,0.01125). exchange(jpy,usd,0.0098). exchange(jpy,eur,0.0075). exchange(jpy,btc,0.0000811). exchange(btc,usd,115.65). exchange(btc,eur,88.8499). exchange(btc,jpy,12325.44).  % Calculate profit for a usd->x->y->usd currency chain profit(First, Second, Profit) :-     exchange(usd,First,P1),     exchange(First,Second,P2),     exchange(Second,usd,P3),     Profit is P1 * P2 * P3.  arb :-     profit(First, Second, Profit),     Profit > 1.0,     write('usd '),     write(First), write(' '),     write(Second), write(' usd '),     write(Profit), nl, fail.  :- arb.  % Results: usd eur jpy usd 1.0040882716200001 usd eur btc usd 1.0120965187500002 usd btc jpy usd 1.0025512896{% endhighlight %}  The next step was to get it to retrieve and parse the JSON from the Priceonomics server. After doing a ton of searches and reading a ton of documentation I was able to get it to work. As a next step I'll try to see if I can get it to return currency chains of arbitrary length.  {% highlight prolog %} :- use_module(library('http/json')). :- use_module(library('http/json_convert')). :- use_module(library('http/http_json')). :- use_module(library('http/http_client')). :- use_module(library('http/http_open')).  parse(I) :-     test(CP=S) = test(I),     atomic_list_concat(L,'_', CP),     [A, B] = L,     atom_number(S,R),     assert(exchange(A,B,R)).  % Calculate profit for a usd->x->y->usd currency chain profit(First, Second, Profit) :-     exchange('USD',First,P1),     exchange(First,Second,P2),     exchange(Second,'USD',P3),     Profit is P1 * P2 * P3.  arb :-     http_get('http://fx.priceonomics.com/v1/rates/', JsonIn, []),     json_to_prolog(JsonIn,PrologIn),     PrologIn = json(L),     maplist(parse, L),     profit(First, Second, Profit),     Profit > 1.0,     not(First = Second),     not(First = 'USD'),     not(Second = 'USD'),     write('USD '),     write(First), write(' '),     write(Second), write(' USD '),     write(Profit), nl, fail.  :- arb.  % Results (Might change each run): USD JPY EUR USD 1.0071833283714342 USD EUR JPY USD 1.007164983424893{% endhighlight %}  I'm sure a Prolog pro would have been able to do this much quicker and better but I had a surprisingly fun time doing it. I got a bit frustrated trying to translate the JSON into Prolog relationships but actually getting it to work made it worth it. Trying a whole new programming category is a great way to get more creative and forces us to think about problems differently. Prolog may not be the most practical language but exposing us to new concepts and approaches makes it valuable.",2,2,2013-06-07,3,"prolog, priceonomics puzzle, priceonomics",731,Fun with Prolog: Priceonomics Puzzle
22,0,Here are the Django and Python packages I've found to be useful after working with Django over the past 18 months.,"#python,#code","{% include setup %} On Tuesday, I shared some  best practices  I picked up while using Django. This is a follow up post to share the packages that I found useful as well as various hiccups I encountered when using them.               django-registration  and  django-social-auth : Combined, these packages let you handle the basic user registration and activation. Most likely, you will end up having to customize them a bit to do what you want. For example, allowing a user to register using an email address instead of a username or requiring an email address for a user who signs up using Twitter. A small issue that annoyed me is that the signals generated by these two packages occur at different points: django-registration generates signals that includes the request while django-social-auth generates signals that contain the response from the OAuth provider. Depending on your use-case, it may be worth it to use the  simple backend  for django-registration, it automatically activates and logs-in the newly registered users, making your app a bit easy to get into.        django-storages  and  boto : If you plan on using S3 to host static content, definitely take a look at these. They provide backends to make it easy to save and access your static content to S3 without having to deal with the AWS API. I ran into some issues using this along with Cloudfront and django-compressor but I was able to fix them by looking at  Stackoverflow .        django-compressor : This is a neat library that will compress and minify your JS and CSS, check if anything’s been updated, generate an upload the result to static files location, and update the HTML to point to the new location. This makes sure that users never end up with older, cached versions of your static files. One thing to note is that you need to make sure that your Javascript are properly formatted and all end in a semi-colon; otherwise you run the risk of the compression failing. I know that there are other Django compressors  out there  but I’ve been happy with django-compressor.        sorl-thumbnail  and  PIL : If you allow users to upload images this is a must have. It provides a standard way of resizing the images and caching the result. The library comes built in with support for cropping and a variety of other processing options so you don’t have to worry about it. One thing to note is that if a user is loading a page where none of the images have been generated yet, it will delay the page load until all of the images are generated. As long as you know the required sizes of all images, you can run a task on the  backend to generate  each of the images. You may have trouble installing PIL in a virtualenv but doing some Googling it should be easy to figure out.        django-extensions : Just a neat library that comes with additional management commands to make developing Django easier.        django-debug-toolbar : This intercepts every Django request and provides some debug information to help you optimize your code. The most useful piece to me is being able to see the SQL queries that are being executed and helps me figure out what needs tweaking/caching.        django-crispy-forms : If you’re using Twitter Bootstrap, this is a library that lets you generate Bootstrap forms in Django.        django-celery  and  celery : This is a way to run tasks in the background. With Pressi, we initially started with some management commands behind some cron jobs but we ended up switching to Celery when we wanted to distribute it across multiple machines and have built in support for threading and error handling. One thing to note is that we used RabbitMQ as the backend but it takes a bit of time to setup and I’m still struggling to understand the ways to manage it. A lot of people have been using Redis as the backend successfully and I think I’ll give that a go in future projects.        mongoengine  and  pymongo : If you’re using Mongo, take a look at mongoengine, which serves as an ORM for Mongo, and is built on top of PyMongo, a Mongo API. Mongoengine makes it very easy to change your models from a relational database to an documented-based one by keeping the field types and model definitions similar. Be aware that document-based databases are significantly different from relational ones and that although cosmetically your models look similar, the interaction with the backend is very different. You shouldn’t switch to MongoDB just because you can - make sure you’re switching for the right reasons. For Pressi, we use a hybrid approach where we use MongoDB to store a user’s social media content with everything else stored in MySQL. Something to be cautious of is that both of these libraries have been evolving pretty quickly and we ran into an issue where we weren't able to consistently connect to a MongoDB instance until we stumbled unto the right versions of the libraries (in our case, 0.6.20 for mongoengine and 2.4.1 for pymongo).        django-haystack : When you’re ready to graduate from implementing a search using QuerySet filters to an indexing backend, take a look at Haystack. It provides a pretty simple search interface that integrates pretty well with Django and supports a few different backends. We ended up settling on the  Xapian  backend because it was supposedly simpler but ran into some trouble installing it inside a virtualenv until I found  this post . Note that although Haystack supports multiple backends, not all features are supported by every backend so make sure the backend you choose supports everything you need. I believe Solr has the most functionality out of the box but we wanted to keep it simple for Makers Alley.        django-postman : We just implemented this for Makers Alley but it’s a very simple way of doing user to user messaging. It comes with the standard messaging features (inbox, reply, archive, delete) but one thing I wish it had was a way to include attachments.        Fabric : I mentioned this in the previous post but wanted to reiterate it since it makes building and deploying your code easy. It also forces you to think about your environment and you end up with a better structured project as a result.        South : Another package I mentioned earlier that makes it significantly easier to deal with database migrations in Django. The only time we've run into issues using South is when two of us were making changes to the same model in parallel branches. Even then it's easy to replace the two flawed migrations with a functional one.        Unidecode : This isn’t a Django specific library but we found it useful when cleaning up unicode data. If you ever get random unicode exceptions in your code, Unidecode should be able to help.        BeautifulSoup  and  PyQuery : If you need to do some HTML scraping in Python, take a look at BeautifulSoup. It turns HTML code into an object that’s easy to navigate and search. After getting more and more familiar with jQuery, I found a python alternative in PyQuery but am still getting comfortable with it. If you come from the jQuery world I’d try using PyQuery first; otherwise I’d try BeautifulSoup.        requests : Just a nice and simple replacement of urllib and urllib2 that makes it much simpler to make HTTP requests. Your code becomes cleaner, more readable, and more expressive.     I tried to highlight the libraries that have made developing in Django easier but I’m sure there are tons more. I’d love to hear about them so do share.",29,2,2013-05-10,6,"Django, python, hacking, coding, web development, startups",1450,Eighteen Months of Django: Part 2
19,0,I wanted to share an example from Pressi where we applied a quantitative approach to solve a problem.,"#meta,#datascience","{% include setup %} The tech world is conflicted about how much math a developer needs. Engineers working on quantitative systems or data science clearly require advanced math and there are also countless engineering roles where math is unnecessary. My experience is that even if you don’t use math, having a mathematical mindset makes you significantly more productive. You’re able to quickly estimate the complexity of various tasks and hone your intuition. You’re also able to quickly recognize patterns when refactoring, especially when working in a functional language. A basic understanding of probability and statistics is a great way to analyze the performance of your code as well as help you model and understand your application behavior. I wanted to share a quick story of how a mathematical approach came in handy when working on Pressi.  First, a little bit of background.  Pressi  is a social media mashup page that takes the content a user posted across a variety of social media networks and creates a “Flipboard” style web page to showcase it. At launch, we had a simple cron job that would run every hour and pull new data for each of our users. Over time, we migrated to a task system that let us run these retrieval tasks in parallel and split across multiple machines. Using this approach, we were able to scale well and handle the increased volume but our hosting costs saw a big jump so we went looking for a solution.  Luckily for us, we tracked the history of each social network data pull (containing user, network, datetime, and # of items pulled) and doing a quick query told us that close to 92% of our requests resulted in no data being retrieved. The intuition behind this is that most people will not be posting on every social network every hour. By eliminating these calls we’d be able to drastically cut our hosting costs.  This analysis got us thinking about the ideal case which is for us to pull a moment immediately after it’s posted. One way to achieve it was to leverage the push updates that some of the social networks supported but we wanted to find a more general way that could tell us when we should pull the data for a particular user/network pair.  To figure this out, we looked at another distribution: the average number of moments shared by a user on a network per day. This let us look at the number of users who were extremely active on social media down to the users that pretty much only had accounts. We then dumped this data into Excel in order to come up with ranges that we’d use to segment our user/network pairs in order to see how often we should attempt to pull their data. For example, a user that on average posted 20 updates a day on Facebook would have their Facebook data pulled every 4 hours but a user who posted on Instagram less than once a day would have their data pulled once a day. This also gave us a way to estimate how many fewer calls we'd need to make compared to what we were currently doing and therefore approximate the cost savings. The result of this update was that we dropped the number of useless requests from ~92% to just over 40%. This was by no means perfect but gave us improvement we needed. An additional update we modeled out but but didn't get a chance to implement was to look at day of week and hourly patterns in order to identify when users were actually posting rather than treat every day and hour the same way. The data clearly showed that users had well defined schedules which would have led to another nice improvement.  The key lesson here was that we started by leveraging the data we collected to identify the major cause of our cost increase and then identified the metric we wanted to optimize. In our case it was to reduce the volume of empty requests we were making while making sure that we did not significantly increase the average number of moments that were retrieved for non empty calls. Otherwise, we could make 1 request a week for each user/network which would pretty much drop the number of useless requests to zero but blow up the number of average moments retrieved per call. We could have chosen a variety of other metrics but went with this one since it was intuitive, easy to model, and easy to test. The other neat property is that it’s self correcting so if a user changes their behavior on a particular network we’d shift them into another bucket.  None of the math we used was very complicated and although we tried playing around with a few statistical distributions to model out the user posting behavior we ended up quickly abandoning those when we saw the impact we’d get from a simple approach. I’d bet that almost every code base has something that can be improved with a little bit of mathematical analysis.",1,2,2013-09-17,4,"math, programming, engineering, quantitative",852,Programming and math
18,0,A few weeks ago I scraped some fantasy football data from Yahoo and decided to visualize it to,#dataviz,"{% include setup %} In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code. As usual, the code is up on [GitHub](https://github.com/dangoldin/yahoo-ffl/blob/master/analyze.R) and I’ll keep updating it as I keep adding newer visualizations and analyses.                              Pretty simple here but highlights how much more valuable the QB position is compared to the others.                                     The  box plot  is a quick way of looking at distributions since it highlights a few metrics at once - the median, the quartiles, as well the outliers. What's interesting here is how many outliers there are at the QB and WR positions, especially how uneven it is for WRs.                                       The density plot shows how the points are distributed by position. This shows a similar story to what we saw in the boxplot but visualizes each of the data points. I suspect the symmetry in the QB position is not unique and is just an artifact of the fact that QBs are heavily represented in the top 100 players each week and if were to expand our dataset we'd see similar distributions for the other positions.                                       Similar exercise to the above but by team. I didn't find a ton interesting here other than Pittsburgh is dominant when it comes to top fantasy players and that Denver and Philadalphia are lacking.                                       This isn't the most useful due to the biased dataset but it does highlight the dominance of some teams compared to others but not much more than that - at least with a quick glance.                                       A bit tough to read due to the volume of teams but paired with the previous one does show that there are a few outliers but many of the distributions are similar.",2,1,2016-09-05,3,"data visualization, fantasy football stats, fantasy football",502,Visualizing fantasy football stats
7,0,Laws are getting out of hand so,"#society,#meta",{% include setup %}                 The Washington Post      Whenever a new law is passed it’s incredibly difficult to have it repealed since there are enough people benefiting who will fight against the removal. This leads to a situation where instead of replacing old laws with new laws we end up with a massive system of laws with new ones that just get thrown on. This leads to a whole slew of inefficiencies and requires experts to help navigate the landscape who themselves benefit from the complexity and want to maintain it as much as they can.  It’s true that there have been fewer laws passed over the past decade likely due to the polarized congress but I would rather see many old and archaic laws repealed in favor of modern ones that are built for the current and future world.  So what can we do about it? An idea I’ve been toying with is to give the opposing voters the ability to name it. The idea is that the law will be active but it will be so terribly named that it will be much easier to repeal in the future. I’m only partially joking and suspect there may be something here. Imagine the type of public support we’d see to repeal awfully named laws.,1,2,2017-02-06,3,"laws, regulations, us government",249,New law? Let the opposition name it
30,0,Engineers always have to balance the quick and dirty work with something more elegant that will take longer. Using a rule of generalizing when n=3 strikes the right balance.,"#management,#code","{% include setup %} Engineers strive to write code that’s general and flexible enough to adapt to support a variety of cases with minimal changes. Unfortunately, writing general code isn’t easy and requires significant thought, effort, and experimentation. The challenge is figuring out the appropriate time to generalize your code.  If you do it too early you may spend unnecessary time writing generalized code that will never be used again. Even worse you may write code that you think is generalizable but ends up collapsing under its own weight under future scenarios. In this case writing minimal code would have served you better since it would have been much easier to adapt or throw away to support the new case.  If you do it too late you most likely spent time doing repetitive work that could have been better spent building a scalable solution that you may end up doing anyway.  My rule of thumb is to generalize at n=3. The first two times I have to support a new scenario or process I'll just do it manually or hacked together. But as soon as I need to do it for the third time I'll start looking for a more generalized solution. At this point it's likely that the third is not the last time I'm going to have to do it and I also have 3 cases to base and test my solution on.  This isn’t a trivial approach but works surprisingly well. It’s incredibly difficult to predict whether a simple script will morph into something more or end up being used once. The easiest way to predict whether it will be repetitive is to wait until it is repetitive - for me that magic number is 3. High enough to weed out the edge cases but low enough to get enough value from being generalized.",0,2,2016-04-07,6,"coding, management, short term, long term, programming, program design",307,Generalize at n=3
29,0,I've recently started using a calendar to track and plan every adhoc task I need to do. This has been a huge help in keeping my productivity high.,#management,{% include setup %} A recent trick I’ve picked up to manage my time a bit better is to take all the adhoc tasks I have to do and scatter them into my calendar for the next few days. This allows me to actually get to working on the tasks and I can make sure none of them are forgotten. Using a calendar also forces me to think about the time I expect these tasks to take and plan around that. I’m nearly always running behind and am constantly shuffling tasks around but it’s much better than my previous system of a text file with a constantly growing list of todos. A side benefit of this approach is that I can split my day into [maker versus manager](http://www.paulgraham.com/makersschedule.html) chunks rather than be at the whim of meeting invites.,1,1,2015-11-19,4,"management, time management, planning, prioritization",142,Adhoc task management
26,0,There are sites that let you export your Instagram photos but they're slow. I wrote my own version that you can install to download your phoots,"#code,#product","{% include setup %}  I just hacked together a quick app to help download Instagram photos. At first, I tried using  Instaport  and  OpenPhoto  but both of them were backed up with others trying to do the same so I decided to create my own. It's basically a really simple python web app that allows you do a quick authentication with Instagram and then lets you downloads all your images to your hard drive.  It should be pretty easy to get started by following the readme on github:  https://github.com/dangoldin/instagram-download",3,2,2012-12-19,1,Download export Instagram photos,98,Self hosted Instagram export
26,0,We idealize founders who quit their jobs to do a startup but there are so many outside the startup world who risk a lot more.,#meta,"{% include setup %} This post has been sitting on my to-write list for multiple years now but it’s a topic I’ve been meaning to write ever since my Makers Alley experience. When we created Makers Alley the goal was to create a marketplace for local designers, woodworkers, and metalworkers to offer their pieces for sale along with some customization. To get the marketplace started we had dozens of meetings with these artists describing what we were trying to do, explaining the product, taking photos, and generally pitching to get them signed up.  We never got the traction we wanted but during the time I met a ton of wonderful and passionate people who were looking to make a living designing and making furniture. This was a world far away from tech and it was thrilling to be a part of it. To get started in tech all you need is a laptop but the people we met were spending tens of thousands of dollars on the spaces, the tools, and the supplies just to get started with no idea if they’d be successful or not. They had a vision and were passionate enough to get into debt to pursue them. Being in the tech world we admire founders who quit their jobs to do a startup but we shouldn’t forget that there are people giving up more, risking more, and all to pursue their passion.",0,1,2018-12-20,2,"startups, entrepreneurship",238,Tech startups are not the only ones taking risks
19,0,I tried using Google's Cloud Vision to identify the food in my fridge. It didn't go very well.,"#code,#meta","{% include setup %} Something that I haven’t quite figured out is how to avoid wasting food. I like to think I keep good track of everything in my fridge but too often I end up finding something in the corner that spoiled and needs to be thrown out. Earlier today I was talking to someone at the office about this problem and how nice it would be if you could just have something that knows everything that’s in the fridge and can track how long it’s been there and an estimate of how long it will last. I’m sure refrigerators in 10 years will have this built in but I wanted to see what I could cobble together in an evening.  Luckily for me Google released a Cloud Vision API and I decided to give it a shot. Turns out implementing it was extremely straightforward, despite Google’s poor documentation, with a quick code search on GitHub that led to me [https://github.com/ramhiser/serverless-cloud-vision](https://github.com/ramhiser/serverless-cloud-vision). Unfortunately, the results were not promising. I ran on three images and while the categorization was surprisingly accurate it was too general. I expected to at least accurate identification for the bottles and cans - milk, ketchup, yogurt but the closest it got was food, ice cream, and gelato. Granted, the photos weren’t staged well and it took me about 15 minutes to get it working but I was still disappointed. The Cloud VIsion service doesn’t offer much customization so I’m going to see how much better I can make it by improving the photos. I’ve included the original photos along with the classification results below. As usual my code is up on [GitHub](https://github.com/dangoldin/fridge-vision) although it was really just a straight up copy and paste from [ramhiser’s code](https://github.com/ramhiser/serverless-cloud-vision) above.                              {% highlight json %}[         {           ""score"": 0.90114909,           ""mid"": ""/m/02wbm"",           ""description"": ""food""         },         {           ""score"": 0.88251483,           ""mid"": ""/m/02phwj2"",           ""description"": ""display window""         },         {           ""score"": 0.81870794,           ""mid"": ""/m/0cxn2"",           ""description"": ""ice cream""         },         {           ""score"": 0.76996088,           ""mid"": ""/m/0270h"",           ""description"": ""dessert""         },         {           ""score"": 0.75129372,           ""mid"": ""/m/02fz11"",           ""description"": ""gelato""         },         {           ""score"": 0.69974077,           ""mid"": ""/m/02rfdq"",           ""description"": ""interior design""         },         {           ""score"": 0.57035172,           ""mid"": ""/m/02q08p0"",           ""description"": ""dish""         },         {           ""score"": 0.54961139,           ""mid"": ""/m/0191_7"",           ""description"": ""retail store""         },         {           ""score"": 0.53331912,           ""mid"": ""/m/031bff"",           ""description"": ""window covering""         },         {           ""score"": 0.51523668,           ""mid"": ""/m/01_bhs"",           ""description"": ""fast food""         }       ]{% endhighlight %}                                     {% highlight json %}[         {           ""score"": 0.87785435,           ""mid"": ""/m/07yv9"",           ""description"": ""vehicle""         },         {           ""score"": 0.78110605,           ""mid"": ""/m/0k5j"",           ""description"": ""aircraft""         },         {           ""score"": 0.77443254,           ""mid"": ""/m/0cmf2"",           ""description"": ""airplane""         },         {           ""score"": 0.7131173,           ""mid"": ""/m/02pkr5"",           ""description"": ""plumbing fixture""         },         {           ""score"": 0.71218145,           ""mid"": ""/m/015y8h"",           ""description"": ""jet aircraft""         },         {           ""score"": 0.66169083,           ""mid"": ""/m/06ht1"",           ""description"": ""room""         },         {           ""score"": 0.5944497,           ""mid"": ""/m/01lgkm"",           ""description"": ""recreational vehicle""         },         {           ""score"": 0.54411179,           ""mid"": ""/m/041x_j"",           ""description"": ""public toilet""         },         {           ""score"": 0.53394085,           ""mid"": ""/m/017_cz"",           ""description"": ""major appliance""         }       ]{% endhighlight %}                                      {% highlight json %}[         {           ""score"": 0.78413528,           ""mid"": ""/m/02phwj2"",           ""description"": ""display window""         },         {           ""score"": 0.647836,           ""mid"": ""/m/02rfdq"",           ""description"": ""interior design""         },         {           ""score"": 0.59498113,           ""mid"": ""/m/0c_jw"",           ""description"": ""furniture""         },         {           ""score"": 0.57692927,           ""mid"": ""/m/0191_7"",           ""description"": ""retail store""         },         {           ""score"": 0.54954523,           ""mid"": ""/m/08790l"",           ""description"": ""boutique""         }       ]{% endhighlight %}",3,2,2016-08-29,4,"computer vision, google, cloud vision, food identification",612,Food identification with Google's Cloud Vision
23,0,Various thoughts I've had about driving before GPS when we had to use an atlas to figure out where we were going.,#meta,"{% include setup %} I’ve been thinking about driving before GPS. I remember my family having an atlas in the backseat that we’d reference for long trips and actually map out our journey - which roads to take, which exits to get off of exits, and the distances involved. My clearest memory was constantly trying to figure out whether we missed an exit or not. The usual solution was to just pay attention for the next couple of minutes and try to use the signs along with the road atlas to figure out where you were on the map. Now, you just type in the destination on your smartphone as soon as you get the car and just start driving. Even if you make a mistake the directions automatically update to correct your course. The amount of time saved by GPS for every trip that no longer needs to be preplanned or adjusted enroute must be incredible. I suspect it’s also changed the type of trips we’re making - rather than going to the same old nearby spots that we know we can get to, we’re confident enough to go beyond that and discover something new, knowing that our phones will bail us out.",0,1,2015-05-09,3,"driving, gps, atlas",203,Driving before GPS
14,0,"Sharing the design and backend changes I've made to this blog since November, 2012",#blog,"{% include setup %} In November, I migrated my Tumblr and Wordpress blogs over to GitHub pages and have been making a few tweaks here and there. I started with the awesome  Jekyll-Bootstrap library  but wanted to share the changes I’ve made. It’s all hosted on GitHub so feel free to fork it.           Design changes              The version I started with didn’t have the Bootstrap responsiveness library so I added that in       Since I’m using it primarily as a blog, I updated the design to emphasize the blog aspect       Consolidated the pagination and social sharing widget to fit on one line       Incorporated some best practices from  Kaikkonen's blog typography guide                  Backend changes              Small improvements to SEO by giving ability to add keywords to each page       Added Open Graph meta tags to control what’s displayed when people share the page on Facebook       Made a few tweaks to the way the sitemap was being generated",2,1,2013-01-24,4,"design, backend, seo, blog",170,Blog updates since November
26,0,"I rant about a few more ui/ux issues I've run into. Login forms resetting emails, sign in vs register placement, new vs existing contact management.",#design,"{% include setup %} I don’t know why, but I’ve become more aware of the UI/UX of various sites and apps that I encounter. Whereas before I might have gotten frustrated about some behavior, I’m now starting to get annoyed whenever I encounter something that’s obviously crummy. Here’s a few of the more recent design anti-patterns I’ve been noticing.       Submitting a login form with the wrong password removes the entered email address. Especially on mobile, where it both takes longer to type and typos are more common, it’s crappy having to type both my email and password again if I made a simple typo in my password or just don’t know which of my passwords I used. A quick hack I saw that makes this a bit easier is to add a keyboard shortcut to your phone to replace “@@” (or any other character set) with your email address.                                Confusing placement of sign in and register. I forget which app I saw this in but as you can screen from the screenshot I’m on the sign in screen and yet the button under the form is to register, which causes the app to load the registration screen. The sign in button is up top which is a confusing flow since the user goes down the page first before having to go back to the top. The fact that the app uses a flat UI makes this worse since there’s not a lot of differentiation between the sign in and register actions.                                   Create new versus add to existing contact. This is probably the most “first-world” one here but without knowing who is currently in your address book it’s impossible to know whether you want to create a new contact or update an existing and contact. My current approach is to choose add to existing, realize that I actually don’t have that contact in my address book, and then go back a few screens and choose create new. A common database operation is “insert or update” - insert if it doesn’t already exist and update if it does. I’d love to have something like that to manage my address book.",0,1,2013-10-23,4,"design, ios, ui, ux",410,Some more design ranting
14,0,I respond to Robert Cringely's PBS post about Apple's acquisition of PA Semiconductor.,#product,"This post is a response to  Robert Cringely's PBS Post . He's giving 2 reasons for the acquisition and I wanted to add to two of his points.          The short term reason is to force Intel to give Apple price cuts for fear that Apple will make their own chips: I do not think that Intel needs to worry about Apple manufacturing their own PC chips as Apple already went through that phase and AMD already provides the necessary pressure on Intel to lower their prices.        In the future, software and OSes will not be tied down to a specific chip so Apple will start manufacturing their own processors to increase their margins: I think the author is on to something regarding the future of processors but I do not think the PC market will change that drastically. Apple will probably start making their own chips for the iPhone and their new gadgets but I doubt they will do the same for the PC market.      Edit: Just found out that Apple already makes a server (Thanks Brian). I have to start doing some research from now on.",1,1,2008-04-25,3,"apple, business, PA semiconductor",197,On Apple buying PA Semiconductor
30,0,Apple's Touch ID is great when my fingers are dry but utterly fails when they're wet. It should have enough history to create a wet profile of my finger.,#product,"{% include setup %} Apple’s Touch ID is great but one thing it doesn't handle well is wet fingers. Even if my hands are a little bit sweaty or not completely dry it's difficult to unlock the phone. Yet as soon as they’re dry the phone immediately unlocks. What’s surprising, especially given Apple's focus on delivering the perfect user experience, is that this is still a problem. I'm not familiar with the hardware behind Touch ID but even if there's some sort of warped fingerprint it should be good enough. The fact that there are a few unsuccessful attempts with the wet thumb followed by successful attempt should be enough to develop a profile for the wet version which can be used on future attempts. Modern products succeed by delivering optimized experiences; future products will need to adapt and grow along with us until they become eerily predictive.",0,1,2016-09-23,3,"Apple Touch ID, user experience, thumbprint",149,A smarter Touch ID
15,0,"The easiest way to become president is to run for a second term.""",#datascience,"Answer: Be elected for a first term, the second term will follow.  It turns out it's pretty likely that a president will be elected to a second term. If we examine all previous Presidential Elections, we will see 8 presidents who failed to get reelected:             President       Result           Benjamin Harrison   Failed to get reelected in 1892       George H. W. Bush   Failed to get reelected in 1992       Herbert Hoover   Failed to get reelected in 1932       Jimmy Carter   Failed to get reelected in 1980       John Quincy Adams   Failed to get reelected in 1828       Theodore Roosevelt   Failed to get reelected in 1912       William Henry Harrison   Failed to get elected in 1836       William Howard Taft   Failed to get reelected in 1912         On the other hand, if we look at all presidents with 2 or more terms, we only see a few Presidents who have failed to get elected. Some of these, like Andrew Jackson, failed to get elected initially but were then able to get 2 terms in office. Grover Cleveland had non consecutive terms in office. In total, there were 16 presidents who had a second term.          President   Result           Abraham Lincoln          Andrew Jackson   Failed to get elected in 1824, was elected in 1828 and 1832       Bill Clinton          Dwight D. Eisenhower          Franklin D. Roosevelt          George W. Bush          George Washington          Grover Cleveland   Failed to get reelected in 1888 (was pres in 1884 and 1892)       James Madison          James Monroe   Failed to get elected in 1808, was elected in 1816 and 1820       Richard Nixon   Failed to get elected in 1960       Ronald Reagan          Thomas Jefferson   Failed to get elected in 1796       Ulysses S. Grant          William McKinley          Woodrow Wilson            Franklin Delano Roosevelt ran for, and won, a 3rd term in 1940 using the idea that one should ""not change horses in midstream."" He did not need to do that since it seems people stick with what they are comfortable with.    Note: If I made a mistake anywhere let me know so I can correct it. The data was retrieved from Wikipedia.",0,1,2009-01-21,3,"politics, elections, presidency""",417,"What's the easiest way to be elected president?"""
23,0,There are a variety of different ways of conducting engineering interviews so I jotted down the various ways I could think of.,#management,"{% include setup %} Technical interviews often come in a variety of flavors and I thought it would be interesting to list as many as I can think of and my thoughts on each one. In general I think there’s value in each type of approach but some are going to be more appropriate than others depending on the person’s experience and role.  - Brainteaser. These are seemingly simple problems that require a trick or insight to get them right. If you’ve heard it before or are familiar you can blow through these otherwise you’ll need to rely on a series of hints to get to the final answer. Probably not the best way to judge someone’s ability since it’s likely removed from the work they’ll actually be doing. - Data structures. A bit more legitimate than the brainteaser approach this question digs into your knowledge of data structures. These usually start with some sense of complexity and then end up in implementing some type of traversal or tree search. The theory and knowledge of these is important but it’s pretty rare to have to implement a low level data structure. - Architecture. This moves higher level and asks you to think about designing a larger application. How would the various components look? If it’s a service what endpoints would be exposed? What are the arguments and results for each of the calls? How would you scale this? What if you needed to make changes? These are a useful way to see how someone thinks and whether they have some familiarity thinking through the architectures of large and complex systems. - Technical. Depending on the domain this gets into the nuances of a language or technology. These range from a rapid fire style that’s asking for descriptions of various HTTP status codes to a deeper dive into the TCP/IP protocol to discussing high level networking or the nuances of particular language or application versions. The goal here is to quickly get a sense if the person knows what they’re talking about or whether there’s only a superficial knowledge. Used alongside some of the other approaches this is a solid way of gauging the accuracy of a resume. - Code test. This is the typical code test where you’re given a problem, a computer, and a time limit. Hopefully the problem is simple enough and offers a variety of implementation options that allow you to see the thought and decision process. The most successful ones involve introducing the problem and making sure everyone is on the same page and an occasional check-in to answer and address any questions. The difficulty here is that not everyone can code in a high pressure environment and you may be missing out on a lot of great people that are strong coders but don’t do well on code tests. - Pair programming. A variation of the code test that tries to make it a bit less stressful is to do a pairing exercise where you’re both working on a problem and bouncing ideas off of one another. The goal is to have the candidate do most of the coding and you act as a sounding board since you do want to get a sense of the person’s coding ability. This also serves the benefit of showing you whether you’d get along since it’s a quick glimpse into how you’d work together on the same project. - Feature addition. This is working with someone on the production code to build out a simple feature. Pair programming is usually done on a predefined problem but this takes an actual problem you’re working on and turns it into a pairing exercise. I haven’t seen this done much since it usually requires a ton of context to ramp someone up to your codebase and every person ends up with a different experience. Nice in theory but I don’t know how well it works in practice. - Unit test fixing. I’ve seen this done a few times and it’s usually set up as a fully written project with a few broken unit tests. It’s your job to go through the underlying code and fix it to get the unit tests to pass. I like this one since it tests something everyone needs to do - go through someone else’s code, understand what it’s trying to do, and make some enhancements without breaking the existing functionality. This requires a fair amount of setup work, especially if you need to support multiple languages, but it’s a great way of testing the skills any developer should have.  The goal of every interview is to make sure the person you’re interviewing will be successful at their job. The best way is to give them something that’s as close to the job itself as possible. If they succeed it’s likely they’ll be able to do the job itself and if they fail it’s likely they wouldn’t be able to cut it. The implication is that you need to structure your process to optimize for this. No single one of these will tell you everything you need to know so it’s important to mix and match to find the combination that gives you the most confidence in your process.",0,1,2017-05-17,5,"interviewing, hiring, recruiting, software engineering, code tests",866,The different flavors of engineering interviews
26,0,There's a common refrain for startups to just ship it and it's more true now than ever since we are constantly inundated with new information.,"#product,#society","{% include setup %} I don’t know whether it’s the pace of modern life or something else but I no longer have a good intuition for recent events. Things that happened a few months ago feel as if they happened a year ago and things that happened a year ago feel as if they happened multiple years ago. An example of this was Twitter increasing a tweet’s character limit to 280. Try to guess when they increased the limit?  I assumed it was over a year ago but turns out it was officially [rolled out](https://techcrunch.com/2017/11/07/twitter-officially-expands-its-character-count-to-280-starting-today/) less than year ago on November 7, 2017. More importantly, and what gets at the subject of the post, there was so much back and forth and consternation and yet now no one really cares.  Talk to anyone involved in startups and you’ll often get advice to just ship and to release early and often. There are always exceptions but these days there really is no reason not to unless you’re working on an extremely risky or litigated field. If you’re worried about consumer reactions or attention there’s no need since they will likely forget everything in a week anyway. We’re so inundated with information now that nothing sticks and we end up getting accustomed to a new normal every day.",1,2,2018-10-25,4,"twitter, startups, lean startup, information",227,Just ship it
18,0,There's a pretty cool visualization that shows your browsing history through the sites' favicons. Interest patterns emerge.,#dataviz,"{% include setup %} I came across a neat Chrome extension called [Iconic History](http://shan-huang.com/browserdatavis/) that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search. My usage has also changed since I stopped using Gmail for my personal email and started using Fastmail. There’s the occasional new site but for the most part I’m a creature of habits - email, search, facebook, and Hacker News constitute the bulk of my internet activity. I’m honestly surprised by how much activity is taken up by a few sites. I suspect most people are similar - a few sites make up the majority of the page views. It would be great to see what this looks like for others and see if any general patterns emerge - I’m sure almost everyone people will have some mix of email, search, and Facebook but I’m curious to see what the outliers are.",2,1,2014-03-25,3,"data visualization, browser history, icons",234,Visualizing my browsing history
23,0,I received an offer for a website optimiziation offer and decided to take them up on it and see where it went.,#meta,"{% include setup %} I’ve been getting a stream of offers to help “optimize” my site and decided to follow through with one and see where it went. The general pitch is to call out existing errors and problems and offer a service to help fix the variety of errors and improve my search ranking. Here’s the text of the most recent email:     Dear business owner of dangoldin.com,   How is it possible that your website is having so many errors? Yes, most of the people share their anger and frustration once they get my email.   Now, I will show you the number of broken links, pages that returned 4XX status code upon request, images with no ALT text, pages with no meta description tag, not having an unique meta description, having too long title, etc., found in your dangoldin.com.   I have a large professional team who can fix all the above issues immediately at an affordable price. I guarantee you will see a drastic change in your Google search ranking once these are fixed.   If this is something you are interested in, then allow me to send you a no obligation audit report.   Best Regards,   XXXXXX     Clearly this is not personalized as every mention of dangoldin.com can be replaced with another domain and have the same effect. The language doesn’t feel natural and is awkward but the author does include a series of technical words and phrases to showcase his knowledge. I wonder if they have A/B tested the hell out of different copies and ended up coming up with this. I recall reading that Nigerian scammers purposely use non-standard English as a way to identify even better marks. If they wrote in perfect prose they’d end up luring many more people into the top of their funnel that would end up backing out later. Much better to get a smaller set of people hooked that have a higher conversion rate.  The day after my reply I received a PDF titled “ Website Analysis for dangoldin.com .” It’s surprisingly well-fleshed out and contains a series of best practices and stats that my site is ranked on. It has the obvious ones such as number of pages indexed by Google as well as some esoteric ones, such as whether it’s listed on “DMOZ.” The analysis ended with a search ranking plan as well as the pricing page with 3 potential plans ranging from $300 to $900 a month. My gut is that this was a dual effort between code and humans with the bulk automatically generated and a human polishing it up. I’m confident that the human component was outsourced given the language and the fact that the firm has presence in India. Generating this was probably cheap but not insignificant and does make me wonder what their conversion rate is. The $300 price point seems high but is in line with the website optimization services out there so maybe these guys have figured out their customer acquisition model.",1,1,2016-06-11,3,"spam, site optimization, marketing",518,Following up on a website optimization offer
31,0,When I migrated my blog to AMP I didn't update the RSS XML generation which caused images to not render since AMP has them in a proprietary amp-img tag.,#code,"{% include setup %} A little less than a year ago I [migrated](http://dangoldin.com/2016/09/05/ampifying-my-blog/) this blog over to [AMP](https://www.ampproject.org/) which required a lot of small tweaks - ranging from automating the markup changes to getting the Disqus plugin to work. One thing I didn’t get a chance to finish until earlier this week was supporting the RSS feed. This blog is hosted on GitHub pages which is powered by Jekyll and comes with a pretty powerful templating engine. One of the predefined templates was the ability to generate an RSS atom feed. It worked by taking the content of each post, escaping it, and concatenating them together into a massive XML file.  With standard pages this approach worked great but fails with AMP since AMP has it’s own markup that isn’t supported by common RSS readers. The biggest problem is that AMP uses the &lt;amp-img&gt; tag to include images while standard HTML has the simple &lt;img&gt; tag. This led to the text content loading fine but none of the images making their way through. Luckily, Jekyll provides a simple way of creating your own template tags and I came up with a super simple one to just do a global replace of amp-img with img. This isn’t a perfect solution since the amp-img tag contains some additional functionality but it’s good enough for getting the images to make their way back into the RSS feed. While testing the XML generation I also realized that the img tags were still using relative paths which is fine when consumed via the browser but not so great when done via an RSS reader. A tag later and this is fixed. I’m hopeful that this is the last of my blog’s AMPification but if there’s anything odd still happening do let me know.  {% highlight ruby %} module AMPToImgFilter   # Very naive for now   def amp_to_img(html)     html.gsub! 'amp-img', 'img'   end end {% endhighlight %}  {% highlight html %}  {% raw %}    {{ post.content | amp_to_img | relative_to_absolute_paths | xml_escape }}  {% endraw %} {% endhighlight %}",2,1,2017-06-20,4,"amp, rss, atom xml, reader",358,Getting AMP into RSS
17,0,We wrote a simple application from Node to Netty and this describes our migration on AWS.,#devops,"{% include setup %} A fun little exercise I had to do was rewrite a simple application from Node.js to Netty to fit into the rest of our stack. The rewrite took a couple of days but the deployment and testing was critical to get right so I wanted to share our approach. To provide some context, the application was an HTTP server that handled ~1,000 requests a minute with each request spawning at most three more to pull in more data.  Make it work. Make it right. Make it fast. The statement is attributed to Kent Beck but I've become a huge fan and try to approach all projects with this mindset. In our case, having a product deployment already available made it simple to test. We would just run some production requests against our new code and compare the responses. If they matched then we knew we did the right thing. Note that if you're on AWS and running an ELB, a simple way of getting HTTP requests without touching any code is through  access logs . Amazon will store each of the requests to the ELB to a file on S3 that you can then easy download and parse.  The next step was making sure it could handle the same load as the old application. The first thing was to run a series of  Apache benchmarks (ab)  so we can get a rough idea of the concurrency and performance on a single request. As part of this test we turned off the caching layer in our application to hobble it as much as possible since if it could handle that, it could handle anything. The final step was using a  script to simulate the requests in the ELB access against the new server and see how it behaved.  The deployment turned out to be the easy part. All we had to do was launch a new server behind a new ELB and swap the DNS record to point to it. We did this for a few short periods before swapping it back so we could do a few final checks before leaving the DNS record pointing to the new ELB. After a couple of days we eliminated the old ELB and instances completely. The chart below shows the transition between the two load balancers.",2,1,2015-01-29,4,"devops, migrating, Node.js, netty",419,Migrating a simple HTTP application on AWS
22,0,While migrating my blog to AMP I wrote a few scripts to automate going from img tags to amp-img tags.,"#meta,#code","{% include setup %} Over Labor Day weekend I migrated my blog to use [AMP](https://www.ampproject.org/) but the first version was definitely a work in progress. One big item I needed to take care of was converting all my images to be AMP compatible by replacing &lt;img&gt; tag with &lt;amp-img&gt; along with the image width and height. I ended up writing a quick Python script to go through each of my posts, find each &lt;img&gt; tag, get the image’s dimensions, and then replace the original tag wit the AMP version. Unfortunately, I ran the script without too much testing and forgot to add closing tags which caused some of the content to go missing.  The solution was to write another script that once again went through every post but instead of replacing every img tag with an amp-img tag it found every amp-img referenced and added a closing tag in case it didn’t have one. These two scripts combined ended up fixing most of the AMP issues but I’m sure there are still a few posts that got warped so if you notice any please let me know.  In the spirit of constantly shipping the code is up on [GitHub](https://github.com/dangoldin/ampification) but is simple enough to not need a ton of polishing. Note that it’s not very robust and has some assumptions based on my blog structure so I would test it thoroughly before applying it to your posts.",2,2,2016-09-08,4,"AMP, img to amp-img, jekyll, amp blog",244,AMP migration scripts
32,0,I went through each repo on my GitHub account which was a nice walk down memory lane and wanted to go over each of the projects briefly talk about each one.,#code,"{% include setup %} Writing up my old projects got me browsing through my GitHub account to see what else I've worked on. Some I'll update when I get a good idea while others I completely forgot until going through the list. I noticed two big themes when going through the list. The first is how much nicer it is to have projects that are in static HTML/CSS/JavaScript since they can be hosted publicly on GitHub and don't require any setup or configuration to start using. The other is how many third party libraries or APIs I've used and how much more difficult everything would have been had I had to build everything from scratch. If anyone is interested in forking and ressurecting some of these I'll be glad to polish it up.  - [Twitter archive analysis](https://github.com/dangoldin/twitter-archive-analysis): At some point Twitter announced that they would allow you to export your entire Tweet history and this was a quick pass at a toolkit to analyze the data and do a few simple visualizations. I wrote a blog post about it [here](/2013/01/19/making-sense-of-my-twitter-archive/).  - [Instagram download](https://github.com/dangoldin/instagram-download): A couple of years ago Instagram changed their policies so I decided to close my account. Before that I needed a way to export my photos. This was a simple app/script that spawns a very basic OAuth web application in order to authenticate you with the Instagram API which allows you to export all your photos.  - [Yahoo fantasy football download](https://github.com/dangoldin/yahoo-ffl): I've been in a fantasy football league for almost a decade now and every year I update this script to scrape the data from the Yahoo fantasy football site. The goal was to use this data to develop a statistical model to help me manage my team but I haven't gotten around to starting that yet. Maybe next year!  - [Runkeeper stats](https://github.com/dangoldin/runkeeper-stats): A pretty simple R script to analyze and map the data that can be exported from RunKeeper. I wrote a blog post about it [here](/2014/01/04/visualizing-runkeeper-data-in-r/).  - [Site analysis](https://github.com/dangoldin/site-analysis): I was frustrated by the slowness of various sites and decided to write a script to see what was taking sites so long to load. This analyzes the top Alexa sites and figures out how much data they're loading and of what types - CSS, JavaScript, images, etc. I wrote a blog post about it [here](/2014/03/09/examining-the-requests-made-by-the-top-100-sites/).  - [Relay Rides analysis](https://github.com/dangoldin/relay-rides-analysis): This script analyzes the JSON search results of Relay Rides (now Turo) and combines it with data retrieved using the Edmunds API to identify the cars that have the best financial return. The return is calculated by looking at the estimated price of the car and dividing it by average money earned per day. The obligatory blog post is [here](/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/).  - [Jersey City parking zone mapper](https://github.com/dangoldin/jersey-city-open-data): Jersey City has a ridiculous PDF that lists the streets and addresses that belong to each zone. I painstakingly extracted, cleaned, and geomapped the data in order to visualize the zones on a map.  - [JS tools](https://github.com/dangoldin/js-tools): Probably my most commonly used code. This is a series of tools hosted on ... that provide some basic utilities that help me throughout the day. The most useful lately has been a way of comparing SQL table schemas but it has a bunch of others.  - [Citibike station directions](https://github.com/dangoldin/citibike-station-directions): A web app that breaks every trip down into a walk to a Citibike station, biking from Citibike station to Citibike station, and another walk to the final destination.  - [Meerkat crawl](https://github.com/dangoldin/meerkat-crawl): To help a buddy out I started mapping out the network relationships between users on Meerkat but quickly ran into a scaling issue. I got to around 5 million connections and wasn't able to figure out how to actually visaulize it in a clean and timely way.  - [Yet another Hacker News reader](https://github.com/dangoldin/yahnr): My attempt at modifying the Hacker News experience to show the top stories over a rolling 24 hour period. This was a good exercise in messing around with pseudo static sites where the content is solely hosted on S3 with a script to push new files every few minutes.  - [Python tools](https://github.com/dangoldin/python-tools): A series of Python scripts that I've writtent to deal with various minor issues. I have a ton more that I'll add to this repo when I find them.  - [MySQL class](https://github.com/dangoldin/mysql-class): I taught a MySQL class at [Coalition for Queens](http://www.c4q.nyc/) and this is the series of slides used.  - [Redirector](https://github.com/dangoldin/redirector): A tiny Node.js app that acts similar to the ""Switcheroo"" Chrome browser extension but able to work across other browsers. It requires a bit of manual set up but then uses the hosts file to intercept web requests and redirect them to another host. A quick write up [here](/2015/02/07/url-redirection-app/).  - [Oyster books crawl](https://github.com/dangoldin/oyster-books-crawl): This was a series of scripts that crawled the Oyster API to pull the available books and then analyzed them to find patterns. A bit sad that this script outlived Oyster itself. I wrote a blog post about it [here](/2014/03/16/fun-with-the-oyster-books-api/).  - [Taxi pricing](https://github.com/dangoldin/taxi-pricing): The goal here was to compare the pricing of taxis across various cities. The two primary dimenisons used were cost per a minute waiting and cost per a mile of driving. Using this information one can then see how different cities and countries value labor costs. The analysis is written up [here](/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/) and [here](/2014/01/09/taxi-prices-around-the-world/).  - [Meeting place finder](https://github.com/dangoldin/meeting-place-finder): A simple script that uses the Google Maps API to come up with an ideal meeting place for a group of people that ensures everyone has the same commute time.  - [Lincoln text analysis](https://github.com/dangoldin/lincoln-text-analysis): An old project that read in the text of Abraham Lincoln's speeches and did a few visualizations of the text. I wrote a blog post about it [here](/2013/02/12/analysis-of-lincolns-words/).  - [Lawdiff](https://github.com/dangoldin/lawdiff): I participated in a journalism meets tech hackathon and this was my team's entry. We looked at proposed state laws and compared them against other states to identify laws that were most likely written by a special interest group. We had a number of false positives but were able to find a bunch of laws that were nearly identical despite being introduced in multiple states.  - [IMDB](https://github.com/dangoldin/imdb): Another early analysis project where I scraped some IMDB data in order to analyze the average age of actors and actresses over time. This came after I watched Misrepresentation and wanted to show that actors and actresses are treated differently in the movie industry. I wrote a blog post about it [here](/2012/05/23/trend-of-actor-vs-actress-age-differences/).  - [Jeopardy parser](https://github.com/dangoldin/jeopardy-parser): I found an open source crawler of Jeopardy clues and made a few updates to make the code multi threaded and able to crawl significantly faster. I then worked with my wife to turn this data into a simple web app that displayed random Jeopardy clues for us to test our knowledge.  - [Map fun](https://github.com/dangoldin/map-fun): Similar to the RunKeeper analysis above this was another pass at summarizing my running data over multiple years but this time leveraging GitHub's map tools. I wrote a blog post about it [here](/2015/01/18/fun-with-githubs-map-tools/).  - [Node toys](https://github.com/dangoldin/node-toys): This was the start of me messing around with Node.js and getting a feel for the framework. One of the fun projects I used it for was evaluating recursive functions using HTTP redirects. I did a quick write up of it [here](/2014/12/31/redirect-recursion/).  - [AWS tools](https://github.com/dangoldin/aws-tools): A super simple script that downloads a list of EC2 instances and then prints the IP, name, and address. The end goal was to make it simple to connect to an instance without going through a manual process of figuring out the appropriate address to use. I ended up not using this that much since it was easier for me to maintain a list of aliases and hosts in a text file. A very basic write up [here](/2014/11/09/some-simple-aws-tools/).  - [Wikilearn](https://github.com/dangoldin/wikilearn): This was one of my favorite projects. The goal was to analyze a Wikipedia article and come up with a visual timeline of all the dates and events that occured. I used an open source library for the visualization piece but ended up running into all sorts of issues analyzing the Wikipedia text. This is where I got a bunch of exposure to NLP but still wasn't able to make it work.  - [Mixergy mp3 download](https://github.com/dangoldin/mixergy_mp3_download): I subscribe to the Mixergy feed and this was my attempt at a script that would just download the available mp3 files and store them for future listening. I'm sure the HTML code of the page has changed since then so the code is most likely broken.  - [Geo data](https://github.com/dangoldin/geo_data): A one of script I wrote to crawl a site and generate a mapping of ZIP codes to counties. I'm not sure why I needed this but I suspect it was for some sort of data analysis project.",41,1,2015-11-12,5,"code, projects, analysis, open source, github",1543,My old projects
35,0,Two different styles of coding are top down and bottom up. I prefer the top down approach since it lets you identify and resolve issues before spending a ton of time on their implementation.,#code,"{% include setup %} Over the years, I’ve noticed two distinct coding styles. Some approach problems top down and will stub out the entire solution using dummy values and methods and come up with a naive solution before fleshing everything out properly. Others will instead take a bottom up approach and try to complete each method entirely before moving on to the next one.  Especially for larger problems, I prefer the top down approach. By stubbing out the various pieces it’s easy to see how everything fits together and makes it easy to identify and solve potential issues before investing a ton of effort into a poor implementation. The other benefit is that I start thinking at a systems level and come up with implementations that tend to be more extensible.  The only time I find myself taking a bottom up approach is when the problem is very well defined and I know exactly what the solution is or when I’m working on HTML and CSS. In that case, and especially with my limited skill, there’s so much coupling between the various components that I can’t avoid going linearly through the components. It does make me wonder whether people who have more frontend experience have also adopted the top down approach.",0,1,2014-09-16,3,"coding, development, engineering",212,Top down vs bottom up coding
28,0,AWS S3 had a massive outage today in the US-EAST-1 zone. The silver lining is that it did lead to a few realizations and lessons.,"#devops,#code","{% include setup %} Today was quite a day. S3, the most resilient of Amazon’s services went down for a few hours in the US-EAST-1 zone and led to a series of failures across a variety of services. There are a ton of lessons one should take away from this - ranging from running across multiple availability zones to being integrated with a variety of cloud providers. The challenge is that it’s not easy; especially when you’re small. At that point you have to prioritize building support for a 0.01% chance of massive failure versus a variety of features and product enhancements to drive your business forward.  As always, there is no black and white answer and your approach should depend on your situation. If you’re working in healthtech or finance and need resilience you should focus on resiliency. If you’re just building a proof of concept consumer app you should focus on building a useful product and not worry about dealing with zone failures. Of course there are best practices you can adopt to make your application more resilient (containerization, statelessness, etc) but they shouldn’t be the primary focus.  Yet this experience provided me with two major realizations: the importance of aggressive edge caching and the value of a loosely coupled system. An aggressive edge caching strategy won’t solve all your problems but the more data that’s cached in your user’s browsers or on a CDN the easier your system can handle internal failure. In the case of an S3 failure this meant that our CDN would be able to serve the last available assets rather than having browser make requests to failing endpoints. A loosely coupled systems buys you time which lets the kinks get worked out in the underlying system. In our case this manifested in our ability to keep collecting event data in Kafka despite our inability to persist to S3. Since Kafka is designed to be a rolling window of events we were able to just consume the events from the last uploaded time as soon as S3 came back up. The outage even gave us the opportunity to tweak some of our configs (from the wonderful [Secor](https://github.com/pinterest/secor) library) to prove that we could persist our event data to Google Cloud Storage if we needed to.  All in all, it was not the most pleasant of days but it did offer a variety of lessons that do contribute to making our systems significantly more reliable and resilient. Not all of us have the ability to test using a [Simian Army](https://github.com/Netflix/SimianArmy) so for the rest of us we get to learn through production failures.",2,2,2017-02-28,4,"aws, s3, cloud provider, devops",443,Lessons learned from today's S3 failure
13,0,"Adding a database column in SQL is easy, except when it's not.","#devops,#code","{% include setup %} A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.  Normally, this isn’t a problem in most cases since you just write a query to specify the desired column order but it makes doing a simple “SELECT *” more annoying and will break naive jobs that rely on a particular column order.  The accepted solution is to create a new table with the proper structure, migrate the data from the original table while using a default value for the new column, drop or rename the original table, and then rename the new table with the original name. It’s a lot easier to see this in code:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Migrate the data INSERT INTO test_new   SELECT id, name, 0   FROM test;  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  One issue with this approach is that if the table is very large, it will take a long time to migrate the data from the original to the new table. In this case a possible approach is to do it piecemeal - if you know you only need recent data you can migrate a subset of the data first to get the table ready, do the rename, and then migrate the rest. In code again:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Migrate some of the data INSERT INTO test_new   SELECT id, name, 0   FROM test   WHERE id > 1000000;  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- Migrate the rest of the data INSERT INTO test   SELECT id, name, 0   FROM test_old   WHERE id <= 1000000;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  If you know for a fact that it's not important to have the old data immediately available you can opt to rename the original table, create the new table, and only then migrate the data. This allows queries that need to write data to this table to complete immediately. The risk is that the legacy data will only be available after the migration completes. The last code block:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- Migrate the data INSERT INTO test   SELECT id, name, 0   FROM test_old;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  The first scenario handles having columns in the right order but the other two can be useful on MySQL as well when tables are large and performance is critical. For high load databases, transactions can be used to make sure that the renames are done atomically - this will avoid an intermittent query writing to a non existent database. It's surprising how a task as simple as adding a column can evolve into a large problem with a variety of solutions when running into a variety of constraints.",0,2,2015-04-23,6,"postgresql, redshift, mysql, sql, database, create column",739,Adding columns in PostgreSQL and Redshift
23,0,I've seen so many iPhone ads today each pushing a specific carrier and yet there's nothing unique to any of the carriers.,#meta,{% include setup %} I had a bit of a lazy Sunday and spent some time watching some football games. During the commercial breaks I saw iPhone 8 ads sponsored by nearly every carrier with the format being nearly identical: a highlight of the features followed by a mention of the carrier. There was nothing there nudging me towards one carrier or another and they all felt like iPhone ads. I’m not sure if Apple is even contributing anything to these but even if they are I suspect the branding outweighs the investment - especially since an ad viewer may decide to get the new iPhone regardless of their existing carrier.  The only explanation I can think of is that the carriers know full well that this won’t have any impact on people switching carriers and is purely a way to accelerate the upgrade cycle and renew the customer contract. This way they can guarantee a 2 year revenue stream and prevent their customer from switching. This feels like an expensive proposition: people are going to stick with an existing carrier if they upgrade a phone and there’s enough iPhone marketing that there’s no need to run yet another ad campaign.  The real winner here is Apple. They built a phone that works across all carriers yet is compelling enough for these carriers to market it. I wonder if this is a holdover from the original iPhone launch that was only available on AT&T. That actually led to a significant number of people switching to AT&T in order to get the iPhone but today’s world is so different I just can’t see this as the reason.,0,1,2017-10-15,3,"iphone, apple, carrier",276,Carrier specific iPhone ads
25,0,Data analysis needs to be fun and enjoyable to do. Otherwise there's enough rote and frustration to make it too easy to give up.,#meta,"{% include setup %} In order to do any meaningful data analysis you need to have fun doing it. Otherwise it becomes a chore that’s extended by each additional analysis you run and each additional failed attempt at an insight. This requires a positive attitude and enjoying the slow, methodical process of discovery and appreciating each iteration while getting closer to the end goal. The vast majority of analyses lead to no new insight, especially when all the easy stuff has already been figured out, and it’s critical to remain the optimist while appreciating the present.  A key part of this is tools. I have a set of tools I’m intimately familiar with and can manipulate them without much thought. It’s this passive approach and behavior that lets me go through the rote work while simultaneously focusing in on the challenging elements of the problems I’m facing. Fast tools are also a requirement. Tools that allow you to quickly get a result prevent you from leaving the zone and leave you ready for the next attempt.  When 80% of the work is rote data manipulation it’s important to not burn out while getting to the 20%. To be successful you need to find the fun in both the data manipulation and the analysis.",0,1,2016-03-21,1,data analysis,214,Data analysis needs to be fun
29,0,Lyft offered a great promotion in NYC - 50 free rides. It's possible to chain a bunch of short rides in order to get a long ride for free.,#meta,"{% include setup %} In honor of their NYC launch,  Lyft  came up with an awesome promotion - 50 free rides, up to $25 each, over the next couple of weeks. This had the desired effect - a bunch of my friends are giving it a shot but since everyone else is doing the same it's difficult to find an available car. And when you do get a car you end up paying the peak demand rate rate.  One way to get around this is to break down a long trip into a series of shorter trips. The driver will have to agree to this but they have an incentive to do so since they will not need to find another passenger and will earn a base fare every trip. A minor inefficiency is that the driver will need to stop to mark the trip as completed and confirm the next trip. It's also easy to go over $25, especially in prime time, so you need to stop more frequently than you think you need to. If only Lyft or Uber showed the real time cost of a trip you'd be able to stop at the optimal times.  We used this approach yesterday when going from midtown Manhattan to Brighton Beach and were able to do it with 6 Lyft trips. The first two ended up going over the $25 rate so we became aggressive stoppers after that. The driver mentioned that a bunch of his passengers used the same approach to get free rides to Long Island and Connecticut. I'm just waiting for Lyft to plug this loophole - a simple way would be to not allow the same consecutive driver/passenger pair.",1,1,2014-08-03,3,"lyft, uber, car sharing",285,Getting the most out of Lyft's 50 free rides
36,0,Just a thought exercise on a new way to build software applications. To support complicated worfklows and systems it's easier to build support for CSV file uploads rather than trying to find the optimal design.,#product,"{% include setup %} Something that I've been thinking about ever since I worked as a product manager focused on internal tools is being able to run a product entirely through CSV file uploads. Instead of building a UX designed to handle bulk operations and complicated workflows you build support for file uploads and handle the business logic entirely on the backend. The motivation is that it’s extremely difficult to build a UI that’s going to be as powerful and flexible as a simple CSV file, especially when outside tools, such as Excel, can help generate these files.  This approach also has the nice property of decoupling the input from the core business logic. Over time, tools and interfaces can be built that are optimized for specific use cases without having to modify any of the backend. Effort can be spent on improving workflows that are already being done rather than building support for workflows that may or may not be common. Permissions and controls can also be added that make the application accessibility to a wider range of users.  Many companies spend tons of time building advanced tools that will never be as powerful as Excel paired with a simple file upload. Workflows vary significantly across users and most products impose a single approach. Why not build more general tools that take advantage of the unique work styles?",0,1,2013-12-21,5,"product management, csv, products, design, software",230,CSV powered products
21,0,How can eBay still not allow people to choose the timezone for a start time? It's currently stuck in PDT.,#design,"{% include setup %}      I recently needed to sell something on eBay and encountered an issue I thought they would have taken care of by now. Apparently you can pick the start time for an auction but it has to be in PDT - there’s no way to choose another time zone. The change is trivial and one would think that a $60B company would be able to support multiple time zones in their core product. Someone brought this up in  the forums  in 2012 and it turns out that time zone support is only present in the forum to allow users to see posts with a local time.  Whenever I see seemingly obvious UX anti-patterns it makes me think there must have been an ulterior motive. In this case, I suspect not having a time zone may lead to a smoother distribution of auction end times which keeps product demand high and can distributes bids more evenly throughout the day. Another reason may be the desire to reduce risk - time zones are difficult to get right and it’s possible that eBay doesn’t want to expose themselves to the liability. The most likely reason may be that they are just not investing heavily in the use experience. They’re already the market leaders and it may make more sense to focus on marketing and selling rather than on making it easy to list a product. If someone’s already started to list a product it’s unlikely that the lack of a timezone will cause him to change his mind - it didn’t in my case.",1,1,2014-06-12,3,"design, ebay, user experience",289,An eBay design rant: timezone support
14,0,Some visualizations done using R on top of my RunKeeper data from 2013.,"#dataviz,#code,#R","{% include setup %} What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to  my GitHub . If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.  PS. It's great when web services allow users to export their data and wish more would start doing the same.    	                               	Cumulative distance. You can see a few flat areas in February and November when I took a break due to some minor injuries.                                                     	Distance run by month. Unexpected drop in November due to a break but pretty solid otherwise.                                                     	Distance run by week. Not much new information here that's not covered in the monthly graph.                                                     	Cumulative time. Very similar shape to that of cumulative distance.                                                     	Cumulative time vs distance. Superimpose one on top of the other to compare the shapes. Started off slowly but started getting faster in October.                                                     	Speed by run. I got significantly faster in October but slowed down again in December.                                                     	Speed by month by distance quantile. The idea here was to look at my improvement in speed but controlling for distance. Echoes the previous chart showing my speed improvement in Oct for the longer distances.                                                     	Speed distribution by distance quantile. Another view that looks at the distribution of my speeds for all runs in a given distance quantile. Not much here but I was expecting to see that I'd have a faster pace for shorter runs.                                                     	Speed vs Distance scatter plot. Another way to look at the relationship between speed and distance but not many new insights here. Slight correlation between speed and distance. This is pretty much because as I got faster I started doing longer runs. It'll be interesting to see how this changes in 2014.                                                     	Speed vs Distance scatter plot clustered. An attempt at clustering the runs by speed and distance. In this case they were basically clustered by distance since the speed didn't vary significantly.                       {% highlight r %} library(ggplot2) library(grid) library(gridExtra) library(reshape) library(scales) library(lattice) library(ggthemes)  data = read.csv(""cardioActivities.csv"", check.names=FALSE)  summary(data)  data <- data[order(data$Date),] # Sort ascending by date data$ymd <- as.Date(data$Date) data$month <- as.Date(cut(data$ymd, breaks = ""month"")) data$week <- as.Date(cut(data$ymd, breaks = ""week"")) + 1 data$distance <- data$'Distance (mi)' data$distance_total <- cumsum(data$distance) data$speed <- data$'Average Speed (mph)' data$time_hours <- data$distance/data$'Average Speed (mph)' data$time_hours_total <- cumsum(data$time_hours) data$time_mins <- data$time_hours * 60 data$time_mins_total <- cumsum(data$time_mins) data$distance_total_norm <- data$distance_total/sum(data$distance) data$time_hours_total_norm <- data$time_hours_total/sum(data$time_hours) data$qs <- cut(data$distance, breaks = quantile(data$distance), include.lowest=TRUE) # Quantile data by distance run  # Generate a new data frame by qs and month to make plotting easier data.qs_monthly <- ddply(data, .(qs, month), function(x) data.frame(distance=sum(x$distance), time_mins=sum(x$time_mins))) data.qs_monthly$speed <- data.qs_monthly$distance/(data.qs_monthly$time_mins/60)  # Summarize the data by qs to make plotting easier data.summary <- ddply(data,~qs,summarise,mean_speed=mean(speed),sd_speed=sd(speed),mean_distance=mean(distance),sd_distance=sd(distance))  # Cluster each of the runs by speed and data m <- as.matrix(cbind(data$speed, data$distance),ncol=2) cl <- kmeans(m,3) data$cluster <- factor(cl$cluster) centers <- as.data.frame(cl$centers)  # Normalize cumulative distance and time data.normalized <- melt(data, id.vars=""ymd"", measure.vars=c(""distance_total_norm"",""time_hours_total_norm"")) data.normalized$variable <- revalue(data.normalized$variable, c(""distance_total_norm""=""Distance"", ""time_hours_total_norm""=""Time""))  png('rk-speed-vs-distance.png', width=800, height=800) ggplot(data=data, aes(x=speed, y=distance)) +   geom_point() +   theme_economist() +   scale_color_economist() +   geom_abline() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Distance') +   ggtitle(""Speed vs Distance"") dev.off()  png('rk-speed-vs-distance-clusters.png', width=800, height=800) ggplot(data=data, aes(x=speed, y=distance, color=cluster)) +   theme_economist() +   scale_color_economist() +   geom_point(legend=FALSE) +   geom_point(data=centers, aes(x=V1,y=V2, color='Center'), size=52, alpha=.3, legend=FALSE) +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Distance') +   ggtitle(""Speed vs Distance - Clustered"") dev.off()  png('rk-speed-month-qs.png',width = 800, height = 600) ggplot(data = data.qs_monthly,   aes(month, speed)) +   geom_line() +   facet_grid(qs ~ .) +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle(""Speed by Month"") dev.off()  png('rk-distance-month.png',width = 800, height = 600) ggplot(data = data,   aes(month, distance)) +   stat_summary(fun.y = sum,     geom = ""bar"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle(""Distance by Month"") dev.off()  png('rk-distance-week.png',width = 800, height = 600) ggplot(data = data,   aes(week, distance)) +   stat_summary(fun.y = sum,     geom = ""bar"") +   scale_x_date(     labels = date_format(""%Y-%m-%d""),     breaks = ""4 week"") +   xlab('Week') +   ylab('Distance') +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Week') +   ylab('Distance') +   ggtitle(""Distance by Week"") dev.off()  png('rk-speed-distribution-qs.png',width = 800, height = 600) ggplot(data, aes(speed, fill=qs)) +   geom_density(alpha = 0.5) +   geom_vline(aes(xintercept=mean_speed), data=data.summary) +   facet_grid(qs ~ .) +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Density') +   ggtitle(""Speed Distribution by Distance"") dev.off()  png('rk-distance-cumulative.png',width = 800, height = 600) ggplot(data=data, aes(ymd, distance_total)) +   stat_summary(fun.y = sum, geom = ""line"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle(""Cumulative distance"") dev.off()  png('rk-speed-daily.png',width = 800, height = 600) ggplot(data=data, aes(ymd, speed)) +   stat_summary(fun.y = sum, geom = ""line"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Speed') +   ggtitle(""Speed by Run"") dev.off()  png('rk-time-cumulative.png',width = 800, height = 600) ggplot(data=data, aes(ymd, time_hours_total)) +   stat_summary(fun.y = sum, geom = ""line"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Time (hours)') +   ggtitle(""Cumulative Time"") dev.off()  png('rk-time-vs-distance-cumulative.png', width=800, height=800) ggplot(data.normalized,   aes(x=ymd, y=value, colour = variable, group=variable)) +   geom_line() +   theme_economist() +   scale_color_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   theme(legend.title=element_blank()) +   xlab('YMD') +   ylab('') +   ggtitle(""Time vs Distance (normalized)"") dev.off() {% endhighlight %}",1,3,2014-01-04,5,"runkeeper, stats, R, running, visualization",1342,Visualizing RunKeeper data in R
30,0,There's an uncanny valley when it comes to representing humans but there's a similar item at play when it comes to advertising as it gets more and more native.,#product,"{% include setup %}            Old Spice ad mocking the uncanny valley (Credit:  Gizmodo )     The  uncanny valley  is this idea that although we keep getting better at depicting people through technology, a few small kinks ruin everything and make people feel repulsed compared to an obvious imitation. Another way to explain it is that we’re a lot more comfortable with cartoon characters that are obviously fake than pseudo-realistic video game characters that look real but have non-human behavior or expressions.  I think this also exists in advertising. I work at TripleLift which allows us generate ads that look and resemble a publisher's website. This means that if a publisher's site has a feed layout where each image is 300x300 with a particular font and typography we’ll use the same style for our ad. At the same time, we make it a point to include a ""Sponsored"" overlay, apply a brand logo, and redirect to the advertiser's website upon a click. Our goal isn't to obfuscate the ad but rather give a publisher an effective way to monetize their content without having to resort to traditional, distracting banner ads that take up the entire page and hurt the consumer experience. The goal is to complement the user experience by providing great looking ads that are relevant to the audience.  It’s likely we’d be able to increase short term performance if we start obfuscating the fact that it's an ad but in addition to being immoral it will fall into some form of uncanny valley. People aren’t stupid and will see something’s off if an ad is pretending to be content. The end result benefits no one and sets advertising back. The site visitor is pissed off by the experience, the publisher loses integrity, and the advertiser may get better vanity metrics without deriving any value. Native is a much better ad format when done right but it comes with risks that need to be properly handled by the industry or we’ll end up with a  depleted field .",3,1,2015-04-12,2,"native advertising, uncanny valley",378,The uncanny valley of advertising
23,0,Having global roaming is a game changer when you're on vacation these days since so many services are now accessed via phone.,#meta,{% include setup %}      It’s amazing how quickly we become entitled to new technological advances. Back in 2008 Louis C.K. had a bit on the Conan O’Brien late night show about airplane wifi and how despite the miracle of actually having wifi on an airplane we still have the gall to complain about the speed.  My modern equivalent is global roaming. I have T-Mobile which offers free global roaming as part of their standard plan and it’s amazing being able  to travel that way. So much of the modern world’s amenities depend on being online at any time that I can’t imagine traveling without it. Using Uber or Lyft would be significantly more difficult. Same for the electric scooter rental apps that require scanning a QR code. Or just being able to quickly look up your location to figure out where you are and how to get to your destination.  All of these are surmountable by findign a nearby wifi spot or talking to a stranger but it’s just so much more convenient with a constantly-on internet connection. And despite T-Mobile offering global data for free I can't help but complain that it's just too slow at the offered 2G speeds. I can still do everything I need but it just takes longer and I’m not used to it. Yet only a few years ago I remember having to constantly be on the lookout for places with wifi just to be able to go online. It really is the modern day version of slow plane wifi.,0,1,2018-12-21,4,"wifi, global roaming, traveling, internet connectivity",270,Global roaming
16,0,I've been volunteering teaching AP Computer Science and wanted to share some thoughts and experiences.,#meta,"{% include setup %} This year, I started volunteering at a program called  TEALS . The long term goal is to improve computer science education in the United States by having tech professionals volunteer their time to teach computer science classes in schools that want to offer computer science classes but don’t have the necessary teachers. Over time, the goal is to have the in-service teachers in each class learn the material so that they will be able to teach it in the future. Currently, the program exists in 65 high schools across 12 states and offers both Intro to Computer Science and AP Computer Science but I’m looking forward to seeing it expand nationwide and into middle and elementary schools.  Throughout the year, I plan on documenting my experiences remote teaching at a high school in Kentucky as well as sharing the lessons I’ve learned. So far, it’s been a little more than a week and I already developed a much bigger appreciation for teachers and the effort required. The most time consuming piece so far has been preparing daily lessons that balance the requirements of the AP test and everyone’s skills and interests while still being engaging enough when delivered via a video chat. I also discovered the difference between lecturing and teaching: my initial approach was to just go through the prepared slides but am now spending a lot more time thinking about tricky concepts and the exercises that will both get the point across and keep students engaged. The best approach so far has been starting a class with a brief explanation of the concepts, going through some tricky exercises, and then diving into code to put it all together.  If you have computer science experience this is a awesome program to volunteer for so please feel free to reach out to me if you have any questions.",1,1,2013-08-18,4,"TEALS, AP Computer Science, teaching, high school",316,On Teaching AP Computer Science
17,0,Political parties are just like product bundles. By looking at one we can understand the other.,#meta,"{% include setup %} I rarely write about politics but it’s an election year and I had an interesting realization. Political parties are just like product bundles. We each have our own issues and policies we’re passionate about but it’s impossible to find a politician, less a party, that has the same views we do. Instead we have political parties that take a few issues and policies and try to wrap them up in a bundle hoping to appeal to enough people to win an election.  Reading the Wikipedia [article for product bundling](https://en.wikipedia.org/wiki/Product_bundling) makes it obvious how closely it fits political parties. From Wikipedia:    Bundling is most successful when:    There are economies of scale in production.   There are economies of scope in distribution.   Marginal costs of bundling are low.   Production set-up costs are high.   Customer acquisition costs are high.   Consumers appreciate the resulting simplification of the purchase decision and benefit from the joint performance of the combined product.   Consumers have heterogeneous demands and such demands for different parts of the bundle product are inversely correlated. For example, assume consumer A values word processor at $100 and spreadsheet processor at $60, while consumer B values word processor at $60 and spreadsheet at $100. Seller can generate maximum revenue of only $240 by setting $60 price for each product—both consumers will buy both products. Revenue cannot be increased without bundling because as seller increases the price above $60 for one of the goods, one of the consumers will refuse to buy it. With bundling, seller can generate revenue of $320 by bundling the products together and selling the bundle at $160.       Each of these is a perfect fit for politics. There are huge economies of scale and distribution for political parties. They’re purely information so there’s no marginal cost and the brunt of the cost is in the formation of a party which is incredibly difficult due to the massive network effects and infrastructure required. People have diverse beliefs with great variance on the most important issues and don’t have the depth to know every issue.  It’s no wonder that political parties are so entrenched but this also provides insights on how to dismantle these bundles. We need to examine history and see how previous bundles have been broken down and see whether those solutions can apply to our political system.",1,1,2016-03-26,2,"politics, bundling",404,Political parties are product bundles
32,0,The first version of most tech companies is a scrapped together product but over time they grow and evolve and start coming up with specialized tools and solutions to new problems.,#meta,"{% include setup %} It’s obvious in hindsight but incredible when you experience it but every successful company has to iterate through a variety of tools as it, and its problems, grow. A typical modern tech startup starts by identifying a problem and using a common web framework to quickly come up with the first pass. But as this company grows new problems and situations arise that the initial solution no longer supports. They may end up having a series of asynchronous tasks and need to start using RabbitMQ with that use case. MySQL may no longer be enough and they start offloading their data to Redshift. That off the shelf web framework is no longer performant enough so they have to split it into multiple components and start embracing strong, statically typed languages.  This tool specialization also goes hand in hand with team specialization. A single engineer doesn’t have the time to do everything so startups need to make the short term decisions and focus on the next couple of months in order to grow. Only when they grow does t make sense to find the critical problems and dedicate time to fixing them. And hopefully by that point you have a larger team that can focus on the deeper problems.  For most of us the problems have already been solved and it’s about figuring out how to adapt the solutions for our systems. Sometimes this requires getting an open source library to work. Other times it may require implementing the code from an obscure academic paper. But the real success comes when you run into problems that no one else has encountered. At that point you’re dealing with extremely specialized problems that you were the first to encounter. These are the scale problems that Google and Facebook are solving and every startup hopes to get there some day. In fact, that is the ultimate mission engineering based companies - solving problems that haven’t been encountered yet.",0,1,2016-02-28,3,"tech startups, companies, engineering",328,Tool specialization and growing companies
36,0,It's very easy to want to be involved in every decision and through in your two cents but it's more important to focus on what actually affects you and trust others to do what's right.,#management,"{% include setup %} A critical component in communicating between various teams is knowing who has what responsibility. Especially with driven people it’s easy to have overlap between various functions - product and design; design and frontend engineering; and frontend engineering and backend engineering. This is both good - because it’s able to focus more eyes on a particular problem and provides a new perspective - and bad  - because people may feel that they can’t move quickly enough and don’t want to cede decision making power. Great teams thrive in this environment while poor teams degenerate into a Dilbert cartoon.  One approach that I’ve been preaching is to standardize on the edge points that can act as a form of “contract” between the teams. At those edges it’s great to have the debates and argue the merits of various implementations but beyond that the ownership should lie with the respective team.  An example is to image two engineering teams - one is a full-stack team responsible for the UI and the corresponding API endpoints for a customer facing application and the other is a backend team that uses this information to run the hidden part of the application - the data collection, the web server, and the various third party integrations. In this case a good intersection point would be the database - both teams leverage it and have their own thoughts on what to store and how to structure the schema. The debate should be centered around these questions rather than how each team builds their own components. Once there’s agreement on the database structure each team can go ahead and work independently of the others.  Similarly, a designer can create a series of mocks that can then be debated with the frontend team. The frontend team may push for a different design that will simplify their code and a designer may push for a certain approach that significantly increases the product’s usability. After both teams settled on an approach they can focus on what they’re great at - a designer may focus on getting the visual details perfect while the front end team can start writing the HTML, CSS, and JavaScript.  By focusing on what we actually need to do our jobs and trusting others to do the same we’re able to skip the politics and move quickly. It’s human nature to be curious and want to know everything that’s going on but it’s a massive hit to productivity. Especially at a startup when speed is critical being able to skip the unnecessary meetings, debates, and politics can make the difference between success and failure.",0,1,2015-08-03,3,"management, politics, efficiency",432,Debate what's necessary and no more
27,0,I got my hands on a dump of IMDB data and wrote up the process required to get it into a state that's useful for analysis.,"#sql,#code,#data","{% include setup %} In 2012 I did a [simple analysis of IMDB](http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/) to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great [post by CuriousGnu](https://www.curiousgnu.com/imdb-age-distribution) that did a similar analysis across a larger set of movies but limited to movies since 2000. I reached out and they were kind enough to give me a DigitalOcean instance containing the data already loaded into MySQL. The analysis should be finished up tomorrow but I wanted to write this post up to share the mundane parts of the process. The janitorial part is critically important to an analysis and it’s important to get it right or the results will may be meaningless or even completely wrong. The [NY Times interviewed](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0) a variety of data scientists and came away with the conclusion that 50 to 80 percent of a data scientist’s time is spent cleaning the data. This is no exception and I wanted to provide a sense of the effort and thought that goes into getting data into a state that’s actually useful.              Lucky for me I already had the data loaded and queryable in MySQL. Most of the time the data is scattered all over the place in a variety of different formats that require a slew of scripts to wrangle and manipulate the data into a useful format.  The first task was to get familiar with the data and I started by looking at sample rows from each of the tables. The table names were descriptive but it turned out that some of them were empty. Running a query that calculated the size of each provided a good idea of where the valuable data was - for my analysis the useful data lived in the title, name, cast_info, and person_info tables.  {% highlight sql %} SELECT TABLE_NAME, table_rows, data_length, index_length, round(((data_length + index_length) / 1024 / 1024),2) ""Size in MB"" FROM information_schema.TABLES WHERE table_schema = ""imdb"";{% endhighlight %}              The next step was figuring out the way the tables related to one another. Since the field names were obvious this was extremely straightforward. The only nuances came due to an unconventional naming scheme - for example the title table contains the list of movies but the other tables map to it via a movie_id column. Similarly, the name table contains people but it’s referenced via person_id in other tables. They key part here was starting with a movie I know and confirming that the results made sense. In my case I chose my favorite movie, The Rock, and made sure that the results of my query made sense.  {% highlight sql %}select * from title t join cast_info ci on t.id = ci.movie_id join name n on ci.person_id = n.id where t.id = 3569260;{% endhighlight %}  After getting a feel for the data it was time to actually think about the data necessary for the analysis. To see what was possible I examined the person_info table which contains a variety of information about each person - anywhere from birth and death dates, to spouse, to various names, to height. In my case looking at the birth and height gave me some ideas but I needed to extract these to make them useful. I ended up creating a table for each one and writing a series of queries to populate each one. This required looking at the format of the data in each of the rows and leveraging various combinations of the locate, substring, and cast commands to transform the text fields into something numeric. The birth date was straightforward since it came in two styles - one was just a year and the other was the full birth day with day and month.  {% highlight sql %}insert into person_birth     SELECT person_id, cast(info as UNSIGNED)     FROM person_info     WHERE info_type_id = 21     AND length(info) = 4;  -- Birthdate is full date so just take the year insert into person_birth     SELECT person_id, cast(substring(info, locate(' ', info, 4) + 1, 4) as unsigned)     FROM person_info     WHERE info_type_id = 21     AND length(info) > 4;{% endhighlight %}  Height was a bit more difficult since it came in a variety of formats. Some were in centimeters, while others were in feet, while others were in feet and inches, with a small fraction having partial inches. Each of these required a complicated series of MySQL commands to convert to inches.  {% highlight sql %}insert into person_height     SELECT person_id, cast(replace(info, ' cm','') as unsigned) * 0.393701     FROM person_info     WHERE info_type_id = 22     AND info like '%cm';  -- No inches insert into person_height     SELECT person_id, substring(info, 1, locate('\'', info) - 1) * 12     FROM person_info     WHERE info_type_id = 22     AND info not like '%cm'     AND info not like '%/%'     AND info not like '%""%';  -- No fractional inches (would also work for no inches but playing it safe) insert into person_height     SELECT person_id, substring(info, 1, locate('\'', info) - 1) * 12 + substring(info, locate('\'', info) + 1, locate('""', info) - locate('\'', info) - 1)     FROM person_info     WHERE info_type_id = 22     AND info not like '%cm'     AND info not like '%/%'     AND info like '%""%';  -- Fractional inches insert into person_height     select person_id, cast(base_height as decimal) + cast(numerator as decimal)/cast(denominator as decimal)     from (     SELECT person_id, info, substring(info, 1, locate('\'', info) - 1) * 12 + substring(info, locate('\'', info) + 1, locate('""', info) - locate('\'', info) - 1) as base_height,         substring(substring(info, locate(' ', info, 5) + 1, 3), 1, locate('/', substring(info, locate(' ', info, 5) + 1, 3))-1) as numerator,         substring(substring(info, locate(' ', info, 5) + 1, 3), locate('/', substring(info, locate(' ', info, 5) + 1, 3)) +1 ) as denominator         FROM person_info         WHERE info_type_id = 22         AND info not like '%cm'         AND info like '%/%'         AND info like '%""%'     ) temp;{% endhighlight %}  Finally it was time to dive into the data. The first query I decided to write was to look at the average age of actors and actresses by year. Writing the query and doing a quick explain caused me to add a few indices to improve the performance but even then it still took over 20 minutes to execute. Having used Vertica and Redshift in the past I knew a columnar database would help but I wanted to keep it free. This led me to [MonetDB](https://www.monetdb.org/).  Somewhat remarkably, installing and setting up MonetDB was a breeze but I had a two hiccups migrating the data. One was creating the equivalent tables in MonetDB which had a slightly different syntax from MySQL and required a bit of trial and error to work through. The other was the actual export of data from MySQL in a way that was also easy to load into MonetDB. I ended up settling on a CSV export that also took into account the various ways to delimit, escape, and enclose the different fields. After getting the migration to work on one table it was just a series of copy and pastes to get the other tables over.  {% highlight sql %}-- MySQL export select * from title into outfile '/tmp/title.csv' fields terminated by ',' enclosed by '""' escaped by ""\\"" lines terminated by '\n';  -- MonetDB import COPY INTO title from '/tmp/title.csv' USING DELIMITERS ',','\n','""' NULL AS '\\N'; {% endhighlight %}  I had no experience with MonetDB and didn’t know what to expect with this entire series of steps being a waste of time. I expected some improvement and it turns out the query that took over 20 minutes to run in MySQL was able to run in just over 30 seconds in MonetDB. I was off to the races. I spent the next bit of time QAing the data and dealing with outliers and edge cases. Some were due to mistakes I made - for example not filtering cast members to only include actors and actresses which manifested itself in an actor that lived to be over 2000 years old. This turned out to be a movie about [Socrates](http://www.imdb.com/title/tt1560702/) with one of the writers being Plato. Some simply uncovered weird data - there's a movie, [100 Years](http://www.imdb.com/title/tt5174640/), which is scheduled to be released in 2115 and led to some old actors and actresses. While others were clearly data mistakes - actors who were born after they died, for example [Walter Beck](http://www.imdb.com/name/nm2917761/) who was born in 1988 but passed away in 1964.          Dealing with these was an iterative process. I ended up settling on removing all non actors and actresses from the queries as well as limiting my dataset to movies produced between 1920 and 2015 while also eliminating all combinations where a movie was produced before a birth. These edge cases are infrequent enough that they most likely wouldn’t have had any impact on the results but going through this process gives us confidence in what we’re doing. The next step is actually going through the analysis which I hope to finish up tomorrow.  If you’re interested in the code, it’s up on [GitHub](https://github.com/dangoldin/imdb); and if you’re interested in the data contact me and I can share a snapshot of the DigitalOcean instance that contains the data in both MySQL and MonetDB.",8,3,2016-05-21,6,"sql, data analysis, imdb, movies, actors, actresses",1629,Analyzing IMDB data: Step 1 - Cleaning and QA
21,0,As technology improves and we rely on it more and more passively it's going to cause huge problems for privacy.,#meta,"{% include setup %} I just discovered that Google launched a new  AdWords feature  to help brick and mortar store owners track the effect their online spending is having in the offline world. The way it works is that if a user sees an ad for a particular store or product on their phone and then ends up close (based on the location sharing option in iOS and Android) to the store in question, Google will use that information as a signal that the ad was the cause of the store visit. It’s not supposed to be perfectly accurate but the idea is that with enough data Google can come up with models that can estimate the actual numbers.  Mapping online spend to offline conversions has been the holy grail ever since advertisers started spending online and with the proliferation of smart phones that track everything we’re getting closer and closer to solving that problem. For centuries advertisers had to estimate and have faith that their spend in newspapers, magazines, and public spaces was having an impact with no good way of measuring the results. The ability to track a person’s location is immensely powerful and we’ll start seeing more and more use cases. Using a similar approach it may even be possible to see what effect a billboard ad has: monitor the locations of your ads and if anyone walks by them assign a probability that they’ve seen it. Then if they end up in your store you can assume they got there by looking at the ad. It’s significantly more difficult than that since people see hundreds of ads a day and the result is not always immediate but even having a tiny bit of data is better than none at all.  This should make everyone a lot more concerned about their privacy. In the past it was simple to have distinct lives - home versus work, inside versus outside, online versus offline - but with our attachment to modern gadgets the lines are rapidly blurring. Being aware is the first step in avoiding being tracked but it’s only a short term solution. As technology improves we’ll rely more and more on passive benefits which when coupled with better and faster data mining algorithms will make it very hard to live “off-grid.” We rely on government to preserve our privacy but I worry that we’re moving too quickly for the legislative process to have any real impact. Bitcoin and other distributed systems may be able to counter this decrease in privacy and I’m curious to see what sort of counter-systems they’re able to produce.",1,1,2014-12-20,3,"technology, privacy, society",448,Passive technology and the decline of privacy
20,0,I revoked my personal AWS credentials without realizing that they were being used on a production system. Lessons ensued.,#devops,"{% include setup %} Earlier this week in a fit of security I went into AWS and revoked my old AWS credentials. I assumed that all would be well but unfortunately didn’t realize that my AWS credentials were being used on a production system that wrote data to S3. Before I revoked them I did see that the recent activity contained S3 but assumed it was just me playing around with the AWS CLI. Of course I shouldn’t have had my AWS credentials used on a live system and of course we updated the application to use its own account. At the same time the experienced taught me a few valuable lessons besides not using personal keys on deployed systems:  - Logging should be consistent and accessible. I knew where the logs for this application were but it wasn’t obvious to the rest of the team. More importantly, people shouldn’t have to log in to an instance to access the logs. Instead they should be pushed to a central repository. We have this for some of our applications but not as many as we should. - Symptom based monitoring really works. I came across this phrase when reading Rob Ewaschuk’s [observations on alerting and monitoring](https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit) years ago and it stuck with me. We had a few alerts on this application - ranging from making sure CPU usage was neither too high nor too low, it had a healthy rate of data in, and it had a healthy rate of data out. In this case the application was receiving data and was processing but it failed to upload to S3. This meant that the first two alerts were fine and it was the network out alert that was triggered. This is exactly what you want - in this case that alert encapsulates the others and is really what affects end users. They care that the data is on S3 and don’t worry about the CPU usage or how the instance is receiving the data.",1,1,2018-06-28,6,"aws, devops, aws credentials, security, monitoring, alerting",332,Using personal AWS credentials in production
23,0,For a very long time I've been using Google Docs instead of learning Confluence but I'm glad I finally made the transition.,#product,"{% include setup %} I’ve been a happy user of Google Docs for years now and have yet to find another product that makes collaboration that easy or simple. It’s a well designed product with a ton of shortcuts that make it incredibly easy to be exceptionally productive. I can dive in and quickly leave a few comments as well as assign some todos knowing that the relevant folk will be notified. And for a very long time I’ve been using it for the bulk of my writing - including specs, design documents, and general note taking.  Yet as much as I enjoy using Google Docs I’ve started using Confluence more and more. We use JIRA and since they’re both part of the Atlassian suite of products they’re both nicely integrated. Confluence is a wiki platform so it provides a whole world of functionality that Google Docs just doesn’t offer - and rightfully so. Confluence comes with a bit more structure that makes it easy to create page templates with structured metadata that can then be organized and displayed based on that metadata. Somewhat remarkably, the Confluence search seems to be better than Google Docs - despite Google being the king of search; I really have no idea why the Google Docs search isn’t better. But the one thing that causes Confluence to win is the ability to have much more structured and better organized tasks. One can assign tasks in Google Docs but they’re treated as second class citizens - they don’t have due dates and are only visible when viewing a document. Confluence, on the other hand, makes it simple to add a todo with a due date on any page and then have all tasks aggregated into a single page. Of course I can have a separate tool tracking all my todos but it’s incredibly powerful to have tasks live next to their context.  I know I shouldn’t be comparing Google Docs with Confluence since they were designed for completely different purposes and yet I’ve resisted moving to Confluence due to how much more usable and friendly Google Docs felt. The lesson here is that we’re always going to favor what we’re comfortable with but it’s important to think about why some tools feel heavier and clunkier than others. There’s likely a reason they’re built and designed that way and are likely to solve our problems better than the tools we love but have to wrestle into submission.",0,1,2017-08-08,6,"google docs, confluence, jira, wiki, collaboration, tools",408,Google Docs vs Confluence
21,0,I somehow got 3 Google Maps routes that all have the same time estimate. Have we reached perfect traffic routing?,,"{% include setup %}            While using Google Maps to pull some directions I stumbled unto a case where every single option gave the same time estimate. I like harboring the idea enough people use Google Maps that they’re able to optimize each route to reduce the total time across the entire system. By having enough data from the tens of thousands of drivers on these roads Google can predict how busy the roads will be due to the directions they’re providing. In turn, they’re able to use this information to predict what the traffic patterns will look like on those roads in the future and use that knowledge now to change it.  I doubt it’s that sophisticated but such approaches will becoming dominant when self driving cars take over. Similar to how modern skyscraper elevators ask for your desired floor and tell you which elevator to go to to reduce your total time once there’s enough scale we’ll be able to have our self-driving cars do the same.",0,0,2017-02-09,3,"google maps, traffic routing, self driving cars",185,Traffic efficiency
18,0,It's both shocking and disappointing that we still don't have a solution for echoes in conference calls.,#meta,"{% include setup %} I’m sure conference calls have improved significantly over the past decade yet they still feel incredibly behind. They’re generally stable but it’s boggling that we still haven’t been able to figure out how to get rid of the echo. Whenever there are two unmuted people in a single room dialed in to the same meeting you hear the effect. As soon as one speaks you hear loud and whiny static that only goes away when one of the participants mutes themselves. The solution is incredibly simple and it’s shocking that there’s no automated solution.  Noise cancelling headphones are not designed for voice cancellation and do much better at low frequency sounds but it just seems that a video conferencing system should be able do a better job. It has a bit more time for the processing, has the data from both channels, and should be able to identify and prevent the echo as it occurs. It can be as simple as detecting a high frequency sound and silencing it or temporarily muting one of the channels. It’s obviously more difficult than that or we would have had this functionality already but it’s just surprising that this hasn’t happened yet. We [have systems](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html) that can make calls on our behalf and yet we don’t have a system that could get rid of a conference call echo.",1,1,2018-12-06,2,"conference calls, ai",238,Conference call echoes
19,0,"IBM's rumored to layoff more than 100,000 people so I wanted to see how it compared against others.",#meta,"{% include setup %} Given the  rumor  of the massive IBM layoffs I decided to pull some others and see how it compared. Surprisingly, the next highest was also at IBM - but in 1993. On one hand, it's odd to see this pattern as if they've learned nothing. On the other, it's a sign that they acknowledge the problem and are willing to adapt. Since the 1993 layoff IBM's stock price increased over 950% and this round may provide another burst.  On a side note, the largest layoffs are available online but I wasn't able to find a non ad-ridden, non-paginated table so hopefully others find these useful.                     Number         Company         Year         Layoffs                             1         IBM         1993         60,000                     2         Sears         1993         50,000                     3         Citigroup         2008         50,000                     4         General Motors         2009         47,000                     5         AT&amp;T         1996         40,000                     6         Ford         2002         35,000                     7         Boeing         2001         31,000                     8         USPS         2010         30,000                     9         Bank of America         2011         30,000                     19         HP         2012         27,000                     11         Daimler Chrysler         2001         26,000",1,1,2015-01-26,2,"ibm, layoffs",209,IBM's rumored layoff
30,0,Each new cconsumer technology category eclipses the former. At the same time they get closer and closer to our bodies. The inevitable outcome is that we will become cyborgs.,"#meta,#society",{% include setup %} The iPhone is the top selling consumer product of all time and a variety of podcasts and articles makes it seem that this is the peak of consumer technology and we’ll never see anything as popular. This is shortsighted. Every new technology achieved wider and wider adoption and eclipsed the previous generation - [laptops eclipsed desktops and smartphones eclipsed laptops](http://ben-evans.com/benedictevans/2014/4/25/ipad-growth). One thing that’s clear is that each generation of tech gets closer and closer to us. Initially we were exposed to computers when we went into the office. Soon we started buying desktops for our homes. After that we decided we wanted laptops that we could carry around with us. Smartphones gave us the ability to carry computers around in our pockets with a full day’s worth of charge.  Smartwatches aren’t very popular now but cellular connectivity may make them even more popular than smartphones. The interactions and designs will need to improve to handle the novel form factor but that itself is an opportunity to get closer to to the senses other than sight. Smartphones we carry but smartwatches we wear. Beyond smartwatches we may end up with technology that gets us closer and closer to becoming cyborgs. At this point we get very close to sci-fi territory with chips that are implanted under our skins or technology that can interface directly with our brains. At that point I can’t even imagine what sci-fi novels will be - everything will seem possible.,1,2,2016-08-08,5,"society, cyborgs, consumer technology, tech products, iphone",251,Consumer tech leads to cyborgs
26,0,Both mobile payments and messaging apps suffer from the same problem: a person can easily use whichever one they need at any point in time.,#meta,"{% include setup %} I wrote a [post](/2018/11/14/what-messaging-war/) almost a month ago describing my relationship with messaging apps: despite the fact that are dozens of messaging apps competing against one another I treat them all as a commodity. The same situation exists with mobile payments. It feels less fragmented than the messaging space and yet there are quite a few players here: Apple Wallet, Google Pay, PayPal, Venmo (owned of PayPal), Zelle, Cash App, in addition to the region specific companies, such as AliPay and WeChat in China to PayTM in India.  Compared to a messaging app getting set up with a mobile payments app is more difficult since you’re often required to link it to a financial account but once that is done you’re generally good to go. The underlying interface is the currency itself so as long as you’re able to withdraw money from these apps you can keep shuffling money between apps without a second thought.  My experience with Apple Pay is illustrative. I started by connecting some of my credit cards to check it out and have been using it at a few stores. A few months ago I went out to lunch with a coworker and instead of paying me back using Venmo he sent an Apple Pay request. I didn’t even know that Apple Pay had social payment functionality. I was curious, accepted the payment, and now my account has a balance like any other app. In fact I can even use this balance to pay at any store that accepts Apple Pay. I didn’t have to do anything special or novel and it worked just like any other payment app. I’m sure there’s a lot of subtlety and nuance I’m glossing over but in my limited use case they suffer the same problem as messaging apps - the offering is commoditized and people will use whichever one they need at any point in time.",1,1,2018-12-02,4,"mobile payments, apple pay, google wallet, messaging",323,Mobile payments and messaging apps
25,0,Rather than keep your knowledge focused embrace your curiosity and try to learn as much as you can about as much as you can.,#meta,{% include setup %} These days it’s incredibly easy to keep learning and find information on any topic but it’s much rarer to find people that are interested in everything they come across. More often you find people passionate about a few domains that are the most relevant to them and don’t bother pursuing knowledge of anything new. This mindset is both sad and irritating - why would someone consciously limit their knowledge?  Beyond knowledge for knowledge’s sake it’s valuable to train your mind to learn and absorb information; you never know when it can come in handy. Beyond making you a generally more interesting person you’ll be able to connect with nearly anyone - an immensely useful ability regardless of what you do. It also adds to your own character and gives you the ability to think from a variety of perspectives - something that seems to be lacking in the modern world.  Many innovations and inventions have come from ideas cross pollinating from one field to another and a micro version of that can be achieved in our own minds by learning and absorbing everything we can. This helps us connect the dots across different disciplines and is able to provide new perspectives on the same old problems.  None of this is difficult to achieve. Just embrace your curiosity and make it a habit. If you find something interesting on Wikipedia keep following the links and don’t stop until you can’t go on anymore. If you start a book and aren’t enjoying it - deal with it and just finish it. See an interesting book mentioned in a blog post? Buy it. Change your mindset to welcome new knowledge and you’ll start seeing these opportunities everywhere.,0,1,2017-05-13,3,"polymath, knowledge, learning",285,In praise of the polymath
6,0,Relationship between innovation and cannibalization.,#product,"{% include setup %}       I was rereading the HBR paper on  Strategies for Two Sided Markets  and came across a passage describing Apple’s mistake of trying to monetize both sides of their market, the consumers and the developers, rather than focusing on one like Microsoft did by giving away the SDK for free.    It got me thinking about Apple’s recovery. Many people credit the iPod with revitalizing Apple but I think there’s more than that. I suspect the bigger reason was the decline of desktop software and the ability to be productive on the web. Suddenly the network effects that existed by having software that only worked on Windows no longer existed. Software started migrating to the web and people were more willing to try new operating systems out. In 2006, I switched to Linux without too much trouble. It was also simple to find help online to deal with the various issues I ran into which made the transition easier. In some ways, Google helped Apple recover by speeding up the move to the web with a more accurate search and a good set of productivity apps.    In general, it’s damn difficult to overcome network effects. Google will not be replaced by a search engine. Facebook will not be replaced by a social network. These network effects will be broken by a behavioral change. Instagram rode this wave of behavioral change of the move to mobile and it was a savvy move for Facebook to make the acquisition. It makes you wonder what Instagram could have become had it stayed independent.    Innovation is cannibalization. By pushing the envelope of technology, pioneering companies cause behavioral changes that will give rise to companies that may end up replacing them. As Clay Christensen  notes , it’s rare for a mature company to put resources behind a disruptive technology that will cannibalize itself but it’s the only way to stay relevant. Only  13% of the companies  in 1955’s Fortune 500 made the list in 2011. It’s amazing to see how quickly things change and the pace is only getting quicker.",3,1,2012-08-12,2,"innovation, cannibalization",387,Eating Yourself - Innovation &amp; Cannibalization
17,0,I migrated by blog to Github pages and wanted to share some notes of the process.,#blog,"{% include setup %} After a few hours of solid work I was able to get my new site up and running on Github pages. I got frustrated with having too many blogs and decided that I should finally get it together and consolidate everything. Within a few hours I was able to get it up and running on Github pages up and migrated my old Tumblr and Wordpress posts. Hopefully this encourages me to write more.  A few notes: 1. The  documentation  for Jekyll is great and makes it very easy to get started. 2. Jekyll comes with a few  migration  scripts that made it easy to move the old blog posts over. 3. There's a pretty strong community around it so it's easy to get started with themes. I ended up using  one  based on Twitter Bootstrap. 4. Github pages provide a custom domain option so you can host your entire site in Github. Other than the fact that it's free, you don't have to worry about your site dying due to heavy load. 5. An issue to be aware of is that the Jekyll parser is pretty strict and doesn't provide very helpful error messages. I had an issue that prevented some posts from migrating because they had a "":"" in the titles. To discover this, I had to migrate a few posts at a time until I was able to identify the issue.",3,1,2012-11-14,1,,245,Github Migration Notes
20,0,I gave the Superhuman email client a shot and while it's a solid product it just wasn't for me.,#meta,"{% include setup %} I’m all about productivity so when I heard about [Superhuman](https://superhuman.com) I decided to give it a shot. I’ve been using it for the past few weeks and while it’s a solid product and well built it didn’t suit me. The onboarding experience is great and there’s a ton of functionality that Superhuman provides that makes it that much easier to go through email. This ranges from a variety of shortcuts (that can all be quickly found using Cmd+K) to functionality that you wish existed in email, such as typing a date and immediately seeing that day’s calendar to being able to copy a whole email message - attachments and all.  I really wanted to like it but unfortunately ran into a few issues that ruined my experience. The biggest was that it was just too slow and would occasionally freeze entirely when loading large emails. At that point my only option was to force quit the app and then go back to Gmail to handle the offending message. I also ran into a few cases where I tried to delete an email but due to the lack of an immediate response I’d hit delete again and would end up deleting a few messages. Given that my account was set up to optimize towards inbox zero accidentally deleting a few emails was a problem since these were important emails that took time to recover.  Superhuman invested a ton in their mobile experience and it shows. The iPhone app is smooth and provides an entirely different experience than the desktop client. Unfortunately, I rarely use mobile email and prefer to do all my emailing on the desktop. I never struggled with mobile email so there was little incentive for me to change my behavior.  If you can justify the $30/month, are a heavy email user, and find yourself spending more time in email than you should then give it a shot. If you’re more traditional, are happy with your current email experience, and don’t have the $30/month to spend you likely won’t invest the time to make Superhuman work for you. My gut is that a lower price and an improved desktop app experience would get me to come back.",1,1,2018-11-21,3,"superhuman, email, productivity",373,Superhuman review
23,0,Every company and team should have components that are mission critical that need to be maintained no matter how quickly they move.,"#management,#product","{% include setup %} The accepted belief is that startups should move quickly and err on the side of speed rather than quality. This makes sense. Startups are so risky that they won’t fail due to making a few mistakes but will fail if they get out maneuvered and out innovated. The big advantage startups have is speed and that needs be leveraged.  The one caveat I’d make is that every company, big and small, should have mission critical elements that need to be maintained when pushing new features and updates. I was reminded of this last week when an unnamed corporate feedback startup sent out the private one-on-one notes people jotted down in preparation for their meeting to everyone within the company. This was a huge betrayal of trust and ruined the good will people had for the company and the product. If they weren’t able to get this basic piece right how are they expected to do the rest? Every company has these mission critical components that everyone needs to be aware of and great care must be taken to ensure they work before every deploying or change. In the adtech case it’s serving ads - if ads aren’t working then publishers aren’t making any money and losing money during each impression. For cloud productivity applications it’s critical that they don’t lose your data - downtime is annoying but at least you can switch to another task while they get back up. If you lose your data and documents you have to figure out exactly what you lost and decide whether it’s worth recreating. Everyone in the company should know what these these mission critical components are and it’s everyone’s job to make sure they’re working as expected since failure carries existential risk for the customer relationship. It’s unlikely that a single bad event will ruin things but as soon as it becomes a pattern it’s likely that that customer will be lost forever and never return due to the faulty first impression.",0,2,2015-12-15,5,"management, product, engineering, quality, software development",336,Know what's mission critical
12,0,I compare the collecting every ticket vs fining trespasser model on trains,#finance,"A few years ago, I was on vacation in Italy and spent a good amount of time on trains. Being from the US, I noticed that my ticket was not checked every single ride. At the same time, not having a ticket and being caught carried a large fine. Having the luxury of time, some assumptions, and some algebra, it’s straightforward to work out how to set the fine to make the two systems have the same expected revenue.  {%highlight txt %} N = number of passengers p = ticket price c = % of passengers that will be checked for a ticket v = % of passengers that are violators F = fine  Np = Np(1-c)(1-v) + Npc(1-v) + Ncv*F  Solving for F, we get that F = p/c. {% endhighlight %}  With these assumptions, the fine only depends on the ticket price and the check rate. For example, if the ticket price is $50 and there are 1,000 passengers, the expected amount collected is $50,000. If the conductor only checks 10% of passengers for tickets, the fine would need to be $500 to make the two systems equivalent. If the two systems are expected to generate the same revenue, but one is cheaper to implement, why is the seemingly non-optimal system chosen? I can think of a few reasons:  - Checking all tickets is easier than setting the check rate and fine - especially if they fluctuate - There are additional roles for the ticket collector other than checking tickets - There’s value in minimizing the volatility of the revenue - The cost of having conflicts between passengers and collectors over large fines is higher than checking every ticket - there may be a fear of a low-likelihood, high-cost event - Union agreements may prevent changing the structure - General cultural differences across continents  As entrepreneurs, we’re constantly thinking about the ideal business model. Noticing and comparing other business models help us refine and develop our own.  I’m always on the lookout for more examples, so if you have any please email me or post below.",0,1,2012-03-05,2,"business models, trains",347,On business models - To collect or to fine?
27,0,Similar to 2016 I've been tracking a few stats every single day ranging from my sleep to food to drinking habits and visualized them all here.,#dataviz,"{% include setup %} Over the past year I’ve been collecting a bunch of statistics for each of my days in the hope that I’ll have time to dig into them and discover some interesting patterns. Unfortunately I haven’t had a chance to do anything other than some simple visualizations but even these provide some insight into my 2017. This isn’t a wholehearted adoption of the quantified self movement but it’s something I am interested in and hoping to expand in 2018. A goal has always been to move beyond visualization and into actual analysis and actionable insights that can help me improve my lifestyle and behavior. I did the same set of [visualizations in 2016](/2017/01/02/year-in-review-2016/) so it's useful to compare them year over and year and see how, and if, my habits have changed.                              Just under 7.5 hours of sleep per night but a bit all over the place.                                       Daily consumption of coffee, tea, alcohol, and soda box plot. Compared to last year the numbers are pretty similar although I did manage to cut down slightly on both my alcohol and coffee consumption - something that I made a concerted effort to this year. Unfortunately the change wasn't as significant as I had wanted to and is a goal for 2018.                                       A much wider breakfast range than last year yet still a bit heavy on the cheese.                                       An improvement over 2016 where I was very much into Chipotle. I still enjoy my burrito bowls but have also introduced salad to the mix.                                       Very similar dinners to last year - still heavy on the salad, chicken, and rice.                                       The most disappointed of the wordclouds and something I need to drastically cut down on in 2018.",1,1,2017-12-27,2,"blog analysis, quantified self",423,Visualizing my 2017 stats
21,0,With a little bit of JavaScript knowledge you can cleanly extract information from a web page while avoiding formatting issues.,"#python,#javascript,#code","{% include setup %} How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.  In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend. Unfortunately, copying and pasting didn’t work so I decided to jump into JavaScript. Hope these steps give a sense of how to approach a simple scraping problem. The idea is to use the browser’s inspect element feature to find the pattern that the element we’re interested in have in common. Then, we use JavaScript to find the elements matching that pattern and extract the information we want.                   1. The page we want to parse - please ignore the quality of my fantasy team.                                    	 2. Use the Chrome ""Inspect Element"" feature to figure out the HTML/CSS of the element we're interested in. In this case, the element containing player name has the class value “name playernote”.                                    	 3. Run a JavaScript command to get all the HTML elements that have those classes.       	{% highlight javascript %}document.getElementsByClassName('name playernote'){% endhighlight %}                                           	 4. Store those HTML elements in a variable so we can quickly iterate through the list. 		{% highlight javascript %}players = document.getElementsByClassName('name playernote'){% endhighlight %}                                           	 5. Use JavaScript to go through the previous list and extract the player name. Then we can just copy and paste the list of names without having to deal with the formatting issues.       	{% highlight javascript %}for (var i = 0; i                         In addition to extracting information, JavaScript can be used to interact with a web page. This comes in handy when you want to automate a certain action on a site that would take too long to do manually. For example, I was able to code up some quick JavaScript that would go through a list of my Facebook friends and invite them to like my startup’s new page. Hope this little JavaScript hack comes in handy and let me know if you have any questions.",0,3,2013-08-26,5,"html, css, javascript, crawling, scraping",529,Extract info from a web page using JavaScript
8,0,Tips to help you write better web scrapers,#code,"{% include setup %} I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.  Hope they’re useful!      Avoid it if possible   It is a bit odd that I’m starting off with this as the first tip but if there are alternatives definitely take a look at those; many sites come with an API and that may be a much better approach. Otherwise, every time there’s a change in the HTML structure you run a risk of breaking your scraper which will leave you scrambling to fix your code. It’s also a good idea to organize your code such that a change in the HTML for one of the scraped items does not break the others. For example, if you want to get the name and address of a restaurant from Yelp, have one method that will get the name and another that will get the address. This will most likely be less efficient so you’ll need to use your judgement to see whether the risk-speed tradeoff is worth it.      Use a library   Unless you’re doing a one off job, use a library. Every major language has one: Python has  BeautifulSoup , Perl has  HTML::TreeBuilder , Javascript has  htmlparser , and there’s no excuse to not use one. If you ever need to go back to make some changes (which you most likely will need to), you’ll be glad you did. You can also find libraries that let you simulate browser behavior by storing cookies and letting you submit forms. This gives you the ability to scrape sites that require a login. Some sites try to prevent scraping by obfuscating their HTML a bit in which case you’ll need to do either a string replacement or a basic regular expression to get it parsed by the library.      HTML/DOM inspectors are a must   Since scraping requires getting specific elements from a web page, we need to understand the HTML structure of that page. For me, doing this work within the browser works best since it gives you the ability to both see the HTML that’s responsible for a certain element and also gives you a console window which lets you test a scraping approach. The two browsers I’ve used successfully for this are  Google Chrome  and  Firefox  with the  Firebug  plugin.      User agent spoofing   Every time your browser visits a website, it submits a request that contains information about the browser. This is why some sites show a different page when you’re using a phone versus a computer. Every once in awhile you will need to trick the site into sending back the proper page by “spoofing” the user agent. A simple way to check if you need to do this is to view the source of a page in a browser and compare it with what you’re retrieving in your code. If they’re different, try changing the user agent and see if that fixes it.      Be clever   Looking at the source of a page may be a bit overwhelming and there may be easier ways of getting at that information so be clever! An example of two approaches that I stumbled across were to spoof a mobile browser and to call the AJAX url directly. Spoofing a mobile browser tends to give you simpler and more lightweight HTML which is easier to parse. Loading the content via AJAX lets you get at the content quicker and usually in a more structured format, like JSON or XML. These approaches won’t work on every site so you need to do some research and experiment a little to understand how each site is setup. After that you can figure out the best approach for your scraper.      Be specific   When scraping, you want to make your scraping code rigorous enough to not fail if the page structure ever changes. A good rule of thumb is to be specific when you write your scraper. Use a specific id rather than a class since the id is guaranteed to be unique. Similarly, avoid an ordinal approach where you reference the 2nd or 3rd div. Sometimes this is unavoidable but try to see if there’s another approach. Another useful tidbit is to use the more content-descriptive identifier in the page. For example, if you see a div with the address you want to scrape and that div has two classes, “location-address” and “blue-highlight”, use the “location-address” one since that’s defining what the content is, not how it’s displayed.      Save the HTML of the retrieved pages   It’s helpful to save each HTML page you’ve retrieved. It takes a few iterations to get your scraping code working and it’s quicker to just have the HTML on disk so you don’t have to download it every time the script runs. Another advantage is that if you discover a mistake in your code, you don’t have to redownload all the pages you’ve already processed. It only takes a few minutes of work and worth doing.      Monitor actively   Scraping is prone to breaking so make sure you monitor the job as it runs. It’s likely that your code will work well on one page but will fail on others. I tend to write my code to be a bit picky at first while I work out the kinks and once I’m confident in it I will build in some logic to deal with a missing value to make sure it continues to run. As I mentioned earlier, storing the HTML of the page will save you time if you need to update your scraper and need to rerun it.      Throttle your requests   If you don’t want your roommates pissed pissed at you, which will happen when Yelp blocks you for 6 months, throttle your requests. The simple way to do this is to have your code wait in between downloading pages and another approach is to use proxies to hide your true IP address. This will make it seem that the requests are coming from a variety of computers and keep your roommates happy.",6,1,2013-01-09,1,Scraping web HTML BeautifulSoup,1092,Web scraping like a pro
23,0,I took a walk down memory lane and thought about the old projects I've worked and what I've learned along the way.,#meta,{% include setup %} This past weekend I was going through some old projects and got a bit nostalgic. Some were my first foray into web programming and startups while others were just me messing around and trying to learn a new framework or language. Each of them have taught me valuable lesson and I thought it would be fun to go through each one and jot down a quick background as well as the lessons learned. I’m doing a high level pass so if any of these are interesting definitely let me know and I’ll do a deeper dive.  - scenepeek.com: I started this with a close friend back during my finance days when I really didn’t know what I was doing. The goal was scrape the web and identify various events that could then be easily surfaced and discovered. This was right before smartphones became popular so it does make one think of what could have been. This was my first real time doing “devops” and working with various instances and configuring Apache. The other big lesson learned was that we probably should have started with some framework to get our project out sooner. Instead we ended up writing raw PHP and building everything from the ground up. -  getpressi.com : Applying the lessons learned from Scenepeek I left a a full time job at Yodle to cofound a startup (initially called Glossi) that would create social media mashup pages. We were accepted into an accelerator and ended up making significant progress but were never able to figure out whether our core customers were consumers or larger companies. We couldn’t commit as a team and ended up floundering until selling to a small advertising agency. At the peak we had a dozen customers and most likely could have turned it into a lifestyle business had we had the maturity and focus. -  makersalley.com : After breaking up with Pressi went in the opposite direction and built decided to build something with a concrete business model rather than waiting for one to fall into our laps. We both liked Etsy and wanted to do something with a community element as well as having to do with physical goods. This was a two sided market play and we were never able to get people to buy expensive furniture online. We got so enamored with our vision of how awesome the furniture and designers were that we focused on getting them rather than on getting customers. -  better404.com : This was a small side project I started to help websites improve their 404 pages. I wanted something that was more passive than building a marketplace and catered to my strengths which were more on the tech side. I don’t have too much time dedicated to this but every once in a while I’ll make some updates with the idea of making it a small passive income generating product. -  jsonify.me : Scratching an itch here but I love the idea of every person having a JSON page that’s a representation of what they are and what they care about. It’s also my “go to” project when learning a new language. It’s a proof of concept more than anything else right now but I’d love to see where it goes. I’m passionate about people owning their data and lending it to third parties as needed and view this is a way to achieve it using existing methods.  On one hand I want to polish some of them off and see what I can do but on the other I’m curious about trying new things. People glorify this idea of a purely passive income but I suspect nothing is that easy and every project will require some ongoing maintenance and improvement to stay relevant.,4,1,2015-10-29,4,"nostalgia, old websites, startups, technology",640,Some nostalgia
11,0,The annual post highlighting my favorite books read in 2017.,#meta,"{% include setup %} For the past few years I’ve been writing two posts a week and my end of the year ritual is to catch up on my writing and finish off the year right. Last year I did a filler post for my favorite books of 2016 and thought it would be interesting to share the 2017 version as well. I’m a big reader and rarely dislike anything I read yet sharing every book I’ve read is a bit of a cop-out so I’ve tried to focus this on the books that I just couldn’t put down and couldn’t wait to resume reading.  * [Deep Work: Rules for Focused Success in a Distracted World](https://www.amazon.com/gp/product/B00X47ZVXM/) (Cal Newport): I heard about this book from Ezra Klein’s [podcast](https://www.vox.com/2017/4/21/15382282/cal-newport-taking-life-back-technology) and knew I had to read it. I wasn’t disappointed. The world is changing rapidly and I feel much more distracted than I did as a kid. Cal Newport calls this out and offers a series of rules for eliminating the crap and concentrating on real and meaningful work. Especially as people are making resolutions for 2018 this is a worthwhile book to read and set the stage for a productive 2018. * [Who Says Elephants Can't Dance? Leading a Great Enterprise Through Dramatic Change](https://www.amazon.com/gp/product/B000FCKL6G/) (Louis Gerstner): Louis Gerstner led IBM in the 1990s and helped it transition from a hardware-focused to a services-oriented business and this is a half narrative, half management book on the transition. IBM was an incredibly large company on the brink of failure and its impressive seeing how they were able to pull themselves away from the edge and reinvent themselves. If you’re interested in business history or managing a team it’s a quick and worthwhile read. * [The Emperor of All Maladies: A Biography of Cancer](https://www.amazon.com/gp/product/B003UYUP58/) (Siddhartha Mukherjee): This has been on my reading list for a while and I’m glad I finally got the chance to read it. The book is incredibly well written and a gripping read despite covering such an intense and depressing topic. Beyond the history of cancer it chronicles the evolution of healthcare and how it’s evolved over the years to fight new threats. * [Before the Trumpet: Young Franklin Roosevelt, 1882-1905](https://www.amazon.com/gp/product/B00J1ISLC6/) (Geoffrey Ward): I’m an admirer of FDR and really enjoyed that this focused on his youth. Many biographies blow through childhood, the teenage years, and college since there’s both more limited information and fewer achievements but Geoffrey Ward does a great job digging through the archives and describing the story of a young FDR. It doesn’t strive to explain him as president but offers anecdotes on top of anecdotes that let readers make the connection themselves. * [Shoe Dog: A Memoir by the Creator of Nike](https://www.amazon.com/gp/product/B0176M1A44/) (Phil Knight): A book that came highly recommended and for good reason. Nike is a huge corporation now but it took an incredible out of perspiration to get there. The story itself is incredibly intense with many twists and turns that highlight how remarkable it is that Nike was able to survive and then thrive. The personalities in the book just draw you into their world and it makes you feel as if you’re there at the moment. * [Creativity, Inc.: Overcoming the Unseen Forces That Stand in the Way of True Inspiration](https://www.amazon.com/gp/product/B00FUZQYBO/) (Ed Catmull): Similar to Who Says Elephants Can’t Dance this is half company biography, half management book but rather than focusing on reinventing and changing the culture of a nearly 100 year old company it’s focusing on how to keep a creative culture alive as a company grows. On its own it’s an interesting and engaging read but what makes it a joy to read are all the hidden Easter eggs behind our favorite Pixar movies.  Looking forward to reading even more in 2018!",7,1,2017-12-31,2,"books, reading",657,Favorite books of 2017
20,0,I had a nice vacation in Paris and wanted to compare the two while the thoughts were still fresh.,#travel,"{% include setup %} I just got back from a 10 day vacation in Paris and couldn't help but compare it against New York. That's what traveling does - forces you to compare what you're comfortable with the novelty you're exposed to. Some make you appreciate what you have while others make you want more. In any case I wanted to share my thoughts while they're still fresh.  - Public transit: One of the first things you notice after living in New York are the public transit systems in other cities. New York has a reputation for having one of the best (one of the best?) in the world and I was curious to see how Paris handled it. The first thing I noticed was how short the platform was - rather than the multiple block stops in New York the Paris platform is enough for a 5 car train - and sure enough that's the size of the Paris trains. Each station I've been to had accurate time estimates and it felt as if the trains ran frequently and I never had to wait longer than 6 minutes although I've only taken it during the day. One thing that's struck me as odd was that it seemed as if every train had their own method of opening the door. In New York the doors open automatically but in Paris you need to either hit a button or pull some sort of level to get the doors to open. The way the stations were labeled felt friendly to tourists as well - each time you had to decide on an uptown or downtown train it would list each of the stops along with the potential transfers along each route which made it very easy to orient ourselves. The last thing I want to mention is price: the NYC subway costs $2.75 right now and you have to pay a fee for the metrocard itself. In Paris the fee is €1.80 which is just under $2 at current rates and you can buy 10 at a time for €1.40 each - significantly cheaper than the NYC subway. - Bike and car share programs: New York has Citibike and Paris has an equivalent version called [Vélib](http://en.velib.paris.fr/). I didn't get a chance to use it so don't have much of an opinion but the rates they offered were significantly lower than a non-annual Citibike pass. A daily Citibike pass is close to $10 whereas you can get day of Vélib for €1.70 and a week for €8. In addition to a bike share program, Paris has an electric car share program with stations prevalent across Paris. I didn't get a chance to use these but it seemed like a really neat idea that reminded me of ZipCar without the burden of needing to return the car to the original destination. - Neighborhoods, not districts: This might be entirely due to where I stayed and wandered but each neighborhood felt like it's own little city. We'd walk around in a neighborhood and it would have everything one would need - a bakery, a cafe, some grocery stores, a few restaurants and bars, a dry cleaning place, and a few boutique shops. It made it seem that one only needs to walk a few blocks to have everything they need. In New York it feels as if there are districts - the flower district on 28th, the diamond district in midtown, the theater district near Time Square, the rug district on 31st, the lighting stores in chinatown - but it didn't feel as if Paris was structured the same way. Paris of course is known for the shopping on Champ-Elysees but that's more the exception than the rule. The only other area that felt like a district was a series of falafel shops in the Marais. Of course this may be completely wrong and only visible through my tourist lens. - Architecture: Compared to New York Paris is ancient and its architecture and layout reflects that. Due to [Baron Haussmann](https://en.wikipedia.org/wiki/Georges-Eug%C3%A8ne_Haussmann)’s work during the 19th century Paris has a consistent look and feel which adds to the beauty. Paris barely has any skyscrapers since the majority of the buildings were construct before the elevator era. I was also struck by how mixed use the buildings were - many of them were businesses on the ground floor while the higher floors were residential. New York definitely has a bit of that but still feels as if it has some areas that are resident focused while others are commercially focused. - Price: Based on my conversions and research I expected Paris to be a lot more expensive than it actually was. The biggest reason was that the exchange rate was hugely in my favor ($1.1 per €1) but even then the cost felt offset by the listed price including tips and taxes. For example, if I go to a restaurant in NYC and have a $14 dish it’ll end up costing me close to $18 due to the tax (8.875%) and tip (~15-20%). At an exchange rate of 1.1 dollars per euro that’s equivalent to a €16.37 dish. We went to a few grocery stores and the prices for fresh food felt reasonable and only a tad bit higher than what we were used to. We also got a chance to look at some posted real estate listings and they seemed cheaper than NYC - but the apartments are generally smaller. This is a pretty biased view since we spent it as tourists and didn’t have to buy clothes or any real house items but I suspect all in all it would be pretty comparable, if not cheaper, than New York. - Panhandlers: In NYC it’s typical for people to look away and rush by someone panhandling but what struck me about Paris was that people would stop and have conversations with them. Even more, people were stopping with their children to chat and seemed to be engaging in meaningful conversations. My French wasn’t good enough to pick up the contents but the fact that people actually stopped and had conversations struck a chord with me. We talk about treating poverty and homelessness but unless we treat them as people and provide proper respect it will be for naught. - Restaurants: Not too much here but one thing I wanted to point out was how diverse the streets of Paris were compared to the “front” of the restaurants. The host and waitresses at nearly every restaurant we ate at had the “classically French” look - I’m not sure whether this was intentional but it struck me as odd given how much diversity we have in NYC. - Public restrooms: I haven’t seen this anywhere yet but Paris has free, public, self cleaning restrooms. It’s a bit slow since you have to wait through the washing cycle for each person but the fact that it’s publically available and free amazes me. - Cabs: For the most part we used the subway but we had an interesting experience when we used a cab. The driver suggested an alternate route to the one provided by his GPS and it took us a bit longer than expected to get where we were headed. Instead of charging us what the meter showed he admitted fault and told us to pay a lower amount. Despite our protests he stuck to the lower amount. I’m not sure if this is a common experience but I’m extremely doubtful something like this would ever happen in NY.  Combined, these make it seem that I prefer Paris to New York but I honestly haven’t figured that out. Paris seems to have more progressive policies than New York but I’m basing that purely on my 10 day trip and actually living and working there may be entirely different. It feels as if Paris takes public services more seriously - the public transit is cheaper, more frequent, and more robust since I didn’t experience a single stall or failure which is sadly a common occurrence in New York. I’m also aware that I’ve only spent 10 tourist days in Paris and may be approaching it through rose-colored classes. I’d love to get thoughts from people that have lived for significant periods in both.",2,1,2016-01-03,3,"paris, new york, travel",1382,Paris versus New York City
20,0,Smartphones have the potential of increasing global literacy in addition to the more obvious changes we're going to see.,#society,"{% include setup %}                              Source:  Business Insider                In 2012 global smartphone ownership surpassed PC ownership and smartphones are still seeing massive growth. The obvious consequence is that many people who’ve never owned a computer are starting to own smartphones and that’s having a huge impact on the world. Almost everything will be affected - not just technology but also business, politics, and general culture. As these smartphones get more powerful and pervasive we’ll see applications that we can’t even imagine right now.  What’s not being discussed is the impact this will have on the world’s literacy rate. In 2010, the  global literacy rate  was estimated to be 84% but increasing smartphone ownership should drive it higher. Having something in your pocket that is both a business and entertainment device will encourage people to learn all its features. Sure one can just familiarize oneself with the various icons and key combinations to achieve certain results but I suspect being exposed to a smartphone’s potential will also serve as motivation to learn more.  Of course, this is just hopeful speculation on my part but I think we tend to view technological change through a tech filter. There’s a whole other world that’s difficult for us to imagine so we tend to not think about it too much. I had a professor,  Prof Levent Orman , discuss the impact that the car had on the world. The direct effect was the replacement of horses but the long term effects were the rise of highways and suburbs and a change in American culture. Smartphones are one of the technologies that will have such an impact, it’s just impossible to know what it will be.",3,1,2013-12-26,3,"smartphones, literacy, education",328,Smartphones and literacy
31,0,Given that the modern world allows zero distribution costs and global reach brands are incredibly important and companies should be leveraging them as much as they can in their growth.,#product,"{% include setup %} Pokemon Go is huge right now. People across all age ranges, demographics, and geographies are getting involved and it’s hard to imagine this sort of adoption for any other game. What I find fascinating is that Pokemon Go was based on the same augmented reality mechanics as Ingress, another game developed by Niantic Labs. Ingress has a loyal following but pales in comparison against Pokemon Go when looking at the user numbers, despite Pokemon Go being less than 2 weeks old while Ingress has been around for almost 4 years.  The Pokemon brand has a huge following and it’s incredible what a strong brand can do in the modern world. With zero distribution costs and instant global reach an existing brand can grow faster than at any previous point. Now more than ever do brands matters. Modern technology has made it easier than ever to enter new markets and quickly launch apps but in this world of commoditization and heavy competition a strong brand can do wonders.  The companies best positioned to take advantage of this new world are the ones with strong intellectual property and brands that can leverage whatever innovation comes along. Right now it's augmented reality but in the future it will be something else. The business will be adapting new innovations that allow companies to magnify and enhance their brands.  This model reminds me of the pharmaceutical industry. I started my professional career working for a pharmaceutical consulting company which gave me a crash course in how the industry works. One of the more interesting insights was that the biggest advantage large pharmaceutical companies have is their sales force rather than their R&D. This allows them to just acquire small biotech companies for their newly developed drugs and have their own sales force selling it. This approach makes sense - you find what you’re great at and focus on applying that as much as you can. This is what Nintendo is doing with Pokemon Go and every brand with global scale IP should be doing.",0,1,2016-07-22,5,"Pokemon go, branding, marketing, business, intellectual property",343,Double down on your brand and IP
26,0,The tech industry has embraced experimentation and we should adopt this sort of attitude in broader society. We'll end up with a much better world.,#society,"{% include setup %} Rather than debating various approaches the best way to determine the ideal option is to run an experiment and look at the outcome. The tech industry has embraced this and is constantly running all sorts of A/B tests to optimize any and all metrics. Unfortunately, this approach hasn’t spread to the rest of society where decisions are based on abstract theories and perceptions. Imagine how much society could improve if we expanded experimentation into policies affecting our cities, states, and countries.  It's not going to be easy since it’s not just pushing new code out but the value would be incredible. Changing the color on a button on a webpage may improve user conversions by a fraction of a percent but testing a traffic congestion policy can improve the lives of hundreds of thousands of people in a big city. Imagine running an experiment to see what sort of education or health policies work best - that could improve the lives of hundreds of millions of people and the sooner we start the better. Rather than getting defensive and worried about change we should be optimistic about the opportunity change can bring.  One way we can do this as a society is to introduce laws with an expiration date that have a higher bar for renewal. It may only take a simple majority to try an experiment but to extend or make it permanent may require 75% of the vote. The goal is to make it easy to run experiments but difficult to get them to stick around unless they’re clearly delivering value.  This is a big shift from the way society has been running but there are signs that this is starting to happen. [Finland is running a basic income trial](https://www.theguardian.com/world/2017/jan/03/finland-trials-basic-income-for-unemployed) but it’s targeted. I’m more encouraged by the [basic income trial in Kenya](http://basicincome.org/news/2017/02/us-ebay-founders-firm-donate-nearly-0-5-million-basic-income-pilot/) which will give the residents of 200 villages a monthly income that will span the next 12 years. We need to see this willingness to run long term experiments and while it’s significantly cheaper to do this in developing countries I’m hopeful that we’ll start seeing these trials everywhere.",2,1,2017-03-22,3,"experimentation, society, basic income",377,Embrace experimentation
10,0,Visualizing the routes of garbage trucks in Jersey City.,"#code,#dataviz,#javascript","{% include setup %}                                    A couple of months ago I took a stab at plotting the Jersey City [parking zones](http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/) after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome [comment](http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530) pointing out that Jersey City has a bunch of open data available, including a near-real time feed of [garbage truck locations](http://www.jciaonline.org/gpsMap.php?view=map), a general [open data portal](http://data.jerseycitynj.gov/), as well as the ability to [request custom data](https://jerseycitynj.seamlessdocs.com/w/records_request). As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at [https://dangoldin.github.io/jersey-city-open-data/](https://dangoldin.github.io/jersey-city-open-data/) and the code is up on [GitHub](https://github.com/dangoldin/jersey-city-open-data).  The approach I took was straightforward. After going to the real time map I opened the network explorer in order to see the HTTP requests being made to update the map with the latest truck locations. It was a single URL call that was returning a pipe delimited file containing the location of each truck. By writing a simple wget script and setting it as a cronjob I was able to capture the truck locations every minute. After a day’s worth of data I combined the files and removed duplicate lines (for when the trucks stayed in a single location). After that it was simple to use the Google Maps API to draw a route for each individual truck. The neat thing here is that 90% of the work was done through simple shell commands. One command to fetch the data every minute, another to combine them into a single file, and then a few others to sort and dedup the data. By the time I got to coding all I needed to do was convert the data from a pipe delimited file into something that could be consumed by the Google Maps API.",7,3,2015-12-12,4,"jersey city, open data, 07302, garbage trucks",397,Jersey City garbage truck routes
14,0,Quick way to connect to an RDS instance using R through an SSH tunnel,"#R,#sql,#code,#aws","{% include setup %} In my quest to replace Excel with R I’ve been spending the past week trying to do everything in R. It hasn’t been that easy with many things taking longer due to me having to reference the R docs but one thing that’s been great so far is being able to quickly run a query on Amazon’s RDS and pull data into a data frame for quick analysis. Being able to wrap this into a reusable function makes things even better. The one thing that makes it tricky was not being able to connect to RDS directly but having to tunnel through an EC2 instance. Below are the steps to replicate the setup.  In your shell, run the following command to set up the SSH tunnel: {% highlight bash %} ssh -L  : :  @  {% endhighlight %}  Now in R: {% highlight r %} install.packages(‘RMySQL’) # Install the R MySQL library library(RMySQL) # Load the library m<-dbDriver(""MySQL"") # Load the driver con<-dbConnect(m,user='username',password='pass',host='127.0.0.1',dbname='db',port=3307) # Connect to the local instance res<-dbSendQuery(con, 'select * from table') # Execute the query data <- fetch(res, n = -1) # Load the retrieved data into a data frame dbClearResult(dbListResults(con)[[1]]) # Use this to free the connection {% endhighlight %}",0,4,2013-11-15,4,"R, RDS, MySQL, RMySQL",218,RDS and R
44,0,There's this idea that media consumption is zero sum and as we consume more online we'll consume less on traditional channels. I'm not convinced this is the case and view it as more of an increase in the total size of the pie.,#meta,"{% include setup %} I’ve seen the point made across a variety of articles that media companies see media consumption as being zero sum - there’s only a fixed amount of consumption that people have and they’re allocating it between a variety of options. The typical example is people abandoning legacy television for YouTube. Thus, the rationale goes, the time people would have spent watching television has been replaced by them watching YouTube.  I understand the concern. People are watching less and less standard television with only time sensitive and restricted content (ie sports) being viewed on television but there’s a world of nuance. People may be watching less and less broadcast television but the content that’s being consumed is growing. Cordcutters pride themselves on not paying for cable television yet they’re still subscribing to Netflix and Hulu. The same shows are being watched - just not on the usual devices. In addition, there’s the rise of multi-screen consumption - we’ll be on our phones or laptops while watching TV. Whether it’s a distraction from a commercial, a way to follow an event on Twitter, or just browsing Wikipedia to get a bit more information, we’re parallelizing our media consumption and increasing the size of the media pie rather than solely switching between platforms.  I found the following graphic which highlights the shift in media consumption over the past century.                               The BrandBuilder Blog                Part has been the rise of new technology to fill in more and more of our days. Radio gave way to the rise of television which started taking up more and more of our time. The internet then came along and gave us another way to access and consume content that. The smartphone blew that out of the water and gave us the internet and media consumption wherever we are. Whereas in the past we’d be watching TV or using the internet at home or at the office with smartphones we can consume content wherever we are. The pie of “total available media consumption minutes” has been growing over time and will only keep increasing. What will people do when we’re being chauffeured around by our self-driving cars? We’ll be using our future devices to consume and entertain ourselves the exact same way we do now. That’s a whole new timeslot for us to consume content that we’ve never had the ability to before.  As more and more of our responsibilities get automated that time will become available. Some will fill it in with creative pursuits while others will view it as another timeslot for media consumption on an ever increasing number of screens and devices. It’s definitely depressing but I’m not optimistic given recent trends.",1,1,2015-10-04,3,"media consumption, television, internet",491,Is media consumption zero sum?
14,0,I rewrote jsonify in go and would love to see what people think.,#code,"{% include setup %} A couple of weeks ago I wrote about the idea of having a  “go to” project  that you use to pick up a new language and earlier this week I finished the bulk of the rewrite of  jsonify.me . It went through a Node.js phase, a Scala phase, and is currently in the go phase. The idea is to give people an open ended and simple way to generate a personal JSON object, similar to how people may have an about.me page but in JSON. This object can then be mapped to any subdomain (mine is at  json.dangoldin.com ) and be referenced by any third party code. For example, you can construct your personal jsonify.me object based on the information in your various social media profiles and then make that information accessible to a variety of sites or pages that can generate it in a variety of ways. One site can turn it into a simple resume while another one can turn into a visual timeline of your history. At the moment it’s entirely open ended with the vast majority of the functionality provided solely through an API. Over time I’ll add some more bells and whistles but I’d love to see the community come up with their own unique JSON format that can then get adopted - similar to the way the hashtag system on Twitter evolved. I suspect it’s going to be significantly more complicated since there’s no 140 character limit but am still interested to see where this goes. Play around with it and let me know what you think!",3,1,2015-10-21,3,"json, personal json, quantified self",288,Jsonify.me 2.0
19,0,It seems we're entering a new era in software engineering where the roles are becoming much more specialized.,#meta,"{% include setup %} I’m not sure if it’s always been this way and I just never realized it but it seems there’s a lot of specialization happening within the software engineering industry. For a long time it felt that the majority of the software industry field was shallow enough that being a strong developer was enough to get into anything of interest. Now it does feel as if the industry is turning a corner and there’s significantly more specialization. There have always been exceptions in the past but it does feel specialization was along the edges - now there’s enough depth in the industry that it’s difficult to be a general software engineer, especially further in a career.  Just by looking at various job postings I see the following roles: frontend developer (and within here there are various splits by technology), data engineer, machine learning and AI engineer, mobile developer (which is further divided by iOS and Android), devops engineer, security engineer, sales engineer, and database engineer. And this just scratches the surface - there’s also a ton of software engineering work happening on the hardware side which comes with its own principles.  On one hand it’s possible to make a solid living as a software engineer that ties various third party libraries together in order to deliver new features. This is by no means easy and requires a solid understanding of software architecture and the ability to quickly process spartan documentation combined with the ability to write code in a futureproof way. At the same time this is very different than pushing the boundaries of a discipline.  It’s definitely possible to specialize in more than one area or switch among them but it requires significant effort and will only get more difficult over time. I don’t know what this means other than to be thoughtful about the roles you’re taking and whether they’re something you want to keep specializing in.",0,1,2017-07-09,3,"software engineering, specialization, software development",321,Increasing software engineer specialization
26,0,"Cities strive to improve quality of life for their citizens and working with private companies is a great, but potentially dangerous, way to improve things.",#society,"{% include setup %} Earlier today I read an interesting piece about the [difficulty facing Chinese bike share companies](https://www.theinformation.com/chinas-bike-share-upstarts-face-tough-road-in-u-s) trying to enter the US market. The primary challenge is that many cities have already signed exclusive agreements with ride sharing companies and in many cases subsidized the initial investment. I find this fascinating since it highlights how even in relatively new industries it’s very easy to encounter established players that are difficult to dislodge - especially when they have the backing of the local government.  It’s great seeing governments trying to improve the quality of life for their citizens by investing in new businesses and technologies but it has to be done right. There will never be a final version of anything since innovation will consistently push newer and superior versions. By investing too much in the current version you may preclude yourself from investing in the improved versions of the future. At the same time if you hold out waiting for the next thing you end up hurting the current experience. It’s a tough balance that’s only getting more difficult as the pace of innovation increases.  Rather than partnering I’d like to see the government offer time based licenses and leases on the various components of city infrastructure. This would allow companies to evaluate the opportunity and use that to determine the price they’d be willing to pay. It would also encourage them to futureproof their developments as much as possible in order to take advantage of a constantly changing world. This wouldn’t work for major infrastructure investments that would last multiple decades, such as subways and roads, but I do see it working out for improvements that last a decade.",1,1,2017-07-13,3,"cities, bike sharing, infrastructure investment",293,Cities and outsourced infrastructure
32,0,The cynic in me says that Netflix waited this long to bring profiles back in order to get more people to stop sharing accounts in order to make their history private.,"#product,#pricing","{% include setup %}               Netflix  recently reintroduced  profiles  so now each household member can get their own recommendations, recently watched items, and instant queue rather than being forced to share the same polluted profile. This is an awesome win for Netflix customers but it’s been bugging me that they didn’t do this sooner; it’s such an obvious feature that it should have been built as soon as Netflix realized that multiple family members would be sharing their account.    The cynic in me says they waited 5 years to bring profiles back in order to force family members to sign up for additional Netflix accounts if they wanted to keep their account history private and the only reason they enabled this functionality now is that they’ve either already captured the bulk of these additional accounts or that customers have been asking for this for so long that ignoring the requests makes Netflix look callous. Even now, the profile functionality remains open with all users of a Netflix account being able to access any of the profiles, forcing household members who value their privacy to sign up for additional accounts.  I may be completely wrong and Netflix is correct that making profiles more complicated will cause significantly more usability problems for the majority of users while only benefiting a tiny percentage of customers that care about private profiles. Simplicity is critical for the success of a consumer product but I’m skeptical when user experience and simplicity are used as excuses to avoid a simple feature change. In this case it could be as simple as adopting a model similar to  parental controls  which are set via the web interface for each profile.",3,2,2013-08-14,2,"netflix, business",316,"Netflix profiles, why now?"
29,0,My mortgage got transferred to a new servicer that offered a worse experience. Why do we have no say in something that will affect us for multiple decades?,#society,{% include setup %} I haven’t seen much written about how consumer protection relates to a product’s user experience but it’s a topic that’s worth exploring. I was reminded of this when my mortgage loan was sold to a new servicer. I came home to find a letter in the mail notifying me that my loan has been sold and that going forward I’d have to use a different payment portal and system. It was simple enough to register but the payment process became less efficient and there was no support for a Mint integration.  This is clearly a first world problem and there are a lot of benefits that come with being able to buy and sell loans. It’s the foundation of our financial system and allows companies to specialize across the entire loan business - some are designed for loan origination while others focus on servicing. This also encourages companies to improve their loan valuation models since if they’re able to identify an arbitrage opportunity they can trade on it and profit.  At the same time it’s frustrating that as a consumer I have no say in what happens and it’s a commitment made on my behalf for multiple decades. I don’t know what the right answer is here. User experience is highly subjective and what works for me may not work for someone else and products should hopefully improve over time yet I think there is something here. As an engineer the simple answer would be to force every consumer facing company to expose all functionality and data via an open API which would allow any experience to be crafted around it but I can’t imagine that actually happening. And maybe none of this will actually matter since AI will get to the point where we won’t need to interact with any of these services directly.,0,1,2016-07-09,3,"consumer protection, ux, loans",309,Consumer protection for UX
20,0,The stereotype of crappy government tools is true so why not apply the open source model to fix it?,#society,"{% include setup %} This past week I had two stereotypical government experiences that got me thinking about ways the open source community can help improve government products and services. There’s a huge different in quality and joy when you compare consumer apps with government services. This is understandable: there’s more competition, fewer restraints, and more money in private enterprise so naturally government offerings won’t be as exciting. At the same time government services affect us much more than a generic app.  Imagine having a civic app that’s as pleasant to use as a modern consumer app. It sounds far-fetched but I wonder whether that needs to be the case. The software development community has leveraged open source to create some incredible tools that make us all orders of magnitude more productive yet every time we have to do something with the government it feels we’re sent back in time.  It doesn’t have to be this way. Given how often we interact with the government wouldn’t it be amazing if we could leverage our combined skills to improve the tools that we’re using? Rather than contributing to yet another open source library why not contribute to improving a government site that you think needs some love?  Of course this is easy to say but there’s a ton of hidden complexity. How is the data itself managed and exposed for development? Who controls the code quality and releases? Who decides what makes it in and what gets rejected? There are a ton of questions that would need to be worked through but I do think there’s something here. We already have the [US Digital Service](https://www.usds.gov/) for the federal government but what if the entire open source model could be applied here as well?",1,1,2017-09-22,5,"open source, government, civic, us digital service, usds",296,Open sourcing government
30,0,"It's surprisingly difficult to count the total number of lines in a GitHub account but using a few shell commands, open source libraries, and Python it's not too bad.",#code,"{% include setup %} It’s surprisingly difficult to count the number of lines of code in a GitHub account. One day I’d like to come up with a fully automated solution but in the meantime I’ve come up with a workaround that gets me what I need.  1. Follow the steps in the following [Stack Overflow](https://stackoverflow.com/a/29012789) answer to create your own command, cloc-git, that fetches a repo and runs another utility, [cloc](https://github.com/AlDanial/cloc), that counts the number of lines in a git repo. 2. Get all your repos into a single file, one per line. 3. Bulk edit the file to have each line be an invocation of the clock-git command and save them all to a single file. For example, a single line of the file should be of the format: cloc-git git@github.com:dangoldin/dangoldin-blog.git 4. In the command line simply execute the file and pipe into an output file, for example sh loc.sh > lines-of-code 5. Once the previous step succeeds you’ll have a single text file with the output of the cloc-git command for every specified repo but the formatting is not the easiest to follow. 6. Run a simple grep command to get every line containing the SUM line: grep “SUM” lines-of-code and save this to the clipboard 7. Unfortunately the spacing is all off so you can’t use the cut command to do a split via the shell so you have to use a simple programming language. I used python and just dumped the contents into a single variable and ran the following command to split it into lines and then retrieve the last value when splitting by a space. 8. Once you have these values just do a simple sum to get the total number of lines.  It’s not very simple and forces you to use a variety of tools to get to the final result, ranging from reading Stack Overflow documentation to some shell commands to some Python scripting. It’s a good example of where having enough breadth of knowledge and experience with a variety of tools turns a hairy problem into one that can be solved relatively quickly. It’s not a perfect solution but for a one-off I’m happy with the results.",2,1,2018-12-13,3,"git, lines of code, shell script",383,Counting the number of lines of code in a GitHub account
21,0,I volunteer tough a four part Intro to MySQL class and wanted to share some of the lessons I learned.,#meta,"{% include setup %} Last Thursday was the last lesson of the four part Introduction to MySQL class I’ve been teaching at  Coalition for Queens  and I wanted to summarize my thoughts while they’re still fresh.  The diversity of the class was amazing and shows how useful affordable technology programs are. You get a mix of people from different backgrounds and different ages that all want to improve themselves and can all contribute in their own ways. Everyone has a unique experience and introducing technology into it may open up new opportunities.  It’s tough to get a curriculum that works for everyone but it’s important to try. Some people grasp concepts quicker than others. Some want to see more hands-on exercises. Some want homework assignments. Some want to be able to split up into groups and work with others on more complicated assignments. The dataset itself needs to be relevant and realistic or people will lose interest. I put the  curriculum up on GitHub  for suggestions but didn’t get any - hopefully others will use it in their classes.  Tools matter. For the first two sessions I used Keynote to generate the presentation and then exported it to a PDF. This approach lacked syntax highlighting which I wanted given the technical nature of the course. I switched to the wonderful  Remark.js  which allowed me to create slideshows using GitHub flavored markdown. This allowed me to integrate exercises into the lecture while incorporating syntax-highlighted examples. One issue was that it wasn’t as straightforward to export it as a PDF and I had to print it to a PDF file via Chrome.  Volunteer teaching is a great way to “teach to fish” rather than just giving a fish while helping develop public speaking skills and meeting a ton of awesome people. If you’re in New York City, work in technology, and believe technical skills are increasingly important you should take a look and try teaching a class at Coalition for Queens.",3,1,2014-10-27,3,"teaching, volunteering, mysql",344,Lessons learned teaching a MySQL class
8,0,Some advice on setting up internet passwords.,#meta,"After my previous post on the lack of privacy, I feel obligated to give some advice regarding internet passwords in order to maintain the privacy that we do have.          Have at least 3 different passwords:           	 E-mail Account          This account controls all your other accounts so protect it as much as you can. All other accounts can be accessed or reset if someone has access to your email.     	 Bank/Financial Accounts          These control your money so use a different password for these than for the rest of your accounts. In addition, you may want to keep your credit card account passwords separate from your bank accounts.     	 ""Fun"" Account          These may not be vital to your survival (unless you are a facebook addict) so a password compromise here may not affect you too much. In addition, these sites may not store your password as securely as the bank accounts so you don't want this password being the same as the other accounts.            A good way to generate passwords is to contain some sort of ""base"" and add some prefixes or suffixes to it in order to come up with the password for the various sites. For example, I can have my base password be ""orange"". For financial sites my password will be ""orangeFIN22"", for my email it will be ""orangeE33"", etc. Then you don't have to remember an entirely different set of passwords yet they are distinct enough to avoid compromising all your accounts with a stolen password.              Don't trust web sites that are able to send you your password over email       If a website is able to tell you what your password is, it means it is storing it in the database as either the password itself or through a transformation that is reversible (a becomes b, b becomes c, ..). This means that the site knows what your password is and can be easily accessed by employees of the site or anyone that has access to the database.        The proper way to handle user passwords is to hash it (one way map) immediately to some obfuscated characters and store those in the database along with an additional field that ensures each row is hashed differently. Then when a user logs in, the site will do this one way map and compare the result against the value in the database; omly if they match is the user logged in.",0,1,2008-06-19,3,"security, passwords, internet",408,Advice on internet passwords
29,0,While going on a Wikipedia tangent I ran across a list of history's largest empires and thought it would be interesting to have some fun with the data.,#dataviz,"{% include setup %} I got a bit distracted today and ended up coming across the Wikipedia page listing [history’s largest empires](https://en.wikipedia.org/wiki/List_of_largest_empires). The page came with a list of the top 140 by land area and just by looking at them you can see there’s a huge range. The British Empire was the largest at 35.5 million square kilometers while the Sumer was the smallest at 0.05. That’s a huge difference - over 700 times - and I thought it would be interesting to plot them to visualize the distribution. As expected, there’s a very steep drop and a long tail. If you add up the land areas of all the empires listed you get just over 455 million square kilometers. That metric itself doesn’t mean anything but it helps to normalize the land areas. The British Empire, for example, is 8% of the total and if you keep going down the list in descending order by size and sum up the percentages you get that the first 55 empires add up to 80% of the total land area. Once again, the total land area is useless metric but it allows us to see how close we are to the [80-20 rule](https://en.wikipedia.org/wiki/Pareto_principle). It turns out not too close - 55 countries out of 140 are just over 39%. The top 20% empires add up to 52% of the land area. If you’re interested in playing around with the data it’s up on the Wikipedia page as well as an [Excel version](/assets/static/data/largest-empires-land-area.xlsx) with cleaned up data.",3,1,2018-11-22,3,"empires, land area, 80-20 rule",281,History's largest empires
33,0,If you're a frequenty user of MySQL you should become familiar with the information_schema.columns table. It's very handy for getting information about all the columns and tables in the MySQL database.,"#code,#sql","{% include setup %} Something that’s been really helpful to me in understanding a MySQL database is the built in  information_schema.columns  table. It provides information on every column in the database and is queryable just like any other table. This makes it easy to quickly find all tables that have a particular column name or all columns that are the same data type. There have been countless times where I knew the data existed somewhere but couldn’t recall which table it was in. Querying the information_schema.columns table for the foreign key helped me quickly figure it out. Below are some sample queries that retrieve data from the information_schema.columns table:  {% highlight sql %}select table_schema, table_name from information_schema.columns where column_name like '%user_id%’;  select * from information_schema.columns where column_name like '%time%’;  select * from information_schema.columns where data_type = 'datetime'; {% endhighlight %}  Since it’s just like any other table, except for being read-only, you can write jobs that access the data. Something I had to recently do was write a quick script to generate fake data. All a user has to do is specify the table name to populate and the script would look up the columns and their types from information_schema.columns and dynamically generate the INSERT statements. For example, if a column was of type varchar it would generate a random text string less than or equal to the length constraint and if it were an int it would generate a random number. It wasn’t perfect and only handled foreign keys that were specified by the user but it was great given the effort. A later version could use the information provided by two other information_schema tables, table_constraints and key_column_usage, to get rid of this manual step. If you’re a frequent MySQL user, familiarizing yourself with the tables in information_schema will make you significantly more efficient.",1,2,2013-12-15,3,"mysql, inforamtion_schema, information_schema.columns",321,Using the information_schema.columns table
8,0,Just some observations I have about pronunciation.,#meta,"Recently I've discovered a few words where I know the definition and spelling but I don't know the pronunciation. The problem is that I think I know how to pronounce them so when I use them for the first time in conversation, or hear someone using them, some confusion arises (as well as making me look like a fool).  I am not sure if this is isolated to me or society as a whole. The world does seem to be getting more and more open so maybe this is a result of that - the spoken dictionary is getting smaller and smaller so many of the words that were commonly used in conversations decades ago are no longer being used.  In any case I think we should all try to identify such words and try to use commonly written words in conversation, we may be surprised by how wrong we are.  For me, the word I didn't know how to pronounce was ""albeit."" I was pronouncing it as all-bite when it turns out that it is pronounced all-be-it.",0,1,2008-05-08,1,pronunciation,170,Pronunciation
30,0,AMP comes with a set of restrictions that make it difficult to get Disqus comments integrated. There's a pretty simple way of doing it using amp-iframe and S3.,#code,"{% include setup %} After migrating my blog to AMP the last task was getting [Disqus](https://disqus.com/) working again. The crux of the issue is that in order to improve page performance AMP disallows blanket script tags (which the Disqus integration leverages) but to make up for it comes with a variety of helpers to include officially support functionality. Examples of this include an amp-youtube tag to include YouTube videos and the amp-vimeo tag to include Vimeo videos. As a generic solution, AMP provides the amp-iframe tag which allows you to include a restricted iframe.  Doing the research it turned out there was no out of the box solution but after a bunch of false starts I came across a great [post](https://labs.tomasino.org/disqus-in-amp) by [James Tomasino](https://twitter.com/mr_ino) where he ran into similar issue and came up with a workaround that was simply creating an additional HTML page for each post that contained the appropriate Disqus code which could then be included via the amp-iframe tag. Unfortunately this approach wouldn’t work in my case since amp-iframe requires HTTPS and my blog is solely HTTP due to being hosted on GitHub pages with a custom domain.  The workaround I came up with is to take the script James came up with and make a few tweaks to it that allow it to be hosted on an S3 bucket. I also wanted to avoid having to build an additional comment HTML page for each post for each new post and made a small change that allowed me to pass the relevant details as GET arguments into the comment iframe page. If you’re interested in the implementation, just take a look at the source of this page or check out [https://s3.amazonaws.com/dangoldin.com/amp-disqus.html](https://s3.amazonaws.com/dangoldin.com/amp-disqus.html).",4,1,2016-09-13,3,"disqus, amp, iframe-amp",306,Supporting Disqus in AMP
28,0,I'm a huge fan of Markdown and love the way it's remained a tool rather than turning into a product. It's a lot more powerful this way.,#product,{% include setup %} I keep on discovering new use cases for  Markdown  the more I use it. My first exposure was when I migrated my blog to GitHub pages from Wordpress and Tumblr. Since then I’ve discovered GitHub flavored markdown which supports syntax highlighting which has been amazingly useful when blogging on tech topics or putting together notes for a tech talk. Just recently I wanted to include some MySQL snippets in a Keynote presentation and discovered the  Remark.js  library which lets you generate in-browser slideshows in Markdown with syntax highlighting.  There’s this desire to turn tools into products and it’s refreshing to see tools stay tools. They are able to stay much more flexible and evolve organically based on the needs of users rather than a preset direction. Markdown is a perfect example of this - it started simple but has evolved to have multiple variations and is used as the base for many other projects. At this point it has become such a standard that it can’t be co-opted by any single product or company.,2,1,2014-11-01,3,"markdown, product, tools",187,"Tools as tools, not products"
35,0,Nearly every API library is the same general steps. I expect we'll see API libraries that will be automatically generated based on a data model or even an AI process that 'reads' the documentation.,#meta,"{% include setup %} A large part of modern software engineering is working with external APIs and services. Whether you want to automate a deployment on AWS, collect payments via Stripe, or track various behaviors using MixPanel, the process is the same - go through their documentation to figure out the available endpoints, the request requirements, and what the response will be. The next step is writing a simple API wrapper around the relevant endpoints that can then be accessed by the rest of the application. Given all the investment in AI research I’d love to see an application that’s able to generate API wrappers in any language for an API based solely on the documentation. Amazon has taken the first steps by [developing a data model](https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/) to represent their API which is then used to generate the actual libraries in a variety of languages. By changing something in the definition they can quickly rebuild the libraries in every language. One can also imagine using this data model to generate the actual documentation. This documentation can then be used to go back to the data model which can then be used to go back to the documentation.",1,1,2015-12-19,3,"api, ai, amazon",206,Automatically generating APIs
30,0,Everyone extols the value of having a cofounder but it's critical that the founders are aware of each other's situations. Otherwise this will cause problems when things get tough.,#meta,"{% include setup %} Countless people have written about cofounder conflicts in a startup but I rarely see anyone talk about how important a similar situation is - financial and personal. There are no problems when things are going well and it's only when things start going poorly, which they inevitably will, that these issues surface.  A founder that doesn't have a lot of savings will have a different relationship to fundraising than the founder who has enough savings to keep going. The former will push to fundraise early while the latter will want to wait and search for the best opportunity.  A founder that's single will have a different lifestyle and priorities than someone who is married or in a serious relationship. The founders will have a different approach to work and may have a hard time agreeing on a culture that fits them as well as the rest of the team.  A founder that wants to start a company is different than a founder who wants to strike it rich is different from a founder who believes in changing the world. Each of them are valid perspectives but put them in a room together and it will be impossible to make even the simplest decisions.  At  Pressi , we had our fair share of founder conflicts and in hindsight a big part of it was how different our situations were - it's definitely beneficial to have a team come from diverse background and provide multiple perspectives but it's also critical that everyone understands where everyone else is coming from. And it's important to do this when things are going well, otherwise they will become bigger issues when things are going poorly and every emotion and event is magnified.",1,1,2014-08-22,2,"startups, cofounders",291,Cofounders and their situations
16,0,I believe Snapchat will be huge if they keep executing the way they have been.,#product,"{% include setup %} The more I use Snapchat the more obvious the potential. The way the product has evolved reminds me of Facebook’s history. Facebook started simply as a profile page for Ivy League college students but due to strong execution and brilliant product decisions has grown into the current behemoth. Snapchat is on a similar path - the initial version was a simple ephemeral photo sharing app but the recent updates seem frequent and massively impactful.            Movie tickets within Snapchat     Earlier today I was messing around and spotted a movie trailer ad for Swiss Army Man that was followed by an option to swipe up to buy the movie tickets that felt native to Snapchat. I didn’t have to click to go to another app and it felt natural to just swipe to get to the next step. And this was a simple case of buying movie tickets. I can imagine this flow expanding to other scenarios that follow a powerful ad with an immediate transaction. This echoes WeChat - the powerhouse app in China that’s an unholy mix of a [social network, a payments platform, and an app ecosystem](http://a16z.com/2015/08/06/wechat-china-mobile-first/). There’s no equivalent of WeChat outside of China and I suspect most think the replacement will look like a WeChat clone. Snapchat feels completely different yet has the potential to be more. WeChat’s foundation is a third party app ecosystem that’s built on top of text while Snapchat is almost entirely visual and asserts a high bar for third party experiences.  This is huge. Interacting with Snapchat is a joy, whether it’s taking photos, playing with the filters, checking our your friends’ stories, or watching the Discover videos and this is due to the masterful job they did with the interactions. They’re not immediately obvious but once discovered are intuitive and consistent across the variety of experiences. Want to go next? Just tap. Want to dig deeper? Swipe up. Want to go back? Swipe down. This is incredibly powerful. By learning these shortcuts Snapchat is able to offer a variety of adventures that users can easily engage with without taking up any additional screen space. This allows every experience Snapchat offers to take up the full screen which keeps us in the moment and makes it easy for us to keep going.  Snapchat is already taking the baby steps of becoming a platform by enabling external parties to build on top of Snapchat. The obvious case are the content producers Snapchat is partnering with but a more telling example is the way they’re approaching geofilters. [Geofilters](https://snapchat.com/geofilters) are created offline, are then uploaded to the Snapchat site, and after approval become accessible in the app. This is a foreshadowing of the Snapchat formula - build out a compelling in-app experience and then follow it up with tools for outsiders to craft their own.  The movie trailer ad can be extended to highlight products - a compelling video of a product that can then be followed up with an option to buy. This can extend into multi-touch - imagine being able to tap on different sections of the screen that drive different experiences. If I’m watching the Olympic trials I can tap on the different players to get some more information about each one or if I’m watching some NBA highlights I can tap on LeBron’s jersey or sneakers to get taken to the Snapchat-integrated Nike Store. And this is just scratching the surface - by focusing on new highly engaging user experiences and setting up the tools to create these compelling stories Snapchat can transition into an incredibly powerful platform.  Snapchat should have no problems monetizing. The advertising industry can be broken down into two major types - brand advertising and direct response. Brand advertising is the typical TV ad that’s focused on building awareness and selling a story and lifestyle. Direct response, on the other hand, is about getting the customer to “convert” and requires every dollar spent to back out into a measurable return. Think of Google Adwords - you search Google for a book, click the sponsored Amazon link, and then buy it - Amazon will then know how much you paid for the book as well as how much the click cost. Using this data they can then optimize their campaigns to maximize profit. Snapchat may be able to sit at that intersection. It’s the perfect platform for high quality brand videos that take up the full screen but can also drop down into transactions - think of my movie trailer/ticket experience earlier today. This will help them get closer to the holy grail of advertising by attributing purchases to brand advertising.  Snapchat is tiny compared to Facebook but major shifts in the tech world have never been direct. New platforms start at the fringes but keep growing until they supplant the incumbents. Google superseded Microsoft by becoming the entry point to the web. Facebook is supplanting Google by bypassing the open web and providing app experiences on every platform. How would Snapchat unseat Facebook? I don’t know but I’m sure Facebook’s watching.",2,1,2016-07-03,3,"snapchat, facebook, google",863,Snapchat's massive potential
33,0,One of the more annoying things to do is split an AWS account into multiple ones as your needs change. This is a quick description of what I had to go through.,#aws,"{% include setup %} When we launched  Pressi , I had it set up under my personal AWS account. Recently, we needed to move it into a separate AWS account and I wanted to share the steps to help others running into the same issue. Unsurprisingly, most of the effort went into planning and figuring out the migration steps and order in which they should be done. We weren’t able to eliminate downtime entirely but we reduced it as much as we could.  The services migrated included Route 53, an EC2 instance, ELB, S3, and Cloudfront. At the high level, we copy every service we can (EC2, ELB, S3, Route 53) to the destination account before redirecting client traffic to the new account. After that, we migrate the remaining services (Cloudfront) and make updates to existing ones (Route 53, EC2) to point to the destination account.  Migrating EC2 and ELB:    	 Create the destination AWS account  	 Create an AMI of the instance on the original account  	 Share this newly created AMI with the destination AWS account  	 Launch the AMI in the destination account  	 Set up the load balancer in the destination account to mirror the original     Migrating S3/Cloudfront:    	 Create an S3 bucket in the destination account and copy the files over from the original bucket to the destination bucket. We used  Bucket Explorer  for this piece but needed to change the file permissions in the destination bucket manually to mirror those in the original account. One thing to watch out for is that S3 bucket names need to be unique so your code will need to be updated to reference the new name.  	 Update the Cloudfront record in Route 53 to point to the destination account. Note that after the migration runs you can also update the Cloudfront record in the original account to point to the Cloudfront CNAME of the destination account.  	 Cloudfront requires unique CNAME records so we give it a temporary name until you kick off the migration. As soon as you do, you will need to remove the CNAME record from the original account and add it to the destination Cloudfront account.     Migrating Route 53:    	 Copy the records from the original account to the destination account.  	 Make sure to update the Start of Authority (SOA) and Name Server (NS) records in the original account to have the same values as the ones in the destination account to speed up the DNS propagation.     Migrating the code:    	 This will entirely depend on the application but the goal is to update your code to reference the services on the destination account.  	 Due to the non-immediate nature of DNS propagation, you will most likely need to run two code bases - one on the original account pointing to some of the original services and one on the new account pointing to the destination services. Depending on the statelessness of your code, this may lead to a variety of sync issues and will require some intricate code to handle properly. In our case, we had MySQL running on the EC2 instance so while the app was running simultaneously under two AWS accounts the database would get out of sync with some users hitting the original setup and others hitting the destination. Luckily for us only a few tables were affected and we had to run a few manual SQL queries to deal with the issue but it could have been a lot worse.     The last step is to update your domain registrar NS records to point to the destination account and wait for the migration to occur. Note that the migration will happen gradually so you should look at the server logs on both accounts to make sure there’s no traffic hitting the server in the original account.  The lesson here is that migration becomes a whole lot easier if you keep your architecture as stateless and modular as possible. This way the services are loosely coupled and can be migrated one at a time rather than having to do everything at once. Your app also becomes significantly easier to scale since additional EC2 instances can be provisioned without having to worry about them getting out of sync. The non-instantaneous nature of DNS complicates the migration but a stateless architecture helps address most of the issues. Our migration didn’t go 100% smoothly but having mostly stateless services definitely helped us avoid major problems.",2,1,2013-08-24,7,"AWS, EC2, ELB, Cloudfront, S3, splitting AWS, migrating AWS",760,Splitting an AWS account
24,0,It's interesting to look at my family's smartphone usage and see if you can guess who's who. Lots of interesting behaviors and patterns.,#meta,"{% include setup %} By examining my family’s phone usage you get an interesting representations of how different generations use their phones. The five of us - parents, younger brother, and younger sister - are all on the same plan and T-Mobile breaks down the usage into phone minutes used, messages sent, and total amount of data used. I played a game with my friends to see whether they’d be able to decipher who’s who but it turned out to be surprisingly difficult and unintuitive. Turns out that my teenage sister uses least talking minutes and data but consumers average number of texts. At the same time, my mom runs a business and has close to 2000 minutes of talk time with the most number of texts sent while only using a moderate amount of data. My brother and I have a similar usage pattern - low minutes and messages but the highest data usage out of the entire family. The biggest surprise is my sister’s data usage that reinforces how much time teenagers are spending via apps and on separate social networks. That stereotype I grew up with of teenagers being constantly on their phones is still true - texting and talking have just bee replaced by siloed apps.",0,1,2015-12-28,3,"smartphones, technology, generations",208,Smartphone usage by generation
28,0,Something I've started doing since joining TripleLift is doing the bulk of my developing on a remote EC2 instance and think it offers a ton of benefits.,"#code,#aws","{% include setup %} One of the first things I was given when joining  TripleLift  was a Macbook Air and an Amazon EC2 instance to do my development work on. Before that, every company I worked at would give me a pretty powerful computer so that I’d be able to do my development work locally. At first, coding on a remote instance took some getting used to but now I'm a fan of this approach.    	 It allows me to work from any computer and paired with the highly portable Macbook Air I can work from virtually anywhere. On the flip side, it relies on a connection to the internet so if the internet ever cuts out it’s difficult to do work unless you also have it checked out locally.   	 It’s a great way to simulate a production environment. Especially on OS X where many packages require significant Stack Overflowing to figure out, being able to install libraries that will be used in production is a great way to work out the kinks and be confident that your code will run as expected.   	 Along these lines, the entire team will end up with a very similar environment which makes it very easy to give and receive help without having to get used to an entirely new environment.   	 If your application relies on EC2 it’s a great way to become familiar with the AWS ecosystem as well as reduce latency between Amazon’s various services. This is useful when you have a significant volume of data going to and from S3 and want to make it as quick as possible.   	 There are a ton of tools to make this easy. I’ve recently discovered the  SFTP plugin  for  Sublime Text  which lets you edit your files locally that are automatically synced to the remote instance. That paired with emacs or vim on the instance are all you need.     The biggest drawback is that you end up relying on the internet in order to get the most out of this set up. It’s possible to have your code synced locally for editing but getting set up to run locally defeats the purpose of having a remote instance since you have to install and configure the various packages. Given that there’s internet almost anywhere I go I think this trade off is worth it.",3,2,2014-01-23,5,"developing, coding, EC2, AWS, Amazon",407,Developing on a remote instance
15,0,"If you're interested in startups, check out the class Peter Thiel is teaching at Stanford",#meta,A great blog I’ve recently started following is  Blake Master’s notes  from Stanford’s CS183 class being taught by Peter Thiel. Peter provides an insightful view of the tech startup world that is valuable to anyone interested in startups and entrepreneurship.,1,1,2012-05-16,3,"peter thiel, entrepreneurship, startups",55,Peter Thiel's CS183
26,0,I worked on a side project while on a flight and it's a completely different experience without the internet. I think everyone should try it.,#meta,{% include setup %} Whenever I fly I try to be at least somewhat productive. This time it entailed finishing up an old blog post and messing around with Node.js on a side project. They say the only way to appreciate something is when it’s gone and that’s how I feel about developing without internet access. It’s such a common occurrence to need to look up the documentation for a particular function or library or search for novel error messages that my approach is completely altered without the internet. Where before a few visits to Google or Stack Overflow would have taken care of the problem now I get to rely on man pages as well as dozens small experiments to figure out what’s happening.  It’s definitely not as efficient as having everything at your fingertips but it’s definitely fun in small doses. In my case I get to brush up on some rusty skills and also get to be a detective when diagnosing bugs. Knowing that you will be coding without internet also forces you to set up a standalone development environment. This requires making sure all the static assets you use are available locally and can be served by your application as well as making sure you have a local database that contains realistic data.  It’s tough to avoid the internet when coding but it’s a worthwhile exercise to attempt a few times a month - it will make you appreciate what you have but also introduce you to a whole new set of skills.,0,1,2014-12-07,2,"coding, internet",258,Internetless coding
19,0,I wrote a pretty basic set of functions to make it easier to work with localStorage and lists.,"#code,#javascript","{% include setup %} I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.  {% highlight javascript %} // Also let caller specify max size of list function addItem(k, v, limit) {   var a = getItems(k);   a.push(v);   if (!isNaN(limit)) {     while (a.length > limit) {       a.shift();     }   }   localStorage.setItem(k, JSON.stringify(a)); }  function getItems(k) {   var a = null;   try {     a = JSON.parse(localStorage.getItem(k));   } catch(e) {}   if (a && Array.isArray(a)) {     return a;   }   return []; }  // Tests/Examples localStorage.setItem('test_list', null);  addItem('test_list', {""name"": ""Dan""}); addItem('test_list', {""food"": ""pizza""}); addItem('test_list', {""beer"": ""Newcastle""});  var l = getItems('test_list');  console.log('Lengths match: ' + (l.length === 3)); console.log('Value 0 matches: ' + (l[0].name === 'Dan')); console.log('Value 1 matches: ' + (l[1].food === 'pizza')); console.log('Value 2 matches: ' + (l[2].beer === 'Newcastle'));  addItem('test_list', {""size"": 2}, 2);  l = getItems('test_list');  console.log('List limit works: ' + (l.length === 2)); console.log('Value 0 matches: ' + (l[0].beer === 'Newcastle')); console.log('Value 1 matches: ' + (l[1].size === 2)); {% endhighlight javascript %}",0,2,2015-02-26,3,"javascript, localStorage, lists",243,Lists and localStorage
17,0,I had to upgrade an HD in a Macbook and I wanted to share my approach.,#meta,"{% include setup %} As a gift to myself I decided to upgrade the RAM and HD in my MacBook. The plan was to replace the old HD with the new one and then use the install disc to install Snow Leopard on the new HD before upgrading to Mountain Lion. Unfortunately, it turned out that I had a bad install disc and had to come up with another approach. The general idea was to upgrade to Mountain Lion first in order to create another boot disc.  1. Upgrade OS to Mountain Lion and make sure to have a copy of the installation file around 2. Put the new HD into an external enclosure 3. Create two partitions on the new HD, one should be 8 GB to hold the Mountain Lion install boot disc 4. Use  Lion Disk Maker  to create the boot disc on the 8 GB partition of the new HD ( instructions ) 5. Shut down the computer and replace your HD with the new HD 6. Boot your computer while holding down the option key 7. Select the Mountain Lion boot disc from the selection screen and install it on the other partition 8. Mountain Lion is now installed on the new HD 9. You can now remove the Mountain Lion boot partition and use that space for something else  An easier approach would have been to just use a USB flash drive or an SD card to create the boot disc. Unfortunately, I didn't have any that had a capacity of more than 8 GB so I had to resort to this hack.",2,1,2012-12-16,1,,286,The joy of upgrading an HD in a Macbook
20,0,Amazon's Fire TV Stick is great. I've been using it to replace my Xbox 360 and a Raspberry Pi.,#product,"{% include setup %}           Photo by  Rancho de la Luna      Earlier this week I set up Amazon’s [Fire TV Stick](http://www.amazon.com/Amazon-W87CUN-Fire-TV-Stick/dp/B00GDQ0RMG) and wanted to jot down some thoughts while they’re still fresh. For the $20 promotion price, it’s a great deal. My alternative was an Xbox 360 along with a Raspberry Pi running XBMC. The Xbox would be used for streaming shows on Netflix and watching older DVDs while the Raspberry Pi would let me watch various files off of a USB stick. I’ve never tried a Chromecast so don’t know how the Fire Stick compares but so far it’s been much quicker to startup and navigate than either the Xbox or the Raspberry Pi. When all you want to do is watch a quick show during dinner it’s a bit frustrating when you’re done eating by the time the Netflix app is ready to use on the Xbox.  Other than speed, the other big benefit has been the ability to use my phone as a remote. This allows both voice search, which Amazon has been doing a good job of transcribing, as well as a real keyboard. Having to type by navigating an alphabetically-sorted on screen keyboard with a laggy controller is not fun.  I haven’t had a chance to install any games or explore any of the other apps but I’m still impressed. It might be time to finally get rid of the Xbox and the old DVDs.",2,1,2014-12-18,4,"amazon fire tv stick, firestick, chromecast, xbox",267,Amazon's Fire TV Stick
18,0,Medium autocompletes suggested tags and it's interesting to see what pops up when you type in Startup.,#meta,"{% include setup %}             While I keep my primary blog on dangoldin.com I also cross post to Medium to get some more views but also because I’m curious to compare the performance. While adding tags to one of these posts I typed in startup and saw the following suggestions and their frequencies: Startup (334K), Startup Lessons (10.8K), Startup Life (3.6K), Startup Marketing (2.3K), and Startup Ideas (1.1K).  The frequencies stood out. It makes sense that the generic term “startup” would have the highest volume but it’s interesting to see how the others compare. I was surprised that the second highest, Startup Lessons, was only 3% of the first one. I was also surprised that the next two were Startup Lessons and Startup Life - they’re both about the same topic but it seems lessons is someone writing about the past while life is someone writing about the present. I suspect it’s catharsis where founders want to write about their mistakes while far fewer want, or have the time, to write about their current startup experience. I know when I was working on my startup I definitely didn’t have the time for writing but at the end it felt great to do a brain dump of everything I learned along the way.",0,1,2018-12-15,2,"medium, blogging",236,Overanalyzing Medium's tag suggestions
21,0,Amazon doesn't make it simple to retrieve all of your highlights and it's surprising given how API centric AWS is.,#product,"{% include setup %} At this point I’ve moved most the majority of my reading to a Kindle. If I want to read a book I’ll only get a physical version if the Kindle version is unavailable. It’s often cheaper, immediately available, takes up no space, and comes with a variety of annotation abilities. My biggest frustration has been how difficult it’s been to export my highlights. I will often highlight interesting passages and quotes and every few weeks will dump them into a [giant text file](https://github.com/dangoldin/quotes/blob/master/quotes.txt) that I will occasionally reference or search.  I wish I could simply export all my highlights into a single file. Instead, the best way I’ve found so far is to use the Kindle Notes and Highlights section and manually copy and pasting them over while removing the unnecessary text captured in the selection. It’s not too terrible since I can at least fetch them in bulk but I wish there was a more automated way. Given how API centric AWS is I’m surprised this hasn’t translated into Amazon’s other products. Building a strong Kindle ecosystem is a great way to improve the overall product and make it that much more costly to switch and building a proper API is the first step.",1,1,2018-02-25,3,"amazon, kindle, api",212,Retrieving Kindle highlights
26,0,Social networks each have their own differentiated offering so it's interesting to see people sharing and posting items that bleed from one to the other.,#meta,{% include setup %} Social networks carry extreme network effects and have massive winner-take-all dynamics. This makes it impossible for two social networks that have the same pitch to co-exist and leads to pretty strong differentiation. Facebook owns relationships. Twitter owns interests. Instagram owns lifestyle. Snapchat is starting to own experience. This is why I find it fascinating when the content from one social network or medium bleeds into another. Twitter doesn’t allow for tweets longer than 140 characters so people overcome that by sharing screengrabs of long form text. I’ve seen the same on Imgur - it’s primarily used for images but often you’ll see someone posting an image of a long story. We have our own unique relationships across each of these networks so it’s not surprising that we’ll sometimes want to communicate something that’s best expressed with a specific medium yet it’s still fascinating seeing it in action. I get the feeling that they’re publicly exploiting a loophole and adding a tiny bit of chaos to the universe.,0,1,2016-09-01,1,social networks,174,Violating the norms of a social network
30,0,Constantly seeing something makes you oblivious to the changes and that's how I feel about technology. Traveling breaks that and lets us see how much tech is actually changing.,#meta,"{% include setup %} If you’re constantly watching something grow it’s hard to notice the magnitude while those further away see it immediately. This is well known for parents not seeing how quickly their children are growing but obvious for distant relatives and friends who get a glimpse once every few months.  I have the same relationship with technology. I’m surrounded by it each day that it’s hard to tell how much it’s changed but a way to combat this bias is by traveling, especially to developing countries.  I’m writing this blog post on a Macbook Air that has a ridiculous amount of battery life at the Delhi airport while tethered to a mobile hotspot running off of my US phone. It's not as fast nor as comfortable as what I get at home or in the office but it's incredible, especially when compared to my two prior trips, a year ago and four years ago. During last year’s trip I had to buy a cheap Android phone and spent countless  hours running around  trying to get a working SIM card. This time I didn’t even have change SIM cards due to T-Mobile’s global roaming plan. Four years ago I didn’t even bother doing anything on my phone and had a 3G USB dongle that required it’s own proprietary software to even connect to the internet.  New apps and products launching is just the surface, the biggest changes are happening to infrastructures that make these experiences possible. These aren’t visible day to day but are unmistakable when seen year to year and traveling makes them apparent.",1,1,2014-12-13,2,"traveling, technology",274,Travel to appreciate technological growth
16,0,Go takes a simple and novel approach to interfaces that's commonly found in dynamic languages.,#code,"{% include setup %} I’ve only been playing around with Go for a couple of weeks but one of the language design decisions I’ve really enjoyed is how interfaces are handled. Coming from a traditional object oriented background it’s typical to define an interface that defines a few method signatures and then explicitly implement that interface in a new class. Below’s a trivial example of this approach in Java:  {% highlight java %} interface Animal {   public boolean isFurry();   public String speak(); }  class Dog implements Animal {   public boolean isFurry() {     return true;   }    public String speak() {     return ""Woof"";   } }  public void aRandomFunction(Animal a) { ..  } // Can take anything that implements Animal {% endhighlight java %}  With this approach a compiler immediately identifies cases where you choose to implement an interface but forget (or mess up) implementing one of the underlying methods.  Go’s approach is different. In go you would define the interface as usual with the expected methods and you would write functions that accept the interface as the argument. But instead of explicitly specifying that a particular object implements an interface you just do it. Then if it turns out you’ve successfully implemented the methods you can use that object wherever the interface is expected. The compiler is still able to point out signature issues since it can tell when you’re trying to use an object with a required method but it’s done in an implicit way. Below’s the equivalent Go code:  {% highlight go %} type Animal interface {   isFurry() bool   speak string }  type Dog struct { }  func (d Dog) isFurry() bool {   return true }  func (d Dog) speak() string {   return ""Woof"" }  func aRandomFunction(a Animal) { .. } {% endhighlight go %}  Dynamic languages frequently use this “duck typing” approach since the variable types may only be discovered during run time so it’s neat seeing it implemented this simply in a static, strongly typed language. The simplicity and novelty of Go’s interfaces make me eager to keep digging and see what else I discover.  https://en.wikipedia.org/wiki/Duck_typing",0,1,2015-07-29,4,"golang, java, duck typing, intefaces",348,The Go interface
35,0,Keep a database clean is underrated. One of the easiest things to do is to drop unused tables. I have a few tricks I've picked up over the years to amke this process easy.,"#sql,#code","{% include setup %} When writing code it’s very easy to accumulate deprecated database tables that end up as zombies - they’re still around and may even be populated and used by a variety of side scripts but if they disappeared and the dependent code was removed nothing would be different. In fact you’d have a smaller code base, a smaller database, and would hopefully improve everyone’s productivity a tiny bit.  Dealing with the tables are are still being populated and read requires a bit of investigative work and knowledge of the product since there’s no simple way of identifying them. But there are a simple ways to identify tables that are no longer updated.  ### 1. The metadata Some databases provide a last updated flag as part of the metadata tables. For example, MySQL contains an update_time field inside the information_schema.tables table for MyISAM tables. Reading the MySQL documentation it also looks as if recent versions will have this set for some InnoDB tables as well.  ### 2. The temporal columns In the case where there’s no metadata for a table you have to resort to a bit of trickery. If your table has any form of a time column then you can write a very simple query - **select min(timestamp), max(timestamp) from table** - to spot the most recent data in a given table. If this date is old you may be able to safely assume that this table is no longer being populated or maintained. Combining this quick trick with data from the informatino_schema.columns table and you can write a very neat query that can run this check across the entire database. For example, you can first run **select table_schema, table_name from information_schema.columns where column_name = 'timestamp'** to identify every table that contains the timestamp column. Then you can automate the creation of a monster query that will generate a checking query for each of the tables and then union them all together. So then you end up with something akin to **select 'table_1' as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_1 union all select 'table_2' as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_2 union all...** The query may take a while depending on the indices but once it does you can quickly sort by the max timestamp to quickly spot the potentially unused tables. A small adjustment you can make to deal with tables that may still be getting populated is to look at the number of rows that exist by day. If you see a huge decline it can be a good indicator that this table may just be getting some noise from an older job and is safe to remove, but only after removing the deprecated job.  ### 3. Snapshot and monitor But what do you do if there’s no metadata and no timestamp column? Ideally you’d have created and updated timestamps in every table. If not you can either add these to be automatically set and see whether anything changes over time or you can just take a manual snapshot today and a few days or weeks later to see whether anything changed. If the table is too large you can compare the number of rows or some summary statistics. The general idea is to compare it against multiple periods of time to see if, and how much, it’s changing.  These are just a few tricks I’ve picked up over the years trying to keep database schemas clean. Most companies do a good job managing the deployment process when generating new tables and writing new code but it’s rare to find companies that tend to their database garden. I believe maintaining a clean database is underrated - it’s valuable to know that everything in your database is used and that you don’t have to worry worry about an obscure script touching an obscure table you’ve never heard of. I’d love to know if people have other tips that can be used to both keep, and get, a database clean.",0,2,2016-05-11,5,"sql, mysql, database cleanup, database schema, metadata",671,Identifying unused database tables
23,0,Another one of my minor workflow frustrations is when applications prematurely save my input if I switch to another tab or application.,#design,"{% include setup %} Every once in awhile I get frustrated by a product experience and turn it into a design anti pattern rant. This time it was updating a bit of information in JIRA, switching to a different tab to look something up, and then going back to realize that my change was saved. Sure it was simple to edit and update the field but it seems the field should have just stayed in edit-mode until I was explicitly done. Since then I’ve been keeping an eye on how many products fall into this trap and it’s a fair amount. In the pursuit of improving efficiency they’re actually hurting it. Many tasks require referencing something else and I suspect there’s a bit of local optimization happening here with them focusing solely on their own product and not where it fits in to someone’s overall workflow.  The solution here is to actually watch users in action solving real-world problems. I’ve done user testing which has been focused on solving a given problem but that only scratches the surface of what a real user test should do. Start with the contrived scenario to get the first 80% done but only by watching people use your product in a natural environment will you get the last 20%.",0,1,2018-01-23,4,"ux, user experience, product design, efficiency",218,Design anti pattern: Tab switching autosave
37,0,The majority of the sites I visit use a slew of JavaScript libraries. I decided to do a quick analysis and see if there are any patterns of the libraries used based on the site type.,"#code,#javascript","{% include setup %} I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.  To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries. Every single publisher used DoubleClick and yet only a quarter of them used Google Analytics while 80% of the social networks I looked at used Google Analytics and only 40% used DoubleClick. The other interesting piece was how many more libraries an average publisher uses compared to a social network or ecommerce site. Five of the 13 publishers I looked at included at least 20 JavaScript libraries while the most libraries included by a social network was 4, which was Pinterest. The bulk of these additional libraries tend to be advertising specific so it’s not that surprising that publishers have more of them but the difference in volume was shocking. I’ve included the data at the bottom of this post in case someone wants to take a stab at it but something on my todo list is to automate the process of gathering this info rather than relying on Ghostery and copy and paste. Once I get get it done I’ll follow up with another post analyzing the larger set of data.  Even if these libraries get cached in the browser it’s still quite a lot of JavaScript that’s executed every time a site is loaded. It’s insane that Forbes is loading 39 libraries every time a page is seen. I suspect most people use AdBlock not because of ads but because of the degraded performance of a site having to load these libraries and the associated images. I’m aware that publishers are in trouble but I don’t think adding more and more libraries to eke out additional revenue is a sustainable model.     # of Libraries by Site      Site  Site Type  # of Libraries      Forbes  Publisher  39    BBC  Publisher  33    The Guardian  Publisher  25    Washington Post  Publisher  24    DailyKos  Publisher  23    NY Times  Publisher  19    USA Today  Publisher  18    Huff Po  Publisher  17    ABC  Publisher  15    Fox News  Publisher  15    Amazon  E Commerce  12    CNN  Publisher  10    ESPN  Publisher  9    Ebay  E Commerce  8    Yahoo  Publisher  6    Pinterest  Social Network  4    Facebook  Social Network  3    Tumblr  Social Network  3    Reddit  Social Network  2    Twitter  Social Network  1           # of Sites by Library      Library  # of Sites      DoubleClick  17    ScoreCard Research Beacon  14    Google Adsense  10    ChartBeat  8    NetRatings SiteCensus  8    Facebook Connect  8    Quantcast  8    Google Analytics  7    Datalogix  7    Right Media  7    Omniture (Adobe Analytics)  6    AppNexus  6    Moat  6    Audience Science  6    Evidon Notice  5    MediaMath  5    TrackingSoft  5    Adobe Test &amp; Target  4    Visual Revenue  4    Aggregate Knowledge  4    Acxiom  4    Google AdWords Conversion  3    AdRoll  3    Criteo  3    DoubleClick Spotlight  3    Facebook Social Plugins  3    Twitter Button  3    Outbrain  3    Quigo AdSonar  3    Atlas  3    Rubicon  3    Advertising.com  3    Krux Digital  3    BrightRoll  3    eXelate  3    BuzzFeed  2    Optimizely  2    Typekit by Adobe  2    AdXpose  2    Casale Media  2    Media Optimizer (Adobe)  2    VoiceFive  2    AdMeld  2    Amazon Associates  2    Facebook Exchange (FBX)  2    OpenX  2    PubMatic  2    TRUSTe Notice  2    Facebook Social Graph  2    MediaMind  2    [x+1]  2    BlueKai  2    Brilig  2    Media Innovation Group  2    Neustar AdAdvisor  2    SpotXchange  2    24/7 Media Ad Network  2    Dynamic Logic  1    Gravity Insights  1    Crazy Egg  1    DoubleClick Floodlight  1    FreeWheel  1    Gigya Socialize  1    MixPanel  1    Specific Media  1    Twitter Badge  1    ValueClick Mediaplex  1    Janrain  1    Parse.ly  1    Yahoo Analytics  1    Burst Media  1    PulsePoint  1    eBay Stats  1    Genome  1    ADTECH  1    Google +1  1    DoubleClick DART  1    Adzerk  1    Effective Measure  1    Mindset Media  1    Rocket Fuel  1    Brightcove  1    New York Times  1    WebTrends  1    ForeSee  1    Google AJAX Search API  1    Integral Ad Science  1    Media6Degrees  1    Adap.tv  1    AddThis  1    AMP Platform  1    DoubleClick Bid Manager  1    i-Behavior  1    Intent Media  1    Lotame  1    Martini Media  1    Media.net  1    Tacoda  1    Tapad  1    TidalTV  1    TradeDesk  1    Turn  1    Undertone  1    SimpleReach  1    Tealium  1    Bizo  1    New Relic  1    Trove  1           Avg # of Libraries by Site Type      Site Type  # of Sites  Avg # Libraries      Publisher  13  19.46    Social Network  5  2.60    E Commerce  2  10.00           Raw Data      Site  Library  Site Type      ESPN  Adobe Test &amp; Target  Publisher    ESPN  ChartBeat  Publisher    ESPN  DoubleClick  Publisher    ESPN  Dynamic Logic  Publisher    ESPN  Evidon Notice  Publisher    ESPN  Google Adsense  Publisher    ESPN  Gravity Insights  Publisher    ESPN  NetRatings SiteCensus  Publisher    ESPN  ScoreCard Research Beacon  Publisher    ABC  BuzzFeed  Publisher    ABC  ChartBeat  Publisher    ABC  Crazy Egg  Publisher    ABC  DoubleClick  Publisher    ABC  DoubleClick Floodlight  Publisher    ABC  Facebook Connect  Publisher    ABC  FreeWheel  Publisher    ABC  Gigya Socialize  Publisher    ABC  Google AdWords Conversion  Publisher    ABC  NetRatings SiteCensus  Publisher    ABC  Omniture (Adobe Analytics)  Publisher    ABC  Optimizely  Publisher    ABC  Quantcast  Publisher    ABC  ScoreCard Research Beacon  Publisher    ABC  Typekit by Adobe  Publisher    DailyKos  AdRoll  Publisher    DailyKos  AdXpose  Publisher    DailyKos  AppNexus  Publisher    DailyKos  Casale Media  Publisher    DailyKos  ChartBeat  Publisher    DailyKos  Criteo  Publisher    DailyKos  DoubleClick  Publisher    DailyKos  DoubleClick Spotlight  Publisher    DailyKos  Evidon Notice  Publisher    DailyKos  Facebook Connect  Publisher    DailyKos  Facebook Social Plugins  Publisher    DailyKos  Google Adsense  Publisher    DailyKos  Google AdWords Conversion  Publisher    DailyKos  Google Analytics  Publisher    DailyKos  MediaMath  Publisher    DailyKos  MixPanel  Publisher    DailyKos  Quantcast  Publisher    DailyKos  ScoreCard Research Beacon  Publisher    DailyKos  Specific Media  Publisher    DailyKos  TrackingSoft  Publisher    DailyKos  Twitter Badge  Publisher    DailyKos  Twitter Button  Publisher    DailyKos  ValueClick Mediaplex  Publisher    Fox News  Adobe Test &amp; Target  Publisher    Fox News  AdRoll  Publisher    Fox News  DoubleClick  Publisher    Fox News  Evidon Notice  Publisher    Fox News  Google Adsense  Publisher    Fox News  Janrain  Publisher    Fox News  Media Optimizer (Adobe)  Publisher    Fox News  Moat  Publisher    Fox News  NetRatings SiteCensus  Publisher    Fox News  Outbrain  Publisher    Fox News  Parse.ly  Publisher    Fox News  Quigo AdSonar  Publisher    Fox News  ScoreCard Research Beacon  Publisher    Fox News  TrackingSoft  Publisher    Fox News  Visual Revenue  Publisher    Facebook  Aggregate Knowledge  Social Network    Facebook  Atlas  Social Network    Facebook  DoubleClick  Social Network    Twitter  Google Analytics  Social Network    Yahoo  Datalogix  Publisher    Yahoo  DoubleClick  Publisher    Yahoo  Right Media  Publisher    Yahoo  ScoreCard Research Beacon  Publisher    Yahoo  VoiceFive  Publisher    Yahoo  Yahoo Analytics  Publisher    Amazon  AdMeld  E Commerce    Amazon  Amazon Associates  E Commerce    Amazon  AppNexus  E Commerce    Amazon  Burst Media  E Commerce    Amazon  DoubleClick  E Commerce    Amazon  Facebook Exchange (FBX)  E Commerce    Amazon  Google Adsense  E Commerce    Amazon  OpenX  E Commerce    Amazon  PubMatic  E Commerce    Amazon  PulsePoint  E Commerce    Amazon  Right Media  E Commerce    Amazon  Rubicon  E Commerce    Ebay  Aggregate Knowledge  E Commerce    Ebay  Datalogix  E Commerce    Ebay  DoubleClick  E Commerce    Ebay  eBay Stats  E Commerce    Ebay  Genome  E Commerce    Ebay  MediaMath  E Commerce    Ebay  Right Media  E Commerce    Ebay  TRUSTe Notice  E Commerce    Tumblr  Google Analytics  Social Network    Tumblr  Quantcast  Social Network    Tumblr  ScoreCard Research Beacon  Social Network    Pinterest  DoubleClick  Social Network    Pinterest  Facebook Connect  Social Network    Pinterest  Facebook Social Graph  Social Network    Pinterest  Google Analytics  Social Network    Huff Po  Adobe Test &amp; Target  Publisher    Huff Po  ADTECH  Publisher    Huff Po  Advertising.com  Publisher    Huff Po  DoubleClick  Publisher    Huff Po  Facebook Connect  Publisher    Huff Po  Facebook Social Plugins  Publisher    Huff Po  Google +1  Publisher    Huff Po  Google Analytics  Publisher    Huff Po  MediaMind  Publisher    Huff Po  Moat  Publisher    Huff Po  NetRatings SiteCensus  Publisher    Huff Po  Omniture (Adobe Analytics)  Publisher    Huff Po  Quantcast  Publisher    Huff Po  Quigo AdSonar  Publisher    Huff Po  ScoreCard Research Beacon  Publisher    Huff Po  TrackingSoft  Publisher    Huff Po  Twitter Button  Publisher    CNN  ChartBeat  Publisher    CNN  DoubleClick  Publisher    CNN  DoubleClick DART  Publisher    CNN  Facebook Connect  Publisher    CNN  Facebook Social Plugins  Publisher    CNN  Krux Digital  Publisher    CNN  NetRatings SiteCensus  Publisher    CNN  ScoreCard Research Beacon  Publisher    CNN  Twitter Button  Publisher    CNN  Visual Revenue  Publisher    Reddit  Adzerk  Social Network    Reddit  Google Analytics  Social Network    BBC  [x+1]  Publisher    BBC  Acxiom  Publisher    BBC  AdMeld  Publisher    BBC  Advertising.com  Publisher    BBC  AdXpose  Publisher    BBC  Aggregate Knowledge  Publisher    BBC  AppNexus  Publisher    BBC  Atlas  Publisher    BBC  Audience Science  Publisher    BBC  BlueKai  Publisher    BBC  BrightRoll  Publisher    BBC  Brilig  Publisher    BBC  Casale Media  Publisher    BBC  Datalogix  Publisher    BBC  DoubleClick  Publisher    BBC  DoubleClick Spotlight  Publisher    BBC  Effective Measure  Publisher    BBC  Facebook Exchange (FBX)  Publisher    BBC  Google Adsense  Publisher    BBC  Media Innovation Group  Publisher    BBC  MediaMath  Publisher    BBC  Mindset Media  Publisher    BBC  NetRatings SiteCensus  Publisher    BBC  Neustar AdAdvisor  Publisher    BBC  Omniture (Adobe Analytics)  Publisher    BBC  OpenX  Publisher    BBC  PubMatic  Publisher    BBC  Right Media  Publisher    BBC  Rocket Fuel  Publisher    BBC  Rubicon  Publisher    BBC  ScoreCard Research Beacon  Publisher    BBC  SpotXchange  Publisher    BBC  TrackingSoft  Publisher    NY Times  Acxiom  Publisher    NY Times  AppNexus  Publisher    NY Times  Atlas  Publisher    NY Times  Audience Science  Publisher    NY Times  Brightcove  Publisher    NY Times  ChartBeat  Publisher    NY Times  Datalogix  Publisher    NY Times  DoubleClick  Publisher    NY Times  eXelate  Publisher    NY Times  Facebook Connect  Publisher    NY Times  Google Adsense  Publisher    NY Times  Krux Digital  Publisher    NY Times  Moat  Publisher    NY Times  NetRatings SiteCensus  Publisher    NY Times  New York Times  Publisher    NY Times  ScoreCard Research Beacon  Publisher    NY Times  Typekit by Adobe  Publisher    NY Times  VoiceFive  Publisher    NY Times  WebTrends  Publisher    The Guardian  24/7 Media Ad Network  Publisher    The Guardian  AppNexus  Publisher    The Guardian  Audience Science  Publisher    The Guardian  BrightRoll  Publisher    The Guardian  ChartBeat  Publisher    The Guardian  Criteo  Publisher    The Guardian  DoubleClick  Publisher    The Guardian  Evidon Notice  Publisher    The Guardian  Facebook Connect  Publisher    The Guardian  Facebook Social Graph  Publisher    The Guardian  ForeSee  Publisher    The Guardian  Google Adsense  Publisher    The Guardian  Google AdWords Conversion  Publisher    The Guardian  Google AJAX Search API  Publisher    The Guardian  Integral Ad Science  Publisher    The Guardian  Media6Degrees  Publisher    The Guardian  MediaMath  Publisher    The Guardian  NetRatings SiteCensus  Publisher    The Guardian  Omniture (Adobe Analytics)  Publisher    The Guardian  Optimizely  Publisher    The Guardian  Outbrain  Publisher    The Guardian  Quantcast  Publisher    The Guardian  Right Media  Publisher    The Guardian  Rubicon  Publisher    The Guardian  ScoreCard Research Beacon  Publisher    Forbes  24/7 Media Ad Network  Publisher    Forbes  [x+1]  Publisher    Forbes  Acxiom  Publisher    Forbes  Adap.tv  Publisher    Forbes  AddThis  Publisher    Forbes  Advertising.com  Publisher    Forbes  Aggregate Knowledge  Publisher    Forbes  AMP Platform  Publisher    Forbes  AppNexus  Publisher    Forbes  Audience Science  Publisher    Forbes  BlueKai  Publisher    Forbes  BrightRoll  Publisher    Forbes  Brilig  Publisher    Forbes  Datalogix  Publisher    Forbes  DoubleClick  Publisher    Forbes  DoubleClick Bid Manager  Publisher    Forbes  DoubleClick Spotlight  Publisher    Forbes  eXelate  Publisher    Forbes  Google Adsense  Publisher    Forbes  Google Analytics  Publisher    Forbes  i-Behavior  Publisher    Forbes  Intent Media  Publisher    Forbes  Lotame  Publisher    Forbes  Martini Media  Publisher    Forbes  Media Innovation Group  Publisher    Forbes  Media.net  Publisher    Forbes  Moat  Publisher    Forbes  Omniture (Adobe Analytics)  Publisher    Forbes  Quantcast  Publisher    Forbes  Right Media  Publisher    Forbes  ScoreCard Research Beacon  Publisher    Forbes  Tacoda  Publisher    Forbes  Tapad  Publisher    Forbes  TidalTV  Publisher    Forbes  TradeDesk  Publisher    Forbes  TRUSTe Notice  Publisher    Forbes  Turn  Publisher    Forbes  Undertone  Publisher    Forbes  Visual Revenue  Publisher    USA Today  AdRoll  Publisher    USA Today  Audience Science  Publisher    USA Today  BuzzFeed  Publisher    USA Today  ChartBeat  Publisher    USA Today  Datalogix  Publisher    USA Today  DoubleClick  Publisher    USA Today  Evidon Notice  Publisher    USA Today  Facebook Connect  Publisher    USA Today  Google Adsense  Publisher    USA Today  Media Optimizer (Adobe)  Publisher    USA Today  Moat  Publisher    USA Today  Quantcast  Publisher    USA Today  Right Media  Publisher    USA Today  ScoreCard Research Beacon  Publisher    USA Today  SimpleReach  Publisher    USA Today  Tealium  Publisher    USA Today  TrackingSoft  Publisher    USA Today  Visual Revenue  Publisher    Washington Post  Acxiom  Publisher    Washington Post  Adobe Test &amp; Target  Publisher    Washington Post  Amazon Associates  Publisher    Washington Post  Audience Science  Publisher    Washington Post  Bizo  Publisher    Washington Post  ChartBeat  Publisher    Washington Post  Criteo  Publisher    Washington Post  Datalogix  Publisher    Washington Post  DoubleClick  Publisher    Washington Post  eXelate  Publisher    Washington Post  Google Adsense  Publisher    Washington Post  Krux Digital  Publisher    Washington Post  MediaMath  Publisher    Washington Post  MediaMind  Publisher    Washington Post  Moat  Publisher    Washington Post  Neustar AdAdvisor  Publisher    Washington Post  New Relic  Publisher    Washington Post  Omniture (Adobe Analytics)  Publisher    Washington Post  Outbrain  Publisher    Washington Post  Quantcast  Publisher    Washington Post  Quigo AdSonar  Publisher    Washington Post  ScoreCard Research Beacon  Publisher    Washington Post  SpotXchange  Publisher    Washington Post  Trove  Publisher          You can also grab the entire set of data  here .",1,2,2013-12-01,6,"javascript, analytics, advertising, google analytics, doubleclick, publishers",855,Drowning in JavaScript
14,0,We just released an update to Makers Alley that lets you buy pieces.,#product,"{% include setup %}            A brief one today.  This past week, Sandy and I have been super busy getting a new version of  Makers Alley  out that allows you to customize and buy furniture. We’re launching with two makers that have items for sale but we’re busy adding more.  Withers &amp; Grain  specialize in using reclaimed wood from the 5 boroughs and do their own wood and metal work.  Mark Grattan  is a furniture designer who has designed a furniture collection for Makers Alley in a geometry-inspired style. Take a look at their pages, watch their videos, and customize their pieces. If you have any feedback let me know.",3,1,2013-03-16,3,"furniture, makers alley, design",165,Makers Alley v2
31,0,People are doing tons of different things on their phones but I can't get over the frustration of how much less efficient I am on a phone than a computer.,#meta,{% include setup %} Smartphones are supposed to be the next big wave but I can’t get myself to be productive on them. Every action takes an order of magnitude longer than it would on a regular computer which prevents from me from starting it in the first place. The challenge is that I’m a power user on a computer able to leverage shortcuts across a variety of programs to be extremely productive. The cost is that when I switch to a phone it’s impossible for me to attain that level of speed which is extremely frustrating.  I see that others are spending a ton of time on their phones and are using it to replace a variety of activities they’d normally do on a full fledged computer but I’m not able to do the same. I wonder what impact this has. Smartphones are great for consumption but I wonder whether they actually improve people’s productivity. It’s much better than not having any digital device and is definitely helping get more people digitally connected but I can’t imagine them replacing actual computers in the short term. What will make smartphones more productive is when they start understanding our intent and predicting what we actually want to do. Something akin to Google Now but instead of showing us what we want to see allowing us to create what we want to create.,0,1,2015-06-13,3,"smartphones, productivity, google now",232,Smartphone productivity
25,0,One of the core lessons I've learned managing a team is that specs and PRDs should be kept fat while stories should be minimal.,#product,"{% include setup %} Most modern software companies use some form of agile as their software development process. There are a variety of different approaches and forms out there and each company ends up with a style that works for them. One of the core beliefs I’ve developed is that stories should be kept light and that time and effort should be spent on the product requirement documents and specs. Investing in the PRDs and specs encourages everyone to understand the high level problem being solved and how the various pieces of the solutions fit together. Otherwise you run the risk of mistaking the forest for the trees where each ticket is done as written but when combined they don’t actually work to make the proper whole.  The other advantage is that it’s just easier to maintain the specs and PRDs as the scope changes. Unless you’re doing something incredibly simple (which shouldn’t incur the overhead of a spec in the first place), the implementation will likely change as you code. And when it does change you want to reflect those changes somewhere so they’re not lost to anyone searching for them in the future. By updating the original spec you’re treating it as the source of truth and prevent others from wasting time hopping around from story to story looking through comments.  There are numerous ways to approach agile and the entire point is to be flexible and find what works given your company’s culture, products, and values. In our case our product is incredibly nuanced with a ton of details that it’s more important for everyone to have the shared context of what we’re doing more than the individual details that change rapidly.",0,1,2018-12-01,5,"agile, scrum, software engineering, tech specs, stories",286,"Fat specs, light stories"
18,0,The old version of the iOS App Store would immediately close when downloading a new app. Why?,#design,"{% include setup %} In old versions of the iOS App Store, every time you downloaded a new app it would close the App Store and navigate to the screen with the now-downloading app. Recent versions of the App Store keep it open and force you explicitly exit. I’m surprised that the App Store didn’t launch with the new behavior - it must have been a conscious decision since the development effort for both seems similar.  The only time the original approach is faster is when users intend to download a single app. I suspect this is actually the most common scenario and someone at Apple decided to design the App Store to optimize for it. Unfortunately, they didn’t consider the frustration of having to download multiple apps - even if one only does this a fraction of the time. Training yourself to hit the home button after downloading a new app is a lot easier than training yourself to scroll to the App Store icon, clicking on it, and resuming the app search. Even if 90% of the time a user is only downloading a single app, the other 10% matters is significant given a large enough cost. One can forgive Apple for launching the iPhone with this behavior - no one knew how people would use the App Store. But why did it take years to release the update?",0,1,2013-03-01,4,"apple, ios, app store, user experience",230,"App Store, what took so long?"
20,0,More than any other move it's shocking to see Microsoft decide to base future versions of Edge on Chromium.,"#meta,#product","{% include setup %} I’m a bit late to the party but it’s unbelievable that Microsoft is going to [move future versions](https://www.windowscentral.com/microsoft-building-chromium-powered-web-browser-windows-10) of Edge to be built on top of Chromium. This is shocking. I can’t separate Internet Explorer from Microsoft. In fact, Microsoft built, launched, and bundled Internet Explorer to drive Netscape into the ground which led to the famous Microsoft [antitrust case](https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp.). It feels as if Microsoft is giving up on Internet Explorer and it’s incredibly surprising. This is bigger than any of the other moves Microsoft has done to show they are a very different company. This is more significant than the GitHub acquisition. This is more significant than supporting their applications on operating systems other than windows. This is moving away from 20 years of history and it shows how different the current Microsoft is from the one of the past. I would have loved to have been a fly on the wall when this decision was discussed - there must have been so much passion in that room. There must have been so many viewpoints: ranging from those that have been working on IE for the past 20 years, to the product team that's responsible for the future of IE, to the executive team that needed to make the call. It really is surprising that they had the strength and conviction to be able to acknowledge that they'd be better off using Chromium.",2,2,2018-12-22,4,"edge, microsoft, chromium, browsers",251,Edge moving to Chromium
22,0,I used the Tweepy library to write a quick script to retrieve a Twitter user's followers and followees multiple levels deep.,#code,"{% include setup %} After reading Gilad Lotan’s  post  where Gilad bought 4,000 Twitter followers in order to analyze them, a  friend  of mine was inspired to analyze his followers to see if he could get any insight and come up with a neat visualization. The first step was downloading a dataset containing his followers and followees as well as the followers and followees for each of those accounts - the idea being that by going two levels deep you see how similar the various accounts are to each other based on who and what they follow and whether there are any patterns.  I offered to write a short script to help him pull the data and it turned out to be easier than I thought due to the excellent  Tweepy library . The biggest challenge was figuring out how to use Tweepy to deal with Twitter’s absurdly strict API limit (15 requests per 15 minutes) since the documentation was a bit sparse but after discovering the Cursor object it became surprisingly easy to iterate through the results and wait for 15 minutes for API errors.  The code currently works in a user id rather than username world since I wanted to avoid making additional calls but that can be implemented in the end to just pull the usernames for every user id in the dataset. The  code’s  up on Github so feel free to try it out and let me know if you run into any issues.",4,1,2014-07-07,3,"twitter, api, tweepy",269,Retrieving a Twitter user's followers and followees
28,0,Self driving cars are inevitable but we're just scratching the surface of the changes they'll bring. The imemdiate effects are obvious. The secondary ones not so much.,#society,"{% include setup %} Nearly every major tech company is pursuing a self driving car future and it’s inevitable that at some point most cars on the road will be completely autonomous. Cheap and easy transportation is the immediate change but there will be massive secondary effects to the shapes of cities and society.  A [college professor](https://www.johnson.cornell.edu/Faculty-And-Research/Profile?id=lvo2 ) used the example of the invention of the car to highlight these sort of effects - if told that cars would be successful most people could have guessed that they’d replace horses and clean up cities. But very few would have been able to predict the rise of highways which led to the development of suburbs and the current structure of the United States.  Self driving cars have that same potential and it’s an interesting exercise to think through the impact. The short term is that fewer people will own cars, our roads will be safer, and that there will clearly be some disruption among the auto manufacturers. The medium and long term are where it gets tricky.  The increases in safety will lead to faster cars which may lead to another shift in where people live. One idea is that people will be able to live further and further away from cities which will lead to a decline in suburbs with more people opting to live in more remote areas while also leading to a boost in urban living.  Self driving cars will not just be ferrying people and one can imagine nearly everything being able to be delivered by self driving car or truck. I like the image of a “carrier truck” that drives around neighborhoods with a series of drones taking off and landing to deliver items along the way. In this sort of world there’s not a great need for physical stores. Taken to the extreme this means that cities will be designed to focus on the social elements. Convenience stores will disappear but restaurants will thrive. Most people aren’t buying everything online but I suspect it’s only a matter of time.  Public transit will have to change. I worry that if the price of self driving cars drops low enough to appeal to most people but high enough to not be affordable by some it will lead to a decline in public transit. At that point since most people wouldn’t care about public transition it would end up in a self destructive loop as more and more people decide to go for the self driving car route which in turn leads to less and less funds being allocated to public transit.  These are just scratching the surface and we’ll have to wait to see what happens.",0,1,2016-02-17,5,"self driving cars, society, culture, futurism, futurist",455,The impact of self driving cars
9,0,Just my 2 cents on the EA/Take Two acquisition,#product,"I'm not sure why no one is pointing this out but it seems that as soon as EA acquires T2, the T2 folks would just leave to create a new studio. They seem independent and I doubt that they'd want to work for EA. And although EA would get the rights to all of the T2 games and may try to develop the series, they might not have the imagination or the guys to do it. In addition, I don't know whether EA would even want to develop such products as GTA and Bully given the violence and public relations ordeal that T2 has been going through the past few years.  What EA does want is probably the T2 sports games, then they'll have a pretty strong hold on the sports games' market.  So as long as EA does not want the T2 human capital it seems like it makes sense - but given the negative response of T2 to the offer, I doubt EA will be able to retain the developers and artists.",0,1,2008-04-29,3,"electronic arts, ea, take two",173,Why does EA want Take Two?
29,0,After getting frustrated with some meetings being booked across multiple rooms I decided to do something about it and wrote a quick Python script to pinpoint it happening.,#code,"{% include setup %} One of the first things felt by a fast growing company is the lack of meeting space. The first few weeks at a new office it’s wonderful to know you can find a room whenever you need it. Yet after a few months and a bunch of extra people you realize you have to book meetings days in advance. And what makes this worse is seeing more than one room booked for the same meeting.  After seeing this happening I decided to do something about it and wrote a quick script to pull the meeting calendar for every room from [Google Calendar](https://developers.google.com/google-apps/calendar/) and then flag the ones having the same start time, end time, and creator.  This isn’t foolproof since it won’t identify cases where someone books multiple rooms for the same time with different times but it's a solid start and already caught a few cases. The code is up on [GitHub](https://github.com/dangoldin/gcal-shaming) so feel free to take a look and provide suggestions.",2,1,2016-10-01,3,"google calendar, meeting rooms, double booking",173,Shaming meeting room hogs
26,0,We're often tempted to write programs to solve simple problems but the shell has a variety of commands that can let us do nearly anything.,#code,"{% include setup %}  It’s surprising how unappreciated shell commands are. They’re incredibly powerful and once understood are able to handle small one-off tasks much quicker than writing even simple scripts. Earlier this week I ran into a small task that highlights the power and ability of the shell.  A few of our applications use the same configuration file which contains a variety of URLs, secrets, and passwords. If any of these applications require a field the it gets added to this growing configuration file. This is clearly not good for security and as part of a larger security revamp we’re moving to application-specific config files. Long term we want to revamp the way we do deploys such that the configuration is kept in environment variables and handled by the build system but as a short term solution we want to split this single configuration file into a file per application.  The solution is straightforward - go through the config reader code of each application, extract the keys, and then find them in the main configuration file. Easy to explain but parsing files is fickle and not very interesting. Luckily for us with a few shell commands we can do exactly what we need to do. We use the cut command and a few pipes to extract the keys from the config reading file and then combine it with the join command to get the intersection. A one liner is all that’s needed to give us the application specific config file.  {% highlight bash %} join -t'=' <( grep '\$configObj' code/application/config/reader.php | cut -d'(' -f3 | cut -d""'"" -f2 | sort ) <( sort ~/config.properties ) {% endhighlight %}  Shell commands are simple but it's the ability to chain them together that gives us an incredible set of tools. We're often tempted to write a simple script but it's very likely that it's possible to do the same with a series of shell commands and pipes.",0,1,2018-05-26,3,"shell, bash, command line",330,Power of shell commands
11,0,In annual tradition I share the top posts of 2018.,#dataviz,{% include setup %}     In the usual end-of-year tradition I want to share the top posts of 2018 - including both the posts that were written in 2018 as well as the posts that may have been written in prior years but viewed in 2018. Given the fact that I’ve been extremely behind in writing this year and am only catching up now it’s clear that 2018 was a weaker year than previous ones. Sorting all pages viewed in 2018 by descending pageviews the first post written in 2018 is in the 57th spot - every other post was written in prior years. That bodes well to the concept of evergreen content but it’s still disappointing that I didn’t have any noteworthy posts in 2018. The lesson for 2019 is to actually write on time and spend the time going into depth versus the more superficial and shorter posts I wrote in 2018. On a positive note it’s good to see that my blog does get a healthy flow of organic traffic despite the weak 2018 showing.  ## Most popular posts written in 2018      Page  Pageviews  Unique Pageviews  Avg. Time on Page  Entrances  Bounce Rate  % Exit      /2018/02/20/analyzing-aws-elb-logs/  71  62  114.38  56  94.64%  81.69%    /2018/08/15/google-calendar-constantly-shipping/  67  66  81.25  58  98.28%  88.06%    /2018/03/03/hunting-for-my-old-geocities-site/  63  56  181.67  51  88.24%  80.95%    /2018/04/28/rise-of-microbrands/  47  45  105.10  35  91.43%  78.72%    /2018/07/21/class-action-settlement-emails/  42  42  27.67  39  100.00%  92.86%    /2018/11/24/code-without-online-help/  41  41  141.00  39  97.44%  95.12%    /2018/11/25/aggressive-code-deprecation/  39  39  432.25  38  92.11%  89.74%    /2018/11/19/computer-history-books/  37  37  0.00  36  100.00%  100.00%    /2018/08/21/incognito-mode-chrome-vs-safari/  36  34  25.10  23  86.96%  72.22%    /2018/01/16/phonetic-distance/  34  28  319.22  24  79.17%  73.53%    /2018/06/05/alb-and-elb-access-log-schemas-for-redshift/  29  26  14.44  19  89.47%  68.97%    /2018/07/22/bulk-discounts-hurt-competition/  29  29  36.83  23  91.30%  79.31%    /2018/07/07/mysql-foreign-keys/  25  10  21.10  2  100.00%  16.00%    /2018/02/03/my-follower-factory/  18  16  81.67  3  66.67%  16.67%    /2018/05/12/curse-of-the-early-adopter/  16  16  56.00  10  100.00%  87.50%    /2018/01/03/learning-docker/  15  15  55.29  5  100.00%  53.33%    /2018/04/25/connect-four-bot-competition/  15  13  13.50  9  100.00%  73.33%    /2018/01/28/moviepass-a-fascinating-business-model/  14  13  93.14  6  100.00%  50.00%    /2018/05/14/memory-as-a-stack/  14  14  18.50  1  100.00%  28.57%    /2018/03/11/crowdsourced-data/  13  13  118.00  9  100.00%  92.31%    /2018/02/09/making-the-most-of-the-subway-commute/  12  12  29.50  4  100.00%  66.67%    /2018/06/01/type-1-and-type-2-tech-specs/  12  11  89.50  5  80.00%  50.00%    /2018/06/28/using-personal-aws-credentials-in-production/  11  11  41.50  2  100.00%  45.45%    /2018/03/18/facebooks-breach/  10  10  33.25  4  100.00%  60.00%    /2018/05/26/power-of-shell-commands/  10  10  60.00  3  100.00%  40.00%    /2018/07/29/privacy-vs-user-experience/  10  8  42.00  1  100.00%  40.00%    /2018/08/18/yahoo-fantasy-football-stats-2018-2019-edition/  10  9  199.20  3  66.67%  50.00%    /2018/07/10/in-a-software-world-humanity-comes-first/  9  9  20.00  4  100.00%  66.67%    /2018/11/21/superhuman-review/  9  8  38.50  6  100.00%  77.78%    /2018/04/12/load-testing/  8  8  53.75  2  100.00%  50.00%    /2018/04/17/secure-at-the-network-level/  8  8  28.60  3  100.00%  37.50%    /2018/10/13/equity-in-the-gig-economy/  8  7  45.60  3  100.00%  37.50%    /2018/10/29/gmails-autocomplete/  8  7  25.00  3  100.00%  62.50%    /2018/01/13/calendar-query-language/  7  7  16.33  4  100.00%  57.14%    /2018/03/25/protecting-data-ouside-of-a-terms-of-service/  7  7  28.00  6  100.00%  85.71%    /2018/04/03/open-sourcing-self-driving-car-data/  7  7  19.25  2  100.00%  42.86%    /2018/01/23/design-anti-pattern-tab-switching-autosave/  6  6  2.00  3  100.00%  83.33%    /2018/02/25/retrieving-kindle-highlights/  6  4  93.50  2  100.00%  33.33%    /2018/11/23/electronic-goods-are-cheaper-than-ever/  6  6  0.00  5  100.00%  100.00%    /2018/11/26/an-ad-on-the-google-search-homepage/  6  3  19.33  1  100.00%  50.00%    /2018/11/20/my-dataengconf-2018-talk/  5  5  12.00  1  100.00%  80.00%    /2018/02/10/optimize-for-keyboard-shortcuts/  4  4  5.00  0  0.00%  50.00%    /2018/10/25/just-ship-it/  4  4  24.00  0  0.00%  25.00%    /2018/11/16/python-3-and-aiohttp/  4  3  3.00  1  100.00%  75.00%    /2018/12/11/the-price-of-aws-vs-github/  4  4  21.00  2  100.00%  50.00%    /2018/11/12/social-security-administration-spoofing-scam/  3  3  0.00  2  100.00%  100.00%    /2018/11/13/a-ux-gem-in-google-slides/  3  3  7.00  2  100.00%  66.67%    /2018/11/22/historys-largest-empires/  3  3  50.00  1  100.00%  66.67%    /2018/11/25/exploring-my-backlinks/  3  3  0.00  3  100.00%  100.00%    /2018/11/30/tragedy-of-the-commons-apartment-edition/  3  3  50.00  2  100.00%  66.67%    /2018/12/03/overcoming-writing-rustiness/  3  2  36.50  0  0.00%  33.33%    /2018/12/10/emr-vs-databricks-costs/  3  3  36.00  2  100.00%  66.67%    /2018/12/13/counting-the-number-of-lines-of-code-in-a-github-account/  3  2  37.00  1  100.00%  66.67%    /2018/11/14/what-messaging-war/  2  2  2.00  0  0.00%  50.00%    /2018/11/18/falling-behind-my-2018-blogging-goal/  2  2  48.00  2  50.00%  50.00%    /2018/11/24/adding-optionality-to-products/  2  2  0.00  2  100.00%  100.00%    /2018/11/28/shell-history-2018-edition/  2  2  0.00  1  100.00%  100.00%    /2018/12/01/fat-specs-light-stories/  2  2  12.00  1  100.00%  50.00%    /2018/12/06/conference-call-echoes/  2  2  21.00  0  0.00%  0.00%    /2018/12/07/speech-recognition-and-a-bunch-of-apis/  2  2  0.00  2  100.00%  100.00%    /2018/12/14/state-of-tech-in-2018/  2  2  0.00  0  0.00%  100.00%    /2018/12/23/quoras-revenue-model/  2  2  0.00  2  100.00%  100.00%    /2018/11/15/limiting-tracking-in-email/  1  1  17.00  0  0.00%  0.00%    /2018/11/17/random-quotes/  1  1  0.00  1  100.00%  100.00%    /2018/11/27/privacy-in-a-face-detection-world/  1  1  0.00  1  100.00%  100.00%    /2018/12/04/how-many-wifi-devices-do-we-have/  1  1  37.00  0  0.00%  0.00%    /2018/12/05/the-golden-age-of-browsers/  1  1  0.00  0  0.00%  100.00%    /2018/12/08/automatic-login/  1  1  6.00  0  0.00%  0.00%    /2018/12/09/avoiding-content-overload/  1  1  0.00  1  100.00%  100.00%    /2018/12/09/the-modern-economy-relies-on-information/  1  1  23.00  0  0.00%  0.00%    /2018/12/12/scenepeek/  1  1  7.00  0  0.00%  0.00%    /2018/12/15/new-code-is-not-a-linear-increase-in-complexity/  1  1  7.00  0  0.00%  0.00%    /2018/12/16/stuck-on-a-problem-take-a-break/  1  1  0.00  1  100.00%  100.00%    /2018/12/17/the-new-company-town/  1  1  0.00  1  100.00%  100.00%    /2018/12/18/i-finally-tried-an-electric-scooter/  1  1  0.00  1  100.00%  100.00%    /2018/12/19/facebooks-latest-hit/  1  1  0.00  1  100.00%  100.00%    /2018/12/19/new-iteration-of-devops/  1  1  57.00  0  0.00%  0.00%    /2018/12/21/global-roaming/  1  1  0.00  0  0.00%  100.00%    /2018/12/22/hackerrank/  1  1  0.00  1  100.00%  100.00%    /2018/12/24/open-public-electronic-and-necessary-government-data-act/  1  1  0.00  1  100.00%  100.00%      ## Most popular posts viewed in 2018      Page  Pageviews  Unique Pageviews  Avg. Time on Page  Entrances  Bounce Rate  % Exit      /2013/08/26/extract-info-from-a-web-page-using-javascript/  7492  7104  424.76  7095  94.56%  93.95%    /2013/06/21/where-are-you-on-the-sales-matrix/  3005  2727  279.80  2727  91.24%  90.68%    /2015/05/26/dealing-with-a-stripped-screw/  1465  1413  341.42  1412  96.67%  96.38%    /2015/09/24/mapping-the-jersey-city-parking-zones-ii/  1325  1036  93.95  1032  75.48%  77.74%    /2017/04/02/slacks-channel-exit-anti-pattern/  972  948  229.50  948  97.36%  97.12%    /2015/04/23/adding-columns-in-postgresql-and-redshift/  924  886  288.61  886  95.37%  95.24%    /2014/02/10/using-virtualenv-in-production/  816  789  324.12  786  96.18%  95.96%    /2016/01/10/cleanest-way-to-read-a-csv-file-with-python/  778  732  484.69  731  94.39%  93.70%    /2017/08/08/google-docs-vs-confluence/  584  572  346.06  571  97.20%  96.92%    /2016/01/03/paris-versus-new-york-city/  511  494  297.60  494  96.36%  96.09%    /2014/02/05/visualizing-gps-data-in-r/  428  379  166.70  375  88.00%  87.38%    /2014/10/01/normalizing-a-csv-file-using-mysql/  381  358  291.88  356  94.10%  93.18%    /2016/08/24/writing-scrapers-as-apis/  323  259  62.96  256  52.73%  56.04%    /2016/08/21/downloading-your-turo-ride-history/  318  278  181.63  179  88.83%  77.99%    /2017/11/07/spark-s-read-jdbc/  276  261  320.75  257  93.00%  91.30%    /2013/01/09/web-scraping-like-a-pro/  271  260  242.14  257  95.72%  94.83%    /2017/05/04/security-across-multiple-aws-regions/  271  266  137.60  266  98.12%  98.15%    /2017/10/09/downloading-your-aim-buddy-list/  221  213  420.22  213  96.24%  95.93%    /2016/02/15/design-your-database-for-flexibility/  219  214  382.22  212  96.23%  95.89%    /2014/09/20/dealing-with-an-rds-replication-issue/  212  202  180.27  202  95.05%  94.81%    /2016/03/10/the-mysql-enum-type/  195  191  256.71  188  97.87%  96.41%    /2013/06/07/fun-with-prolog-priceonomics-puzzle/  163  150  403.20  150  91.33%  90.80%    /2014/05/03/gap-fills-and-cross-joins-in-excel/  161  155  468.57  155  96.13%  95.65%    /2016/10/10/setting-up-secor-for-kafka-010/  153  147  231.56  146  93.84%  94.12%    /2016/05/11/identifying-unused-database-tables/  150  143  305.25  142  95.07%  94.67%    /2016/07/17/coding-puzzle-word-transformation-through-valid-words/  146  134  195.92  134  91.04%  91.10%    /2013/08/24/splitting-an-aws-account/  143  140  201.80  135  98.52%  96.50%    /2013/05/17/adding-attachments-to-django-postman/  140  128  299.38  126  92.86%  90.71%    /2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/  134  111  186.34  109  78.90%  78.36%    /2013/07/17/scraping-yahoo-fantasy-football-stats-with-scrapy/  133  131  132.00  130  96.92%  96.24%    /2017/04/16/amp-and-subscription-paywalls/  127  120  146.60  117  94.02%  92.13%    /2011/01/08/fun-developer-interview-question/  124  114  412.42  114  89.47%  90.32%    /2016/12/31/amazons-peer-to-peer-marketplace/  117  111  157.67  110  93.64%  92.31%    /2017/07/08/yahoo-fantasy-football-stats-2017-2018-edition/  114  101  161.13  100  89.00%  86.84%    /2013/04/12/why-dont-cellphones-have-a-dialtone/  113  108  343.20  108  95.37%  95.58%    /2017/06/20/getting-amp-into-rss/  110  102  227.43  101  92.08%  87.27%    /2017/12/01/measuring-sprint-efficiency/  110  108  301.83  106  96.23%  94.55%    /2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/  107  104  87.33  104  97.12%  97.20%    /2014/01/04/visualizing-runkeeper-data-in-r/  104  90  357.76  89  84.27%  83.65%    /2016/09/13/supporting-disqus-in-amp/  103  75  77.10  73  72.60%  70.87%    /2013/12/23/getting-a-sim-card-in-india/  98  96  253.50  95  96.84%  95.92%    /2015/04/26/aws-service-limits/  92  87  79.22  83  96.39%  90.22%    /2014/12/31/redirect-recursion/  91  90  143.00  89  98.88%  98.90%    /2016/12/24/comparing-public-transit-systems-new-york-vs-london/  91  88  506.33  88  97.73%  96.70%    /2014/05/02/migrating-from-linode-to-digital-ocean/  85  80  294.17  80  92.50%  92.94%    /2017/02/21/advice-for-coding-bootcamp-graduates/  84  83  808.50  82  97.56%  97.62%    /2016/08/29/food-identification-with-googles-cloud-vision/  83  74  109.70  74  89.19%  87.95%    /2016/11/13/recursive-redirects-with-aws-lambda/  82  75  237.00  74  93.24%  89.02%    /2016/12/10/word-clouds-and-text-similarity/  78  73  422.33  73  91.78%  92.31%    /2015/11/22/why-are-netflix-and-spotify-so-different/  76  74  516.00  74  97.30%  97.37%    /2017/04/23/the-golden-age-of-big-data-tools/  74  66  251.56  66  89.39%  87.84%    /2018/02/20/analyzing-aws-elb-logs/  71  62  114.38  56  94.64%  81.69%    /2014/09/16/top-down-vs-bottom-up-coding/  68  67  29.50  67  97.01%  97.06%    /2016/04/03/ben-thompsons-laddering-up-and-building-bigger-moats/  68  60  72.20  59  86.44%  85.29%    /2018/08/15/google-calendar-constantly-shipping/  67  66  81.25  58  98.28%  88.06%    /2013/08/28/simplicity-vs-power-in-product-design/  66  64  53.89  12  83.33%  46.97%    /2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/  65  64  76.54  54  90.74%  80.00%    /2017/01/26/shame-on-united-and-bank-of-america/  64  63  37.00  63  98.41%  98.44%    /2018/03/03/hunting-for-my-old-geocities-site/  63  56  181.67  51  88.24%  80.95%    /2015/02/01/mysql-vs-postgresql-sort-order/  61  56  394.60  55  92.73%  91.80%    /2014/05/16/examining-ssh-login-requests/  59  54  657.20  54  90.74%  91.53%    /2016/05/22/analyzing-imdb-data-actors-vs-actresses/  58  51  46.63  49  67.35%  67.24%    /2016/11/27/visualizing-your-aws-costs/  58  53  58.50  52  92.31%  89.66%    /2017/03/19/refactor-driven-development/  56  50  104.33  50  90.00%  89.29%    /2017/04/11/sql-is-the-perfect-interface/  50  45  30.00  41  82.93%  78.00%    /2016/09/05/visualizing-fantasy-football-stats/  49  46  126.33  44  93.18%  93.88%    /2015/06/09/a-mysql-group-by-nuance/  48  47  17.00  44  95.45%  95.83%    /2016/06/22/messaging-app-fragmentation/  48  40  81.33  40  87.50%  81.25%    /2017/05/10/using-options-to-play-snapchats-quarterly-results/  47  46  146.00  45  97.78%  97.87%    /2018/04/28/rise-of-microbrands/  47  45  105.10  35  91.43%  78.72%    /2014/03/18/goodbye-gmail/  46  44  647.20  44  88.64%  89.13%    /2016/12/13/automatically-taking-screenshots-of-html-elements/  46  45  157.67  45  93.33%  93.48%    /2017/11/23/improving-jekyll-generation-speed-for-amp-pages/  46  42  118.57  42  83.33%  84.78%    /2016/03/05/aws-ec2-instance-arbitrage/  44  41  137.00  41  92.68%  90.91%    /2016/06/18/aws-stripe-and-wework/  44  43  56.00  42  97.62%  95.45%    /2017/04/08/quality-over-quantity-nextdoor-vs-craigslist/  44  41  99.00  40  92.50%  88.64%    /2013/11/15/rds-and-r/  42  40  901.50  40  95.00%  95.24%    /2014/04/04/postgresql-fibonacci/  42  39  607.00  39  92.31%  92.86%    /2018/07/21/class-action-settlement-emails/  42  42  27.67  39  100.00%  92.86%    /2018/11/24/code-without-online-help/  41  41  141.00  39  97.44%  95.12%    /2015/04/06/redshift-meets-excel/  40  40  7.00  39  100.00%  97.50%    /2015/04/19/dont-scrape-into-a-dropbox-folder/  40  39  196.00  39  97.44%  97.50%    /2016/06/11/following-up-on-a-website-optimization-offer/  40  32  174.22  32  78.13%  77.50%    /2018/11/25/aggressive-code-deprecation/  39  39  432.25  38  92.11%  89.74%    /2013/08/02/a-brief-history-of-manufacturing/  38  38  186.00  37  97.30%  97.37%    /2015/05/26/dealing-with-a-stripped-screw/?usqp=mq331AQECAEYAQ==  38  38  0.00  38  100.00%  100.00%    /2018/11/19/computer-history-books/  37  37  0.00  36  100.00%  100.00%    /2013/06/21/where-are-you-on-the-sales-matrix/?usqp=mq331AQECAEYAQ==  36  31  364.80  31  83.87%  86.11%    /2018/08/21/incognito-mode-chrome-vs-safari/  36  34  25.10  23  86.96%  72.22%    /2017/02/09/traffic-efficiency/  35  34  56.50  34  94.12%  94.29%    /2017/10/20/schedule-automation-using-google-spreadsheets-and-slack/  35  29  34.55  28  75.00%  68.57%    /2018/01/16/phonetic-distance/  34  28  319.22  24  79.17%  73.53%    /2015/05/30/date-range-generation/  31  28  35.40  26  92.31%  83.87%    /2016/04/05/the-best-code-is-no-code/  30  29  715.00  29  96.55%  96.67%    /2014/08/30/managing-settings-files-in-django-projects/  29  29  713.00  29  96.55%  96.55%    /2016/06/06/word-clouds-in-r/  29  28  90.00  26  96.15%  89.66%    /2018/06/05/alb-and-elb-access-log-schemas-for-redshift/  29  26  14.44  19  89.47%  68.97%    /2018/07/22/bulk-discounts-hurt-competition/  29  29  36.83  23  91.30%  79.31%    /2013/11/23/im-joining-triplelift/  28  26  16.50  25  68.00%  64.29%    /2017/09/09/apartment-rental-arbitrage/  28  26  92.50  26  92.31%  92.86%,0,1,2018-12-25,3,"blog stats, google analytics, data visualization",1457,Top posts of 2018
25,0,"Despite being economically better, we're so used to the idea of owning items that it will take time for us to shift our mindsets.","#society,#pricing","{% include setup %} The sharing/rental economy is getting stronger and stronger and will have a massive impact on societies - especially cities. One thing that’s been on my mind is how it will fit into human behavior and biases. We’re so used to making infrequent or one time payments and then owning something that moving to a rental or sharing model might be difficult. For example, I don’t own a car and mostly rely on a combination of public transportation and CitIBike to get around. The rare times I need a car I’ll use either Zipcar, Hertz 24/7, Lyft or Uber depending on my exact situation yet each time I make the decision I can’t help but think about the cost. I realize that in the grand scheme of things it’s much cheaper than owning a car but during the moment itself it’s draining. It’s similar to the of unbundling TV - it’s much cheaper to just pay a dollar per episode to watch a TV show than pay more than $100 each month for cable but do people actually want to be thinking about spending the dollar each time? I suspect most would rather pay the premium for the entire bundle and the option of watching anything instead of feeling as if they’re being nickeled and dimed. I have no idea whether this is innate in human behavior or something that we’ve just grown accustomed to. I suspect it’s the latter - there are countless items we pay for individually and don’t think twice about it. What will make the sharing economy universal is when we start treating the majority of our purchases as per use rather than a lifetime subscription.",0,2,2015-03-21,3,"sharing economy, rental economy, human behavior",281,The sharing economy and human behavior
16,0,I used to think business frameworks full of bs but now find them very useful.,"#meta,#product","{% include setup %} Tom Tunguz wrote a  great post  yesterday sharing the frameworks he uses to evaluate and analyze startups. For this post, I’m not interested in the content (which is great for anyone building a company) but I am interested in the concept of business frameworks and their application. When I was younger and came across a “business” framework I would dismiss it as obvious and move on. Now, I’m aware of how valuable a good framework can be. A good framework imposes structure that leads to a clearer though process with better results. At the same time, it needs to be simple to apply but be expressive enough to describe the complexity of a business. Being human, we also don’t want to think about our own fallibility and weaknesses which makes it difficult to critique our businesses. We also want to solve problems on our own rather than share our uncertainties with others. A framework serves as an impartial third party where you go through and fill in the blanks until you discover you aren’t in as good of a shape as you thought. Now you can work on growing your company instead of avoiding self-criticism.  I recall struggling to fill out the Lean Canvas for Pressi after reading one of the lean startup books. It took me a few hours with multiple breaks and online searches but I ended up with a much better understanding of our business. It became easier to see where the risks were and gave us tons of ideas around acquiring new users and generating revenue. None of these ideas were groundbreaking and I’m sure we would have gotten to them eventually but it was valuable getting to them earlier since we were able to take them into account when building the product.  I’m not sure why investors don’t require pitching startups to share these instead of a pitch deck, they seem much more useful.",1,2,2013-06-25,4,"business frameworks, startups, lean canvas, business model canvas",328,Business frameworks are actually useful
28,0,I have a very loose memory of my childhood. I wonder whether kids these days will have much higher fidelity versions of theirs as they grow up.,#meta,"{% include setup %} A fun exercise is picking a random day in the past and trying to recreate it using the various tools at our disposal. In my case the most useful ones are my calendars, both personal and work, the photos I took, and Foursquare/Swarm. As long as I was vigilant in documenting the events it’s simple to figure out what I did. We lose a bit of the mystery when we document our lives and we no longer have long discussions trying to recreate events with friends. I don’t know whether this is better or worse but we’ll probably see more and more of this happening. Our phones are already collecting our location and video is becoming increasingly popular. Now we have high fidelity versions of recent events but only vague memories of our childhood. I wonder whether kids that are growing up now will have access to accurate memories of their childhood when they grow up and what the impact will be.",0,1,2015-01-25,2,"technology, memories",167,The changing fidelity of the past
19,0,Sometimes all it takes is letting a difficult problem for into the subconscious for it to get solved.,#meta,"{% include setup %} When struggling with a difficult problem that I just can’t figure out I’ve found that it helps to just stop focusing on it and work on something else. Oftentimes that break is enough to get the problem into the subconscious where after a bit of stewing it just clicks. It may feel as if you’re admitting defeat and giving up but that break gives you the respite that is necessary to find the passive inspiration to solve a problem. People say that they often have the best ideas in the shower - in fact Archimedes supposedly found his “Eureka” moment in the bathtub - but I get my moment of inspiration when I’m either waking up, falling asleep, or doodling. In all those cases my mind is wandering and miraculously decides that it actually wants to tackle the problem.",0,1,2018-12-16,1,problem solving,142,Stuck on a problem? Take a break
19,0,I ran a data scraping script over two days that ended up clobbering my Dropbox folder. Exciting times.,#data,"{% include setup %} Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.  Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted. In addition to being over my 6 GB limit, the syncing was completely stopped and Dropbox was using up my entire CPU. I had to figure out a way to deal with this while holding on to the scraped data.  Since Dropbox was entirely unusable, I disabled it on my travelling machine and did a bit of investigation with the data I had with the hope of running it on the complete dataset when I got back home to my primary computer. When I finally got back home I saw that the scraping job was still running and had downloaded around 791 thousand files into one folder that totaled 11.7 GB.            The solution seemed straightforward - move the files out of Dropbox into a separate directory and then let Dropbox recover itself. Sadly that didn’t quite work. First, doing a “mv * targetfolder” ended up causing an issue with the globber since there were too many files for bash to handle. The fix was simple - move the entire folder and then rename it to the destination folder - but it took me a few attempts until I stumbled unto it. Second, Dropbox was in such a wretched state that it refused to do anything. The solution here was a bit more involved. I had to log in to the Dropbox site, remove the data from the UI, unlink Dropbox from my computers via the website, and then relink them via the app on the computer.  Two lesson here: do not save your results in Dropbox and when downloading hundreds of thousands of files do not save them in a single folder.",0,1,2015-04-19,3,"scraping, dropbox, lesson learned",372,Don't scrape into a Dropbox folder
30,0,Every since I moved my blog over to AMP it took a very long time to build it due to the CSS inlining requirement. Now it takes 15 seconds.,#code,_layouts/default.html                                                      |   570 | 16663.10K | 4.277 _layouts/post.html                                                         |   562 |  3595.71K | 4.104 _includes/themes/amp/default.html                                          |   570 | 16658.09K | 3.984 _includes/head.html                                                        |   570 | 11934.64K | 3.607 _includes/themes/amp/post.html                                             |   562 |  3590.78K | 3.306 _includes/metadata.json                                                    |   570 |   461.92K | 2.420 _includes/setup                                                            |  1612 |    11.02K | 1.367 _includes/tags_list                                                        |   559 |    73.99K | 0.700 sitemap.xml                                                                |     1 |    62.20K | 0.251 archive.html                                                               |     1 |   120.81K | 0.119 _includes/posts_collate                                                    |     1 |   120.80K | 0.111 atom.xml                                                                   |     1 |  1739.13K | 0.065 tags.html                                                                  |     1 |   108.78K | 0.064 _includes/pages_list                                                       |     2 |   103.64K | 0.050 _includes/header.html                                                      |   570 |   709.16K | 0.041 index.md                                                                   |     1 |   106.79K | 0.019 sitemap.txt                                                                |     1 |    34.99K | 0.011 _layouts/page.html                                                         |     6 |   246.62K | 0.008 done in 15.077 seconds.| | |,0,1,2017-11-23,4,"jekyll, amp, build, speed",184,Improving Jekyll generation speed for AMP pages
22,0,As I read I collect quotes in a GitHub repo and just wrote a simple endpoint to fetch a random one.,#meta,"{% include setup %} Besides blogging I’m a big reader and normally get through read a book a week. Most of my reading is done on Kindle and whenever I come across any interesting passage or quote I highlight them. Then every few weeks I go through my highlights, add them to a single [file](https://github.com/dangoldin/quotes/blob/master/quotes.txt), and dump it into a GitHub [repo](https://github.com/dangoldin/quotes). Earlier today I decided to do something fun and wrote a simple [endpoint](https://bots.dangoldin.com/quoteme) to fetch a random quote. The majority of my reading is nonfiction and this is reflected in my quotes. In addition, I’m a sucker for biographies and computer history books so they’re an even larger share of the quotes. I also like to throw a few idioms into the mix. So if you’re ever in the mood for a random quote take a look at [https://bots.dangoldin.com/quoteme](https://bots.dangoldin.com/quoteme).",4,1,2018-11-17,3,"quotes, bots, reading",151,Random quotes
37,0,I've decided to start learning Scala and have had great success learning it through the Project Euler problems. The biggest benefit has been being able to access other solutions once I'm able to solve a problem.,#meta,"{% include setup %} Over the past week I've been learning Scala. The initial motivation was that our API code is in PHP and in dire need of a rewrite. And since we've been rewriting our other critical applications in Java we want to leverage the JVM as much as possible. At the same time, we want to keep the code simple, expressive, and maintable while being fun and easy to write. I've heard great things about Scala so decided to give it a shot.  My first step was to install the  Play framework  and play around with the examples but I quickly discovered that while I could understand and tweak it, I needed a better Scala foundation to actually work on a real project. One of my favorite ways to learn a new language is to go through the Project Euler problems in a new language. The problems are a fun balance between mathematics and computer science and gradually build up in complexity which aligns itself well with the build up in my coding skills. The other big benefit is that solving the problems gives you access to other peoples' solutions which you can use to improve your code and knowledge. Normally looking at other people's code isn't the most impactful but in this case since you've already solved the problem you have the background to actually absorb the new patterns and styles.  So far, I've found Scala fun and expressive but tend to write it in a Java-like way. I'm definitely seeing significant changes in my style though. The very first solution looks just like Java while the most recent one is definitely functional. The plan is to work on a few more problems this weekend and then take another stab at the Play framework.",1,1,2015-02-13,4,"scala, programming, learning a new language, project euler",303,Learning Scala
8,0,Some thoughts on making subways in NY better,#design,"Source:  inhabitat.com     The combination of taking the subway every day and reading design books had me thinking of ways to improve the subway user experience, other than the obvious one of making it cleaner.    One thing that struck me is the feeling you get when you see the train leaving the station. It&#8217;s annoyingly stressful and makes me wonder how long I have to wait until the next train comes. Anything that can avoid this outcome would make waiting for the train a better experience. A way to do this is to limit the sensory feedback provided by seeing and hearing it leave. To avoid seeing the train until the last minute, subway stations can be designed to have stairs that need to be climbed in order to get to the platform. This way, the train and track will be hidden until the platform is reached. Making the train quieter would reduce the noise and prevent you from being aware that a train has left.    Of course, another cheaper and simpler way to deal with this is to just have something to occupy your time when waiting for the next train - that&#8217;s why having an iPad preloaded with ebooks is great.",1,1,2012-01-12,2,"subways, nyc",219,Improving the Subway User Experience
26,0,A quick guide on how to set up HTTPS on an EC2 instance that's running Nginx without having to upload the SSL certificate to an ELB,"#code,#devops","{% include setup %} I recently needed to set up HTTPS for my side project,  better404.com . Amazon makes it easy to  set up  by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.       Get an HTTPS certificate. I bought three certificates from Namecheap a couple of months ago when they were running a  promotion.     Go through the certificate generation process. I found  this guide  that explains how to do it detail and worked well for me. The only things I had to change where the  Nginx certificate configuration folder path (/opt/local/nginx/conf/certs => /etc/nginx/certs/) and replacing the filenames to be more specific (domain.* to better404.*). Note that this process is not immediate and you will need to send the contents of your CSR file to the SSL provider and they will respond back with the SSL certificate to use.     Enable SSL in Nginx. The previous guide provides some information here and I’m including the relevant parts from my configuration. I chose to redirect all traffic to HTTPS rather than supporting both simultaneously. {% highlight nginx %}#Nginx config server {     listen *:80;     server_name www.better404.com;     rewrite        ^ https://$server_name$request_uri? permanent; }  server {     listen *:80;     server_name better404.com;     rewrite        ^ https://$server_name$request_uri? permanent; }  server {     listen *:443 ssl;     server_name better404.com;      ssl on;     ssl_certificate certs/better404.pem;     ssl_certificate_key certs/better404.key;{% endhighlight nginx %}     Allow Nginx to bind to the IP address. One thing that’s not mentioned in the guide and required a bit of digging around is that you need to allow Nginx to bind to the non local IP address - otherwise it can only access the private IP address set by AWS. There’s a  quick guide  on how to do this I found on StackOverflow.     If you have any questions feel free to leave a comment and I’ll try to help out.",4,2,2014-07-15,4,"https, ssl, ec2, nginx",375,Set up HTTPS on EC2 running Nginx without ELB
29,0,As general tools get better and better we'll see a decline of niche tools for specific use cases. This will further entrench the position of the market leaders.,#meta,"{% include setup %} I have a few sites that are “first stops” for specific use cases. I’ll go to Google Maps for directions, Foursquare for ideas of where to go, and Amazon whenever I need to buy something. They’re great most of the time but what’s interesting is what happens in the failure case. At that point my primary tool is no longer sufficient and I need to move on to secondary options. In these cases I tend to not have a well defined set of fallback options - for most of them I’ll fall back to the general case of using Google and then exploring from the search results. The only clear exception is Foursquare in which case I’ll go to Yelp before moving on to a general Google search. What’s surprising is that the fall back option usually leads to a successful outcome. Maybe I should switch my approach to start with the general search first and only move on to the specific tools when it fails. I wonder if we’re converging to a world run by fewer, smarter, and more powerful apps.  Data begets data  and as we supply more of it to the leaders we entrench their position, making it significantly harder for new companies to launch. We need regulation that enforces data mobility and allows people to export all the data that they’ve contributed and share it with whoever they’d like.",1,1,2015-05-22,4,"data, monopoly, niche tools, google",243,The decline of niche tools
29,0,To learn React I worked on a simple webapp to help me explore the RGB colors. React is great but the JS build system is getting quite complicated.,#code,"{% include setup %}      The best way to learn a new technology is to play with it so to learn React I started a simple project I termed “[color-fun](https://dangoldin.github.io/color-fun/)"" ([GitHub](https://github.com/dangoldin/color-fun)). The general idea is to let you specify a starting color along with a step size for each of the digital primary colors and see the color progression. By messing around with various combinations one can get a pretty good sense of the way the RGB color scheme works. To make it a bit less boring there’s also a “random” option to generate a new value combination and a new color row.  While simple it was my first real attempt at a React project and - apologies for the pun - color me impressed. It was straightforward to get started and felt intuitive and direct. Oftentimes when learning a new framework it feels as if you’re learning a brand new way to do things without a good understand of the behind-the-scenes magic. With React there’s a fair amount of magic but it’s intuitive and doesn’t prevent you from getting up and running. I’m positive my code could be cleaner, less repetitive, and organized better but it logically makes sense and if someone were to suggest a different way of organizing it I’m sure I’d understand it immediately.  The biggest challenge was figuring out the modern JavaScript toolchain. I come from a world where one could do front end development by linking to a jQuery CDN but we’re far removed from those days. Now there’s webpack and browserify as well as a whole slew of highly-recommended libraries all available via npm. It’s great seeing this growth and evolution but it still doesn’t feel as simple as it ought to be. The biggest benefit of learning to code JavaScript is the ability to run it directly in the browser and while helpful, these tools do keep pushing us further away. I’ve had to deal with some complex build systems - ranging from make to maven to gradle - and I can imagine how challenging they can be to a new developer. The quicker one can get to actually writing the code the better and reducing friction in the toolchain is necessary to get more people into coding. Alongside the great new frameworks there needs to be a well fleshed out build system.",2,1,2017-04-30,5,"react, colors, rgb, javascript development, frontend engineering",409,Having some fun with the RGB color model
36,0,Following up to my Drowning in JavaScript post I take a look at the top 100 Alexa sites and see how many external files they're loading and the impact that has on page load time.,"#datascience,#dataviz","{% include setup %} Since writing the  Drowning in JavaScript  post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.                                                          Average load time.  To get a general sense of the data this plots the average time it took to load each URL. The interesting piece here is that multiple foreign sites take a while to load (163.com, qq.com, gmw.cn, ..) - I suspect a big reason is that there's quite a bit of latency since I'm based in the US. Another observation is that many news sites tended to load slowly (huffingpost.com, cnn.com, cnet.com). The Google sites loaded extremely quickly (all less than 1 sec) as did Wikipedia.                                                                          Load time boxplot.  This provides a bit more information on the load times by showing the min/max values as well as the median and the percentiles. Not a significant amount of new insight here.                                                                          Number of requests.  Huge variance here as well - rakuten.co.jp loaded almost 800 external files while the Google sites are all less than 10.                                                                          Number of request vs time to load.  This leads to the question of whether sites that are loading more files take a longer time to load. By plotting a scatter plot between the two it's pretty clear that all things being equal more files increase page load time.                                                                          Number of requests vs time to load linear fit.  A simple regression of load time as a function of the number of file requets confirms this. On average, each additional file leads to an additional 20 milliseconds of load.                                                                          File type frequency.  We can also take a look at the most common type of file requested. As expected, images are the majority of requests followed by JavaScript.                                                                          File types by url.  Not a lot of insight here but the colors sure are pretty. One thing that does standout is that if a site has a significant amount of file requests they tend to be of multiple types.                                                                          File type correlation.  We can plot a simple correlation of file type found on a page to see whether there are any file types that tend to be included together. Not much going on here.                                                                          Multiple linear regression.  And just for fun we can run a regression to see whether a particular file type leads to significantly worse load than others. I was hoping to show that having a lot of JavaScript hurts performance but that doesn't seem to be the case. I suspect it's due to the innate time differences it takes to load some sites (in particular sites outside the US) vs others.                      As usual, the code’s up on  GitHub .",11,2,2014-03-09,4,"site speed, javascript, phantomjs, data analysis",842,Examining the requests made by the top 100 sites
30,0,I revisted an analysis from 2014 where I looked at the HTTP requests being made by the top 100 Alexa sites in order to see what affects page speed.,"#datascience,#dataviz","{% include setup %} A [few years ago](http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/) I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they're making.                                                          Average load time.  Pretty similar to last year with most of the top platform sites being incredibly quick. The surprising thing is that on average sites seem to have gotten slower but this can be entirely due to me having a different internet connection - something that on its own is an issue.                                                                          Load time boxplot.  Similar distribution to two years ago but so much more variance. No idea why this would be the case.                                                                          Number of requests.  Many more requests being made in 2016 than in 2014. In 2014 no site made over 1000 requests but in 2016 we see it happening with 3 sites.                                                                          Number of request vs time to load.  Expected and similar results to 2014. The more requests a site is making the longer it takes to load.                                                                          File type frequency.  Pretty similar distribution to 2014 but we do see much higher numbers across the board and a relative decrease in JavaScript and an increase in JSON and HTML.                                                                          File types by url.  Not much here but seems that there's a bit more variety of content types compared to 2014 although still heavily dominated by images.                      As usual, the code’s up on  GitHub  but you'll need to go back in the revision history to get access to the old data files.",8,2,2016-11-18,4,"site speed, javascript, phantomjs, data analysis",557,Comparing the web requests made by the top sites: 2014 vs 2016
31,0,It's easy to procrastinate and delude ourselves into ignoring simple tasks but all it takes is to take the first step. After that taking the next step becomes much simpler.,#meta,"{% include setup %} Numerous times I’ve procrastinated on doing something convincing myself that it would either take too long or just wasn’t worth doing but more often than not when I finally take the first step I’m able to quickly complete the task. I don’t know why our minds encourage procrastination but I suspect it’s not just me. I’ve been combating this tendency by recognizing that it’s happening and forcing myself to just do something, as simple as it is. A small task typically turns into a series of small tasks in which I’m able to make a significant amount of progress. In fact, there have been numerous times where I’ve even been able to achieve “flow” - despite being hesitant in the first place. It doesn’t seem like much but 10 minutes here and there do add up. Whether it’s coding up a simple feature, doing a quick data analysis, or just jotting down a few ideas, it’s infinitely more valuable than staring at a phone.",0,1,2015-01-17,2,"productivity, efficiency",168,Take the first step
18,0,I joined TripleLift's engineering team as a data wrangler and am excited about the space and opportunity.,#meta,"{% include setup %} Just a quick update on my professional life. I recently joined  TripleLift ’s engineering team. I met the founders while at  ERA  and liked the problem they were solving. It was also time for me to move on from my other projects so when I found out they were growing it was a pretty easy decision. Being a startup, it’s hard to pinpoint exactly what everyone’s responsibilities are since everyone becomes a generalist but I’ve been focused on the data side. This entails developing our various data pipelines, leveraging the data we have to improve performance and unlock new opportunities, and doing some light data science to help model and understand the native advertising space.  In 2012, US marketers spent close to $15B on internet display ads and this number’s only growing. The old banner ad model is a huge chunk of it but it’s being supplanted by better, smarter technologies that provide significantly higher engagement rates. Social networks have embraced native advertising that blends into their products rather than being delegated to a sidebar or a banner that’s immediately ignored. Native advertising is a much better approach and I believe it’s going to take up the majority of display budgets within the next couple of years. TripleLift is doing some incredibly awesome things by allowing publishers to design sites around their content and then easily integrate ads that enhance the user experience rather than disrupt it while advertisers are able to get their ads in front of users that actually enjoy seeing them. It’s an evolution of display advertising that’s better for everyone involved that I can’t see it failing. Clearly I’m excited.  PS. TripleLift is hiring so let me know if you want to talk about the various roles and what we’re doing.",2,1,2013-11-23,4,"triplelift, advertising, native advertising, display advertising",305,I'm joining TripleLift
27,0,It's so much easier to offer advice to others than to take it yourself. This is just a post of how I'm trying to overcome it.,#meta,"{% include setup %} Since becoming active in the startup scene, I've been meeting a ton of founders and am annoyed by how much easier it is to offer suggestions than to apply them to myself. My most common suggestion, in true lean startup fashion, is to advocate a quicker or cheaper way to validate the market before building a product and yet it’s extremely difficult to take my own advice. I’ve been working on  Better404  on and off for two months now and know I should get it in front of potential customers and yet I keep on making minor tweaks and updates to the product.  I'm not sure if this is due to how much easier it is to say something versus executing it or just a fear of failure but it’s something I’m becoming more and more aware of. At least now I’m aware of this bias and and can work on correcting it. What’s been working well so far is coming up with a framework that can be used to evaluate a startup in a similar space and then using it to evaluate my own. The challenge is coming up with the framework without biasing it knowing that you will be using it to evaluate your own project. Nonetheless, this additional indirect step does help.  I’ve been looking at other SaaS businesses and listing what they could be doing better and what they’re doing well as well as what I’d do if I were in their situation. Having a concrete target for this sort of analysis is a great way to come up with a large number of specific suggestions and although many of these aren’t relevant a few can be applied to what I’m working on.  To succeed, it’s critical to be able to look at your startup from an unbiased, external perspective. This is even more true as a solo founder since there’s no one else bringing their own ideas and experiences. I’d love to know how others are overcoming this.",1,1,2013-09-16,3,"suggestions, advice, startups",341,Offering suggestions
40,0,Marketplace startups are very popular these days and do provide value but I'm just not sure why there are so many home cleaning ones out there. That doesn't seem to be the best market for this type of model.,#product,"{% include setup %} The recent rise of marketplace startups is great and benefits all except the incumbent. They provide much needed liquidity and transparency to markets that helpfully reduce costs to the consumer and increase volume to the provider.  Yet I’m surprised by the number of home cleaning service startups out there. I’m aware of  HomeJoy ,  Exec ,  GetMaid ,  MyClean , and  HandyBook , but am sure there are countless other copycats. The Uber approach works because it’s for an immediate service with a one time transaction where the value provided is somewhat of a commodity. This is not the case with home cleaning services. The range of quality among cleaners varies significantly more than the quality among drivers and I’d be willing to have a good cleaner come in at a slightly inconvenient time rather than a poor cleaner at the perfect time. And once I find a cleaner I like I’d want to book them directly rather than go through the company again. This way I can get a lower rate while also giving a cleaner more than they’d otherwise make from using the service. This would violate the company’s terms but I don’t see how they can be enforced.  I’m sure there’s still a large market for such cleaning services, I’m just not as  optimistic as others  seem to be. I see it being great for one off events - cleaning after a party, getting your apartment ready a visit from the parents, or preparing your home for sale. I just don’t see how this is a huge market that warrants all these startups. I understand it’s just the beachhead but it doesn’t seem to be a very strong one. Have the better ones already been done? Uber is dominating car service. Food delivery is full of one-time transactions that need to be met quickly but it’s already a mature market. Laundry is another one and there are two startups I’m aware of working on it - Washio and Prim. HandyBook also offers handyman services although they seem to be more focused on home cleaning. I really don’t know why home cleaning startups are so popular.",6,1,2013-12-09,1,,386,Why are there so many cleaning startups?
35,0,DevOps is a necessary skill for all software engineers to know. It gets you closer to the hardware while letting you think at a more abstract level of how your application will actually work.,#devops,"{% include setup %} I’m becoming increasingly convinced that DevOps is a necessary skill for any software engineer to have. It gets you closer to the hardware and helps you understand the way your code will actually run and where it fits within the tech stack. It also provides independence when working on new projects since it gives you both the knowledge to understand the needs as well as empowers you to make them happen. This is especially important when working at a small company where there’s immense value in having general skills that can be used to make progress independently without need to disrupt others. My first job writing code I only had to worry about my small patch of software but over time I’ve slowly picked up a variety of DevOps skills that help me write better code. Below are the skills that every software engineer should know - they may not all fall under traditional DevOps but I believe they’re essential for anyone writing code. If you have any others let me know and I’ll add them to the list.  - **Shell commands**. Almost any task can be done efficiently using a series of shell commands. Being aware of the various commands and their options makes it simple to find specific files, summarize data, or just monitor a server. The most important ones are ls, cd, pwd, rm, wc, find, grep, head, tail, less, sort, cut, sed, and curl. - **Configuration of various applications**. This might be specific to Linux/Unix but knowing the default configurations of common applications and where the settings are is important when setting up an application and diagnosing problems. There have been countless times I installed something on my box only to run into issues with unexpected behavior. Being able to examine the configuration and grep through the logs is critical to understanding what’s actually happening. - **Package managers**. Both the ones by provided by the operating system (apt and yum) as well as the language specific ones (pip, npm, gem, CPAN). There already so many powerful open source tools and libraries available that it’s rare to find something that can’t at least serve as a starting point for whatever you’re building. - **Web architecture**. This is a big one but it’s crucial to understand how the internet works from the time you type in a web address to a browser to what’s going to happen on the server. A good starting point is to look at Amazon Web Services and be able to explain each of the services offered, how they’re used, and how they fit together. Even better is to play with them to get a sense of how they can be used. - **Setting up and deploying an application on a brand new VPS**. Using AWS or Digital Ocean it’s trivial to get a virtual private server (VPS) but it will come completely empty. Being able to log in, install the necessary software, and get it responding to external requests is vital for anyone writing web code. Even better is deploying multiple applications on the same VPS and configuring DNS to make it work. - **Back of the envelope calculations**. Having a sense of how long various types of requests and operations take is important in understanding the type of hardware you need and where optimizations can be made. Another good skill is thinking in terms of “bytes.” This helps you estimate how much memory your code is using and makes it easy to understand what solutions are scalable and which will end up causing problems.",0,1,2014-12-26,4,"devops, aws, vps, engineering",591,DevOps for the rest of us
21,0,Startups need to push boundaries to grow and I share an experience we had pushing a boundary at Makers Alley.,#meta,"{% include setup %} Startups need to use everything in their arsenal to grow. A big part of it is playing in the grey area between moral and immoral. Do you create fake users and comments to portray an active community? Do you reply to posts on Craigslist trying to get visitors to your site? It’s also much easier to play in this area when you’re a startup - you’re most likely too small to be noticed and even if you are the press won’t spend much time on it. Google already gets a ton of flak every time someone complains about losing business due to a search engine update, imagine what would happen if a Google employee was caught spamming Craigslist.  It’s important for all companies, and especially startups, to test these moral boundaries but there’s no clear answer of what the boundaries actually are, just shades of gray which will vary from company to company and from team to team. I believe that until you get some resistance you need to keep on pushing otherwise you never know that you’re doing enough.  At  Makers Alley , our lesson came when we wanted to increase the amount of makers signing up. We decided to create pages for all makers in an area and then email each of them a link to “claim” their page. In order to make the pages look appealing we took images and descriptions from their individual sites. The results were mixed: as expected most emails didn’t even get a response but the ones that did had a wide range of reactions. Some of the makers gladly claimed their page and loved that their content was automatically pulled. Yet others were pissed that we used their copyright images on our own site without their permission. We weren’t comfortable knowingly upsetting some users and quickly removed the images from the unclaimed pages. We continued running a few other tests to try to maximize the “sign up rate per email” and ended up settling on a simple email that asks the makers to sign up and getting rid of the claim functionality. But without making the misstep in the beginning we wouldn’t have been able to settle on this approach. I do worry that less scrupulous companies have a higher chance of success but I can only take actions I’m comfortable with.",1,1,2013-06-20,3,"startups, morals, boundaries",396,Pushing moral boundaries
20,0,Startups that cater towards developers should adopt an API first approach and share it publicy with their potential customers.,#product,"{% include setup %} When building a SAAS product geared towards developers the quickest way to start is to build an API. One can even make the argument that the MVP should just be the API documentation. This benefits both sides. Potential users of the API will know exactly what to expect and have a clear understanding of the functionality and limitations and you can quickly see if there are any issues or inconsistencies in what you’re building. Some non-fiction authors will share a table of contents with potential readers in order to get feedback and this extends that idea to companies and their products. Especially when your primary users are developers this is a simple way to share your idea and approach without resorting to buzzwords or even relying on a beautiful site design.  The best example of this approach is  Stripe . At launch, they had a beautiful API that you were able to use without creating an account. After seeing how it well it worked it was an easy decision to register for a full account. Tons of companies adopt an API-first approach for their internal systems and it’s not a lot of work to extend this to the external world. There’s definitely a risk in doing it since you’re exposing more of your internals but if you claim to be developer friendly it’s the best way to actually prove it.",1,1,2014-09-01,4,"API first startup, product strategy, saas, product",239,API first startups
7,0,My thoughts back on the year 2012,#meta,"{% include setup %} I'm finally in a position to do a ""Year in Review"" post that I’m comfortable writing. In March I left my full time job to pursue  Glossi , our startup, full time. In May, we were accepted into an  incubator  and had an amazingly productive four months. Unfortunately, none of us were passionate about the direction Glossi was headed and we’ve ended the year pursuing a new venture,  Makers Alley .  I learned a ton about myself but along with that came the realization of how much I don't know. Working as one of three cofounders on a startup forced me to develop a much broader set of skills. I understand the whole technology stack better and can actually make a passable website now (mostly thanks to  Bootstrap ) but I can also discuss trademark and copyright law with lawyers as well as do some tax planning with an accountant. I'm not too interested in pursuing law or accounting as a profession but it's great being able to follow along and chime in every once in a while with something useful.  More important than the skills, I've learned more about myself in the past year than I have in my entire professional life. I've discovered that money is not as important to me as I thought and that the control over my day is important to me, even if it does mean more work for less pay. I have a better understanding of my strengths and how they are best applied. I also know what my weaknesses are and am working on a few goals for 2013 which I’ll publish over the next few days. I'm still trying to figure out my exact passions but at least I'm starting to acknowledge that they need to be discovered.  Lastly, I want to acknowledge how lucky I am to even be in this position. It's wonderful having a  wife  and family who support me through all this, I know it can't be easy.",5,1,2013-01-01,1,2012 year,345,Year in Review 2012
6,0,Enabled Apache2 modules in Ubuntu manually,#code,"The Apache enabled modules are found in  ""/etc/apache2/mods-enabled""  as a set of .load and .conf files. If the modules you want are in the  /etc/apache2/mods-available  folder but not in  ""/etc/apache2/mods-enabled""  folder, just copy the .load and .conf files over (note that the .conf file may not exist).    If there is no file in the mods-availble folder, you will need to create a new .load file in the mods-available folder to point to a module in  ""/usr/lib/apache2/modules"" . To do this, create a .load file containing the line  ""LoadModule xxx /usr/lib/apache2/modules/yyy.so""  where xxx is the name of the module and yyy is the file name. After creating this file, you can just copy it over to the mods-enabled folder and restart apache using  ""sudo /etc/init.d/apache2 restart"" .",0,1,2009-01-23,2,"apache2, ubuntu",140,Enabling modules in Apache2 under Ubuntu
33,0,By getting a variety of information from Zillow you can estimate the cost of owning an apartment and then comparing that against the rent to see what kind of value you're getting.,#meta,"{% include setup %} Housing is one of the largest expenses and it’s worth trying to get the best deal you can. When it comes to finding an apartment rental there are a few tricks I’ve picked up that help find the best value. Generally, the approach is more suited for apartments that are being rented out by an owner rather than a development being rented out by a single company since there’s just more information out there.  The idea is pretty straightforward - estimate the total costs the owner is paying and then see how that compares to the listed rent. At least in the New York City area, the primary expenses when owning a home are the mortgage, taxes, and the maintenance fees. Each of these can be fetched or at least estimated using Zillow.  To calculate the monthly mortgage payment, Zillow provides the Zestimate and you can use a simple mortgage calculator assuming a common 20% down payment and a 30 year loan to get the monthly mortgage payment. The taxes may be available on the listing as well but if not they may be available from public records usually available online or estimated from another unit in the same building. The maintenance fee is usually the trickiest to get since it’s not typically listed unless the unit is for sale. The best way I’ve been able to get it is by looking at units in the building that are listed for sale and adjusting their maintenance fee by the ratio of floor space since maintenance fees are usually calculated based on square footage.  Adding these three numbers up should hopefully give you something that’s more than the rent. Then to see how good of a deal the rent is you just take the ratio of rent to our estimated monthly cost: the lower it is the better the value. I did this with a few units in the area and it turns out the majority of the ratios fell between 0.75 and 0.9 which makes sense since renting should be cheaper than owning.",0,1,2017-09-09,4,"apartment, rental, ownership, rent vs own",349,Apartment rental arbitrage
14,0,Messaging apps are extreemly fragmented and it's fascinating to think about the space.,#product,"{% include setup %} The messaging space is fascinating. There are probably hundreds of apps available with pretty massive fragmentation. Onavo collected the following data to indicate the reach of the various messaging apps by country and while WhatsApp (owned by Facebook) is clearly dominant there are some countries that WhatsApp is a fringe player, especially among Asian countries.       via TechCrunch    It’s shocking how dominant the local companies are in Asia. WeChat is the behemoth in China. In Japan it’s Line. And in Korea it’s KakaoTalk. I don’t know whether it’s as simple as nationalism or that the local companies just have a much better understanding of the market and were able to build better products.  I just view messaging apps as utilities.There’s no need to restrict myself to a single app and I use them reactively. If someone messages out via iMessage I’ll use that. If someone uses WhatsApp I’ll use that. And so on. If a friend asks me to use a particular app I have no problem downloading it and giving it a shot. They have limited network effects and there’s no reason to restrict yourself to one.  I suspect most people feel the same way. They probably have an app that’s the goto with their most frequently messaged group but if they’re part of another group that has their own principal app there’s nothing stopping them from using it.  The most dominant apps will be the ones that are able to leverage them to become utilities. Tencent has built a massive business on top of WeChat which acts as the digital hub in China. WeChat is not just for messaging but is essentially the operating system for mobile in China. It can be used to interact with a litany of services in china - including payment at physical stores, booking ridesharing services, and serving as an authority on identity. Nothing like this exists in the US or Europe and it’ll be interesting to see what comes out.",1,1,2016-06-22,4,"messaging, wechat, facebook messenger, whatsapp",356,Messaging app fragmentation
28,0,After getting disconnected one too many times due to crappy internet service and losing my SSH sessions I discovered Mosh which makes remote development a lot easier.,"#meta,#devops","{% include setup %} Since I do a fair amount of web development having flaky internet is a big hit to my productivity; especially when I have a half dozen open SSH sessions that bulk disconnect every few minutes. After being thwarted one too many times by spotty internet at the office I decided I had enough and started looking for alternatives. One of the tools I discovered was  Mosh . Mosh allows you to open a remote session just like you would do with SSH but unlike SSH it’s robust enough to handle networking disruptions. In fact, I can start a Mosh session on Friday afternoon before leaving the office for the weekend, let my computer go to sleep, and then have it automatically resume as soon as I get back to the office on Monday and wake my computer up. I’m still amazed at how well it works and only wish I discovered it sooner.",1,2,2015-05-12,3,"mosh, shoddy internet, flaky internet",161,Mosh trumps shoddy internet
24,0,I will watch lectures and listen to podcasts at twice the original speed. Why don't we speak and listen at twice the speed?,#meta,{% include setup %} Whenever I watch some online lectures or listen to a podcast one of the first things I do is change the speed to either 1.5x or 2x the original. Sometimes I’ll have to skip back or reduce it back to the normal speed but for the most part this approach saves me tons of time and I like to think that I absorb the same amount of information. But the fact that I can absorb and process information at twice the speed makes me wonder how much more productive I’d be if every conversation I had occured at twice the speed. Is there some physiological reason we don’t speak at twice the speed? Is there a cultural factor? Does this information density vary based on language?  We can train ourselves to get faster and faster at processing aural information but there must be some limit and I suspect there’s a wide range in the information density of various languages. If this is the case I wonder if there’s some conclusion that can be drawn about that culture or society. Most likely the bottleneck is on the transmission side - the effort to produce language is more than listening and it requires both our brains to form thoughts and our mouths to turn them into sounds which are the limiting reagent.,0,1,2016-02-27,3,"language, speech, information processing",225,Aural information density
26,0,"Humans are wired for stories so we should make all talks, whether technical or not, be delivered as a narrative and focus on a story.",#meta,"{% include setup %} Last week we hosted a meetup describing the evolution of our data pipeline over the past 5 years - starting with a simple script to aggregate log files to a fully fledged big data system relying on a slew of technologies ranging from Kafka to Spark to Redshift. I find most meetups and presentations more focused on the present which is valuable but does a disservice to the audience by not describing the history, motivations, or thought process. No system lives in a vacuum and it’s much more interesting to look at the evolution of a system rather than focusing on its current state.  Seeing a large and complicated system for the first time is overwhelming but it’s incredibly unlikely it was built that way. Most likely it required multiple iterations with some subsystems being replaced by others until the present state was reached. Going over the evolution of a system reveals that process and makes the final result more approachable and relevant.  Stories are a core part of the human experience and we’re wired to think in narratives rather than standalone facts. The most interesting talks and presentations are done in the form of a story with an obstacle that needs to be overcome, a series of potential solutions and setbacks, and finally an approach that solves the problem once and for all. Technical talks don’t always lend themselves easily to a narrative but done right they end up being incredibly sticky and enjoyable. If you’re giving a talk on a technical topic, or any topic for that matter, see if you can do it as a story rather than listing a series of facts.",0,1,2017-08-28,3,"public speaking, meetups, presentations",280,Focus on the story
30,0,I recently moved my personal code over from Linode to Digital Ocean. I was hesitant at first and worried about the migration but it went as smoothly as possible.,#devops,"{% include setup %} Ever since I saw that Digital Ocean charged $5/mo, I’ve been meaning to migrate my sites and projects over from Linode but have been wary of dealing with the various issues that would ensue. I finally bit the bullet earlier this week and it went surprisingly smoothly.  My biggest concern was forgetting to copy some files that specified some esoteric settings I came up with when I first set up the projects. Luckily I didn’t run into this issue and most of the effort was spent in trying out my sites and looking at the log files to see which libraries were missing.  Here’s a quick overview of the process - the Digital Ocean  migration guide  was a big help.  - Follow the guide and install/configure rsync on both boxes - Use rsync to migrate the relevant files and folders. In my case it was everything in /var/www, the sites-enabled apache folder, and the MySQL dump - Install apache, MySQL and the other likely required packages. An overkill approach would have been to list every package on my old box and install it on the new one. - Load the MySQL dump into the new instance - Restart apache and go through the configuration settings. All the issues were due to disabled apache modules (headers, deflate, expires) and enabling them resolved them. - Go through each of the configured sites and make sure they worked. No downtime wasn’t a requirement for me so I ended up changing the DNS settings one by one confirming that each site ran properly. The PHP sites worked immediately but the python sites needed some packages installed via pip. - Copy the cronjobs from the old box to the new one  None of my projects were complicated and the migration went as smoothly as possible. Most of the time was spent waiting for the files to copy or packages to install and I never ran into an issue that didn’t have an immediately obvious fix. The error log was a big help - it gave me a quick way to identify problems and the missing packages. If you’re on the fence about migrating to Digital Ocean and the only thing holding you back is worrying about the migration I suggest just going for it. Worst case is you spend a few hours playing around with a new box.",1,1,2014-05-02,4,"linode, digital ocean, migration, devops",403,Migrating from Linode to Digital Ocean
16,0,I ran into a pretty frustrating design choice when using the Chipotle app earlier today.,#design,{% include setup %}         Wanting to avoid a busy lunch rush but hankering for Chipotle I decided to download their app to order ahead. It’s a straightforward app and everything went as expected until I had to enter the expiration date for my credit card. The way the app is set up is that you’re expected to choose the month first followed by the year. Unfortunately it prevents you from picking a month in the past. One can probably guess what problem this leads to: if the expiration date is in the future but the expiration month is before today’s month the app rejects the month change until you change the year. The screenshot illustrates the design.  I tend to be more passionate about usability issues than most - especially ones that are obviously wrong and trivial to fix. I suspect in this case in the desire to make the user experience better by not allowing a user to select a date in the past it actually had the opposite effect and decreased the usability.,0,1,2016-05-14,5,"design, usability, date selection, user experience, chipotle app",196,Expiration date selection design anti pattern
23,0,It's possible to use PostgreSQL to define recursive relationships. In this case we define a Fibonacci number generator using a PostgreSQL query.,#code,"{% include setup %} Earlier today I was researching whether it was possible to generate Fibonacci numbers using a SQL query. A Google search turned up a  short PostgreSQL  query that uses a recursive approach. Since this is recursion, the query starts by defining a base case and then goes on to define a generation step with a stopping limit.  {% highlight sql %}with recursive f as (     select 0 as a, 1 as b     union all     select b as a, a+b from f where a < 100000 ) select a from f {% endhighlight %}  It’s interesting to see the edge features of a language and I find that query languages tend to have the most striking ones. My experience with the various SQLs has been that at the basic level they’re very similar but diverge significantly at the edges.",1,1,2014-04-04,2,"postgresql, fibonacci",151,PostgreSQL Fibonacci
23,0,I never realized this until I read The Idea Factory but cell phones don't have dialtones and landlines do. Why is that?,"#meta,#history","{% include setup %} While reading  The Idea Factory , I came across an interesting passage that explained why cell phones don’t have dialtones:   Meanwhile, Phil Porter, who had worked with [Richard] Frenkiel on the original system, came up with a permanent answer to an interesting question. Should a cellular phone have a dial tone? Porter made a radical suggestion that it shouldn’t. A caller should dial a number and then push “send.” That way, the mobile caller would be less rushed; also, the call would be connected for a shorter time, thus putting less strain on the network. That this idea—dial, then send—would later prove crucial to texting technology was not even considered.    It’s amazing that although this suggestion was made in 1971, we’re leveraging it more than 40 years later with text messaging. How many other technologies and businesses are built on top of SMS that wouldn’t have existed without this decision? I’m sure an SMS-like technology would have come along regardless of this decision but it still makes me wonder how significantly past technological decisions influence us in the present.  An additional meta thought: this is an example of one of those things that gladly lives in the subconscious that has no reason to bubble up to consciousness. I’m sure if someone were to ask me point blank to compare dialtones between landlines and cell phones I’d immediately get it but without a push I never would have thought of it. I wonder how many other connections there are stuck in our heads waiting for a spark to bring them into our consciousness.",1,2,2013-04-12,4,"cell phones, dialtone, land lines, technology",277,Why don't cell phones have a dialtone?
28,0,Viewing a long article as a single page should preserve my spot on the page. It's really not a difficult tech problem and a big usability win.,#design,"{% include setup %}            When reading a long form piece, I favor the single-page view. Unfortunately, I usually don’t find out that it’s longer than a page until I’ve finished the first page. At that point, I switch to the single page view which causes the entire page to reload and I have to skim the page to find the spot where I stopped reading.  Why haven’t any of the major content sites dealt with this yet? It’s not a technical problem. All they’d need to do is have an anchor tag on the single page view to indicate the spot that the reader should be shown if they click the “View as Single Page” link after reading the first page. Do they just want to force their readers to look at the ads again? I’m sure this small change would save people hundreds of hours each day.  If you know of any sites that handle this scenario well, let me know; I’d love to check them out and give them a shout out.",0,1,2013-05-22,4,"usability, design, UX, long form content",196,"Save my reading spot, damn it"
27,0,As software gets more and more evolved it becomes more and more human. Yet once software is prevalent it can fallback to technology we already have.,#society,"{% include setup %} Google Duplex was announced earlier this year as a way to automate tasks that require a phone call - such as booking an appointment. Rather than calling yourself google has an assistant that understands human speech and speaks convincingly enough to do these simple tasks. Relatedly, a few days I read [an article](https://gizmodo.com/google-is-reportedly-looking-to-take-over-call-centers-1827379911) describing how Google is also pushing Duplex to be used in call centers. A bit surprising but makes a ton of sense since the vast majority of the calls are relatively simple tasks that companies are already trying to automate as much as possible using Interactive Voice Response (IVR) systems.  Now imagine both of these becoming successful: we end up in a world where my consumer Duplex is having a conversation with a call center’s Duplex. If only there was a way for machines to communicate directly with each other in a standard protocol instead of depending on some advanced natural language processing.  On a more serious note it’s pretty amazing that that we’re getting to a world where computers are actually able replicate human behaviors. Rather than building systems that can speak to each other via APIs we’re instead building systems that have to speak human first.  This is the same thing that’s happening with self driving cars. Building a self driving car is much more difficult in a world with humans driving than if every car was driven by software. In fact, if we didn’t have human drivers on the road today I suspect we’d already have self driving cars.  The irony is that as software gets better and becomes universal is when it could be dead simple. The software first has to convince us it’s good enough to be human before it can act as a machine.  Maybe this is what’s going to save us from the AI apocalypse.",1,1,2018-07-10,4,"ai, software, humanity, google duplex",319,"In a software world, humanity comes first"
24,0,To dump a large agg table we wrote a simple script to slice into small date based chunks and handle each one individually.,#code,"{% include setup %} One of the major changes we made when building the latest iteration of our data pipeline was moving our key agg tables over from MySQL to Redshift. Despite the migration we thought it would be prudent to archive these tables. The challenge was that some of these tables were hundreds of gigabytes so doing a simple mysqldump wouldn’t work. The reason these tables were so large is because they included a date dimension which led to our [archive script](https://github.com/dangoldin/python-tools/blob/master/archive_tables.py). The script works by generating a sequence of shell commands that slice the table into chunks by date, gzip each chunk, and upload it to an S3 bucket. This keeps each individual chunk small enough to archive while making sure all the data is captured. It’s not the most elegant solution but it’s obvious and it’s quick. The one piece that’s missing is the table schema which can be fetched separately.",1,1,2017-11-18,4,"mysql, archive, data, mysqldump",158,Archiving large MySQL tables
20,0,Similar to the blog analysis I ran in 2016 I repeated it for 2017 to see what stood out.,"#code,#dataviz","{% include setup %} I have a set of [scripts](https://github.com/dangoldin/blog-analytics) I wrote in 2016 that aimed to [analyze my posts](/2016/06/12/analyzing-my-blog/) over the years and hopefully offered up some insights. I’ve updated them for 2017 but rather than posting every single visualization I thought it would be more valuable to highlight the ones that seemed the most relevant and interesting.                              The year is not quite over but I'm defintiely behind on my posts that I hope to power through by the end of the year.                                       Similarly, the total word volume is lower and in addition to fewer posts it seems when I do write my posts are shorter.                                       I wanted to avoid only writing on the weekend and this year has been an improvement. I strive to write during the week to keep it more balanced but it hasn't been as easy to find the time and I end up having to catch up over the weekend.                                       A somewhat interesting chart many of my posts are about tech companies and this shows the trend of companies I write about over time. Surprisingly not much has changed over the past few years and it seems I've settled on a pattern.                                       I like the concept of word clouds since with a quick glance you're able to get a sense of the topic. In this case it's definitely leaning more on the tech side as well as high level software development concepts.",2,2,2017-12-21,3,"data visualization, blog, analysis",361,Analyzing my blog: 2017 edition
14,0,Is the MongoHQ hack causing more companies to bring their security in house?,"#meta,#product","{% include setup %} Over the past few days my inbox has been filled with security alert emails caused by the  MongoHQ  database hack. I’m impressed by the number of customers MongoHQ was able to sign up - they spanned the gamut from sites that I don’t even recall signing up for to startups that have been getting significant buzz.  If a database as a service company is able to get hacked it doesn’t leave me optimistic about the way other companies are securing our data. As much as these “as a service” products make our lives easier they bring an increased risk to our business and more importantly our customers. Sure their security will be better than someone who’s setting up a MongoDB instance for the first time but that has to be balanced against the fact that a hosting site offers a much higher reward for a hacking attempt. Access to the infrastructure provides a lot more information than hacking an individual site.  I used to believe that doing security internally was dumb but now I’m not so sure. No one will care about hacking a small site and if it turns out that the site is becoming successful you can dedicate the resources to properly secure it. At the same time, with so many people sharing passwords across multiple accounts it only takes one careless site to undermine the efforts of all the others.  Some of the security alerts I’ve received have mentioned that they plan on managing their database internally rather than relying on a third party; I wonder if this is the start of a trend.",1,2,2013-11-07,4,"MongoHQ, security, hack, infrastructure",275,Security in the wake of MongoHQ
20,0,The iPhone 5C is more expensive than people expected to avoid cannibalizing iPhone 5S and serves as a 'Decoy effect',"#product,#pricing","{% include setup %}  Many people  expected  the iPhone 5C to be priced low in order to compete with the cheaper Android phones in countries without carrier subsidies. The news that the 5C’s starting price was $549 left many in the tech community  surprised and concerned  with many believing that the price needed to be lower than $400 in order to compete worldwide.    I'm definitely speculating but I believe the reason for such a high price for the 5C was to avoid cannibalizing the sales of the 5S while also framing the comparison to be iPhone vs iPhone instead of iPhone vs Android. When people go smartphone shopping they see that the 5S is “only” $100 more than the 5C and pay the difference for the more premium product. If the 5C were significantly cheaper people would be comparing it to a similarly priced Android phone which may encourage them to go with the Android or they'd compare it against the 5S which would get many to purchase the much cheaper 5C instead.     Localytics  put together a  great chart  showing the share difference between the 5S and the 5C which supports this view. The 5S is outselling the 5C in every country by a large margin. And although Apple could get higher sales volume with a cheaper 5C, I believe that the cannibalization of the 5S and the reduction in margins would have made Apple less profitable overall. At the same time., charging a lower price for the 5C to get more market share at the cost of profit may have been the right long-term decision.     	  		   	      Pricing products that are sold worldwide is a tough problem since every country has its own economic environment with unique shopping behaviors. This challenge is magnified for products that can easily be bought in one country and resold in another. The opportunities also vary significantly: the US is a mature smartphone market where carrier subsidies exist and make the buyer less price sensitive while in China many people are getting smartphones for the first time and lack carrier subsidies that will help reduce the initial sale price. We’ll just have to wait to see whether this was the right pricing decision.              Note: This reminded me a pricing table the Economist had a while back that looked like an error. There were three options: 1) a web-only subscription for $59, 2) a print subscription for $125, and 3) a print and web subscription for $125. In this case, the last two options were the same price but option 3 offered more value so it was clearly better than option 2. Yet by having option 2, the Economist got people to compare option 3 against option 2 rather than option 3 against option 1. This phenomenon is called the “ Decoy effect ” and worth understanding, especially if you’re a marketer.",6,2,2013-10-01,6,"apple, iphone, 5S, 5C, pricing, marketing",572,Why's the iPhone 5C so expensive?
12,1,Startups can compete by focusing on the details and doing things quickly,#product,"A frustration I’ve been experiencing more and more is having to reload a webpage in order to change the date range in the options. If a company expects me to keep a site open for more than a day they should make it easy for me to update the options. The big example is Google Analytics - I open up a page, choose a date range, and get to see my charts. If I keep the tab open and want to want to run the same analysis the next day, I’m forced to reload the page to even be able to include today in the date range. It’s an unnecessary action for the user and it would be easy to correct this behavior with some simple Javascript.    Such small details don’t matter individually but together they reflect a lack of empathy for the user that impacts a company culture. We should always be striving to make a user’s experience better and doubly so whenever it’s actually an easy fix. Other easily fixable examples I’ve seen are clearing entire forms when there’s an error with one field and not highlighting the field that’s giving the error.    I suspect the reason these aren’t fixed is a managerial problem. The application works and there’s no reason to go back when there are all sorts of new shiny things that can be built. No one wants to do a cost vs value analysis for these minor fixes so they stay the way they are. I suppose you need to either build things the right way immediately, fix it without letting anyone know, or resign to leaving it alone.    There’s a reason startups tend to have better products. They don’t go through analyses to determine whether to make minor changes, all it takes is for someone to decide that something needs to be fixed and the next deployment, probably within a few hours, will have it solved. Combined with the massive sense of ownership that comes with working at a startup, that’s a lot of improvements that would be done at a startup but not a larger company.",0,1,2012-07-28,1,startups,361,"The Startup Advantage - Details, Details, Details"
20,0,A simple script to capture of a screenshot of an HTML element that can exist anywhere on the page.,#code,"{% include setup %} I’ve worked on a variety of scraping projects that required spinning up a browser (via selenium) and having it browse a variety of pages unattended in order to capture some data. The two most recents ones [scraping my account data](https://github.com/dangoldin/turo-automation) from Turo and the [fantasy football stats](https://github.com/dangoldin/yahoo-ffl) from Yahoo. These were relatively straightforward since the browser was used purely to navigate from page to page with the actual data capture done by parsing the underlying HTML.  Recently I needed to expand this approach to get screenshots of specific HTML elements on page. Taking a generic screenshot was easy since selenium comes with a built in function but expanding this to handle elements that were out of view and needed to be cropped took a bit of research. I found two StackOverflow responses that made this simple: the [first](http://stackoverflow.com/questions/37882208/get-element-location-relative-to-viewport-with-selenium-python) explains how to scroll to a specific HTML element and the [other](http://stackoverflow.com/questions/15018372/how-to-take-partial-screenshot-with-selenium-webdriver-in-python) explains how to screenshot an element. Putting these together was extremely painless with the resulting code below. The only nuance was incorporating the scroll amount into the crop in order to get the math to work out.  {% highlight python %} OUT_DIR = 'out'  fn = str(uuid.uuid4())  filename = 'screenshot-' + fn + '.jpg' filepath = os.path.join(OUT_DIR, filename)  self.driver.execute_script(""return arguments[0].scrollIntoView();"", el) self.driver.execute_script(""window.scrollBy(0, -150);"") self.driver.save_screenshot(filepath)  scroll = self.driver.execute_script(""return window.scrollY;"") location = el.location size = el.size  if size['height'] == 0 or size['width'] == 0:     continue  im = Image.open(filepath)  left = location['x'] top = location['y'] - scroll right = location['x'] + size['width'] bottom = location['y'] + size['height'] - scroll  im = im.crop((left, top, right, bottom)) im.save(filepath.replace('.jpg', '-2.jpg')) {% endhighlight python %}",4,1,2016-12-13,3,"selenium, screenshots, HTML element screenshot",318,Automatically taking screenshots of HTML elements
23,0,I've often found that the AirBnBs I stay in have spotty wifi and end up fixing it by picking the appropriate channel.,#travel,"{% include setup %} Nearly every time I travel I use AirBnB but a fairly common problem is spotty wifi. I don’t know whether my expectations are higher than the typical guests but in a significant number of my stays the wifi has been spotty. Rather than complain or just accept it I’ve started doing my own simple wifi network maintenance to help improve the speed. The nice thing thing is that this is entirely doable - both because you can connect to the network but also because you have access to the physical router.  The critical piece is being able to access the router’s admin panel. This is usually hosted at the router’s IP address which, on a Mac, you can find by holding the Option button while clicking the wifi icon in the top toolbar. This gives you a router IP address which is usually 192.168.0.1 or 192.168.1.1. There’s going to be a login page but people rarely change the default username/password and these can be found either online via some Googling of “router model default admin password” or by looking at stickers on the router itself. As a last resort you can physically reset the router to factory settings but you need to make sure you know what you’re doing since you will be reverting all the settings and potentially causing significant issues.  Now that you have access to the router’s admin panel you can poke around and see if there are any settings or options that look useful. I usually try to disable all the bells and whistles to focus on actual wifi speed and latency but the biggest help is just picking the appropriate channel. If you live in an apartment building your wifi signals will overlap with those from other apartments. This means that if you’re using the same channel as everyone else there will be a lot of noise which will hurt your performance. The nice thing is that OS X (and I’m sure other OSes have their own equivalents) comes with a utility called “Wireless Diagnostics” which lets you scan all the wifi networks to determine the best channel to use - which is the one that has the least overlap with others. Once you run the scan and get the optimal channel you would just make that the default channel in the router’s wifi settings and you should be good to go. Modern routers come with a gamut of options and customizations but I’ve found that picking the optimal channel gets you most of the way there.",0,1,2018-12-21,2,"wifi, airbnb",446,FIxing spotty AirBnB wifi
22,0,I don't understand why Uber is pursuing self driving cars so aggressively. It only carries risks and hurts their compettive advantage.,"#meta,#society","{% include setup %} Self driving cars are inevitable and yet I’m surprised by how aggressive Uber is in contributing to the space. Uber is winning right now due to massive network effects. For most drivers and passengers Uber is the primary option and they only switch when Uber is either in surge if you’re a passenger or if you’re a driver when no passengers are available. Self driving cars eliminate half of the market. They won’t need to balance multiple apps on their phones and won’t need to go back and forth trying to find a passenger. It will all happen behind the scenes and do a much better job than any human would. They’d be as likely to work with Uber as any of their competitors. In fact, the entire protocol may evolve to be open with owners setting up their cars to start picking up and dropping off passengers when they’re not in use. The equivalent of how you can sell electricity back into the grid without having to do a ton of extra work. Imagine being able to own a car and just let it roam so it starts earning.  It’s unclear why Uber is driving this change - self driving pose a risk and diminish their competitive advantage. Maybe the outcome will eliminate individual car ownership and Uber wants to own a fleet of these cars. In that case pushing for this result makes sense but carries a world of risks - why wouldn’t car manufacturers both produce the car and have it part of a fleet? The other option is that they accept it’s not ideal but feel as if they have no choice since if others achieve it first they’ll be in an even worse position. Or maybe Uber does think they’ll own the market by the time self driving cars are a reality and at that point no one else will even bother to compete.",0,2,2016-05-28,2,"uber, self driving cars",322,Uber and self driving cars
23,0,Lately I had a thought of data that doesn't exist on any one machine but instead is constantly traveling across the world.,#meta,{% include setup %} A thought experiment that's been on my mind lately is this idea of ephemeral data. Imagine a computer sending a message to another computer and then immediately deleting it. This message then gets sent from computer to computer without actually getting saved down anywhere. It doesn’t have a permanent home and just hops from machine to machine. If that computer shuts down before the message is passed on then it’s lost forever. Only by being on a machine when it receives a message do you get to see it - otherwise it keeps going along on its infinite journey. I like to think of this as information that only exists in wires and is constantly crossing the world at the speed of light.,0,1,2017-08-07,3,"data, information, speed of light",127,Ephemeral data
18,0,I wrote a quick script to make it easy to generate fake data for SQL-like databases.,"#code,#devops","{% include setup %} A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and  hacked one together  in JavaScript. It’s currently a part of my js-tools  GitHub repo  and suggestions are welcome. One thing I definitely need to add is the ability to specify the range of possible values for each field rather than using a hardcoded distribution.",3,2,2014-05-29,4,"sql, fake data, generate sql data, javascript",162,Generate fake SQL data using JavaScript
16,0,Does it make sense to apply the EU model of member countries to US states?,#meta,"I've recently been thinking about whether the US can move to a EU like model with each state having control over it's own policies but sharing a  single market  and monetary union. In addition, competition is well regulated and a shared budget exists. In addition, it looks as if this shared budget is a little over 1% of the  Gross National Income  of the individual countries (1) - imagine a Federal tax rate of 1%.    Clearly, the states would then have to handle more of the lower level administration but that may be for the best. Each state is different and must be governed differently. States with a large agricultural focus should have different policies than states with a large technology focus. States with a highly religious population should have different policies than the more atheist states. Under such a system, some states may end up doing better than others in the short term but if it becomes obvious that certain policies work, the other states would have adapt in order to compete, thus improving the US as a whole.    I believe that such a system plays on the strengths of the federal government as well as the strengths of local governments. It will still be easy to travel from state to state, use the same currency, and not deal with trade barriers but each state will have it's own social and cultural policies that reflect its population.    It just seems that the federal government cannot pass laws that will be beneficial to all states at once and so there is some form of a standstill. Maybe this focus on a more state-centered model is the approach to take.    In the future, I would like to take a look at the economic growth of the individual countries before the creation of the EU as well as after the creation of the EU adjusted for the overall growth of the world markets. I have a feeling it would show that the creation of the EU encouraged the growth of the individual markets.    Notes: (1)  http://en.wikipedia.org/wiki/European_Union#Budget",3,1,2008-09-15,3,"politics, united states, european union",370,Should we apply the EU model to the US
19,0,I had to deal with some requests off of Google's CDN taking close to a minute to succeed.,#devops,"{% include setup %} I’m not sure whether this is a recent issue but earlier this week I started noticing that many HTTP requests to  Google's CDN  were taking close to a minute to complete. In particular, this blog would take almost a minute to render since it uses two fonts and an old version of jQuery both hosted by Google.            After some investigation it turned out that the issue seemed to only happen on Chrome Canary (43.0.2351.3 canary (64-bit)) and even occured when visiting the URL directly. Neither standard Chrome, Firefox, Firefox nightly, nor a simple curl requested had this issue - it seemed to be a purely Chrome Canary issue.  I didn’t spend a ton of time investigating the root cause since it seemed to be browser specific but ended up implementing two simple solutions to deal with the problem. One was self-hosting jQuery (via GitHub pages) and the other was loading the fonts asynchronously using Google’s JavaScript implementation. This allows the content to load without having to wait for the fonts or jQuery to be available. This will occasionally cause a bit of a flicker as the text gets redrawn with the newly loaded font but I prefer this to have my site not load for nearly a minute.",1,1,2015-04-12,2,"google cdn, ajax.googleapis.com",242,Dealing with an unresponsive Google CDN
21,0,Data science doesn't need to be complicated. Anyone who can write a script can contribute and we should encourage that.,"#datascience,#meta","{% include setup %} Data science has earned the reputation of being complicated and inaccessible to those without an advanced degree but it doesn't have to be this way. The goal of data science is simply to unlock insights and value from data. There's no need to make it more complicated than that. Of course, there are times where the data requires some domain knowledge or is just too big for someone without the necessary experience to work with but I believe that most places have enough low hanging fruit that anyone who can write a quick script can contribute and do data science.  This can be as simple as looking at a site's log files to figure out the most popular pages and how long they take to load in order to identify slow pages that can be sped up. Another quick task can be writing some queries to provide summary statistics across varying dimensions and visualizing them to see if any patterns emerge. A more advanced project can be going through a codebase and implementing a system to help track metrics in a way that makes future analysis easier. None of these require advanced quantitative knowledge and there's no reason that anyone should feel unqualified to dabble in data analysis. In my experience the most value has come from someone noticing something interesting and asking the right questions that led to a more thorough analysis. The more people that approach data with a curious mindset the more valuable a company's data becomes.  There's always the risk of discovering something  spurious  so it's important to validate discoveries but I'd rather have signals and noise than silence - especially if this encourages more people to become interested in data. At first, this can pose a problem for the people who need to deal with the noise but over time people will become more aware of what's valuable and can help identify areas of further analysis. This is the way to build a data driven culture - not by hiring a few data scientists.",1,2,2014-06-02,2,"data science, analysis",346,Approachable data science
10,1,"I share some more thoughts on conducting tech interviews""",#code,"{% include setup %} When conducting interviews, I've developed the following criteria for a good interview problem:    	 Avoid brain teasers - they tend to be hit/miss and some people don't really do well under this type of problem  	 Challenging - the answer should not be immediately obvious and the should require some creativity  	 Rare - similar to above, the problem should not be a common question in order to get  	 Flexible - the problem has multiple solutions and can be modified on the fly for different skill levels     I've found that the following problem satisfies the criteria and gives a pretty good sense of a developer's skill level.  The problem starts of as a simple scenario:  You need to write a program that will accept a list of words. After the words are entered, the user will enter words and your program will need to indicate whether the entered word was in the original list. How would you design this program and what data structures would you use?  The typical answer to this question is to either store the initial word list in an array, a tree, or a hash. If it's an array or a tree, we talk about the Big O of the solutions and compare that to just using a hash. At this point you can get a sense of whether the person you're interviewing understands basic data structures and knows the use cases of each one.  To dig deeper, I add a twist:  Now imagine you were transported back in time and it turns out your program uses too much memory and you can't keep track of every word. Do you have any alternative solutions?  The creative solutions start appearing here and you can get a pretty good sense of the problem solving skills. For example, a proposed solution at this point is to use word roots or repetitive letter combinations in a tree like structure to reduce the memory usage. We then talk about the algorithm that would need to be written and try to point out possible problems and see how they would be addressed.  And a final twist:  Let's say you still do not have enough memory and but you find out that you don't need to be correct all the time. Can you think of any solutions that can achieve this?  At this point, many people will try to come up with a heuristic or machine learning technique to try to identify words that resemble the words previously entered. We can then talk about both how to construct the algorithm as well as talk about the accuracy of the approach. It turns out that for these solutions it's difficult to quantify the trade off between error rate and space requirements.     At this point the concept of a Bloom Filter is brought up, either by me or by the person I'm talking to. If it's by me I go through the basic concepts (bit array, hash functions, probabilistic data structures) and can get a good sense of whether this is understood or I need to dig deeper. It's great when you can see the moment that someone ""gets"" the value of this data structure and knows immediately how to use it. At this point we discuss the trade off between the size of the bit array and the number of hash functions. If there's time, we'll work on deriving the relationship between the two as well as talk about where they can be used in the real world.  I wish I could remember how I came up with this problem - I think it stemmed from me encountering Bloom Filters for the first time as well as reading a few articles about spell checking and dictionaries.",0,1,2011-01-08,3,"interviewing, puzzles, brainteasers""",647,"Tech Interview Question"""
21,0,This is a comparison of taxi prices across various cities. I've included a few visualizations showing the magnitude of the differences,"#dataviz,#code,#R","{% include setup %} I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on  worldtaximeter.com  and combining that with the data from  taxiautofare.com  and  www.numbeo.com  in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute. Since each city’s fare came in local units I also had to convert to miles (sorry world) and US dollars (sorry again). Using R we generate the fares for the various combinations of distances and stoppage times and start diving into the data. As usual, the data and code are up on  GitHub  with contributions, corrections, and suggestions welcome. I’d also love to get the real rates for the cities so either do a pull request or let me know what they are in the comments and I’ll update the post.      City  Base  Inc Dist  Per Dist  Per Min  Dist Cvr  Fare Cvr  $ Base  $ per Mile  $ per Min  Ratio  $ per Hr      New York  2.50  0.00  2.50  0.50  1.00  1.00  2.50  2.50  0.50  5.00  30.00    Mumbai  19.00  1.50  12.35  0.50  1.61  0.02  0.32  0.33  0.01  39.77  0.50    London  2.20  2.00  1.70  0.52  1.61  1.64  3.61  4.49  0.85  5.31  50.71    Amsterdam  2.66  0.00  1.95  0.32  1.61  1.36  3.62  4.27  0.44  9.81  26.11    Tokyo  712.00  0.00  188.00  56.00  1.61  0.01  6.84  2.91  0.54  5.41  32.26    Aberdeen  2.40  0.90  1.10  0.37  1.61  1.64  3.94  2.90  0.61  4.79  36.41    Austin  2.54  0.20  1.30  0.67  1.61  1.00  2.54  2.09  0.67  3.12  40.20    Baltimore  1.80  0.15  1.36  0.44  1.61  1.00  1.80  2.19  0.44  4.98  26.40    Barcelona  2.05  0.00  0.98  0.38  1.61  1.36  2.79  2.15  0.52  4.15  31.01    Berlin  3.00  0.00  1.58  0.41  1.61  1.36  4.08  3.46  0.56  6.20  33.46    Boston  2.60  0.23  1.73  0.54  1.61  1.00  2.60  2.79  0.54  5.16  32.40    Chicago  2.25  0.18  1.11  0.37  1.61  1.00  2.25  1.79  0.37  4.83  22.20    Dublin  4.09  1.00  1.03  0.37  1.61  1.36  5.56  2.26  0.50  4.48  30.19    Edinburgh  3.00  0.52  1.20  0.36  1.61  1.64  4.92  3.17  0.59  5.41  35.13    Ibiza  3.25  0.00  0.98  0.35  1.61  1.36  4.42  2.15  0.48  4.51  28.56    Las Vegas  3.30  0.00  1.49  0.53  1.61  1.00  3.30  2.40  0.53  4.53  31.80    Los Angeles  2.85  0.18  1.67  0.50  1.61  1.00  2.85  2.69  0.50  5.38  30.00    Madrid  2.04  0.00  0.98  0.32  1.61  1.00  2.04  1.58  0.32  4.93  19.20    Malaga  1.42  0.00  0.84  0.34  1.61  1.36  1.93  1.84  0.46  3.98  27.74    Mallorca  3.00  0.00  0.80  0.29  1.61  1.36  4.08  1.75  0.39  4.44  23.66    Manchester  2.35  0.43  1.00  0.28  1.61  1.64  3.85  2.64  0.46  5.75  27.55    Melbourne  3.20  0.00  1.61  1.04  1.61  0.89  2.85  2.31  0.93  2.49  55.54    Montreal  3.45  0.00  1.70  0.63  1.61  0.93  3.21  2.55  0.59  4.34  35.15    New Delhi  40.00  0.00  15.00  1.67  1.61  0.02  0.67  0.40  0.03  14.46  1.67    Paris  2.20  0.00  1.14  0.75  1.61  1.36  2.99  2.50  1.02  2.45  61.20    Rome  2.80  0.00  1.52  0.44  1.61  1.36  3.81  3.33  0.60  5.56  35.90    San Diego  2.50  0.00  1.67  0.46  1.61  1.00  2.50  2.69  0.46  5.85  27.60    San Francisco  3.10  0.00  1.39  0.47  1.61  1.00  3.10  2.24  0.47  4.76  28.20    Seattle  2.50  0.16  1.55  0.52  1.61  1.00  2.50  2.50  0.52  4.80  31.20    Sydney  3.40  0.00  2.06  0.91  1.61  0.89  3.03  2.95  0.81  3.64  48.59    Toronto  4.25  0.14  1.74  0.53  1.61  0.93  3.95  2.61  0.49  5.29  29.57    Vancouver  3.20  1.00  1.85  0.50  1.61  0.93  2.98  2.77  0.47  5.96  27.90    Washington DC  3.00  0.00  0.93  0.26  1.61  1.00  3.00  1.50  0.26  5.76  15.60    Zurich  6.00  0.00  3.80  1.15  1.61  1.10  6.60  6.73  1.27  5.32  75.90    Beijing  13.00  3.00  2.30  0.30  1.61  0.17  2.21  0.63  0.05  12.34  3.06    Shanghai  14.00  0.00  2.40  0.50  1.61  0.17  2.38  0.66  0.09  7.73  5.10    Moscow  245.00  0.00  26.53  14.00  1.61  0.03  7.35  1.28  0.42  3.05  25.20    Bangkok  35.00  0.00  6.00  1.67  1.61  0.03  1.05  0.29  0.05  5.78  3.01    Buenos Aires  1.81  0.00  1.00  0.18  1.61  1.00  1.81  1.61  0.18  9.20  10.50    Cairo  2.50  0.00  1.25  0.28  1.61  0.14  0.35  0.28  0.04  7.19  2.35    Dhaka  250.00  0.00  35.00  4.17  1.61  0.01  3.25  0.73  0.05  13.51  3.25    Istanbul  2.80  0.00  1.73  0.33  1.61  0.46  1.29  1.28  0.15  8.44  9.11    Jakarta  6000.00  0.00  3550.00  500.00  1.61  0.00  0.49  0.47  0.04  11.43  2.46    Lagos  3.32  0.00  3.06  0.16  1.61  1.00  3.32  4.93  0.16  31.58  9.36    Manila  50.00  0.00  13.60  1.75  1.61  0.02  1.10  0.48  0.04  12.51  2.31    Rio de Janeiro  4.70  0.00  1.70  0.37  1.61  0.42  1.97  1.15  0.16  7.38  9.35    Seoul  2800.00  0.00  1050.00  206.00  1.61  0.00  2.63  1.59  0.19  8.21  11.62      Using this information we can run a few interesting analyses:                                                          USD per minute vs USD per mile.  The most obvious check is to see the most and least expensive cities by the two dimensions we have - distance and time. The results are expected - Asian and African cities tend to be the least expensive and European cities being the most expensive. Within Asia there's pretty significant variance with South and Southeast Asia being the cheapest but Seoul and Tokyo being more expensive. A city that stood out was Lagos - it has the one of the lowest per minute fares but one of the largest per mile fares. I don't know why this is the case but I suspect it has something to do with t sure why this is the case other than the roads being in poor condition and the price needing to take that into account.                                                                          Keep time fixed at 10 minutes but vary distance.  The idea here is to look at how quickly the prices increase by distance for different cities. This echoes the previous chart of Europe and Lagos having the highest per mile fares.                                                                          Keep distance fixed at 4 miles but vary time.  Similar to the previous plot but look at the way price will increase as a function of time. Not much new data here.                                                                          What does $10 get you?  Another way to look at expenses is to see the maximum time and distance $10 will get you in different cities. This is similar to looking at the inverse of the per minute and per mile prices.                                                                          What does $10 get you (zoomed)?  This zooms in the bottom left corner of the previous plot. Turns out that having $10 in an expensive city doesn't go very far.                                                                          Ratio of $ per mile vs $ per minute.  The goal was to see how many times a mile was more expensive than a minute for the different cities. The reason we see such high ratios is that the price of gas has a lower variance from city to city than the cost of labor - this leads to cities with low labor casts having significantly higher ratios.                                                                          Heatmap of fares as a function of time and distance.  I wanted this to be a bit more insightful in order to be able to compare all cities against each other across both dimensions but the extreme differences make it difficult to visualize. This highlights once more how expensive Zurich is compared to the other cities. The heatmaps below cluster the cities by the sum of price per mile and price per minute in order to visualize them along similar price scales.                                                                          Heatmap of the fares as a function of time and distance by city (1st quartile).                                                                           Heatmap of the fares as a function of time and distance by city (2ng quartile).                                                                           Heatmap of the fares as a function of time and distance by city (3rd quartile).                                                                           Heatmap of the fares as a function of time and distance by city (4th quartile).",15,3,2014-01-09,3,"taxi prices, visualization, analysis",1707,Taxi prices around the world
26,0,"Donating is a great way to pay our success forward and I wanted to share the organizations I donate to - Wikipedia, the ACLU, and the EFF",#meta,{% include setup %} It’s that time of the year when many organizations are ramping up their donation efforts and I wanted to share the organizations I donate money to. I feel incredibly lucky to be where I am and being able to donate to worthy causes is a great way to pay it forward. Everyone is passionate about something and donating to that cause is incredibly worthwhile and valuable.  [Wikipedia](https://www.wikipedia.org/). The need for education is critical to a functioning society and unfortunately this has been magnified recently by the explosion of fake news. Wikipedia is incredible at providing factual information and I find myself visiting it multiple times a day. It’s both education and entertainment since it’s just so easy to get lost in its labyrinth. Out of all the tools and services I pay for Wikipedia offers by far the highest return.   [American Civil Liberties Union](https://www.aclu.org/). Especially over the next few years our rights will be incredibly important and the work the ACLU is doing is a key part in making sure they exist in the future. Democracy is being challenge around the world and keeping it safe is something we need to do for future generations. Rather than losing our liberties one small piece at a time the ACLU needs the ability to engage in the small skirmishes and battles that strengthen our freedoms.   [Electronic Frontier Foundation](https://www.eff.org/). Being in the tech industry I like to think I understand the dangers of technology better than most and the EFF has both an increasingly important and increasingly difficult job ahead. Technology has eliminated friction across the globe and democratized a wealth of information but the flipside is that it makes it incredibly easy for governments and agencies to monitor our digital worlds. This will be a bigger and bigger issue as more and more of our lives are captured digitally and the EFF is the bulwark keeping them secure.,3,1,2016-12-09,6,"donation, wikipedia, american civil liberties union, electronic frontier foundation, aclu, eff",327,It's donation season
22,0,I wrote a simple script to get all members of a Slack channel and then send them each a privat message.,#code,"{% include setup %} One of my more recent “management automation” tricks was to write a simple script that gets all active members of a Slack channel and then sends them a direct message. I’ll often want to poll the entire team and ask them to fill out a survey or submit a questionnaire but the response rates tend to be poor. But if I send a message to people directly I end up with a much better response rate. It turns out that in my case I was able to get a greater than 100% improvement in response rate by using this approach. In a group channel there’s a lot going on so it’s likely that some people don’t see the message or decide they’ll do it later but inevitably forget. But by messaging them directly it sends a pretty strong signal that I care about the response and prompts people to just get it done. Despite the success I am hesitant to overuse it since it may lead to people ignoring these direct messages as well. As they say, with great power comes great responsibility. As usual, the code is up on [GitHub](https://github.com/dangoldin/automating-management/blob/master/spam_channel_members.py) and suggestions and pull requests are welcome.",1,1,2017-06-30,1,slack,206,Send private messages to all members of a Slack channel
22,0,AMP is meant to improve the performance of mobile sites but there's a big hole that allows one to bypass paywalls.,#product,"{% include setup %} A variety of publishers are adopting [Accelerated Mobile Pages](https://www.ampproject.org/) (AMP) to speed up the performance of their sites on mobile. In fact, I’m using AMP to power my entire blog on both desktop and mobile and it’s significantly faster than my old, heavyweight site. But I’m a small time blogger and to get real publishers on board AMP needs to support a variety of monetization options - including ads and subscriptions -  that are able to generate the same revenue they’re getting without AMP.  While browsing the Wall Street Journal I came across an AMP article page and saw that AMP had introduced a subscription paywall feature so I was curious to see how it worked. Looking through the source and network calls it’s powered by a small JavaScript snippet called amp-access-0.1.js. This controls whether the page throws a paywall overlay or the full article. It turns out that by messing with the network calls and blocking “amp-access” from loading it’s possible to get access to the full content.  Blocking ads is one thing but being able to bypass a subscription paywall is quite another. It’s already possible to get access to the paid Wall Street Journal articles by reaching them through Google but that doesn’t always work and the Wall Street Journal can stop that any time. Being able to bypass AMP’s subscription functionality, on the other hand, is something that AMP needs to address in order to get wider adoption - especially by the premier publishers.                                               Original WSJ AMP article with a paywall access blocker.                                                             The entire article is accessible if amp-access is blocked.",1,1,2017-04-16,6,"AMP, accelerated mobile pages, subscriptions, paywalls, advertising, wall street journal",328,AMP and subscription paywalls
23,0,The difference in taxi fares between New York City and Mumbai is huge and highlights the difference in the cost of labor.,#datascience,"{% include setup %} Something else that struck me during my trip to India was the difference in taxi fare between  New York City  and  Mumbai . I expected them to be different but the magnitude of the difference was shocking. In NYC, the base fare is $2.50 and increases 50 cents for each additional 1/5th of a mile or 60 seconds of not moving. In Mumbai, the rate starts at 19 rupees (~32 cents) and includes the first 1.5 km. After that it’s 12.35 rupees (21 cents) for each additional km and 30 rupees (50 cents) for an hour of not moving.      Distance (mi)  Wait Time (min)  Total NYC Fare ($)  Total Mumbai Fare ($)  Est NYC Gas Cost ($)  Est Mumbai Gas Cost ($)  Est NYC Driver Profit  Est Mumbai Driver Profit      1  1  5.50  0.39  0.18  0.24  97%  38%    1  2  6.00  0.44  0.18  0.24  97%  45%    1  5  7.50  0.59  0.18  0.24  98%  59%    2  1  8.00  0.72  0.35  0.48  96%  33%    2  2  8.50  0.77  0.35  0.48  96%  38%    2  5  10.00  0.92  0.35  0.48  97%  48%    5  1  15.50  1.71  0.88  1.20  94%  30%    5  5  17.50  1.91  0.88  1.20  95%  37%    5  10  20.00  2.16  0.88  1.20  96%  45%    5  20  25.00  2.66  0.88  1.20  97%  55%    10  5  30.00  3.57  1.75  2.40  94%  33%    10  10  32.50  3.82  1.75  2.40  95%  37%    10  20  37.50  4.32  1.75  2.40  95%  44%    10  30  42.50  4.82  1.75  2.40  96%  50%    20  10  57.50  7.14  3.50  4.80  94%  33%    20  20  62.50  7.64  3.50  4.80  94%  37%    20  30  67.50  8.14  3.50  4.80  95%  41%    50  0  127.50  16.58  8.75  12.00  93%  28%    100  0  252.50  33.15  17.50  24.00  93%  28%    1000  0  2502.50  331.40  175.00  240.00  93%  28%      The differences are crazy. A short ride will cost $5 in NYC but only 40 cents in Mumbai. Even if we look at the limit where we’re always moving and there’s no stopping, a NYC fare will cost 7.55 times   1   that of one in Mumbai. Given these differences, I was surprised to discover that gas is 40% more expensive   2   in Mumbai. If we assume an average car gets 20 miles a gallon, it works out that in NYC the profit to the driver is over 90% of the total fare whereas in Mumbai it’s closer to 30%. The fare pricing echoes this: standing still for an hour costs 50 cents in Mumbai but $30 in NYC. This is simplified since there are many other factors at play, ie the  NYC medallion system , but it’s still a massive difference in labor costs.  This reminds me of something I read about the pricing of soda in grocery stores. In the US, a 12 pack of Coke is only slightly more expensive than a 20 oz bottle whereas in countries with lower labor costs they’re much closer to the actual unit costs. The reason is the same - the cost of labor in US contributes the most to the cost of an item whereas in countries with lower labor costs it’s the item cost that’s the bulk of the final item price.   1  7.55 = 2.5/(1.61 &times; 12.35/60)  2  $3.50/gallon in NYC vs 78 Rupees/gallon ($4.80) in Mumbai",5,1,2013-12-29,4,"taxi pricing, taxi fares, NYC, Mumbai",485,Taxi pricing in NYC vs Mumbai
28,0,When traveling I've developed the habit of buying a history book about that area. It's a great way to add a new dimension and improves my trip.,#meta,"{% include setup %} While traveling I've developed the habit of visiting a local bookstore and buying a book that dives into the history of that area, sometimes it’s the city and other times the country. This gives me something relaxing to do in the evenings after a day spent running around but also adds another dimension to my trip. The history heightens my experience when exploring the city since I’m able to relate what I read to the real world - be it landmarks, sculptures, or the culture.  Last year during my trip to Paris I got “[How Paris Became Paris: The Invention of the Modern City](https://www.amazon.com/How-Paris-Became-Invention-Modern-ebook/dp/B00GC53AEA/ref=sr_1_4?s=books&ie=UTF8&qid=1483062652&sr=1-4&keywords=paris+city)” which covered the growth of Paris and how it introduced many features which we associate with a modern city: sidewalks, a public transit system, night time street lighting, and many others. Beyond being an engaging and immersive read it felt magical to read a chapter about a bridge before bed and then walking over that same bridge the next day with my new knowledge. This happened multiple times during the trip and each time the book improved the real world experience.  This year, I went to Portugal and bought ""[Conquerors: How Portugal Forged the First Global Empire](https://www.amazon.com/Conquerors-Portugal-Forged-Global-Empire-ebook/dp/B00UEL0HNK/ref=sr_1_1?s=books&ie=UTF8&qid=1483062923&sr=1-1&keywords=conquerors)"" which covers the early history of Portuguese colonization. This was a flashback to high school history but was much more engaging this time around. Seeing the street names, plazas, and sculptures named after the personalities (villains?) in the book made my exploration more real. I even went out of my way to check out an Afonso de Albuquerque statue in a small Lisbon park.  This is a habit that I encourage avid readers to adopt. It gets you to read something you never otherwise would have read and adds a new layer to a trip. So far I've only done it for international trips but will start doing the same for domestic and local ones as well. The world is so rich in history and it’s amazing to be able to experience and feel that connection.",2,1,2016-12-29,7,"traveling, reading, books, history, paris, portugal, lisbon",360,Traveling? Buy a local book
13,0,Gmail has a new feature that autocompletes messages and it's surprisingly accurate.,#product,{% include setup %} Ever since I’ve been managing I’ve spent an inordinate amount of time in my inbox. What’s both sad and amazing is how accurate the new Gmail autocompletions (officially called [Smart Compose](https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html)) have gotten. They’re incredibly accurate for common phrases and are likely leveraging very similar technology to what Google is using for their autocomplete search. It’s great for finishing off that last sentence and even acts as a real time phrase thesaurus that helps suggest alternate phrasing.  It does make you think about the information Google is collecting to make this happen. By using Gmail we’re already trusting Google with our sensitive information but this feels like a step beyond. The tech blog linked above addresses this concern by indicating that it’s only used for common phrases but it’s still sending the message contents to the autocomplete servers to get a prediction.      Gmail autocomplete request       Gmail autocomplete response   Google has numerous businesses beyond advertising and has a strong incentive to get privacy right but it’s still shocking how embedded Google is in our lives - personally and professionally.,1,1,2018-10-29,4,"google, gmail, autocomplete, privacy",222,Gmail's autocomplete
15,0,It turns out that sums aren't commutative when working with infinite series. Who knew?,#math,"{% include setup %} Maybe I never learned this or maybe I forgot but while reading [Prime Obsession](https://www.amazon.com/Prime-Obsession-Bernhard-Greatest-Mathematics/dp/0452285259) I came across a concept that blew my mind. We all learn about infinite series and how some converge (think 1 + ½ + ¼ + .. = 2) and some diverge (1 + ½ + ⅓ + ¼ + ..) but it turns out that not all convergent series are the same. The same numbers, added in a different order, can lead to a different resulting sum. These series are called [conditionally convergent](http://mathworld.wolfram.com/ConditionalConvergence.html). This is incredible since I always assumed that addition was commutative but it turns out even fundamental ideas are violated when dealing with infinite sums. I still can’t wrap my mind around how this makes any sense but the math doesn’t lie. The example below is from the book but I’d love to see others so I can continue wrapping my head around it. I’ve been out of school for a while now but this discovery brings me back.                              The alternating harmonic series.                                       Move the terms around so we have the 1/n and 1/(2n) terms next to one another, but keep those where n is a multiple of 4 alone.                                       Group these 1/n and 1/(2n) pairs together.                                       Simplify these 1/n and 1/(2n) pairs.                                       Factor out 1/2 from the series and we have 1/2 of the original series. The sum of the alternating harmonic series is ln(2) but by changing the order around we can have it equal to ln(2)/2. That's amazing.",2,1,2017-02-19,5,"convergent series, infinite series, math, analysis, commutative",359,Math is incredible
29,0,"In a 1980 talk,Steve Jobs talks about the need to improve hardware in order make software more accessible and usable. Usability is what's driven technology, not core functionality.",#meta,"{% include setup %}     Earlier this morning I watched a Steve Jobs talk from 1980 where he discusses Apple and the relationship between hardware and software. An interesting piece comes at the 12:30 mark where he addresses the question “Right now software is powerful enough, what impact will improvements in hardware have on software?” His answer is great: “[We will] start chewing up power specifically to help that one on one interaction go smoothly and specifically not to help the calculation...  start applying that power to remove that barrier”  Sure the response is very Jobsian but the underlying point is significant. It’s only a tiny bit about what the software actually does; the majority is realizing that people will actually be using the software to solve problems and building tools for that experience. I remember starting with DOS on the family computer and being blown away when I first used Norton Commander. Similarly to when I saw Windows for the first time and saw my first smartphone.  Most hardware improvements over the past 30 years led to improvements in usability, not functionality. Processors in 2014 are 100,000 times more powerful than those in the early 1980s and a majority of the improvement went into user experience - better UIs packed into smaller devices. Without usability improvements computers wouldn’t be nearly as ubiquitous as they are now and would primarily stay a hobby for engineers. Each usability improvement brings aboard a whole new set of people. You can make the case that the same thing occurs with programming languages - very few people were writing assembly code at its peak compared to C code, and fewer people were writing C code at its peak than JavaScript.  Usability improvements are still happening but they’re taking the form of cloud and background services - akin to the way Google Now provides contextual information and the way Siri handles voice recognition. As sophisticated as they are, they will only get better as hardware improves.",0,1,2014-09-21,3,"usability, functionality,",337,"Software is good enough, why improve the hardware?"
27,0,I'm currently using Google Analytics for my blog but am going to also start using MixPanel and a bunch of meta data to track additional info.,#blog,"{% include setup %} Every time I launch a new website one of the first things I do is add  Google Analytics  to start gathering data. This blog was no different but I’ve recently been wondering whether Google Analytics is the right way to measure a blog. It’s great for tracking the total number of visitors, where they’re coming from, and how long they’re staying but I wish there was something that was optimized for blogs rather than something that was designed as a general solution.  My ideal blog analytics tool would help me understand both how readers are finding my content as well as how they’re engaging with it. It would also automatically segment my blog’s readers so I’d be able to quickly tell the different types of readers I have and what each of the groups is interested in seeing. Many of my posts end up being shared but I only discover that when I look at the referrals in Google Analytics. I’d also love to get a notification whenever one of my posts is shared on another site or social networks so I can participate in the discussion as it’s happening rather than days later.  I’m not sure about the available tools but I have some experience with  MixPanel  and am going to see if I can jerry rig it to do what I want. It’s great for doing simple funnel and segmentation analysis but it should also be flexible enough to do a ton more stuff. One thing I’ve been thinking about is generating additional meta data for each of my posts (topics, number of words, number of images) and then feeding that into MixPanel to see what impact they have. I’ll share both the source code and results once I have it up and running.",2,1,2013-11-26,4,"blog, analytics, google analytics, mixpanel",308,Blog analytics (blogolytics?)
26,0,When running raw SQL make sure to use table aliases in your queries even when they're not required. This insures you against future breaking changes.,#sql,"{% include setup %} One of the best habits to develop when working with SQL is to always refer to fields through an alias. Numerous times I decided to just take a shortcut and ended up regretting it later. Even if you’ve tested your query to make sure it works there’s no guarantee that a future change to a table schema won’t break it.  Let’s say you have the following two tables - with items.category_id corresponding to categories.id  {% highlight sql %} create table items (     id int,     name varchar(20),     category_id int,     owner_id int );  create table categories (     id int,     code varchar(4) ); {% endhighlight %}  It’s straightforward to join the two tables to get some basic info:  {% highlight sql %} select i.id as item_id, name, category_id, code from items i join categories c on i.category_id = c.id; {% endhighlight %}  Let’s say we test the code and deploy to production. It works perfectly until someone adds a “name” column to the categories table. All of a sudden our query stops working with a helpful “Column 'name' in field list is ambiguous” error. The reason is that the query doesn’t specify which source table for the name column. The solution is to simply prepend the items table alias to the name field and we’re back to a functional query.  {% highlight sql %} select i.id as item_id, i.name, category_id, code from items i join categories c on i.category_id = c.id; {% endhighlight %}  This issue is tough to check against since it requires searching your entire codebase every time you need to alter a table. A better approach is to always specify the schema and avoid the issue altogether. Especially in a quickly growing engineering team where multiple people are working on the same code base it’s very easy to run into these sorts of issues that may only get discovered in production. Although most ORM frameworks abstract this away it’s sometimes necessary to dive down into raw SQL and this is one of those small best practices that is a tiny bit of additional effort to significantly reduce a future risk. Avoid learning this lesson the hard way.",0,1,2015-06-27,3,"sql, queries, best practices",370,Ambiguous SQL queries
33,0,Advances in technology have reduced friction and made mass surveillance the norm. We need to reintroduce friction in order to protect our privacy and force governments to prioritize their tracking and monitoring.,#society,"{% include setup %} One of the best arguments I’ve heard against mass surveillance is that the marginal cost has dropped to nearly zero which warps the system. Since so much of our world is digital it costs the government nothing extra to collect each additional data point. Given these incentives it’s no surprise that the government was able to get the major companies to provide a dedicated feed of the data they were collecting - modern technology has enabled both the collection and analysis of massive amounts of data.  This infrastructure is something we’ve never had before. In the past surveillance carried a sizable cost - beyond the warrant one would need to either install wiretaps, manually intercept mail, have people followed, and generally hire people to do both the data collection as well as the analysis. These constraints necessitated making tradeoffs and prioritized those that carried the largest risk.  It’s impossible to undo the technological advances and we wouldn’t want to. At the same time we need to do more to introduce friction back to surveillance. The goal isn’t to achieve 100% privacy but to make it costly enough that governments need to think about who and what they’re tracking. The obvious way is to start using end to end encryption - I’m sure isolated cases can be cracked but cracking it at scale would be a monumental task.",0,1,2016-10-09,2,"mass surveillance, security",229,Friction and mass surveillance
21,0,I got a free trial of LinkedIn business plus and had a fun time figuring out how to cancel it.,#product,"{% include setup %} It’s just too easy to rant against LinkedIn but I can’t help it. They recently offered me a free month of business plus so I took them up on it. Little did I know (although I should have expected it) that canceling would be a maze that I still may not have escaped.      The cancel screen hides the downgrade to free option and automatically chooses a paid “recommended account” with a bright clickable “Downgrade Account” button. And then, when you actually do manage to downgrade, it’s not clear from the account settings page that you downgraded since it still displays as the premium account option. Maybe when my free month is up it will downgrade or maybe I’ll get charged - how am I supposed to know? I do see a note that says canceled and I suspect I’m in the clear but there’s no way to actually confirm other than contacting support.      It’s one thing to optimize your funnels to get more conversions and revenue is one thing but tricking your users to subscribe is entirely different. If your product relies on this sort of “optimization” you really should think of another business model.",0,1,2014-08-18,4,"linkedin, optimization, user experience, ux",226,Trick your users into staying
28,0,MaxMind made a decision in 2002 that has led to some unfortunate unintended consequences. It's important to think about the societal impact of the code we write.,"#code,#society","{% include setup %} Earlier today I read an [article about MaxMind](http://fusion.net/story/287592/internet-mapping-glitch-kansas-farm/), a company that offers an IP address to geographic location mapping service, making a seemingly minor decision in 2002 that that led to unintended consequences that have been going on since then. The article goes into detail about the decision and the effect but the main idea is that it’s not a prefect system and they needed a way to approximate some IP addresses to particular locations. Lo and behold these locations are now seeing tons of harassment from law enforcement and various strangers online.  This is a perfect example of how a quick fix to a seemingly simple problem can lead to a world of problems that can impact others without you even knowing. I can imagine myself running into that problem and making the same decision. It’s unlikely I would have thought about the people that may have lived at those coordinates or that people would actually be using this information to track people down.  There’s a lesson here for everyone who’s writing software: at the end of the day all the code we write will have some effect on people and we need to be mindful of that. We’re not going to stop making mistakes but we should take the time to consider the impact of every line of code we write.",1,2,2016-04-10,3,"buggy code, maxmind, geolocation",233,Unintended consequences
15,1,It seems it's a conflict of interest to be giving your employees free Adwords.,#product,"According to this  blog post   Google  gives their emplo﻿yees $1 a day to advertise on Google. The intent is to give employees the perspective of an Adwords user in order to improve the product. In addition to giving employees exposure to Adwords, this also has potential to increase the competition in  Adword  auctions by causing bids to increase and leading to more revenue  for Google. Given that Google has more than 10,000 employees worldwide this can have an effect on smaller advertisers.     Other people have written about Google advertising for their own products and giving employees free money can have a similar, although smaller, effect.",3,1,2010-08-25,3,"google, adwords, employees",148,Google Giving Employees Free Adwords?
28,0,I updated the Citibike directions app to be more general and provide directions from any NYC address to another rather than just Citibike station to Citibike station.,#code,"{% include setup %} To coincide with the launch of Citibike, I wrote a  simple web app  that provided cycling directions from one Citibike station to another. The biggest piece of feedback I received was that people care about getting from place to place rather than from one Citibike station to another. Based on this feedback, I  updated the app  to provide directions from any New York City address to another by breaking every trip down into three steps: the first is to walk to the nearest Citibike station, the second is to bike from one station to another, and the last is to walk to the destination. A limitation I ran into is that Google’s  Direction Service  doesn’t support different transit methods for multiple waypoints. This, combined with my desire to get it out there, is why the design’s not as good as it should be. I’ll see if I can improve it over the next few weeks. People have also been telling me this needs to be on mobile so I’m going to use this as an excuse to jump into mobile development. I’m excited.",3,1,2013-06-04,4,"citibikenyc, citibike, bike sharing, google maps api",204,Citibike Directions: Second Attempt
24,0,The US Congress recently passed an act to make government data available and accessible. There's a lot left but it's a great start.,#data,"{% include setup %} The US Congress recently [passed](https://e-pluribusunum.org/2018/12/21/congress-made-open-government-data-the-default-in-the-united-states/) [HR-4174](https://www.govtrack.us/congress/bills/115/hr4174) (The Open, Public, Electronic, and Necessary Government Data Act) which is intended to make all public government data available and accessible. Over the years I’ve done my fair share of poking around various government datasets - both public and private - and while the data was generally available it was rarely accessible. More often than not the data would be available via a scanned PDF which required some heavy OCR work to extract anything useful or the slightly easier PDF parsing code. Even when the data was in CSV files I often ran into formatting issues or inconsistency between the column documentation and the data contents themselves. The most important available datasets will always have people willing to go through the grunt work of cleaning them up but it’s the fringe datasets that end up having too much friction for researchers and developers to dig into them. I’m glad the government is moving to make the data accessible as well since it is the strongest way to make it actionable.  It’s definitely not going to perfect to start as it’s incredibly difficult to get consistent and clean data. Imagine a survey being done every decade. An obvious way to structure it is to have a set of files for each decade containing a description of the available fields and then a file containing the data itself. You can have each survey’s files being internally consistent but there are no guarantees that they will be consistent across surveys. What if some fields or questions were renamed, added, or removed? That makes it more difficult to compare the results over time. To maintain consistency across years all of the surveys would need to be a part of the same data set and use consistent values and formatting in order to make it dead simple to compare one year to another.  In an ideal world the datasets would actually come in prepackaged as database dumps. Then you’d be able to just load into your database with the appropriate database definitions, foreign keys, and field constraints. I can’t wait till the day all government data is in [third normal form](https://en.wikipedia.org/wiki/Third_normal_form).",3,1,2018-12-24,3,"open data, government, databases",382,"Open, Public, Electronic, and Necessary Government Data Act"
33,0,It's easy to get bogged down and focus on implementation but a nicer approach is to think about the interfaces. This leads to a  hgiher quality codebase that's easier sustain and adapt.,#code,{% include setup %} An idea I’ve been preaching over the past few days is to start thinking in terms of interfaces when thinking about writing code rather than the actual implementation. It’s a higher level of abstraction that leads to a higher quality and more scalable product. Rather than focusing on the details it’s better to think about the components and how they’ll interact with another - this also makes it easy to put in a crappy implementation for now while making it easy to modify and rewrite in the future. As engineers there’s a strong desire to obsess over the perfect code which can lead to a significant amount of refactors and rewrites without translating into actual business value. Thinking in terms of interfaces and components forces you to get the design and architecture right and leaving the implementation details for later. A side benefit for me has been being able to take pride in the design and flow and not worry about the code itself - allowing me to write code at a much faster place and sprinkle a series of todos for the parts of the code that I know need improving.,0,1,2015-12-02,4,"coding, software engineering, development, engineering management",195,"Think interfaces, not implementation"
27,0,Readmill announced that they were shutting down and it got me thinking about third party apps and what it would look like to host them locally.,#product,"{% include setup %} It never bothered me when apps were acquired and shut down but the  Readmill news  hit me hard. It was one of the truly “free” ebook readers and never got in my way. It fit my behavior perfectly - I would download my books from wherever, drag them into the Readmill web app, and have them permanently accessible on my iPad after a quick sync.  My first reaction was wishing that it would be open sourced but that got me thinking about third party services. Numerous people have been saying how dangerous it is to rely on third party services but until Readmill it never really mattered to me. Sure, in the abstract it’s better to have everything hosted on your own but in reality it’s impossible to get to the same level of quality and experience for everything we use. We’re constantly balancing tradeoffs and we’re biased to favor the short term factors, such as ease of use and simplicity of set up, rather than long term ones, such as privacy, control, and data ownership.  I did some research on self hosting and came across  Sandstorm  - it’s pitched as a “personal cloud platform” and seems to be the solution to this reliance on third party apps. The idea behind it is that you have your own server and can download and install various cloud apps that will then have access to whatever data you give them. I’m eager to try this out. In the case of a Readmill replacement - I’d love to be able to host something on my own server to act as the backend and then download an iPad app that can connect to it. Both the iPad app and server can still be updated as new versions are rolled out but there’s no risk of the apps being shut down.  The business model would resemble Wordpress. The technology itself would be open source but if someone doesn’t want to run their own server they can pay to have their apps hosted somewhere else. There’s also room for a marketplace of premium or specialty apps that can be sold similar to the way themes and plugins are sold for Wordpress. People are already buying apps on the various app stores - it’s not a big leap to imagine people purchasing apps for their personal servers.",2,1,2014-04-07,3,"cloud, hosting, personal cloud",397,Self hosted services
38,0,There's some weird stuff going on in SEO land. Someone is ripping off my mom's site (along with a ton more) and monetizing them via AdSense. Just trying to do some investigation to see what's going on.,#product,"{% include setup %} My mom owns a small local business in suburban NJ,  The Do Re Mi School , that’s akin to an after-school program where music, dance, art, language and math is taught. Being surrounded by a family of engineers, we’ve been helping her on the tech side and my brother created the web site she’s been using it for the past couple of years. It’s based on Drupal and allows her to make changes without having to dive into the tech details. This approach has been working well she’s recently started using YouTube, Facebook, and Twitter to help with her marketing and social efforts.  Earlier this week, she sent an email saying there was a new website, bestnewjerseyartsschool.com, that was completely ripping off her site. They claim to be Do Re Mi and have copied various parts of the content, including paragraphs of text and various images. They’ve even created a YouTube video, www.youtube.com/watch?v=lvE6tBl8xU4 that's embedded on the site’s home page.  Looking at the whois and dns info doesn’t reveal much since they’re using a privacy protection service. All I know is that the domain was registered in April using WildWestDomains, is protected by DomainsByProxy, and that the name servers are under  hbuse.com  which claims to be ""Hosting by unbelievably sweet elepehants"" and yet doesn’t contain any real content and is not in Google's index. The interesting thing is that looking at the whois info for hbuse.com indicates it was registered by someone with a PO Box in Nevada with an email address that also seems to be anonymyzed. I found a site,  yougetsignal , that allowed me to search for other sites that were all hosted on “hbuse.com” and came up with a list of ~270 sites that all look to be site rip offs. They all seem to follow a pattern of having the location, the service, and possibly an adjective (best being the most common). Below's a screenshot of some of the sites that I found sharing the host.      I don’t know what the motivation behind these. They are all running adsense so I suspect part of it just a way to generate easy ad revenue but the more cynical part of me thinks it’s a way to blackmail the existing sites into buying these fake domains to avoid SEO penalties.  It’s terrible that honest business owners have to deal with these things. They don’t have the background to know what to do and many are not even aware that their brands are being manipulated, damaged, and monetized. There’s also an SEO risk that these rip-off sites will start dominating the search results and hurting business even more. And since these sites are using AdSense, Google is able to generate more revenue.  I’d love to know what I should do next but my current thinking is that I should send a DMCA takedown notice for my mom’s site and report this entire list of rip-off sites to Google and hope that they stop AdSense from running on the sites and remove them from search results.  I worry that as it becomes easier and easier to generate written content using software we’ll see more and more of these scenarios where it’s going to become increasingly difficult to find the source of the original content and real site owners are hurt.  Update: So the way my mom discovered this other site is because she got a notice from Getty images that she was using one of their images without licensing it. When she asked which image they sent her a link to that other site that has her contact info on it. I guess that answers the question as to why that other site still uses the actual contact information.",3,1,2013-10-20,4,"seo, adsense, blackhat, blackmail",667,What the SEO?
29,0,I found a bunch of fliers from a computer show that I attended in the mid 1990s. It's crazy to see how much better computers are these days.,#meta,"{% include setup %} While doing some spring cleaning I discovered a bunch of fliers from various computer shows I attended in the mid 90s. Bsed on the Windows 95 and Windows NT promotions I suspect this must have been in 1995 or 1996. What’s striking is how much better our computers are. It’s one thing to be abstractly aware of Moore’s Law but shocking to actually see it. The top of the line model in 1995 was $2,500 and came with a 4 GB hard drive, 64 MB of RAM, a 200 MHz processor, and a 33.6 kbps modem. Adjusting for inflation, this is equivalent to $3,700 in 2015 dollars. With that budget you can a top of the line computer with an order of magnitude more of everything and still have enough leftover for a smartphone which is also an order of magnitude more powerful than a computer from the mid 1990s.",0,1,2015-05-06,4,"computers, windows 95, windows nt, tech history",462,Computer show fliers from the mid 1990s
26,0,To write great code engineers need to embrace end to end ownership and not think their work is done when they open a pull request.,#management,{% include setup %} Great engineers assume end to end ownership of their products. Rather than focusing on one feature at a time they understand how it fits in with the rest of the product and think about the impact it will have on users and the business. This leads to code that scales with the product while being able to be maintained and developed by a small team. But you can only have this with everyone embracing full ownership over a product.  This idea can be expressed via an ownership hierarchy. The idea is that all engineers are responsible for writing code but the best ones want their code in products that’s loved by the end users. By moving up this hierarchy you develop a larger sense of ownership than someone who just wants to knock out some tasks.  - I opened a pull request: This is the start for every engineer. We all write code and some may consider it done when they open a pull request - leaving the rest up to someone else. - My code’s merged into master: The next level is a tiny bit beyond - in this case it’s not just that the code was written but that it has also been merged into the main branch. - My code’s deployed to production: At this point we’re at least aware that the code isn’t the end goal and that we want to make sure it’s out in the real world. - My code is being used in production: We’re finally at the point where we care that our code is actually being used. Code that’s deployed but unused doesn’t matter and we strive to write code that’s actually used. - Users love my code: The peak is building products that are loved by users. This is what drives great products and should be the goal for every bit of code that’s written and deployed.,0,1,2016-01-23,3,"software engineering, product ownership, engineering management",315,The ownership hierarchy
24,0,Credit cards charge some pretty high fees to merchants and it seems there's an opportunity to make these much more variable and intelligent.,"#finance,#product","{% include setup %} While reading today’s [Stratechery update](https://stratechery.com/2017/patreon-reverses-itself-the-patreon-backstory-and-the-reality-of-fees-patreons-mistake/) on Patreon changing their pricing model I came across the following passage:  > Of course this wasn’t great for creators: payments made for a creation released on November 2 wouldn’t land in creators’ bank accounts until some time after December 1; the reasoning, though, was clear — credit card fees. Here I can certainly bring personal experience to bear: credit card fees are really expensive (they are by far Stratechery’s largest expense)! While the exact amount varies by network (that is, American Express is the most expensive), most credit card fees are ~$0.30/charge plus anywhere from 1.5% to 3.5% of the total amount charged.  > That $0.30 is more important than it might seem; to use Stratechery as an example, I charge $10/month or $100/year — that suggests that I earn $20 less from annual subscribers than I do from monthly ones. However, because I have to incur that fee 12 times a year for monthly subscribers, but only once for annual subscribers, the difference is actually only about $16 (by the way, monthly subscribers can easily upgrade to annual accounts; I push a seemingly lower revenue option because it turns out annual subscribers have significantly higher lifetime value — a higher up-front commitment ends up being a longer commitment). The importance of that fee looms even larger for Patreon’s “charge-per-creation” creators, who often charged only a dollar or two for their creations: were Patreon to charge credit cards immediately, over 30% of payments would go towards fees; however, by bundling payments over the entire month fees are much more manageable.  Clearly credit card issuers and banks need to make some money on their credit cards but these rates seem a bit aggressive, I don’t know the details but I’m sure a chunk of that is to cover cases of fraud. That makes sense but it seems there’s an opportunity to charge lower and lower rates for each recurring subscription charge. The intuition is that the likelihood of a person not being able to cover the subscription charge is much higher the first time they get billed rather than each subsequent one since they’ve built a strong payment history. Beyond this, it does feel as if there’s something a bit more intelligent that can be done around these merchant rates. They were instituted decades ago before the growth of modern data analysis methods that can be run for each individual and had to be set for the whole population. Imagine how optimized these rates can be now given modern tools. This is a huge opportunity and I’m surprised there haven’t been any companies trying to tackle it - maybe it’s due to the complexity of the space or network effects but it just feels as if there’s something here.",1,2,2017-12-14,4,"credit cards, merchange rates, fees, finance",488,More intelligent credit card merchant rates
12,0,No matter the type of code we're writing there's always depth.,#code,"{% include setup %} Lately I’ve found myself thinking more deeply about the code I’m writing. No matter how small the task or script I’ll think through the implications of my approach and whether I should be doing anything differently. This doesn’t mean I’ll always pick the more correct and flexible approach and more often than not I’ll choose the quick and dirty one to save time but the thought process itself is valuable since it gets me in the habit of questioning and constantly improving my code. The following is an example that illustrates this approach.  Years ago I wrote a [basic app](https://yahnr.dangoldin.com/) that shows the most popular Hacker News stories over a 24 hour period. It crawls the Hacker News homepage every 15 minutes and parses the DOM in order to track a story’s total points along with a few additional stats. It’s an extremely simple script that’s been running unchanged for multiple years. And yet a few days ago while checking up on it I discovered that there was a minor bug - there was a small change to the HTML structure which caused the number of comments to no longer be parsed.  The screenshot below shows a typical Hacker News entry - the top row contains the title and the domain and the bottom row has the points, the submitter, and the comments. Looking at the source code we can see that the number of comments is part of the last &lt;a&gt; entry in the bottom row.          When I wrote the code years ago it was the second entry so all my code did was look at the second &lt;a&gt; element and extract the number. At some point over the past few years the flag and hide options were added which caused my logic to fail. The fix was to look at the new element containing the comments but there are multiple ways to handle it. The simplest is to just realize it’s the 4th element and go off of that - but once again if the structure changes we will run into the same issue we just did. Another approach is look at the last element in which case we’ll be good as long as the comments element is always the last one. Yet another approach is to go through each of the cells and search for the “comment” string. If we find it then we use that cell, otherwise we move on until we do. This will work no matter where the comments section since we’re going through each one. With significant effort, one can even extend this into a learning system that is able to infer where the various bits of content are in a wide range of possible structures. There’s no right answer and it’s a series of tradeoffs but it illustrates how there’s significant depth and choice in the simplest of coding problems. Hundreds of these seemingly minor choices occur every time we write a script and thousands when writing a large project. Getting into the habit of questioning our choices is how we end up writing wonderful code as a habit.",1,1,2017-07-04,5,"programming, software engineering, hacker news, scraping, html parsing",545,Thoughtful code
30,0,It turns out my blog is not as hidden away as it feels since there are quite a few links to it from some much larger sites and publishers.,#meta,{% include setup %} I enjoy [Hacker Noon](https://hackernoon.com) and often find myself coming across an interesting article on the site. Yesterday I got a pleasant surprise from an old coworker who sent me an [article](https://hackernoon.com/how-to-create-a-slack-bot-that-messages-all-members-of-a-workspace-in-8-minutes-32a5b52838be) about building a Slack bot that mentioned of one of my [blog posts](http://dangoldin.com/2017/06/30/send-private-messages-to-all-members-of-a-slack-channel/). A few months earlier the author and I had a conversation in the comment thread and I’m glad she found the conversation useful enough to write a much more thorough post.  I don’t write write for publicity and genuinely enjoy the process of clarifying my thoughts through writing so it’s definitely rewarding to see a link back to my blog. It feels good knowing that my blog is interesting enough to warrant a link and it’s definitely appreciated.  Since I only discovered this backlink accidentally I got curious about how many others I missed over the years and decided to dig in. I found a free [backlink checker](https://lxrmarketplace.com/seo-inbound-link-checker-tool.html) and after searching for my domain name discovered that there are actually quite a few linkbacks. Some are list aggregators - such as [top books](http://www.largeheartedboy.com/blog/archive/2016/11/online_best_of_73.html) or [quantified self visualizations](http://quantifiedself.com/2015/01/represent-year-numbers/) while others are more interesting - there are quite a few links to a 5 year old [blog post](http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/) about the lack of a dial tone in cell phones as well as a link from [CityLab](https://www.citylab.com/transportation/2017/06/in-new-york-city-bikeshare-is-faster-than-cabs-when-it-matters-most/530469/) for a post about Citi Bike trip planning.  It turns out some of my posts and projects actually do make their way past my blog. It’s not much but it’s definitely cool.,8,1,2018-11-25,4,"articles, blog posts, backlinks, content",312,Exploring my backlinks
15,0,If you want to laugh read the Google Voice transcriptions of foreign language voicemails.,#meta,"{% include setup %} A fun post today. I went through Google Voice’s attempts at transcribing my grandmother’s Russian voicemails into English. As expected they don’t make any sense but they do provide some comedic relief. and as expected they make no sense but there’s some gold . Google has one of, if not the, best data science teams around and it shouldn’t be too difficult to train Google Voice to detect the language before providing the transcripts. I [wrote](http://dangoldin.com/2016/01/17/poor-neglected-google-voice/ ) about this last January and it’s somewhat shocking that they did a redesign of Google Voice and yet didn’t address the actual functionality. But hey, at least we get to laugh.  > Monica Travis to be in Justin King bed and said Doorman YouTube free movies to be I hope sister row. It's big social hello. Mortgage guy, this is Janice knock knock who's there?  > Garden Workshop is Stevia. Mr. Gage, just way more. the most good woman in comma Kim Hurst media that woman workshops  > hello period Daniel Costa Papa Toyota Ohh meet up with you the phone open a test in Corpus loan of Markham. Phineas and wish you a receipt for that. Gollum Commit What the door it's okay?  > Glen Burnie, I was wondering if I go screw the smoker.  > Betty who travis to be there to pick up s\*\*\* up? It's official love it's working and not feeling India. It's reema so logical Cochran Thomas. It's about at you as soon as it would take a little as I'm in an orchard in Glasgow, Kentucky Carquest in the very bored on the way, then you come over to f*** management. Workshop as smoothly Mr. Otero. Did they see me at the time of the month is when it's in your mouth when you can the number style?  > I'm doing that and onion crisps simple the showroom races you a quote your motor bike. It does wine is the demo step yeah your mother week anyway to me with this. What is it took me? So long some more when you want us to go in and wasn't able to reach anyone with anyone this line is Cecilia was not with your Papa was returning a step of the way, it's vince carter. So this will choose mostly was going give me a personal today.  > Nautica think I get to Milford area that you didn't rush for booking with Shelby. It's been delivered so now we're that we're headed to bed afterwards. We won't you would thank you? Good. I miss you, so she was I was on my trucker that one listen scott with them, but they're one in the same workshop. That was okay.  > Burning cos overview to give me a ring if you're not thomas to your order it is noon or so, so sorry. I bring an umbrella. What you're too busy doing your mom I was just sooner line. No, other way. I moved ohh cook with your background. Which is 100 good.  > Meagan good onion, Google Chavez today is there will be a resource constrain your head ocean subaru take a cab which is 322 and the year to be a driver of the system. It today, it's way over now. water stick with my special  > Yoga Workshop disturb you will be supported or bring you dinner with her phone is dying. It's a big is Rose. Rose my juice. 30th september with that if she's not approach the Smothers press tiny good Shabbos issue that.  > Monica just now getting to see when you might with this weight needs. So f****** in Delaware, so f****** strain your God with you for spring you.  > Tinker what's up? It's Rebecca roaches to work. Cuz my music this morning.  > burning Cargo chops Timber your new merchant, or do not wish him too. So that was in the store, and you have a short deborah. I'm with West moses press 1 now.",0,1,2017-11-30,2,"google voice, transcriptions",674,Google Voice transcription fails
8,0,Doing some data visualizations on my Twitter archive,"#dataviz,#datascience,#code","{% include setup %} I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with  matplotlib . The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one ""gotcha"" was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I'm more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I'd like to explore how my usage of Twitter has evolved and also get to play around with the  NLTK library .  I've committed by ugly code to github if anyone wants to play around with it:  https://github.com/dangoldin/twitter-archive-analysis . I know the code is ugly. I'll clean it up one of these days.                              Apparently, I like to tweet evenings and nights.                                       You can see I like to take my Fridays and Saturdays easy. Since I also tend to tweet more frequently at night this indicates I'll go out Friday and Saturday nights.                                       I was pretty much quiet since I got on Twitter in 2008 but have been more consistent since 2012.                                       I must admit this one's here mostly because I wanted to do a heatmap but this does reinforce that I've been a more active on Twitter since 2012 and that I'm less active on Friday and Saturday.                                       I started off barely saying anything but it looks as if I'm consistently around ~90 characters per tweet.                                       I wanted to see whether my behavior around tweeting, retweeting, or replying has changed over time but this doesn't make it very clear due to the number of lines going on so I decided to normalize it - see next chart.                                       Now we're on to something. In the beginning I was basically posting short tweets about my life but more recently I have been more involved in the community aspects.",3,3,2013-01-19,1,twitter data visualization matplotlib nltk quantified self,570,Making sense of my Twitter archive
23,0,There's a type of application that's tough for either devops or core engineering to administer alone and requires effort on both sides.,#meta,"{% include setup %} Lately I’ve been pondering about applications that exist between core engineering and devops. Applications that have existed for years and have widespread adoption have best practices that many devops engineers have mastered - think of Apache or nginx. On the other extreme you have single page apps that exist solely in the browser and don’t need any support from the devops team after they’re deployed. Yet there’s a whole range of applications in the middle that don’t fall neatly into either of the camps. They have a gamut of configuration options that are heavily dependent on the workload which makes it difficult for either side to manage individually. In my limited experience these tend to be common in the big data ecosystem - Spark, Druid, and Kafka are great examples. Go through any of the documentation and you discover how complex the configuration can get and how tough it is to get it right. Without having any prior experience it’s difficult to see how one can get it immediately right. It requires understanding the use case, the expected volume, and a fair amount of experimentation to get right. This is not something that can be determined by devops or core engineering alone and is ideally a group effort.",0,1,2017-11-15,2,"devops, engineering",211,DevOps and Core Engineering application gap
27,0,There's no way to truly protect data using a terms of service. Instead you need to allow third party apps to run inside your walled garden.,#meta,"{% include setup %} Since the Facebook/Cambridge Analytica news broke I’ve been thinking about how a company can make private data available without depending on a terms of service to enforce its usage or retention. As we’ve seen, terms of service are easily ignored and it may take years to notice that your data has been compromised.  The only way of securely sharing data with third parties is to not actually give it to them. That seems like a contradiction but there is a way out. Rather than shipping your data to them you instead have them provide their code to you. Their code can then run within your walls and you’re able to audit it to make sure it’s working as promised. This means giving third party developers a limited set of methods that can be used and preventing any but the most minimal data from leaving the system. That means as a developer you’re working in somewhat of a black box since even debugging gets difficult. In addition, as the platform you’re incurring the additional cost of hosting and executing these third party applications.  Maybe it can be structured in such a way that the developers pay you for the computing they use but that seems like a tough sell. As I write this it seems similar to writing a smartphone app: you’re limited to a finite set of APIs and need permissions from the users for nearly everything. In fact, our system would be even more restrictive since you would not be able to access any of the secured data. It would be akin to writing an app for the Apple App Store but not being able to use your own database - instead you would write your app to depend on a database that Apple provided that you would not be able to access.  This is a ton of hoops to jump through but unfortunately feels as if it’s the only way to have some form of data portability - at least in spirit.",0,1,2018-03-25,4,"data privacy, data protection, facebook, cambridge analytica",337,Protecting data ouside of a Terms of Service
25,0,When doing text message based two factor authentication it's important to actually be secure by not showing the unlock code on a lock screen.,#product,"{% include setup %} The purpose of two factor authentication is to prevent unauthorized access to your accounts by requiring a device other than a password to verify that it’s actually you. Usually this is a text message to a phone or an app such as Authy or Google Authenticator. Being paranoid and despite the inconvenience I chose to do it for the vast majority of my accounts that support it but some are significantly more secure than others.  In particular, developers need to be careful when doing text message based authentication and make sure the code is not visible during a lock screen. Twitter includes the login code as the first word in the message whereas Bank of America does it right and makes sure the code is not visible without unlocking the screen. It’s a seemingly tiny difference but highlights how important it is to get security right.    	  		  			   			 Bank of America obfuscating the code  		  	  	  		  			   			 Twitter including the code on the lock screen",0,1,2015-06-20,4,"security, two factor authentication, twitter, bank of america",203,Properly handling text based two factor authentication
28,0,While reading about how Google can't compete in the audio home automation space I started thinking about why we'd be so against audio ads vs display ads.,#meta,"{% include setup %} I’ve been reading a lot of articles lately on how the shift to audio interactions at home, namely Echo, have the potential to disintermediate Google. The gist is that Google makes the bulk of its revenue from search ads that will show up alongside the organic results. At that point the user can either click on an organic result powered by Google search or an ad powered by an auction. This approach works for the web where we can process the page at a glance but doesn’t translate neatly in an audio context. Imagine asking the Echo to give you the local weather but instead of that getting a list of weather related options first. That would be an utter flop but we tolerate it on the web where our eyes have multiple of orders more information bandwidth than our ears.  The entire idea of bandwidth of the different senses is fascinating and it’s both incredible and obvious how much of the world depends on our physiology. It shouldn’t be surprising at all but but we take so much of it for granted that noticing these things is an enlightening experience. We naturally gravitate toward designs that fit our biology but taking a step back and thinking why we’re designing them this way can inspire a whole new thought process.",0,1,2017-01-11,5,"advertising, google, search ads, audio ads, amazon echo",225,Information bandwidth of audio and display ads
27,0,I had a bunch of old useless files I didn't want to permanently get rid of so I started using AWS Glacier to back them up.,"#aws,#devops","{% include setup %} I’ve been trying to reduce the amount of stuff I have and a big part of it is old electronics. I’ve been selling off old headphones and random cables but the one thing that’s been more difficult to get rid of is older hard drives. I know that most of the stuff on them is junk that I’ll never see again but it’s still tough to just throw it away. They’re reminders of previous jobs and old projects that are a part of my identity that are tough to permanently delete with a click. Many of them are unique in the world and only exist on an old hard drive. I realize it’s foolish to keep them around but it’s tough to let go.  I thought about moving the stuff over to Dropbox but upgrading to a paid plan to store a bunch of files I’d never touch again seemed wasteful. I wanted something that was a one time upload, was cheap, and gave me peace of mind. I recently read about  Amazon’s Glacier  product and it fit the bill perfectly. It’s a penny a month for each GB stored but with additional fees for retrieving old files. Looking at the amount of files I want to store this would cost me less than 20 cents a month and allows me to get rid of a ton of old drives. I spent a couple of evenings this past week tarring up these old files and transferring them to Glacier using the  Simple Amazon Glacier Uploader  app.  Glacier is a great fit for data that’s difficult to throw away but unlikely to be accessed and is significantly cheaper than Dropbox. I’m in the process of going through more and more of my data and seeing which of it would be a good fit for Glacier.",2,2,2014-10-12,3,"aws glacier, dropbox, backups",317,AWS Glacier
22,0,Apple introdued a new feature in iOS 10 to alert users that they're connecting to an open and unsafe wifi network.,#meta,"{% include setup %}           While exploring the city earlier today I ended up wandering too close to the Google building and somehow got connected to their guest wifi network, GoogleGuest, and noticed that iOS 10 gave me an “Security Recommendation” notification. My first reaction was that this was an Apple jab at Google but It turns out that iOS 10 introduced a [new feature](https://www.engadget.com/2016/07/22/ios-10-unsecured-networks/) to let people know that they were connecting to an open network. The intent seems to be to warn users that they may not be on a secure connection but it’s a bit hidden away and didn’t actually prevent me from connecting: it was more of an FYI.  It’s clearly a minor feature but I think it reinforces the stance Apple has been taking in favor of [user privacy and encryption](http://www.apple.com/customer-letter/). They’re positioning themselves to be the antithesis of Google and this is a small way of driving that point home.",2,1,2016-10-02,4,"apple, security, privacy, encryption",184,iOS wifi security recommendation
16,0,The post office should introduce price discrimination to make up for the $5B loss in 2012,#pricing,"{% include setup %} I recently came across Jeff Jordan’s  post  about revamping the post office so it’s no longer losing more than $5 billion a year. Jeff suggests the obvious solution of raising prices but I think a more clever approach would be to start price discriminating. Everyone who needs to mail a letter has to pay 46 cents for a stamp but why not come up with tiered pricing. People who need to send something urgently can pay more than a dollar while others who only care that the letter arrives can pay 20 cents. The postal service would need to ensure their systems are able to track how full or empty each shipment is but this would allow them to ship the cheaper, less urgent mail with the more urgent mail to maximize the shipping space. Another way to price discriminate would be to give a discount for mail that’s picked up at the post office within a few days rather than being delivered to the home.  I took a quick look at the USPS  financials for 2012  and if the average price of a first class delivery increases from 42 cents to 52 cents, the post office would be profitable given the same volume. I realize that’s a 24% jump in price but if it’s done via a price discriminatory approach, such as introducing multiple price points based on delivery guarantees, it won’t feel as drastic.                                 Service Line               Revenue               Volume               Unit Price               New Price               New Revenue                                               First Class               $28,867               68,696               $0.42               $0.52               $35,721                                   Standard               $16,428               79,496               $0.21               $0.21               $16,428                                   Shipping + Packages               $11,596               3,502               $3.31               $3.31               $11,596                                   International               $2,816               926               $3.04               $3.04               $2,816                                   Periodicals               $1,731               6,741               $0.26               $0.26               $1,731                                   Other               $3,785               498               $7.60               $7.60               $3,785                                   Total               $65,223               159,859               $0.41               $0.45               $72,078                     Airlines have been price discriminating since Sabre launched in the 60’s, coupons have been around for 100 years now, and retailers have been offering discounts on out of season items for even longer. Hardware and software improvements are streamlining operations all over the place and are allowing companies to price more efficiently than ever. I’d love to see the government do the same.",2,1,2013-03-05,4,"post office, usps, postal service, price discrimination",416,Discriminatory Pricing in the Post Office
23,0,"As data becomes more important in software, it will pose a challenge for startups that do not have access to this data.","#data,#meta","{% include setup %}  I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.    I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies. Imagine trying to write language translation software without having access to Google’s data? Or trying to do audio transcription by relying on publicly available data? It’s going to be impossible to compete by relying on publicly available data source while large companies build out their internal data monopolies - especially by using their existing products to  subsidize the cost  of collecting this data. Data also begets more data. By giving us great experiences, we’re willing to provide more and more information that is then used to launch new products which have us surrendering more and more data.    No matter how good an algorithm is it still needs data to be useful and I hope we’re not shooting ourselves in the foot by volunteering our data so easily. I’d love to see companies that collect user-contributed information be required to have it shared with their users so that they can have it used by other services. It’s not going to solve everything but it’s a step in the right direction.    Successful startups have always had to overcome challenges so the data monopoly problem will just be more of the same and should hopefully lead to some new approaches. An example that comes to mind is how  Duolingo  is able to generate revenue by selling document translations that are transformed into language lessons that are then done freely by the community. I’m excited to see new business models that are able to innovate past this data gap.",2,2,2013-07-21,4,"data monopoly, startups, business, entrepreneurship",343,Beware the data monopoly
24,0,I wrote a simple script that takes an 'On-Call' calendar in a Google Spreadsheet and posts the current week's schedule to Slack.,"#code,#management",{% include setup %} Recently we adopted the concept of owning your own up time for our engineering teams. The goal is to encourage a stronger sense of ownership and actually give the teams the autonomy to approach their development and release process the way they’re comfortable with. Before this we relied on a single on call every week that would be responsible for monitoring all issues and escalating them to the appropriate team. One minor side effect of this change was that I now had to manage the on call calendar and post the new rotation on Slack every week. Since this was a good opportunity to mess around with the Google Spreadsheet and Slack APIs I decided it good be a fun little project.  The spreadsheet has a set of columns with a header - the important ones being a “Current” column indicating whether this is the current week and driven by a spreadsheet formula and a set of columns indicating the on call for that particular team. All the script needs to do is find the “current” row and generate a Slack message highlighting the on call engineer for each team.  Turns out the code was ridiculously easy to write. The [gspread](https://github.com/burnash/gspread) Python library provides a very simple way of reading a Google Spreadsheet and all I need from Slack is a way to pull the list of users (which I could have just kept in the Spreadsheet) and post a message. The code is [up on GitHub](https://github.com/dangoldin/gsheet-slack) and I hope to expand it to handle more of the standard admin work given how easy it was to get this working.,2,2,2017-03-04,5,"code, management, automation, google spreadsheets, slack",277,Automating admin work: Spreadsheets to Slack
15,0,Bundling makes sense for low quality products. High qualit products should be sold separately.,#pricing,"{% include setup %} In the quest to reduce the amount of stuff I own I've been going through various cabinets and boxes and trying to list everything on eBay. The most common items are old cables with no corresponding devices (or any ideas what these devices even are) and old DVDs.  Looking at the historic prices for these items doesn't make me happy - a Lenovo laptop charger is less than $10 while a Raging Bull DVD is a couple of bucks. But this entire process got me thinking about bundling. Bundling makes sense when selling cheap products. It's not worth the time to list these individually and it's likely that there are only a few people interested in each item. Bundling them makes it more likely that various items will appeal to a variety of buyers and increase competition. The Lenovo adapter may appeal to one person while a Game Boy charger may appeal to another. By having them in the same lot they are competing against each other and are willing to bid higher to get what they want.  High quality items are competitive on their own. The sum of the individual sales will be greater than the value of the bundle. The intuition is that bundling premium items when buyers only want a single item will decrease buyers' willingness to pay more for the extra items.  HBO's the standard example - it has enough consumer demand that it can stay independent and charge a premium. Other channels need to bundle and subsidize each other. I suspect many would be more successful breaking out, such as ESPN, but they're either stuck in contracts or fear change.",0,1,2014-07-29,4,"bundling, pricing, auctions, HBO",278,Low quality? Start bundling!
7,0,Some thoughts on GroupOn launching their scheduler,#product,"{% include setup %}       I'm not surprised that someone came out with an  online scheduling tool  for SMBs. I am a bit surprised that it was GroupOn though. I suspect many smaller companies have tried doing it but found selling to the SMB much more difficult than they expected. Since GroupOn already has penetration in the SMB space they may find it a lot easier, especially if, as it looks, they will be offering it free to any business that runs a GroupOn promotion.    This brings a good amount of value to the business:        Cheaper appointment booking system since fewer people will use the phone     Reminding customers about upcoming appointments      The real value is going to GroupOn though:        GroupOn can see how busy the businesses are (not just from GroupOns)     GroupOn can start offering a finer capacity management product - Imagine being able to see a haircut for $10 if you go in the next hour but $20 if you book it for tomorrow.     GroupOn will have major visibility into the way businesses operate and will be able to relate it back to the customer      I'll be watching this to see how it turns out but I'm glad to see this space innovating. As businesses get more comfortable running their business online it will become much easier for new companies to attack this space.",1,1,2011-12-08,2,"groupon, small business tools",246,GroupOn Scheduler
29,0,I get around one spam blog comment a week and wonder what the goal is. They're clearly spam and I don't understand how they add value to anyone.,#meta,"{% include setup %} Ever since I’ve started blogging I’ve been getting around one spam blog comment a week.  Disqus  does a nearly perfect job of flagging them so I don’t understand the motivation behind it. They’re obviously spam and my readers are suave enough to never click on any of the links. There’s also little, possibly none, SEO value since they’re all loaded asynchronously and every link has a rel=”nofollow” property. And if the goal is to spark a discussiong and raise awareness they're so poorly worded that no reader will take them seriously. The only thing I can think of is that these companies pay a third party service to grace tangentially related blogs with content on their behalf and these third party services go the cheapest possible route in both effort and quality.",1,1,2014-10-26,2,"spam comments, blog",140,Spam blog comments
31,0,"At Pressi, we decided to hire someone from Odesk to help us come up with a list of prospects. We ended up finding someone on Odesk using a 'screening' approach.",#meta,"{% include setup %} While working on  Pressi , we found a niche selling ""social media mashup pages"" to colleges and small universities. Once we discovered it we needed a quick way to find these colleges and identify the contact details of their marketing or social media directors. Searching for this information was not the most efficient use of time for our small team so we went looking for other options.  Two options that stood out were  Mechanical Turk  and  Odesk  but they were designed for quick and simple tasks. Using them for complex tasks would result in poor quality results. One advantage that Odesk had was that it allowed us to work with the same person for many tasks - something we couldn't figure out how to do using Mechanical Turk. This allowed us to come up with a set of potential candidates based on their project interest and skillset. We gave each of them the same set of problems to do and compared the results. Using this approach we discovered someone who was the right balance of cost and quality and we ended up working with her over the next few months to compile this list.  There's a lot of talent on platforms such as Odesk and Mechanical Turk but it's not easy to find. A good approach is to develop a set of tasks that you're looking for someone to do and use that as a proxy for an interview. Giving this set of tasks to a few dozen people will lead to a few that stand out and can hired on for longer term work.",3,1,2014-07-31,3,"odesk, remote working, on demand economy",279,Hiring people on Odesk
27,0,Sales approaches can be seen as varying by inbound vs outbound and proximity to customer. They all work but we're trending towards a self serve model.,#sales,"{% include setup %} Something I’ve been thinking about is the variety of sales approaches. On one extreme, you have pharmaceutical companies sending sales reps to visit doctors offices to try to get them to prescribe their drugs. On the other you have companies such as MixPanel and Dropbox which rely on a self serve approach. And in between you have companies such as NewRelic which offer a self-serve trial and try to upsell you with emails from a sales rep.  Depending on a product’s complexity and its cost structure your sales approach may be limited but it’s always worth seeing the other approaches available and if any of them may fit. It’s likely that an approach that didn’t work a year ago may work right now. A simple way to check is to look at newly launched competitors in your space and see how they’re acquiring customers.  After trying to come up with an exhaustive list of approaches I figured out it's easier to just rank them across two dimensions:          Proximity : This is both physical proximity as well as familiarity with your customer. It’s much easier to sell when you’re in the same room as them and know their story than when you’re sending out a generic email.        Inbound vs outbound : A customer already having an interest in your product is much better than trying to interest him from scratch.     Here’s my attempt at coming up with matrix showing where different companies would lie based on their sales approach.      It’s possible to be profitable by being in any spot; a higher acquisition cost will just lead to a higher price. That’s why an Oracle installation can  cost millions  of dollars a year and why the enterprise Dropbox product is around  $125/user/year . I believe that current trends favor businesses in the inbound/self-serve quadrant. This is due to people becoming more comfortable with technology as software gets better and easier to use and the ability for companies to offer free-trials with near-zero marginal cost. A corollary is that there’s an opportunity to compete with businesses outside the quadrant by creating simpler, cheaper versions of their product. The first version will suck compared to the existing products but as long as it’s cheaper and still solves a problem you should be able to get some customers and revenue (a la  Lean Startup ). Over time, you can continue to grow and keep on building our product until you’re competing with the existing companies (a la  Innovator’s Dilemma ).",4,1,2013-06-21,7,"sales, innovator's dilemma, lean startups, mvp, saas, startups, business",456,Where are you on the sales matrix?
17,0,It's much easier to be a small business these days due to how cheaply information flows.,#society,{% include setup %} In many ways modern capitalism is about bigger and bigger companies owning more and more industries. We’re seeing with the major tech companies moving into new verticals to keep growing. We’re seeing it with the media and telecom companies going on acquisition sprees aimed at both vertical integration and horizontal scale.  Yet there are also ton of small business being launched across a variety of industries - especially in consumer products. The world is much more open now and if you offer something unique and compelling it’s easier to find customers than at any other time in history. You don’t need to invest in a ton of marketing or advertising so long as you know your audience and figure out the best channels to reach them.  The modern world is great at reducing friction. At first it was friction in the transport of physical goods with the standardization of the shipping container and railroads. Now it’s the reduction in information friction. In the past it made a ton of sense to vertically integrate your business and control each step so you can control as much as possible. That’s still advantageous if you can swing but but it’s so much easier to be small and access the transparent market to find what you need. There are numerous vendors for anything you need and you can find exactly what you need. Moving from small to medium is a whole other story but it’s easier to be smaller than ever.,0,1,2018-12-09,2,"small business, modern economy",251,The modern economy relies on information
24,0,I just finished migrating my blog to AMP and am happy with the outcome. The site's snappier and feels much better on mobile.,#meta,"{% include setup %} Ever since AMP was announced I’ve been meaning to migrate my blog but hesitated due to the fear that it would take an inordinate amount of time and would be laden with edge cases. But over the Labor Day weekend I decided to give it a shot and see how far i could get. A quick GitHub search showed two promising repos - [amp-jekyll](https://github.com/juusaw/amp-jekyll) and [amplify](https://github.com/ageitgey/amplify) - and I gave them both a shot. They approach AMP integration in two different ways - amp-jekyll creates an AMP version of every post and has it live in a separate folder structure while amplify is a comprehensive theme. This made the amp-jekyll integration much easier since it’s designed to work parallel to the existing blog but I wanted to do a full rewrite.  I ended up cloning the amplify repository and manually importing a few of my blog posts to see how they’d render, handle the images, and look under the different style. After playing around with amplify I realized it would actually be straightforward to integrate directly into my blog as an additional theme. After copying over the design files and the libraries required to inline the SCSS I was left with making a few changes to the CSS to get it to resemble the previous design. All in it took a few hours to get my blog migrated to AMP and it’s incredibly quick - especially on mobile (if you haven’t given it a shot please do).  There are still a few things I need to take care of but I’m pleasantly surprised by the ease and simplicity of the transition and the resulting performance. The major problems I need to take care of before I call this a success:  - Disqus integration. I’ve been using Disqus to manage comments and it would be a shame if I had to ditch it. Based on a few StackOverflow and forum posts it looks as if it’s possible to get Disqus working by forcing an iframe with the comment section but I’ll have to figure out how this works. - Various styling fixes: Since I ended up starting with the amplify CSS there are a few inconsistencies that I still need to take care of - especially on some of my older posts that have some ugly inline CSS. - Img to amp-img: To be AMP compliant you cannot have any img tags and instead must use amp-img. This sounds straightforward but amp-img requires you to specify the dimensions of the image which I have not been doing. It looks as if there’s a plugin for this in amp-jekyll and I got it working locally but need to get it working on GitHub pages. - JavaScript heavy posts: I have a few older posts that depend on D3 for visualizations and I’m going to have to rewrite those posts to include the animations as amp-iframe elements. This seems straightforward but I’m sure I’ll run into some hiccups when I actually sit down to do this. - Build times take forever: This is my biggest issue so far. Since AMP requires all CSS to be inlined it means that every CSS change causes the entire site to be regenerated. Before AMPifying, Jekyll would build the site in around 10 seconds and now it takes nearly 2 minutes. I don’t have a good fix for this but a simple solution may be to have different CSS for the different types of pages to avoid a full site regeneration with every style change. While developing I solve this problem by moving all but a few posts out of the _posts folder in order to reduce the number of pages that need to be generated. Then when I’m happy with the outcome I’ll move the other posts back and let it go through the full generation. This is extremely hacky and I wish there was a better solution here.  I’d love to know what the readers of the blog think and whether they’re noticing any improvement so if you have any feedback please let me know. And I’m aware that I have yet to get Disqus set up to work with AMP but in the meantime let me know via [Twitter](https://twitter.com/dangoldin).",3,1,2016-09-05,5,"AMP, Google AMP, jekyll, blog design, mobile design",712,AMPifying my blog
26,0,Sharing a pricing strategy I've been using for smaller consulting projects where I will steeply discount the rate if I go over my project estimate.,"#product,#pricing","{% include setup %} I've been doing some consulting work over the few months and wanted to share a pricing model that’s been working well for smaller projects. I’ll sit down with the client to understand the scope of the project and work with them to break it down into smaller, more manageable components. Based on this break down, I’ll estimate the time required for each piece and come up with an estimated total time. I charge my usual hourly rate for the work that falls within the estimated time but will charge a steeply discounted rate for every hour that goes over.  This aligns incentives since it gives me an incentive to complete the work within the estimated time and helps the client feel more comfortable that the project won’t run past its estimate for the sake of me working more hours. The other major benefit is that it forces us to scope out the project together so we’re both on the same page and avoid any future surprises.  I don’t think this approach would work well for larger projects since those are significantly more difficult to estimate but any feature can be broken down this way. For larger projects, I think trying to bill weekly as  advocated by Patrick  is the right approach if you’re able to agree with the client on it. In my case, I have multiple projects going on simultaneously so it's been difficult getting a weekly rate worked out.",1,2,2013-09-28,3,"consulting, pricing, contracting",250,Pricing small consulting projects
27,0,I'm working on a project using Netty - a low level framework that forces you to create your own HTTP requests from packets. A wonderful learning experience.,#meta,"{% include setup %} I’m currently working on an application using  Netty , a low level network framework, and it’s given me a wonderful education of the HTTP protocol. Prior to this project, every web application I’ve worked on has leveraged a framework that removed the low level details. They built the HTTP requests from multiple packets, took care of various encoding issues, dealt with keep-alive connections, came with built-in support for sessions and cookies, and in general made it extremely easy to get a web server up and running.  Writing a Netty application is completely different than using a framework such as Django, Ruby on Rails, or Node. You get a much better understanding of how TCP and HTTP work and doesn’t actually take that much time once you get the hang of it. Building an HTTP request from individuals packets was completely novel and finally getting it to work made me feel the same way as when I built my first site. If you’re interested in or currently working in web development and you haven’t worked with a low level framework, take a weekend off and give it a try - it’ll give you a newfound appreciation of how the modern web works.",1,1,2014-07-14,4,"netty, http, low level, web framework",209,Getting low level with HTTP
23,0,Two common problems in Excel is filling in missing values and doing a cross join. Here are two ways to do it.,#code,"{% include setup %} During my consulting years I’ve done a ton of Excel and noticed people getting frustrated by two seemingly simple operations. The first is getting a worksheet with gaps in a column and needing to fill it with values from the cells above and the second is doing a cross join between two sets of values.  The solution to the gap filling can be done by explaining the solution in such a way that it can be implemented via an Excel formula. The best I could come up with is “If a gap is a value, take the value of the closest non empty cell above it, otherwise keep its value.” We can create a formula in another column that takes this approach and after coming up with the new cell values and pasting them over the originals. In the image below, the formula in cell D2 is  =A1  and the formula in D3 is  =IF(A3="""",D2,A3)  with D4 down being relative copies of D3.      The cross join problem is similar - we have two sets of values and need to enumerate each combination. The key point is realizing that we know what the values should be in a particular row and deriving the formula to get those values. My approach uses integer division to get the value in the first column and modulo to get the value in the second column although any function that’s deterministic should work. In the image below, the formula in cells D2 through D25 is  =INDEX($A$2:$A$5,(ROW()-2)/$H$2+1)  and the formula in cells E2 through E25 is  =INDEX($B$2:$B$7,MOD(ROW()-2,$H$2)+1) .      The file with the two approaches can be grabbed  here .",1,1,2014-05-03,3,"excel, cross join, gap fill",315,Gap fills and cross joins in Excel
22,0,It seems possible to make money by buying reserved instances and then selling them at a discount on the AWS market.,#aws,"{% include setup %} While reserving some EC2 instances earlier this week I discovered that Amazon allows you to [sell](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-buying-guide.html) reserved instances you’re no longer using. Usually the prices the third parties are offering are very close to the fair market value but I wondered if there was an arbitrage opportunity by reserving a longer term instance and selling it for a series of shorter term leases. The [typical discount](https://aws.amazon.com/ec2/pricing/) for buying a 1 year reserved instance is 30% while buying one for 3 years can get over 60%. The idea being that if you can get an instance for a 60% discount over 3 years and then sell it for 3 one year terms at a 25% discount you end up coming out ahead. Of course the challenge is that Amazon constantly drops prices so a 60% discount now may be equivalent to something much smaller three years later. There’s also the risk of no one purchasing your instances but that seems unlikely since you can always undercut Amazon’s official price. The other factor is the discount rate since you’re paying up front for 3 years worth of an instance. During that time you could have taken that money and invested it elsewhere which could have led to a better return but which would have been unlikely when you’re getting a 30% discount over the course of a year.  Unfortunately, based on the Amazon seller [documentation](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-selling-guide.html) it looks as if you can't actually split a 3 year reservation into 3 single year reservations and you'll be charged a 12% fee. There's always the option of using an instance and then selling it at a premium for a term that Amazon is not offering but I doubt it's worth it given the risks and the restrictions placed by Amazon.",3,1,2016-03-05,5,"aws, ec2 instances, reserved instances, ec2 marketplace, arbitrage",316,AWS EC2 instance arbitrage
15,0,Dealing with the madness of having multiple accounts for each of my startups and projects,"#meta,#product","{% include setup %}            Over the past year, I cofounded two startups and launched a bunch of side projects. Since they all had potential, I wanted to make each as standalone as possible and ended up with dedicated accounts for each. This meant that I had a flood of accounts for each, ranging from the various Google products to Sendgrid and AWS to Freshbooks and Quickbooks. Unsurprisingly, this turned out to be an unmanageable pain in the ass.  It gets worse. We ran into a trademark issue and had to change our company name from Glossi to  Pressi  and transfer our branded assets. This meant handing over our domain and since we’re heavy users of Google’s products, losing access to our email, our documents, and our calendars. To migrate, we had the great fortune of having to forward the important emails and share the important documents to our new account.  This led to me an epiphany that we’re using today. Only have unique email. Everything else can be managed through individual accounts until it’s necessary to create company accounts. And even then, only create accounts that are absolutely necessary, which will typically be the financially dependent ones (Freshbooks, Stripe, etc). This allows us to not worry about having a flood of Google tabs open and we get to avoid the adventure of figuring out whether a doc we’re looking for has been shared on a personal or company account. When something does need to be shared with someone outside the company, we share it with our corporate clone and manage it from there.  By no means is this a perfect solution but it works for me and I only wish I stumbled unto it sooner. How do you make it work?",1,2,2013-02-25,4,"startups, multiple accounts, time management, productivity",311,Account management madness
29,0,Airbnb claims that it's too hard to localize their product but on the flipside you have companies such as Uber that built their entire product around local markets.,"#product,#society","{% include setup %} Yesterday I read an [interesting piece](http://www.nytimes.com/2016/03/06/business/airbnb-pits-neighbor-against-neighbor-in-tourist-friendly-new-orleans.html) on Airbnb in New Orleans. The content itself isn't new - it highlights the typical conflict between those that view Airbnb as violating local ordinances and ruining the city and others who believe that Airbnb brings value and is helping New Orleans rebuild after Katrina.  But what was interesting was the repeated claims of Airbnb and the other rental marketplaces that it’s just not scalable to follow local policies for every city and it’s up to the users to know their local regulations and follow them. I understand it’s difficult to localize complex products but these responses just feel like an excuse.  > Representatives of the larger home-sharing companies have met with New Orleans officials, but they are seldom heard from in more public forums. Officials of Airbnb and VRBO (Vacation Rentals by Owner, a HomeAway brand that is popular in New Orleans) point out that they operate in so many places they cannot possibly get into the specifics of local policy; they are merely private businesses offering services to consumers. So it is up to New Orleans and other cities to devise their own regulations, and up to users to follow them.  > According to Mr. Rivers, Airbnb and VRBO told his staff that it would be too onerous to adjust their software to accommodate every regulatory arrangement for thousands of municipalities around the world. Spokesmen for Airbnb and VRBO confirm that rewriting their platforms in this way is not practical.  Contrast this with Uber. They also run a marketplace that’s highly sensitive to local regulation but work within the confines of the law (including pushing to change legislation). Both Uber the company as well as the Uber app have adapted a localized view. When I open up the Uber app in New York City I see a variety of options that I don’t see in other places. In fact, Uber can even push idiosyncratic updates that may only last a couple of days - for example a special [“De Blasio” ride option](http://techcrunch.com/2015/07/16/uber-launches-de-blasios-uber-feature-in-nyc-with-25-minute-wait-times/) that came with a 25 minute wait time.  The goal of technology companies is to come up with elegant solutions to real world constraints. Uber has embraced it by building their company and product to embrace local differences while Airbnb adopted the attitude of a single product for the whole world. I’m confident if Airbnb wanted to build a flexible product that worked for local markets they’d be able to and it would actually be a fun and interesting product and engineering challenge. Startups need to embrace their challenges and this feels like Airbnb being complacent. I understand there’s a high cost to localize Airbnb and it comes with a world of risks but if they do it right they’ll be able to capture significantly more share and markets.",2,2,2016-03-06,5,"airbnb, uber, local laws, economy, startups",498,"Airbnb, Uber, and local laws"
25,0,I moved away from Gmail to Fastmail and couldn't be happier. Gmail just got to be a big pain in the ass to use.,#product,"{% include setup %}        Over the course of the past year I’ve become more and more pissed off at Gmail. I loved using Gmail when it launched - it made writing and reading email a pleasure. It was simple, clean, and responsive. Now it’s the opposite. All actions feel slow. The initial page load takes a substantial amount of time and then I get to wait for the various page elements to load - including a chat list that I’m almost never signed into and integration with a slew of other Google products. Loading emails or new tabs is noticeably slow and the search is sluggish for a company whose main product is a search engine.  This past weekend I decided to see what was out there and discovered  Fastmail . I’ve been using over the past few days and it’s been great. Emails are quick to load and send and the navigation feels snappy and responsive. I’m not sure how well it will work as my inbox grows but so far I’m impressed. It feels like Gmail when it launched almost 10 years ago. It feels odd to describe it in terms of Gmail since I’m bashing it but I can’t think of a better way.  I did a quick anecdotal test by looking at the networks tab in Google chrome as each loaded. Gmail loaded in 6 seconds after making nearly 150 requests and retrieving 338 kb while Fastmail loaded in a little over 300 milliseconds after making 18 requests and retrieving 128 kb. Repeating this a few times showed similar results. Others seem to be having the same issue since Google’s first auto-suggestion is “why is gmail so slow” when typing in “why is gmail”. The switch is also much simpler than I expected - I just have Gmail forwarding everything to my Fastmail account and it’s completely transparent to the outside world. In the future I plan on migrating everything to use my new email address but for now this is a good intermediate step. I haven’t heard much from others moving away from Gmail so I’d love to hear your experiences if you made the switch.",1,1,2014-03-18,2,"fastmail, gmail",409,Goodbye Gmail
38,0,I've hailed rideshares where the driver canceled the ride as soon as they found my destination. There's an obvious solution here by having ride sharing companies take that into account when pricing rides to less-profitable destinations.,"#product,#pricing","{% include setup %} One of the more frustrating modern, first-world problems is booking a Lyft or an Uber ride and having the driver cancel the ride a soon as they find out where you’re going. While against policy it is a rational decision by the driver. Why agree to a trip somewhere where it'll be difficult to pick up another passenger and then have to return to where you started and incur a toll? Of course you run the risk of a complaint but I suspect most people will silently accept the misjustice so the expected value of canceling the ride is in your favor.  Given how much effort Lyft and Uber are expending on price prediction it seems there's an obvious solution here: take into account the expectation of future rides based on the destination when calculating the current trip's payout. They know how likely a driver will be to get another ride at the destination at the expected arrival time and can use that knowledge to price appropriately. This encourages drivers to take these less rewarding trips but does shift the cost to the customer. This strikes me as a fair solution though - imagine if the driver knew everyone's destination and the individuals had to bid for a ride. The drivers would take the destination and bid into account when determing who to pick up. There are obvious reasons to not make the destination visible to the driver before picking but there are ways ride sharing companies can make these less profitable trips more sustainable. This is one of the decisions that's all about finding the balance between the driver and the passenger but in this case there's room to move closer to the driver. In the end it will benefit the passenger as well who will avoid having to deal with a canceled ride, which is usually to a destination that requires punctuality, such as a train station or an airport.",0,2,2017-11-20,4,"ridesharing, lyft, uber, abandoned rides",327,Ride sharing ride bailing
35,0,"Scripting languages make it easy to move quickly with small steps. On the other extreme some languages let us take slow, large stpes. The goal is to find a language that maximizes actual speed.",#meta,"{% include setup %} The most common way of making sure code works is by going through the “develop-run-test” loop. We write some code that we expect to have a certain behavior, we run the code and trigger that behavior, and then we confirm that the results are what we expected. And we keep iterating, hopefully making more progress with each new iteration.  One thing I’ve noticed is that this pattern varies drastically for me depending on the language I’m working with. I’ll cycle through iterations much quicker in Python than I will with Java. Part of it is that my Java projects are larger and take a longer amount of time to start but I suspect the bigger benefit is that Java’s strong and static type system makes it easier to take larger coding steps than I’d be able to with Python. For example, if I need to write a method to extract data from a JSON object I’ll approach it very different if I’m doing it in Python than I would if I were doing it in Java. With Python I’d jump into the REPL and walk through a few examples and make sure I handle the the various edge cases whereas with Java I’d place a lot more faith in the IDE and it’s litany of warnings.  Each language comes with it’s own pros and cons and it’s impossible to find a single language that fits every use case. The goal is to pick the appropriate language for the job at hand - and this may involve starting with one and moving to another one as the problem domain changes or the team grows. The ideal language is one that’s able to maximize the product of the iteration speed as well as the step size. Taking frequent, small steps is equivalent to taking fewer, bigger steps - the aim is to maximize the resulting speed - not the individual inputs. A great language paired with a strong development environment achieves both speed and step size.",0,1,2015-08-20,3,"programming, Python, Java",338,Development cycles across programming languages
33,0,We ran into an issue where our Spark jobs suddenly started taking hours to run. Digging into it we discovered it was due to the way we were accessing tables from MySQL.,#code,"{% include setup %} Yesterday I spent a bit of time investigating one of our Spark jobs that had suddenly shot up in run time. The purpose of our job is to collect all the events we see in an hour and generate a variety of aggregate tables and files that can then be loaded into various systems. When we first wrote the job it took about 45 minutes to run but as we've started seeing much higher data volume the job time has crept up to to 90 minutes. And for some reason yesterday the jobs were not completing even after 2 hours. There was clearly something odd happening.  After digging into it I discovered it was actually a problem with the way we were fetching data from MySQL. In order for us to process our data we load a variety of fact tables from MySQL that are then joined to the raw log level data in Spark. We ingest about a dozen tables this way and it turned out that Spark was just hanging waiting for MySQL. Examining the slow query log it became immediately obvious what the issue was: we were seeing a ton of queries that were ""SELECT * FROM (SELECT * FROM table) table WHERE 1=0"" quickly followed by ""SELECT a, b, c FROM (SELECT * FROM table ) table."" It turns out we were using the [read.jdbc](https://spark.apache.org/docs/2.0.0/api/R/read.jdbc.html) function that under the surface Spark does two queries - the first to get the schema and the second to get the data. It's designed to work on a table but what we were doing was passing in a subery.  I suspect the origin of this creativity was to give us a way to do more advanced querying but it came at a pretty big cost when trying to fetch a table. For one of our simple tables there was a multiple order of magnitude improvement by doing ""SELECT * FROM table WHERE 1=0"" instead of ""SELECT * FROM (SELECT * FROM table) table WHERE 1=0"" despite them fetching the exact same data. The reason for this is that MySQL isn't smart enough to realize what we were doing and has to actually evaluate the inner query before the where clause. This adds up when the underlying table keeps growing.  Making a one line change to fetch the tables rather than rely on the subqueries had an immediate performance improvement. Our jobs that were still going strong after 4 hours started completing in less than an hour and this was all due to improving a bit of SQL. It's remarkable how trivial, yet impactful some optimizations are and it's important to keep profiling and understand the slowest parts of the system. Only by fixing those do you improve the whole system. This is obvious and has existed since the 1960's by virtue of [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law) but it's still relevant today as we embrace more and more big data and parallel processing tools.",2,1,2017-11-07,4,"Spark, read.jdbc, sql, code optimization",505,Spark's read.jdbc
23,0,I used Foursquare to plan my trip to New Orleans and had a great time. Technology is making travel a lot easier.,#product,"{% include setup %}                                    Last week, my wife and I took a vacation to New Orleans and it was the first time we used Foursquare to plan a trip. I asked friends for suggestions, looked at other Foursquare lists, and did some online research to create  my list  of 25 places that I wanted to visit while there. These places ranged from tourist magnets such as Bourbon Street and Cafe Du Monde to the more local places like Cafe Envie and Port of Call. Our typical approach in New Orleans was to go to a neighborhood we wanted to explore and then visit the places that were nearby on our Foursquare lists. Out of the 25 places I had on my list, I ended up visiting 16 which is a bit low but I made up for that by visiting a bunch of local places that I wouldn’t have discovered otherwise.  Before I had a smartphone, I remember drawing a map every time I went somewhere new so I wouldn’t get lost. I’m no longer doing that but still need to come up with a rough plan of when to visit the various places. The next step would be something that takes my list and applies a route finding algorithm to come up with an agenda that gets me to visit all the places while taking into account distance, open hours, and the venue type. Combine this with  Google Glass  and you get  a pretty awesome way of exploring a new city. I’m excited.",2,1,2013-02-22,4,"foursquare, google glass, travel, augmented reality",288,Vacationing with Foursquare
22,0,We still view our products as being software but in the future we'll start seeing our products and technology as our equals,#product,"{% include setup %} In 2013, I gave myself a goal of running 1000 miles. I used RunKeeper to record my runs and used its goal feature to track my progress and quickly see how much I had left. Two days before the new year, I was able to hit my goal and got a little notification from RunKeeper congratulating me on achieving my goal. This small notification got me thinking about how emotion is built into our products. RunKeeper doesn’t care whether it was a 1 mile or 1000 mile goal - the reaction I get would be the same. Yet if I shared these two achievements with my friends, the reactions I get would be completely different. Sure, an algorithm could be designed to treat accomplishments of various difficulties differently and can even be adapted to take into account that to some people, running one mile is equivalent to others running 1000 miles.  I doubt a smarter algorithm would actually make a difference. We might appreciate the intelligence of the algorithm but we’re not going to believe that this digital praise was authentic or that our software actually cares. We already have Google Now promising to give us the information we need when we need it and Amazon is trying to ship products to our doors before we even place the order. Yet as smart as these are, they’re not emotional. Even Siri is just an algorithm. As technology gets smarter, I wonder whether future generations will feel this way. We’ll continue to see these improvements as simply smarter software and better data but I doubt future generations will feel the same way. We’ve seen how dumb our technology has been and won’t be able to think of it as anything more than software. Future generations will be born and grow up in a world surrounded by smarter and better versions of what we have and won’t be saddled with this bias. Many believe the singularity will happen in our lifetimes but I think this will have the larger effect - that we’ll start viewing technology as our equal.",0,1,2014-01-19,3,"emotional products, technology, society",350,Emotional products
41,0,As managers we need to make our teams as productive as we can be. To do that we need to figure out what to measure since that's a clear way of improving. But how can we find the appropriate metrics?,#management,"{% include setup %} A key part of management is getting out of the way and building out processes that help your team be as productive as possible. At the same time, you can’t change what you can’t measure. Combining these two makes it clear that to improve, whether people or process, you need to start measuring and tracking the appropriate metrics.  In software engineering, some things are easy to track: how many bugs there are, how quickly they’re resolved, how much code are written - but rarely tell the whole story and may lead to perverse incentives. The common example is measuring developer productivity through number of lines of code written: a smart developer would purposefully write verbose and long winded code to get their metric up.  Then there are the items that are hard to measure but actually drive productivity: improvement as an engineer, simple and expressive code, code that’s easily changed. These are incredibly difficult to measure, especially at scale, but if you’re able to focus on improving these you’ve found the holy grail.  By being creative it’s possible to come up with proxy metrics and approximations despite not being able to find easy ways of measuring the actual performance drivers. Think of these as traits that productive teams have and should be encouraged. There will always be exceptions and many are susceptible to gaming but they’re much better than nothing.  Besides the usual suspects (velocity, bugs, test coverage), I plan on tracking the following proxy metrics. Individually they don’t tell the whole story but taken together I hope they’ll be a good way to help improve the productivity of an engineering team. Note that a requirement for these was that they would be easy to collect, ideally automated.  - Pull request size: I believe pull requests should be as small as they can be. Larger pull requests are harder to code review and carry more risk. - Pull request file variance: Not a 100% sure about this one but I suspect there’s a difference in pull requests that are isolated to a small set of files rather than dozens. It may indicate that our code is not as cleanly laid out or architected as it should be and may be worth cleaning up. - Pull request activity: Another soft one but I want to see whether the amount of comments and changes a pull request has carries any meaning. I think junior engineers tend to have more feedback on their code versus more senior developers and measuring this may be a good way of discovering that. The challenge is that this one is easily gamed and we should all want to encourage discussion of code in order to come up with as high quality code as we can. - Deploy frequency: The more we deploy the more useful code makes it out into the real world and we should strive to deploy as often as we can while maintaining a high quality bar. We’re not at continuous deployment yet but hopefully this will help us get there.",0,1,2016-08-07,3,"engineering measurement, metrics, productivity",507,Engineering management: Measuring the unmeasurable
35,0,It's easy to get carried away and start rewriting all parts of your code. Don't do this and treat it like an ill patient that requires multiple transplants - do one transplant at a time.,#management,"{% include setup %} When working on new features it’s easy to keep increasing scope until you end up doing a full rewrite of your code. Don’t. It’s healthy to refactor code as you go but you need to be wary of how many things you’re changing and the risks those changes carry. Code will get stale unless it’s constantly maintained and updated as the rest of the product evolves but trying to change too much at once will make it difficult to diagnose issues and increase the odds of bugs in production.  The analogy is that of an extremely sick patient. That person may need a variety of transplants but it’s dangerous and stupid to replace multiple organs at once. Instead you should find the most critical one to replace and do that. After the body adjusts to that transplant you move on to the next most critical one. Otherwise the body will go into shock and reject the organs.  Bad code is similar to this patient. There are countless things that can be improved but if it’s doing a critical job keeping a product alive you need to treat it carefully. Replacing everything at once may end up working but more likely it will cause a slew of problems that will be tough to diagnose given the various changes. It’s much better to approach code like a sick patient - make a change, release, and monitor to make sure everything is going well. Once you’re confident that the code is functioning as expected you can move on to the next most critical item. Over time you end up replacing the critical components while reducing risk.",0,1,2016-04-27,4,"engineering management, coding, software development, software engineering",277,Avoid full body code transplants
26,0,I had to use the Edmunds API a couple of weeks ago and was pleasantly surprised by how easy and simple it was to use.,#product,"{% include setup %} As part of the  RelayRides analysis  I needed to estimate the price of a car and stumbled across the  Edmunds API . I came in with some low expectations but was pleasantly surprised by how well it worked. I thought I’d need to go through a data cleanup process to make sure I was using the correct arguments in the HTTP requests but somewhat remarkably the Edmunds API was able to properly handle nearly every request.  It’s unbelievable how happy a good API makes me. Dealing with various edge cases is a huge time suck so having an API that works as expected the first time you try it is incredibly refreshing and highlights the amount of crappy APIs we’ve all had to deal with. I’d expect this to come from a small, product focused company or at least be built in house but it turns out Edmunds partnered with  Mashery  to develop their API. It definitely makes a case against keeping development in house.",3,1,2015-06-19,2,"edmunds, api",185,The Edmunds API
20,0,Thinking about Netflix and bundling is giving some ideas about the future of video and what we'll start seeing.,#product,"{% include setup %} Something that’s been stuck in my head is the relationship between Netflix and bundling. On one hand, we’ve been wishing that cable came unbundled so we’d be able to just pay for the shows we want to watch. On the other hand, we have Netflix which is striving to let us stream every TV show and movie whenever and wherever we want. Why don’t we care that Netflix is actually a bundled product?  I'm sure the major reason is that it’s just not worth worrying about since Netflix is only $7.99 a month; especially when cable TV bills can easily go past $100. Maybe we like the new shows that are exclusive to Netflix (House of Cards, Hemlock Grove, and Arrested Development) and are happy to pay for them; the rest of the content available on Netflix is just an added benefit. Maybe we just don’t view Netflix as being a bundled service at all: the reason I have Netflix is to be able to watch anything I want when I want.  I wonder about the reasons because it helps me think about the future:     Does Netflix want to be the central repository of all video content that can be accessed at any time? What happens when the existing content producers keep raising licensing fees to extract as much as they can?   Does Netflix want to focus on producing its own content? Is it just a TV channel with a unique distribution channel and monetization approach? Does this mean that we’ll start seeing competing TV show/movie producers creating their own Netflix like service? How easy will it be for consumers to find this content if it’s heavily fragmented?   Will the future consist of niche shows and movies? Kickstarter has been used to raise money for the  Veronica Mars  movie as well as Zach Braff’s  “Wish I was here. ” Will we just have thousands of shows that are just supported by small groups of passionate fans?     I suspect we’ll see a combined approach. Mass market won’t be going away since we all want to stand around the water cooler and chat about the latest episodes but we will start having more and more shows and movies that are catered to our interests and passions. This specialization has been happening throughout the 20th century to our physical products and it’s going to extend to the emotional ones. I don’t know whether it’ll be Netflix, Kickstarter, or some unknown company that’ll make it happen but I do believe it’s inevitable.  Disclosure: I own Netflix stock.",2,1,2013-05-05,5,"netflix, video, kickstarter, media, entertainment",451,"Netflix, bundling, and the future of video"
30,0,"Whenever I learn about a new tool or technology I will do a simple Google search, '{name} vs', to see how the product compares against others in its space.",#meta,{% include setup %} A simple trick I’ve started using when learning about new tools or technologies is to just type in “{name} vs” in Google search and see what pops up. This relies on the wisdom of the crowds and Google’s prediction to give me insight into other items in the same space. Oftentimes going to the corporate site gives me standard marketing copy but looking at comparisons provides the actual details: What do people use it for? What are the competitors? How does it compare against them? What are some success stories? What are some failure cases?  Looking at just the site and reading a few white papers only gives you the positive elements. What’s more important is the knowledge of what the tool or technology doesn’t do. This allows me to actually think about the vision for what I’m building and use that knowledge to anticipate the potential issues. Then by seeing what problems others have had I can determine whether they may be relevant to my problem. Otherwise I may end up building something only to realize that my entire foundation was flawed and needs to be rebuilt.  Of course this isn’t a panacea and it’s impossible to get everything from a Google search but it’s quick and easy enough to not need a second thought. You might not discover anything new but if you do it would have been well worth it.,0,1,2017-02-04,5,"tools, technology, product, comparisons, google autocomplete",238,Identifying product weaknesses using Google autocomplete
21,0,We need to stop looking for short cuts and do the necessary work. Focus on the means not the ends.,#meta,"{% include setup %} Entrepreneurs are familiar with the elevator pitch. The idea is to give a pitch in 30 seconds (the duration of an elevator ride) that is compelling enough to an investor that it leads to a follow up meeting where you can go through your pitch deck. An entrepreneur coming up with an elevator pitch is similar to a politician trying to come up with sound bites that are easily digestible, look good on the news, and stick in people’s minds.  Why are we so intent on diluting our message? So much substance is lost when we simplify and condense. We mock politicians when they speak in sound bites and yet we do the same thing when we pitch investors. We both want to draw attention to ourselves and stay top of mind but why take shortcuts? Investors will come to you if you build a great product, get customers, and generate revenue. Voters will support you if you empathize with them and support their community. We need to stop looking for the easy way out and just do the work, success will follow.",0,1,2013-03-09,5,"elevator pitch, politics, startups, business, success",188,Just do the work
23,0,"Adding new code often feels easier at the moment but it can significantly increase the branching, and thus complexity, of your code.",#code,"{% include setup %} When it comes to software development we often add features simply because it’s easy. And almost always they are - just add an additional optional argument or two to a function and suddenly you’ve expanded your application’s functionality. The catch is that this assumes that this new code is a linear increase in complexity but it’s not.  Computer science has the concept of “[Big O notation](https://en.wikipedia.org/wiki/Big_O_notation)” to measure how a function behaves as a function of it’s input. A “Big O” of O(N) is linear while O(N^2) is quadratic. The implicit goal is that you should strive to write your code to minimize it’s complexity with the ultimate goal being O(1). The same approach can be applied to code complexity. How much will the new functionality affect the complexity of the code? A seemingly simple code change may change the “exponent” of your code’s complexity and a complex code change may actually reduce your code complexity. The code change is not always correlated with the complexity that is being introduced.  [Cyclomatic complexity](https://en.wikipedia.org/wiki/Cyclomatic_complexity) was a concept introduced in the 70s to measure code complexity. It does so by measuring the number of branches in your code. A simple example is to think about a simple “if” statement: just adding a condition adds a new flow to your code which may have further implications down the line. The concept is more than 40 years old and is still incredibly useful in keeping ourselves honest.  Adding a single feature is often easy and so we do it but we don’t consider the fact that dozens of these small changes increase the overall complexity of the code significantly. This makes future code changs more difficult to make, test, and maintain. It’s critical to think of the code you’re writing not purely in terms of the complexity of the code itself but of the complexity it introduces to the the rest of the code base.",2,1,2018-12-15,2,"software engineering, code complexity",328,New code is not a linear increase in complexity
24,0,The NY Times did some very neat visualizations on spotting fake Twitter followers and I ran an open source script to plot mine.,#dataviz,"{% include setup %} Last week, the New York Times [ran an expose](https://www.nytimes.com/interactive/2018/01/27/technology/social-media-bots.html) on the massive amount of follower fraud happening on Twitter. Unsurprisingly, when you can buy tens of thousands of followers for a few thousand dollars it’s not very likely that they’re going to be real. Anyone who has used Twitter for even a nominal amount of time would have quickly discovered that there’s a rampant amount of bots. Some leave cryptic comments, others like and retweet, while others follow; most do all of the above.  One of the cool ways they explored these fake followers is by plotting the growth of followers over time. Each point is a follower with the x-axis showing what number follower they were and the y-axis indicating when they joined Twitter. The idea here being that if you see a stretch of new followers that all joined Twitter at roughly the same time they’re likely bots.  This was a pretty cool way to look at it and one of my friends, [Geoff](https://twitter.com/geoffgolberg), shared an [open source script](https://github.com/elaineo/FollowerFactory) that would pull and plot the data. I ran it for my meager, but amazing, 680 Twitter followers and am happy to report that there’s no obvious fraud pattern.",3,1,2018-02-03,4,"twitter, followers, twitter follower visualization, follower factory",226,My follower factory
29,0,Google's done an incredible job analyzing our photos and making them easily searchable. What happens when every bit of digital content we produce is analyzed and data mined?,#meta,"{% include setup %} Last Friday, Fred Wilson [wrote a post](http://avc.com/2016/05/feature-friday-photo-search/) lauding Google’s photo search. I’ve had the same experiences. In the past couple of months I’ve made numerous searches without expecting a useful result but in nearly every case I was pleasantly surprised. Just in the past week I wanted to search for a short story I wrote while in middle school that I digitized at some point over the past few years. My first attempt was to search for “paper” which got me too many results to parse through. But for my second attempt I tried “essay” and was able to find a photo of one of the hand-written pages. It was simple to look at the date I uploaded that one page to find the others. A couple of days ago I was out of town but needed my passport information to fill out an online government form. Turns out that I have a photo of my passport on my Google account - I backed it up years ago as I was traveling so I had proof in case anything happened to it.  On one hand I’m clearly impressed by how accurate the searches are but does make me worry about how much information we inadvertently share that can be indexed. It’s hugely convenient now but it’s impossible to predict the future and see how it will be used. I wonder how many photos we’re currently sharing that we assume are indiscernible to these automated systems. The vast majority are safe for now but given the pace of technological progress I’ll be shocked if software isn’t more accurate and faster than humans in 20 years. And photos are just a small piece of the puzzle - every bit of digital content we produce will be data mined until it can’t reveal any more. All this will benefit us in the short term but I wonder the world will look like when everything we produce can and will be analyzed and understood by machines.",1,1,2016-05-08,5,"google, photo search, machine learning, ai, google photos",340,Google's photo search is eerily incredible
37,0,As rote work gets more and more automated it's important for people to keep learning and making connections between various fields and topics. This improves our creativity and lets us focus on what we're good at.,#meta,"{% include setup %} I'm a strong believer that one needs to keep learning and to not get content with the knowledge they have. This can come in the form of new experiences or challenges but should be seen as a learning opportunity. Throughout school we have a structure in place to help us learn but after we graduate we have to take the responsibility ourselves. Unfortunately, many people don't and even take pride that they haven't read a book since college.  As more and more of the rote work becomes automated it's important to develop the creative mind set that can take topics from a variety of fields and blend them together to create something new. The irony is that our education and professional systems are becoming more and more specialized. As our knowledge of an area becomes deeper it's harder to be a generalist and we end up focused on only a few areas and skills.  My solution is to read as much as a I can across a variety of fields: fiction and non-fiction, classics and modern fiction, even the occasional textbook. The goal isn't so much to recite everything from memory as it is to plant some remnant thoughts in the back of my mind that may end up being useful in the future. It's the difference between not knowing that there's an answer to a question and knowing that an answer exists. The latter gives you the ability and confidence to find the answer whether the former gives you an exuse to give up. The other benefit of this cross-disciplinary approach is that you get great at identifying patterns. These act as a shortcut and make it easy to absorb new information as well as spot patterns in the wild.  These days it's too easy to get our knowledge bite-sized through the various social networks, articles, and headlines but that information is fleeting. The way to really absorb information is through focus and being able to spend more than a few minutes on a particular topic. Otherwise it's just wasted time and effort.",0,1,2015-09-20,3,"learning, knowledge, automation",352,Keep on learning
27,0,Rather than offloading work to other teams engineering teams should strive to internalize the externalities by supporting business cases as soon as they become technically feasible.,#product,"{% include setup %} Recently I've adopted the practice of having the engineering team support other team when the core technology can support it - even if hasn't been fully built in to the product. This may require manually adding entries to a series of database tables that or manually pulling reports that aren't yet availabe via the UI. Despite being an inefficiency for the engineering team it provides a variety of benefits that outweigh this minor inconvenience.  - Engineers understand the business better. By being closer to the actual use cases engineers understand how the product is used and the problems that other teams are solving. This can have huge wins in the future when there are multiple implementation options available and the developer needs to pick one. Being able to pick the right one can significnatly change the cost of doing future development work.  - Work gets done faster. Engineers love their efficiency and as soon as they end up having to do the same thing twice they'll think of ways to automate it. This is the perfect way of shifting the problem to the person best suited to solve it. A support person may need to come up with a series of inefficient workarounds while an engineer will solve the underlying, core issue.  - Improved quality. Being closer to the actual use case will improve the quality of the code since the developer will know how their code will be used and will understand the options available. The other benefit is that it will become a lot easier to see usability issues as well as improvement opportunities that can be implemented in the future. Bugs will also get caught earlier since the developer will be more likely to monitor the behavior if they were the ones that worked on it. The fact that they used it will provide more motivation to monitor it when it's out in production.  - Prioritization makes sense. Oftentimes it's not obvious why one feature is prioritized over another. Being able to work through some business cases and experience user frustrations is a great way to get a look at the product from a different perspective.  - No workarounds that end up being ""grandfathered"" in. It's common to see users come up with workarounds to support a use case not officially supported. This places a big burden on the engineering team that may end up needing to support it since a particular workflow now depends on it. By exposing the problem to engineers earlier it's more likely that this situation is avoided.  Years ago I read how Kayak has their  customer support phone number get routed  to the engineering team some percentage of the time. The goal is to expose engineers to customer problems with the idea that an engineering can actually solve a problem once it bothers them enough times. I view this as ""internalizing externalities"" - rather than offloading engineering costs to other teams (product managers, QA, customer support) we should address the problems as soon as they arise. This ensures the quality stays high while aligning the engineering team with the rest of the company.",1,1,2015-09-15,4,"engineering, management, product management, code quality",531,Internalizing externalities
27,0,Yesterday I gave a quick talk about tips and tricks to get better as a software engineer - both in the short term and the long term.,"#meta,#management","{% include setup %} Yesterday I had the privilege of giving a talk at [HackReactor](http://www.hackreactor.com/) titled “Things I wish I knew” which was an amalgam of the various themes and topics I’ve been blogging and thinking about. While going through the blog I came up with two themes for the topic - the first was tactics that would make someone a better programmer immediately and the second was how to improve as a developer over time.  ### Short term tips to become a better programmer - **[Generalize at n=3](http://dangoldin.com/2016/04/07/generalize-at-n3/)**. Rather than coming up with the perfect abstract solution right away my rule of thumb is to start thinking about that on your third iteration of solving the same problem. This will ensure you’re solving a problem that will recur while giving you enough perspective to actually develop a useful abstract solution. - **[Think carefully about your database](http://dangoldin.com/2016/02/15/design-your-database-for-flexibility/)**. Compared to changing a database changing code is much simpler. Code is mostly stateless and you don’t need to worry about backfills or migrations. - **[Focus on interfaces, not implementations](http://dangoldin.com/2015/12/02/think-interfaces-not-implementation/)**. Instead of obsessing over the perfect implementation it’s more important to think about how your application works and the way it’s architected. This way you can always change the implementation of a single method or function to make it better without having to gut and rewrite the entire application. - **For dates and times, just use UTC**. A very common refrain online but only worry about timezones when displaying data to users. - **[Use GitHub for documentation](http://dangoldin.com/2016/08/14/integrating-poorly-documented-open-source-libraries/)**. Sometimes Documentation and StackOverflow don’t have exactly what you need. A good resource is to use GitHub’s code search and find actual examples of the relevant code being used.  ### Getting better over time - **[Learn to appreciate DevOps](http://dangoldin.com/2014/12/26/devops-for-the-rest-of-us/)**. Not many people love DevOps but I’m a strong believer in understanding how your code will run and be deployed. It gets you more familiar with the entire lifecycle and allows you to be more creative with your solutions. - **[Have a sample project to learn new languages](http://dangoldin.com/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/)**. In addition to tutorials I like having a project that I will implement in new languages to learn them. This repetition highlights the major differences between the languages and allows me to work on a sample application that mirrors my interests. - **[Approach code like you approach the gym](http://dangoldin.com/2016/03/13/approach-work-like-the-gym/)**. We spend more than 8 hours a day working but imagine if we approached it like we do the gym. Sure people that go to the gym every day without a plan are better off than those who don’t go at all but they pale in comparison to those that go with an agenda. How do we turn every line of code we write into something that’s as focused as a workout? - **Read the classics**. Despite being a relatively young field software engineering has had a ton of great books written and rather than spending time reading blog posts (including this one!) it’s worthwhile to go read the classics.",8,2,2016-11-30,5,"software engineering, software development, coding skills, improving, hackreactor",533,Becoming a better developer
18,0,Coursera can now offer classes for credit and I'm excited by the changes this will bring to education,#meta,"{% include setup %} I was excited to read that Coursera can now offer classes for college credit. I’m optimistic that this is a start of a trend that will change higher education. At first, this will be adopted by the motivated student - the one who took AP classes in high school and the price sensitive student - the one who took community college classes before transferring to a university. But over time, this will spread until it’s the dominant approach. It’s simply better. It may be more expensive to get a digital class together but it’s primarily an upfront cost that will be spent on getting the best lecturers, developing engaging lectures, and creating varied course content that’s optimized for different learning methods. This content can be accessed from anywhere and is always available.  These classes will serve as building blocks for entirely new curriculums and courses of study. Dependencies between classes will be created so if you’re having trouble with a part of one class you can quickly go to another class that covers that topic in more depth. Companies can get involved as well and help bring the costs down. For example, a company can subsidize a classes and in return be allowed to recruit students who score highest on various sections. Companies can even create their own funnels based on the performance over a set of classes. I don’t know if this is the right approach since it will lead to corporate influence in education but I’m excited by the opportunities. This is something prior generations never had and just a shadow of what future generations will have. I'm just glad it's so easy to keep on learning after college.",0,1,2013-02-08,3,"coursera, online education, udacity",284,More thoughts on online education
13,0,A simple analysis to see whether having weekend vs weekday election days matter,#datascience,"I found an  op-ed  in the NY Times that claimed that the best way to increase voter turnout was by having election day fall on a weekend. They provide a few examples but nothing too detailed. I tried pulling in some data and seeing if I could come to the same conclusion. I used two data sets:  voter turn out by country  and  election dates by country .    Combining this data into one table, and ignoring the missing data:          Country   Election Type   Date   Day of week   Weekend   Turnout           Czech Republic   Presidential Final   Fri 2/15/08   6   No   85%       South Korea   Parliamentary      Wed 4/9/08   4   No   75%       Canada   Parliamentary      Tue 10/14/08   3   No   76%       Czech Republic   Parliamentary      Fri 10/17/08   6   No   85%       United States   Presidential      Tue 11/4/08   3   No   54%       Romania   Parliamentary      Fri 11/28/08   6   No   81%       Russia   Presidential      Sun 3/2/08   1   Yes   61%       Malta   Parliamentary      Sat 3/8/08   7   Yes   94%       Spain   Parliamentary      Sun 3/9/08   1   Yes   73%       Italy   Parliamentary      Sun 4/13/08   1   Yes   90%       Iceland   Presidential (Cancelled)      Sat 6/28/08   7   Yes   89%       Austria   Parliamentary      Sun 9/28/08   1   Yes   92%       New Zealand   Parliamentary      Sat 11/8/08   7   Yes   88%         It does seem as if they are on to something - the average turnout for weekday election days was 76% while the average turnout for weekend election days was 84%. This wasn't a very rigorous examination and I am sure there are many more issues that factor in to the voting process but it does make intuitive sense.",3,1,2008-10-24,3,"voting, elections, politics",305,On weekend voting
36,0,We're seeing a conflict playing out between Google and Apple when it comes to user experience vs privacy and I'm glad that Apple is finding ways of keeping data private while improving their AI models.,#product,"{% include setup %} These days it feels as if every tech behemoth is competing with every other tech behemoth but one of the more interesting battles has been between Google and Apple. Google is the accepted leader in ML and AI and they leverage it to offer a better and constantly improving user experience. Apple, on the other hand, has been stressing that unlike Google it doesn’t make money off of data mining your data and is instead focused on privacy.  More data is an incredible advantage when it comes to training AI models and Google is king. We all want privacy but the question is whether the benefit of privacy outweighs the convenience of opening up your data. Every person has a different opinion but as these AI models improve the improved experience will take precedence over the benefits of privacy for more and more people until only a minority grasps to their data.  That’s why I was glad to see [this mention](https://techcrunch.com/2018/06/29/apple-is-rebuilding-maps-from-the-ground-up/) of Apple leveraging iPhone sensors to collect anonymous data snippets that preserve privacy while feeding into the models. The resulting models may not be as great as those built on identifiable information but it’s much better than the alternative.  > The secret sauce here is what Apple calls probe data. Essentially little slices of vector data that represent direction and speed transmitted back to Apple completely anonymized with no way to tie it to a specific user or even any given trip. It’s reaching in and sipping a tiny amount of data from millions of users instead, giving it a holistic, real-time picture without compromising user privacy.  > If you’re driving, walking or cycling, your iPhone can already tell this. Now if it knows you’re driving, it also can send relevant traffic and routing data in these anonymous slivers to improve the entire service. This only happens if your Maps app has been active, say you check the map, look for directions, etc. If you’re actively using your GPS for walking or driving, then the updates are more precise and can help with walking improvements like charting new pedestrian paths through parks — building out the map’s overall quality.  I’m hopeful that this model succeeds and proves that great predictions are possible while relying on anonymized data. More importantly, the fact that the data itself is anonymous may even lead to it being open sourced and help the industry compete against those leveraging proprietary data.",1,1,2018-07-29,4,"google, apple, machine learning, artifical intelligence",420,Privacy vs user experience
15,0,I had a CSV of fantasy footabll stats and used MySQL to normalize it.,"#data,#sql","{% include setup %} As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database. I thought it would be useful to share the commands below and provide some context for those interested in learning more about MySQL and the data import/cleanup process.  The first step is to create the table that we'll be loading the CSV file into {% highlight sql %}create database stats; use database stats;  create table orig_stats (   week int,   name varchar(100),   position varchar(20),   opp varchar(50),   passing_yds float,   passing_tds float,   passing_int float,   rushing_att float,   rushing_yds float,   rushing_tds float,   receiving_tgt float,   receiving_rec float,   receiving_yds float,   receiving_tds float,   return_tds float,   twopt float,   fumbles float,   points float ); {% endhighlight %}  Now we load the CSV file into the table making sure to specify the options properly. In my case this took a few attempts to deal with the line endings. {% highlight sql %}LOAD DATA INFILE '/tmp/stats-2014.csv' INTO TABLE orig_stats FIELDS TERMINATED BY ',' LINES TERMINATED BY '\r\n' IGNORE 1 LINES ; {% endhighlight %}  Next step is to create the tables we want to end up with. In my case I wanted to normalize the data which required designed a new set of tables. A big assumption made here was that a player will not get traded from one team to another. This is definitely not correct in the real world but it is good enough for this exercise. If we wanted to allow for trades we would have a separate table that would map a player to a team by week. {% highlight sql %}create table teams (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(100),   PRIMARY KEY (id),   UNIQUE (name) );  create table positions (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(10),   PRIMARY KEY (id),   UNIQUE (name) );  create table players (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(100),   position_id int,   team_id int,   PRIMARY KEY (id) );  create table schedule (   week int,   home_id int,   away_id int,   UNIQUE (week, home_id, away_id) );  create table stats (   week int,   player_id int,   passing_yds float,   passing_tds float,   passing_int float,   rushing_att float,   rushing_yds float,   rushing_tds float,   receiving_tgt float,   receiving_rec float,   receiving_yds float,   receiving_tds float,   return_tds float,   twopt float,   fumbles float,   points float ); {% endhighlight %}  Now it's on to the hard part. We want to take the data in the original stats table and convert into a properly normalized data set. The strategy here is to start with the simple tables and work our way up to the more complicated ones leveraging the normalized data we created at each step. The first two tables are teams and positions and we can derive them from the ""position"" field in the original stats table by splitting the position field into two and realizing that the left side is the team and the right side is the position of given the player. {% highlight sql %}insert into teams   (name)   select distinct(substring_index(position, ' - ', 1))   from orig_stats order by position;  insert into positions   (name)   select distinct(substring_index(position, ' - ', -1)) as pos   from orig_stats order by pos; {% endhighlight %}  To generate the players table, we get the player position and team from the stats table and then find the associated ids from the teams and positions tables. The key assumption here is that there are no two players with the same name, on the same team, and the same position. {% highlight sql %}insert into players   (name, position_id, team_id)   select p.name, pos.id, t.id   from (     select name, position,       substring_index(position, ' - ', -1) as pos,       substring_index(position, ' - ', 1) as team     from orig_stats     group by name, position, pos, team   ) p   join teams t on t.name = p.team   join positions pos on pos.name = p.pos; {% endhighlight %}  We can figure out the schedule by getting a list of the unique matchups in the original stats table. Since the games are symmetric we only need to look at the rows that are home games. {% highlight sql %}insert into schedule   (week, home_id, away_id)   select s.week, t1.id, t2.id   from (     select week,       substring_index(position, ' - ', 1) as home_team,       substring_index(opp, ' vs ', -1) as away_team     from orig_stats s     where opp like '%vs%'     group by week, home_team, away_team   ) s   join teams t1 on t1.name = s.home_team   join teams t2 on t2.name = s.away_team   order by s.week, t1.id, t2.id; {% endhighlight %}  Putting everything together we generate the new stats table by doing the relevant lookups in the tables we created. We can ignore the redundant fields (name, position, opponent) and the only thing we need to watch out for is duplicate players. In this case there are two names, Zach Miller and Alex Smith, that need to be made ""unique"" by also looking at their team. {% highlight sql %}insert into stats   (week, player_id,   passing_yds, passing_tds, passing_int, rushing_att, rushing_yds, rushing_tds,   receiving_tgt, receiving_rec, receiving_yds, receiving_tds, return_tds,   twopt, fumbles, points)   select s.week, p.id,   passing_yds, passing_tds, passing_int, rushing_att, rushing_yds, rushing_tds,   receiving_tgt, receiving_rec, receiving_yds, receiving_tds, return_tds,   twopt, fumbles, points   from orig_stats s   join teams t on substring_index(s.position, ' - ', 1) = t.name   join players p on s.name = p.name and p.team_id = t.id; {% endhighlight %}",0,2,2014-10-01,2,"mysql, cleaning data",958,Normalizing a CSV file using MySQL
19,0,Craigslist has become the web's fertilizer. Everyone is using it to grow their business - whether legitimately or not.,"#meta,#product","{% include setup %} Craigslist has become the fertilizer of the web. This realization came to me last week when I needed to get a replacement phone and decided to search for one on Craigslist.Filtering past the obvious scams I thought I find a legitimate offer and reached out. Within a few minutes I received the following response from ""Kyle"":   Hi a guy bought this from me, but I can tell you where I got it from. I got 3 of these from http://enetcweb.com/dibzees and I resold them for some extra money. The trick is to watch for bidding to slow down and then put in a bid. That's what I do and I win most of the time.    And this was from a listing that seemed legitimate! The vast majority of listings were clearly fraudulent that promised either amazing deals or was the same posting duplicated a dozen times with slightly different wording. Even beyond the fraud and scams there are probably tons of startups trying to take advantage of the network that Craigslist offers. Some are listing their products and services to validate their market and others are reaching out to owners of various listings trying to sell them on something.  At Makers Alley we posted a variety of products to Craigslist to understand the market. How many people would click through to our site? Would people more interested in buying custom furniture from individual makers or from a brand? Would anyone go through the entire checkout process?  We only received responses to ads that were positioned as independent makers and each of these responses was from another startup trying to get us to sign up for their platform. This feels like a perverse version of  ""The Gift of the Magi""  - startups exchanging services with other startups - without any real consumers benefiting. I'd love to know what percentage of Craigslist is startups interacting with startups without either knowing the identity of the other. I suspect that for some verticals the number is shockingly high. This is also a massive example of how powerful the network effect is - the tiny sliver of value available in Craigslist is still enough to keep it useful.",1,2,2014-04-25,2,"craigslist, business",373,Craigslist: the web's fertilizer
37,0,Doing a group by on a derived column can be dangerous if it has the same name as one of the originals. It's much better to be explicit with the columns and group by you're using.,"#sql,#code","{% include setup %} I had a bit of fun with MySQL earlier this week when trying to explain a non obvious “group by” behavior. It’s fairly common to want to manipulate a field in order to transform it into something more useful. The difficulty arises when you want to keep the original name. Below is some SQL code that highlights the odd behavior.  {% highlight sql %} drop table if exists dan_test;  create table dan_test (   id int not null,   id2 int not null );  insert into dan_test (id, id2) values (1,1), (2,2), (3,3);  select * from dan_test;  select id, case when id = 1 then 2 else id end as id, id2 from dan_test;  select id, sum(id2) from dan_test group by id;  select case when id = 1 then 2 else id end as id, sum(id2) from dan_test group by id;  select case when id = 1 then 2 else id end as new_id, sum(id2) from dan_test group by new_id; {% endhighlight sql %}  With the second to last query it’s not obvious which id field the group by is referring to: the original from the table or the derived field? It turns out it’s the original field which can cause problems if you’re unaware of this subtlety. There are a few different ways to deal with this situation, including grouping by the derivation formula, but my favorite is to use a brand new field as in the last example above.",0,2,2015-11-21,2,"mysql, query issues",244,More MySQL fun
22,0,Google offering free Adwords credits to businesses might have some pretty large effects on the auction marketplace and drive bids up.,#meta,"{% include setup %} Since we’re using Google Apps for Business for our startup, we’ve been getting a bunch of emails trying to get us to sign up for Adwords. The latest promotion is offering a credit of $300 if we spend $100. It’s a pretty common marketing tactic and tons of companies have similar promotions. What’s special about Google is that they’re running an auction for every single click and by giving some businesses free money, they’re driving the prices up for the entire market.  Without being at Google, it’s impossible to know what effect this has but I tried to do some estimates. According to the  NY Times , Google had 4M businesses on Google Apps at the end of 2011 and by June of 2012 it was up to 5M. So over the first 6 months of 2012, Google Apps gained 1M businesses. If each of those businesses was offered $300 a credit and took Google up on it, there would be an extra $300M in the Adwords market provided by Google, in addition to that business’s Adwords contribution. If we compare this against Google’s advertising revenue during the first  two quarters  of 2012, $20,750M, we get 1.4%. That may not seem that big but think of every bid on Google Adwords increasing by 1%. Of course, not every business took Google up on the offer so the true number is going to be lower but I’d bet even then it’s still enough to affect the market.  I was trying to think of an analogy but had trouble. I initially thought this is similar to a casino giving some of the poker players free money but then realized that the majority of that money will still end up in the hands of players. I then thought it would be akin to ebay offering free credit to some buyers, but once again, most of the value would still captured by the marketplace participants; either by sellers getting higher prices or by buyers being able to buy more or better items. I finally settled on the idea of an amusement park offering free passes to random people. Those people end up making the park more crowded for the visitors who paid yet still buy concessions and the amusement park profits.  I suppose this is what happens when the dominant company in a market full of near-zero marginal cost products runs a promotion. I don’t know whether Google has a responsibility to their existing businesses to keep the auction fair but I know if I were advertising on Google I wouldn’t be too happy with Google giving free credits to my competitors.",2,1,2013-05-15,5,"Google, Adwords, auction, bidding, PPC",461,Google's “free” Adwords credits
23,0,Earlier today I gave a talk at the DataEngConf and wanted to share my slides for those that couldn't make it out.,#data,{% include setup %} I had the privilege of giving a talk today at DataEngConf. Unfortunately the talk was not recorded but you can grab the slides [here]({{DATA_PATH }}/DataEngCof_NYC_Data_Startups_Dan_Goldin-Scaling-a-Data-Pipeline-Mystery-to-Mastery.pdf). The theme was going over the evolution of the TripleLift data pipeline from the early days where we were sampling events on the client side to the current iteration of a fully fleshed out Lambda architecture. Take a look at the slides and if you have any questions I’d be glad to answer them in the comments.,1,1,2017-10-30,3,"dataengconf, scaling a data pipeline, adtech data",95,Slides from my talk at DataEngConf
19,0,Getting a new router and having to update network settings highlighted how many network devices we actually have.,#meta,"{% include setup %} Benedict Evans wrote a [great post](https://www.ben-evans.com/benedictevans/2014/5/26/the-internet-of-things) back in 2014 that started with the observation that while our grandparents knew how many motors they owned we have no idea and that while we know how many of our devices connect to the internet our children will not.  It sounded nice at the time but I lived it today. Verizon sent me a new router and rather than taking the small amount of time to change the wifi network name and password back to the previous version I decided it wouldn’t be that hard to just update the wifi settings in the various devices. After I updated my phone, computer, and Fire TV Stick I realized I still had many more to go and decided, albeit too late, that it would be easier to just revert the settings. What makes changing the settings difficult is that updating the wifi settings on devices without a keyboard is just incredibly difficult. For some, such as the Fire TV Stick, you have to type using a remote and an on-screen keyboard. For others, such as the Echo, you have to switch back and forth between wifi networks to get everything set up.  I can’t imagine what it will be like when we actually have hundreds of devices that all need to connect to a wifi network and the settings change. There will have to be a way to make the network settings easier to manage and I wonder what that will be.",1,1,2018-12-04,2,"wifi networks, devices",259,How many wifi devices do we have?
29,0,Some people are making the case that WeWork deserves it's massive valuation and I just don't see it becoming the foundational layer the way AWS and Stripe have.,#product,"{% include setup %} I have been subscribed to [Stratechery](https://stratechery.com/) for almost a year now but have recently started listening to the [Exponent podcasts](http://exponent.fm/). One of them, titled [Pickaxe Retailers](http://exponent.fm/episode-071-pickaxe-retailers-2/), makes the case that WeWork has an appropriate valuation due to their ability to leverage their strong brand and become the utility layer for real estate as well as provide a slew of products to their tenants. Similar to the way AWS has eliminated the need to run your own data center and Stripe has eliminated the need to acquire merchant accounts and negotiate with vendors, WeWork may do the same for physical space - both commercial and [residential](http://www.fastcompany.com/3055325/from-wework-to-welive-company-moves-members-into-its-first-residential-building).  While the explanation is reasonable it’s tough for me to buy into it. The decision to use any product boils down to how easy is it to switch and what’s the cost/revenue potential. In the case of AWS it’s incredibly costly to switch. You have to incur the cost of updating your code and deployment to make sure it will run on a new platform, retraining your team, and if you plan on switching to your own datacenter then hiring for roles you’ve never had to deal with. Added to this you have Amazon constantly cutting costs while innovating on new products. The value in switching only comes at massive scale - even Dropbox is getting beaten up over their move away from Amazon instead of focusing on building a more compelling product.  Stripe is in a similar situation. Despite providing a seemingly simple service it’s difficult and costly to replace. Stripe contains customer data and has a slew of products, for example subscriptions, that make it tough to switch. Imagine having to ask every customer to re-enter their credit card information. At the same time, Stripe gets cheaper and cheaper as your volume increases which makes it less and less compelling to replace.  I just don’t see these sort of arguments holding true for WeWork. AWS and Stripe both run services that start of cheap and get even cheaper as you scale. They’re both unbelievably sticky and have a growing cost of switching. WeWork has neither of these. The actual office space may be great but over time it gets easier to make the decision to rent your space directly rather than pay WeWork’s margins. WeWork does provide additional services that make their space great when you’re small. Unfortunately, these same services can easily be outsourced as you grow. For unlimited coffee you go with [Joyride](http://www.joyridecoffeedistributors.com/service/page/cold-brew-iced-coffee-kegerators-coffee-kegs/). For office cleaning and maintenance you go with [Managed by Q](https://managedbyq.com/). For food you can go with one of the hundreds of delivery startups. Every service WeWork provides can be had via a separate company..  Picture a small company starting out on AWS, using Stripe, and renting an office at WeWork. As they grow it’s easy to imagine them still using AWS, still using Stripe, but no longer at WeWork. Netflix is the perfect example. They’re a public company with a current market cap of just under $41 billion. Yet they’re still on AWS. And Amazon is a competitor! I can’t imagine any public company using WeWork as their primary office space solution.",6,1,2016-06-18,6,"aws, stripe, wework, coworking space, scale, business",558,"AWS, Stripe, and WeWork"
29,0,Recently I've been thinking about the optimizations I've made when working on my computer. They're tiny on their own but can add up to a ton of time.,#meta,"{% include setup %} A pretty trivial post but something I’ve been doing for a while now is keeping my dock as a vertical bar on the right of my screen. I started doing this years ago when I was working on Windows and it was too difficult to track every single program that was running. At that point I was in finance and would have a dozen Excel workbooks open and needed to be able to quickly switch between them. The only way I could do this with a bottom toolbar was by making it extremely thick which would take up too much space. Moving it to the side solved that problem and I stuck with it as I moved to Ubuntu and now OS X.  These days I have significantly fewer programs running at once and I’m much better at hopping between programs but it’s surprising what a good dock location can do for efficiency. Getting everything right only saves a fraction of a second but doing this countless times a day adds up. As important as tools are, the way they are access and used is just as important. The challenge is getting stuck with a suboptimal routine that you’re used to and not moving to a more optimal one since the short term cost is high. The greatest example of this is probably the  Dvorak keyboard layout  - in theory it’s significantly faster than QWERTY and yet virtually everyone is using the slower QWERTY approach. It’s just good enough.",1,1,2014-11-13,4,"productivity, dvorak, keyboard, docks",257,Productivity optimization
17,0,I have a bunch of memories of growing up with computers and wanted to share them.,#meta,"{% include setup %} Inspired by yesterday’s post I decided to compile a list the memories I’ve had growing up with computers - hopefully they spark others. I tried to keep these in chronological order but I made some mistakes.  - The increase in the number of colors on a monitor and the various *GAs (CGA, EGA, VGA, XVGA) - Booting of one floppy disk and then running programs off another - Having both a 3.5 inch and a 5.25 inch floppy disk drives - Playing Prince of Persia for the first time - Using Norton Commander rather than the DOS command line - Shareware and the tons of paper catalogues selling games by mail - Upgrading to a 486 DX2 - Installing Windows 3.1 from a ton of floppy disks - The wonderful blue screen of death - Getting a second phone line in order to use dial up - Getting AOL instant messenger and my first screenname - Upgrading to a 56k modem - Using NetZero as an ISP - Hosting my first website at Geocities (wish I knew where it was and could dig it up) - Lycos and AltaVista - Finally getting a cable modem - Learning Pascal and C++ in high school - Warcraft 2, Starcraft, Diablo, Total Annihilation, Shattered Galaxy and LAN parties - Using Google for the first time - Signing up for the Gmail beta - Getting my first smartphone, a Motorola Droid I  Since then, I’ve had a ton of experiences but they feel incremental. I guess being steeped in tech for so long gives that perception.",0,1,2014-04-13,3,"computers, technology, history",248,Some computer memories
17,0,"Tools are essential to developer productivity and tools for static, strongly typed languages are significantly better.","#meta,#java","{% include setup %} Good tools are essential for developer productivity. Imagine how tough it would be to write code in an editor that didn’t have any of the features we use on a daily basis - syntax highlighting, smart spacing, shortcut keys, auto completion, etc. It takes time to get used to all the tools available but once we’re familiar with them we’re orders of magnitude more productive.  For the past year my primary language has been Java although I’ve gotten to do a fair amount of JavaScript, Python, and PHP as well. As great as Sublime is, I’m much more productive in Eclipse. It has nothing to do with the editor and everything to do with the language. Eclipse is able to provide a lot more functionality due to Java’s static, strongly type nature. Some examples are being able to quickly rename variables and methods, move packages, and quickly identify dumb mistakes in method signatures and typos. I suspect similar tools exist for weakly typed or dynamic languages but I can’t imagine them working as well as they do in Eclipse. Strongly typed and static languages are able to get rid of an entire class of errors that are common with scripting languages - typos, forgetting to add an argument to a method call, messing up a type - that the time saved typing gets replaced with the time spent debugging. For many tasks this tradeoff makes sense but larger projects that involve multiple developers and require higher performance would benefit from moving to a strongly typed, static language.  I’m learning Scala which seems to combine the flexibility and expressiveness of Python with the safety and performance of Java. So far I’m cautiously optimistic but we’ll see where I am in a month.",0,2,2015-03-10,5,"strong typed, weakly typed, static, dynamic, programming language",293,"Strongly typed, static language tools"
23,0,Using AWS to scale is great but make sure to be aware of your service limits or you might be caught unaware.,#devops,{% include setup %}            Something I haven’t seen mentioned much is that AWS has  service limits . The only way to find out that you’re hitting one is when an instance fails to launch with the error message “Your quota allows for 0 more running instance(s)” with a link to open a support ticket and request a higher limit.  This is a serious problem when you’re at the instance limit and depend on auto scaling for high loads. The instance limit prevents you from scaling to meet the demand and the only reasonable option is to file a support ticket and hope someone is able to get to it in time. The doomsday option is to temporarily shut down secondary or tertiary instances to make room for the critical ones.  I understand why Amazon chose to implement it but I wish they had better support around messaging when you’re close to the instance limit or whether your current setup can push you over. There’s nowhere in the UI where you can see the limit but luckily there’s a simple command to do it via the  command line interface  - aws ec2 describe-account-attributes --region=us-east-1.  If you’re running a quickly growing stack you should make sure to monitor your service limits so you’re not caught unaware. Also make sure to monitor the limits per region and within a VPC as they each have their own.,2,1,2015-04-26,4,"aws, devops, service limits, cloud computing",270,AWS service limits
33,0,For the first time I saw contextual behavior in the Google Hangouts app when my wife asked me where I was. The app gave me the helpful option of sharing my location.,#meta,"{% include setup %}            Yesterday I got a reminder of how deep smartphones and tech companies have gotten into our lives. After spending a day volunteering at the [C4Q](http://www.c4q.nyc/) office, I got a text from my wife asking me where I was. When I opened the Hangouts app I saw an option to share my current location. This is the first time I’ve seen a contextual behavior in Hangouts. I’ve seen it before in other apps - a Gmail alert telling me I forgot to attach a file when the text has “please find attached” or Google Calendar defaulting to a weekly repetition if I put “weekly” in the meeting title - but this is the first time I’ve seen it happen in Hangouts. Basic contextual behavior is relatively simple to support and can just require a simple word search but it has incredible potential as more and more data gets collected. Our smartphones are with us wherever we go collecting data each step of the way. Right now the behavior is formulaic and standardized but soon enough our phones will act as personal assistants - keeping track of everything in our calendars while understanding everything we have going on. This has the potential to drastically simplify our lives but we may be making a Faustian bargain in the process.",1,1,2015-02-16,5,"google, hangouts, smartphones, technology, privacy",237,Smarterphones
16,0,I just wanted to share my thoughts on the Microsoft walking away from acquiring Yahoo.,#product,"A little unexpected but I think this was the right decision for Microsoft for a couple of reasons:  1. Jerry Yang was doing everything in his power to prevent MS from acquiring Yahoo - what type of message does that send and how must the employees of Yahoo feel if their CEO is acting this way. 2. In order to compete with Google, the merger must have gotten done quickly and smoothly, this would not have been the case in the case of a hostile takeover. 3. There have been rumors that many of Yahoo's employees were just waiting for the merger to happen in order to cash in on their new accelerated vesting and compensation packages, immediately leaving Yahoo there after. 4. A lot of the Yahoo employees and MSFT employees and investors did not want the merger to go through. From what I know the largest group that wanted the merger to go through were the Yahoo investors, albeit at a higher price. 5. Yahoo will now have to deal with a variety of problems: shareholder lawsuits, talk of Jerry's management ability, sudden price drop. All these may in fact lead to another Microsoft offer in maybe a few months at a much lower price - in which case it will probably be accepted.  The only downside that just immediately to my mind is that MS and Yahoo are in deep water and do need to do something in order to compete with Google - but I do not think the merger would have necessarily helped them, given the merging difficulties and the corporate culture clash.",0,1,2008-05-04,3,"microsoft, yahoo, acquisition",266,On Microsoft walking away from Yahoo
29,0,Amazon has a great concept with VPCs but unfortunately they don't work across regions. There's a simple workaround here that allows you to secure your cross region communication.,"#devops,#code","{% include setup %} As great as AWS is there’s still a major gap in the way cross-region support are handled. It’s boggling that there’s no single screen to see every one of your instances and you’re forced to do it a region at a time. Beyond the cosmetic it’s not-obvious how to get instances from multiple regions to communicate securely with one another. On one hand Amazon has the neat concept of a Virtual Private Cloud (VPC) that allows you to create a group of machines that act as if they’re on the same network. This makes it simple come up with some pretty neat security rules - for example only allowing for an instance to communicate with the outside world via port 80 but with its network on other ports. Using a combination of VPCs and security groups one can come up with a pretty intricate security system.  Unfortunately this doesn’t work across regions. Instead, Amazon [suggests setting up a VPN](https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/) that can bridge the two networks. This makes sense if you have a dedicated DevOps team and the scale necessary to support this but I was looking for a much simpler solution. All I wanted was to have a few instances (powered by OpsWorks) that were scattered across the world be able to communicate securely with the primary system that was hosted in the US East region. After a minor back and forth with the AWS support team we were able to come up with the following, somewhat elegant, solution. Note that I contributed minimally here and almost all the credit should go to AWS support.  The solution was to update the setup recipe of our instances outside the US East region to whitelist themselves on the relevant security group (code below). In theory it’s simple but there were a few small gotchas that needed to be handled:  - Deregistration. Since there’s a maximum number of rules that a security group may have it’s important to also add a shutdown recipe that removes the instance from the security group. - Permissions. It turns out that every instance that starts up within OpsWorks has an IAM role which needs to be updated to support the AuthorizeSecurityGroupIngress and RevokeSecurityGroupIngress actions. - Stack settings. While possible to hardcode the relevant ids and values into the OpsWorks recipe it makes a lot more sense to make this dynamic and driven by the Stack settings.  I wish Amazon supported VPCs that could span multiple regions but it turned out that this workaround wasn’t as difficult as I thought. There’s still the risk that this approach will fail when we hit the security group rule limit or the API calls fail but by then I’m hopeful there’s a legitimate solution.  {% highlight ruby %} # Authorize require 'aws-sdk' ec2 = AWS::EC2::Client.new(region: 'us-east-1') resp = ec2.authorize_security_group_ingress({   group_id: 'sg-####',   ip_permissions: [     {       ip_protocol: ""tcp"",       from_port: 443,       to_port: 443,       ip_ranges: [         {           cidr_ip: ""#{node[""opsworks""][""instance""][""ip""]}/32"",         },       ],     },   ], })  # Revoke require 'aws-sdk' ec2 = AWS::EC2::Client.new(region: 'us-east-1') resp = ec2.revoke_security_group_ingress({   group_id: 'sg-####',   ip_permissions: [     {       ip_protocol: ""tcp"",       from_port: 443,       to_port: 443,       ip_ranges: [         {           cidr_ip: ""#{node[""opsworks""][""instance""][""ip""]}/32"",         },       ],     },   ], }) {% endhighlight %}",1,2,2017-05-04,5,"aws, security groups, virtual private clouds, cross regions, vpc",545,Security across multiple AWS regions
38,0,Going to the gym every day will get you stronger and stronger but having a workout plan will make it even better. Now apply that philosophy to work where we spend more than 8 hours a day.,#management,"{% include setup %} If you go to the gym you end up getting a lot more out of it if you approach your workout with a plan in mind. The same thing happens with work. If you go every day you will inevitably get better but if you come in with concrete goals and ways to push yourself you’ll be in a much better position. It’s not as easy to measure your performance at work compared to the gym but just taking the first step and realizing that you want to improve is already beyond how most people approach work. Just by thinking about your performance you improve your ability to identify your strengths and weaknesses. Doing this on a consistent basis gets you into the habit of being introspective and improves your self-awareness, which is necessary to improve.  I’ve approached this problem by jotting down notes and thoughts throughout the day and then having a non-disruptive, scheduled time each week for me to go through and digest it. Some weeks are slower than others but more often than not I gain valuable insight on what I did well and what I can improve. Writing these notes has become a habit and I don’t even need to focus on it anymore. The value of these is that over time you end up having a log of your improvement and can acknowledge how much you’ve grown as well as how much is still left. I’ve also discovered themes from week to week that have helped me improve as an engineer and a manager. If you haven’t been approaching work the way you approach the gym you’re losing valuable time and opportunity to improve yourself. You’re already spending at least 40 hours a week at the office so you should make it as valuable as you can.",0,1,2015-12-20,4,"progress, self improvement, self awareness, working out",309,Have a work “workout” plan
35,0,One of my favorite economics book is by Mancur Olson and discusses the effect special interest groups have on countries. Given all the change we're seeing in the world it's useful to revisit it.,#society,"{% include setup %} I studied economics in college but only encountered Mancur Olson years later when I picked up “[The Rise and Decline of Nations](https://www.amazon.com/Rise-Decline-Nations-Stagflation-Rigidities/dp/0300030797)” at a bookstore years later. It proposes a simple theory that explains why some countries succeed while others fail - as a country maintains growth and stability it ends up developing more and entrenched interests that hobble future growth. Olson uses a variety of historical economic examples to prove this point - ranging from the British empire to Germany and Japan post World War II - that make a compelling case. This also ties in nicely with the sharing economy since it ties into the idea of regulatory capture - where government agencies are lobbied by special interest groups to introduce laws that defend their market position. This is currently playing out with the sharing economy - the taxi and limo lobbies are fighting Uber while hotels are challenging AirBnB. Both Uber and to a slightly lesser degree bring significant value to society but spend money on lawyers and lobbyists that could be spent on pushing the economy and world forward.  Another simple but only obvious after you hear it point is that small groups are able to achieve significantly more than large groups. Since the value to an individual in a small group is significant that one person would be willing to fight and invest to make something happen. The cost, on the other hand, is distributed across an order of magnitude more people so each person has very little incentive to challenge it. I haven’t read the book in a while but I think the example he uses is of the farming lobby. Farmers have a very strong incentive to pressure the government for farm subsidies and are willing to vast both time and money to make sure their representatives push for it. To them the value of this legislation may be thousands of dollars and they’re willing to spend nearly that much to see it through. Everyone else is hurt by this but to them the cost is pennies - this leads to the effect that small groups are able to drive more change than larger ones.  With the economy undergoing significant changes in the coming years it’s critical to understand what’s happening and Mancur Olson provides a wonderful set of theories to help understand the change.",1,1,2017-02-13,3,"mancur olson, political economy, sharing economy",397,Mancur Olson and the changing economy
28,0,I recently replaced Evernote with Sublime Text and it's been great. I get the full power of the Linux shell and avoid getting locked in to Evernote.,#meta,"{% include setup %} Although there’s been a ton of services launching trying to help people do everything under the sun I’ve been finding myself going back to simple tools. One of these has been replacing Evernote with text files that are synced via Dropbox after getting annoyed with Evernote one too many times. It’s great, I edit files in  Sublime Text  and came up with my own naming format to make search easier. If that fails, I just use grep and find and almost always find what I’m looking for. Since the files are plain text, every Linux command is a tool. It’s trivial to do bulk search/replace using sed or compose one liners to do various filters and counts. It’s easy to sort files by time or size and being able to do a regex search comes in handy when you only have a vague idea of what words you used when writing a note.  I wish more services would approach their products the same way - only focus on one thing, do it well, and allow other services to integrate with it in a simple way. This belief has been the foundation of the  Unix philosophy  but sadly most businesses haven’t embraced it due to a desire to encourage lock in and increase switching costs. Hopefully this changes in the future.",2,1,2014-06-08,3,"productivity, evernote, sublime text",232,Replacing Evernote
25,0,Brendan Eich launched a new browser that's taking a new approach to advertsing. It's definitely faster but it's going to be a tough journey.,#product,"{% include setup %} Trying to launch a new browser seems like a fool’s errand and yet if there’s anyone that can do it it’s [Brendan Eich](https://en.wikipedia.org/wiki/Brendan_Eich), who in addition to creating JavaScript also ran Mozilla. Given his pedigree I decided to give his new browser, [Brave](https://brave.com/), a shot. It’s definitely a bit on the rough side compared to the mainstream browsers but it’s surprisingly fast. The speed improvement comes from a built in adblocker rather than having it implemented via slower browser extensions. At the same time Brave wants to pay publishers for their content by partnering with higher quality advertisers in order to serve benevolent ads that should also be priced at a premium.  The difficulty with this approach is that tracking users is what makes the advertising so valuable. Being able to track users allows advertisers to see what users care about, their purchase intent, as well as a whole slew of demographic information based on their consumption behavior. Eliminating this will cause advertisers to be shooting in the dark. There’s a reason Google and Facebook are eating up nearly 80% of every advertising dollar - they’re leveraging their data to provide extremely targeted and effective advertising that will be tough to do without the ability to track users.  I can see the case that Brave can centralize the tracking and allow users to opt into sharing this data with various partners. The challenge is getting advertisers on board with this as they would have to trust Brave for their reporting and getting users to opt in to this. I know very few people who’ve used adblock and then decided to switch back to a full ad experience. Brave has a tough road ahead.",2,1,2016-06-23,4,"brave, adblock, browsers, Brendan Eich",290,The Brave browser
22,0,What if instead of spending our times staring at our phones during our commute we spent it talking to one another?,#meta,"{% include setup %} Every day I take the subway to and from work and can’t help but think how that time could be better spent. Some people are browsing through photos, others are playing games, while others are just staring off into space listening to their headphones, while the last few are just asleep. All these are individual actions and can be done at any time but the subway is a forced melting pot and feels like a missed opportunity.  What if instead of being in their own world everyone decided to turn it into some sort of social event - imagine meeting someone who can help professionally, listening to someone explain a complex topic, or if you’re lucky and willing to risk it - participating in a civil debate. There are dozens of ways to leverage a couple of dozen people stuck in the same enclosed area for 30 minutes. Instead we’re all spending our time doing things that can be done when we’re home alone.  We’ll see if I have the courage to actually try to do something like this. It’s something that will likely go horribly wrong but the upside seems worth it.",0,1,2018-02-09,4,"subway, commute, productivity, networking",196,Making the most of the subway commute
10,0,Amazon continues their world domination by acquiring Whole Foods.,#meta,"{% include setup %} Earlier today Amazon announced that it is acquiring Whole Foods. I can’t count how many different lines of business Amazon has but this is yet another step in its quest to capture every consumer dollar spent. Unsurprisingly, people spend the most on [necessities](https://www.bls.gov/news.release/cesan.nr0.htm) which is basically food and housing followed by clothing and transportation. In 2015, a whopping $600B was [spent at grocery stores](https://www.statista.com/topics/1660/food-retail/) in the US - and this doesn’t include eating or drinking out. Amazon has massive ambitions and getting into these categories is a necessity. Amazon has already been getting into food and clothing but this is a huge step into the brick and mortar world which will open up a ton of possibilities.  There have been a lot of interesting observations about the value this acquisition brings Amazon. These range from giving Amazon distribution locations in urban areas to doubling down on grocery delivery with AmazonFresh. I like to think that the fact that anyone can come up with a justification is proof enough that the acquisition makes sense. This is especially the case given Amazon’s history of jumping into adjacent markets and businesses. Relentlessly executing in one area gives Amazon the ability to leverage that experience and infrastructure to attempt something new (Brad Stone does an incredible job of covering this in [The Everything Store](https://www.amazon.com/Everything-Store-Jeff-Bezos-Amazon-ebook/dp/B00BWQW73E)). Amazon started with books, introduced other goods, then after figuring out their logistics expanded this to third party sellers. And in their spare time they somehow managed to build AWS. No one but Amazon knows how Whole Foods fits into Amazon’s vision but I wouldn’t be surprised if this leads to even more Amazon domination.",3,1,2017-06-16,4,"amazon, whole foods, ecommerce, business",294,The Amazon juggernaut
18,0,I have a small script that analyzes Jira in order to help measure squad and sprint efficiency.,#code,"{% include setup %} A few days ago I [wrote](/2017/12/01/measuring-sprint-efficiency/) about using average number of sprints to complete a story as a way to measure a team’s sprint efficiency. Unfortunately at that time I had a pretty hacky [Jira analysis script](https://github.com/dangoldin/automating-management/blob/master/jira-analysis.py) that I was too ashamed to share but it has been cleaned up enough for me to not feel too much guilt. It’s available on GitHub and comes with a few additional bells and whistles. One is specific to the way we work where we label relevant stories with a priority (priority:1, priority:2, etc) based on our planning process so we can hold ourselves accountable to spending the appropriate time on our initiatives. The other is a simple way that calculates the story points done by assignee. It’s a dangerous metric to use since story points are variable and not all work is measured via story points but it’s yet another metric that can help highlight or sharpen a potential issue.",2,1,2017-12-07,3,"jira, developer efficiency, developer productivity",169,Jira analysis script
28,0,"Data pipelines are typically complicated and tough to build and maintain. There's a simple, serverless implementation that can be done using AWS's Elastic Load Balancers and Lambda.","#code,#data","{% include setup %} Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.  An idea I’ve been toying with is a “poor man’s data pipeline” that could be built in a serverless way and can scale to massive volumes. It turns out that a pretty simple data pipeline can be built using two AWS services: Elastic Load Balancer (ELB) and Lambda. This data pipeline doesn’t have the true streaming that Kafka provides but for simple aggregations and a tolerable 5 minute delay it’s extremely cheap and robust.  The way it works is by setting up an Elastic Load Balancer with [access logs enabled](http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html) but not actually associating it with any EC2 instances. Then every time an event is generated it would just be making a request to the ELB with the data specified as query string parameters. Note that this is extremely cheap since at this point we’re not paying for any computer and are just paying for traffic to the load balancer. Note that this is a hack since it will cause every single response to have a 503 status code but can be easily remedied with an simple web service that does no actual processing and responds with a simple 204.  Once we have access logs enabled we set up a Lambda function that gets executed every time a new S3 log file is generated. This lambda function downloads the S3 file and rolls it up via a custom function which can then be setup to export the resulting data wherever it needs to go. Note that at the moment Lambda still has a series of [limits](http://docs.aws.amazon.com/lambda/latest/dg/limits.html) that may prevent this from working at incredibly high volumes but even then one can set up Lambda to make a simple HTTP request to an external service with the log file path which can then be processed.  The [code](https://github.com/dangoldin/poor-mans-data-pipeline) is short and sweet and is up on GitHub along with a guide on getting started. If you have any questions or suggestions I’d love to hear them.",3,2,2016-11-12,4,"data pipeline, aws, lambda, elastic load balancer",435,A poor man's data pipeline
15,0,Stratechery published a great post describing the state of the tech world in 2018.,#meta,"{% include setup %} I’m a big fan of the [Stratechery](https://stratechery.com/) newsletter and eagerly read every article that comes out. This past week a post came out that gave a wonderful description of the [state of tech in 2018](https://stratechery.com/2018/the-state-of-technology-at-the-end-of-2018/):  > This, then, is the state of technology in 2018: the enterprise market is thriving, and the consumer market is stagnant, dominated by the “innovations” that a few large behemoths deign to develop for consumers (probably by ripping off a smaller company). Meanwhile a backlash is brewing on both sides of the political spectrum, but with no immediately viable outlet through competition or antitrust action, the politics surrounding technology simply becomes ever more rancid.  This felt like such a potent and accurate summary of 2018 that I wanted to repot it here. It does feel as if there’s no room for consumer apps anymore and yet we’re seeing a ton of competition in the enterprise. This is the opposite of the world 15 years ago when the enterprise world felt static and the consumer world was alive. I like to think that the tech world goes in cycles and in another 15 years (hopefully sooner) we’ll be in a renaissance of consumer tech.",2,1,2018-12-14,3,"startups, technology, stratechery",213,State of tech in 2018
22,0,I finally tried an electric scooter in Mexico City and it was awesome. Wish they could come to New York City.,#product,{% include setup %} I live in the NYC area which has Citi Bike but doesn’t allow any scooter companies to operate. In fact it actually disallows any electric bikes/scooters/skateboards/hoverboards and only makes an exception for pedal-assisted bikes. And since I haven’t been traveling much the closest I could get to trying an electric scooter was reading the endless series of articles about them.  Luckily for me I went on a vacation to Mexico City and got the chance to try a Lime scooter. It was great. I use Citi Bike whenever I can in NYC so I didn’t expect the scooters to be that much different but it felt much more liberating with significantly less friction. The big difference is that there is no docking station which is always a concern in NYC: a station may be empty when I’m looking for a bike or a station is full and I need to dock a bike. The electric powered also makes it much easier to just hop on and go. I don’t mind the Citi Bike pedaling but the scooter only needed a small kick-start and then quickly accelerated to over 14 mph. The scooter was also much smaller and lighter than a Citi Bike which added to the feeling.  I read some articles complaining about the clutter introduced by these scooters but didn’t feel any of that. Sure there were random scooters in random places but it never felt overwhelming and actually made me appreciate them more knowing that they’re so easily accessible. I’m not surprised that they haven’t made their way into NYC yet but hope they’re able to in the future.,0,1,2018-12-18,3,"electric scooter, cities, citi bike",278,I finally tried an electric scooter
28,0,The inbox is becoming a critical piece of our workflows and we need to have better standards to add functionality rather than relying on 3rd party apps.,#product,"{% include setup %} There are only a few tabs I consistently keep open all day on my computer - Gmail, Google Calendar, Hacker News, and New Relic. Out of these, Gmail is the most important with my entire day running through it. The value of having a presence in the inbox hasn't been lost on companies and there are a ton of third party apps that make Gmail more useful -  Rapportive ,  YesWare ,  ToutApp , and  Boomerang . Even Google itself has been providing ""Lab features"" to augment the default inbox behavior.  One thing that bothers me is that this additional functionality is only provided via browser extensions. The only time I recall seeing an interactive behavior is when a form is embedded in an email and even then it's at the mercy of the  email client's implementation . I'd love to see a new inbox standard adopted that allowed email messages to be richer and more interactive rather than having to rely on a separate browser extension. Imagine being able to send out surveys that can be completed without leaving an email, emails that are able to show whether a product is available in real time (I know this can be done via server side rendered images but it's a hack), or being able to change the content text based on the time the email is viewed. These are trivial examples but this would open up potential for uses that we can’t even imagine. Of course, we’d have to be even more wary of smarter spam and inbox trickery but the potential value is worth that cost.",5,1,2013-07-27,6,"gmail, inbox, rapportive, yesware, toutapp, boomerang",289,The power inbox
16,0,I recently finished Bruce Schneier's Liars and Outliers and wanted to write a quick review.,#books,"{% include setup %}            I’ve been a fan of Bruce Schneier ever since I read his  post about security theater  in the post 9/11 world. As soon as I discovered that he wrote a book,  Liars and Outliers , I added it to my to-read list and just finished reading it over the weekend. It’s one of those books that is obvious as you read it but spawns a ton of thoughts. He develops a framework that he uses to analyze security and trust in individuals, organizations, and differently-sized societies.  Trust is the foundation that’s allowing the world to become faster paced and interconnected. We’re interacting with people all across the globe, our organizations and businesses are larger than ever, and we’re more dependent on technology than ever. Modern life depends on these complex trust systems and Schneier does a great job explaining the various interactions and the impact technology is having. As others have said, the 21st century will be about data and the rise of social networks, wearable computers, and the quantified self movement are an indicator of the type of data that will be collected. We need to make sure proper systems are in place to prevent abuse and Liars and Outliers provides a great framework to think about these issues and prepare us for the data century.",2,1,2013-02-26,4,"bruce schneier, security, trust, liars and outliers",256,Book Review: Liars and Outliers
26,0,One of the best ways to onboard as an engineer is to understand the database and how various tables and fields relate to one another.,#meta,{% include setup %} I’m convinced that the best way to ramp up as a newly hired engineer is to go through the database. Rather than relying on outdated documentation or discovering undocumented features the database is the actual source of truth and defines both the limits and the capabilities of the application. You can examine the relationships between the various objects as well as the litany of features and options that are supported. It’s definitely more difficult to get up to speed on a database rather than documentation or a demo of the UI but the knowledge gained is significantly deeper. Especially when you’re going to be working on features that depend on the database it’s incredibly useful to know how the database is laid out and set up. On its own a walk through of the UI provides a high level overview of how it works but coupling that with the database allows you to internalize the connections and actually understand how the user interactions feed the data and vice versa.  One of the most interesting benefits is seeing the progression of a company’s products and features. A typical database will contain a litany of fields and tables that stick around after the underlying feature or product is antiquated. These features have no documentation and the only way to know what they’re used for is to spend hours going through version control history or talk to someone who was actually around. Despite these fields and tables no longer being used they’re valuable for the context they provide. Seeing the evolution of a product allows you to identify what worked and what didn’t work and serve as as a springboard for new ideas.  Databases are rarely part of an engineer’s onboarding process and are mostly on a “need to know basis.” Only when you’re working on a particular feature do you have to understand the relevant schema and even then you’re not expected to go beyond what you’re working on. This is the wrong approach and there’s a ton of implicit knowledge in our databases that make the entire team more productive. Coupling this with a walkthrough of the UI and the API is a great way to learn and relate the various concepts.,0,1,2015-07-25,3,"engineering, management, onboarding",377,Use the database Luke
34,0,We use the OpenStates API to download bills from various states and compare them against each other in order to find similar language that will indicate bills written under a 3rd party influence.,"#datascience,#dataviz,#code","{% include setup %} This past weekend I participated in the  Bicoastal Datafest  hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn't be very easy, especially in two days, but I did meet  Llewellyn , a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.  We ended up with the following approach that we were able to code up during the weekend: 1. Use the OpenStates API to get the URL of the bills 2. Download the bills and convert each to raw text - from PDF and HTML 3. Extract 8 word phrases from each bill, excluding stopwords 4. See which phrases were duplicated across states 5. Examine the duplicate phrases to see which bills are most likely duplicates  Somewhat surprisingly, this approach led us to discover the following duplicate bills:   Firearms Freedom Acts   Shared the phrase:  manufactured without inclusion significant parts imported another state                                Indiana                                                        Michigan                                 Prohibit US government officials from enforcing firearm-related acts   Shared the phrase:  accessory ammunition owned manufactured commercially privately state remains                                Arizona                                                        Tennessee                                 Prevent pharmaceutical substitution of opioid drugs   Shared the phrase:  bear labeling claim respect reduction tampering abuse abuse                                New York                                                        New Jersey                                The code's up on  Github  so if you have any ideas or improvements - contribute and help out. In two days we were able to get something useful done and it's exciting to see what we can discover if we stick with it.",9,3,2013-02-05,5,"bicoastal datafest, hackathon, politics, laws, natural language processing",543,Identifying duplicate bills across states
30,0,Part of being a manager is doing a lot of rote tasks that help improve the team's productivity. I've spent some time automating it with a variety of scripts.,#management,{% include setup %} One of the biggest lessons I learned when I became an engineering manager was how important the basic operational elements. These are all the things that need to get done outside of code and allow the whole team to be as productive as possible and range from reminding people to do code reviews to creating dashboards to highlight key metrics to enforcing an on-call process. These tasks are important yet repetitive so being a good engineer I’ve spent some time automating them. There’s still a long way to go but strong engineers have a mindset that they want to automate as much repetitive work as possible in order to focus on unique and novel challenges.  This attitude can be applied to management as well. By automating the menial stuff you’re able to focus on the tasks that require a human touch. Nearly every product geared towards developers exposes some sort of API which can be used to automate most rote work. The approach I’ve taken so far is extracting data from Redmine and GitHub via their APIs and exposing the results in a simple dashboard powered by [freeboard](https://github.com/Freeboard/freeboard) as well as on Slack. Since every company has a unique setup with their own set of tools and processes it’s difficult to come up with a universal solution but modern day tools make it incredibly easy to get started with some sort of automation.,1,1,2016-09-18,2,"engineering management, automation",240,Automating management
16,0,AWS is constantly shipping new products and features and it's incredible how productive they are.,#product,"{% include setup %}           I never thought about this until [Shaun](https://twitter.com/szach) brought it up but now I see it all the time: AWS ships code quicker than any other company that size. Some are simple feature improvements to existing products, such as an advanced instance search, additional configuration options for ELBs, and new instance types, but others are entirely new products, such as Lambda, EC2 Container Service, and the newly announced code management suite.  What’s even more impressive is that they provide infrastructure to thousands of other companies and the cost of failure is massive. A breaking feature will affect thousands of companies and cost a ton of money and hurt their reputation. Yet they keep on shipping and launching as if they were a newly launched startup.  I’d love to know the process Amazon has in place that drives these releases - both how they’re able to code at such scale as well as the test process they use to make sure no existing functionality breaks due to new features.  The half serious joke here is that Amazon is always shipping - physical items from Amazon.com and digital services from AWS.",1,1,2014-11-24,2,"aws, product",210,AWS: Always shipping
37,0,"I felt crappy after a cold so went for a walk to clear my head. Thinking about nothing inspired me to come up with 3 resolutions that have made me feel happier, productive, and more inspired.",#meta,"{% include setup %} This weekend I felt discouraged after getting cold but dragged myself outside to go for a walk and clear my head. I ended up getting a cup of coffee and just sat on a bench for 30 minutes and letting random thoughts go in and out of my head. It turned into a form of personal meditation where some ideas crystallized and stuck around while others quickly disappeared. It felt great (so great that I ended up going back to my apartment and doing a bunch of chores I’ve been putting of!), especially considering that I had to drag myself away from the couch to even go for the walk and would have much preferred to just sit on the couch and watch football.  While sitting on the bench I made three resolutions that are making me feel more inspired and productive. They’re all simple and only require commitment but I’m definitely benefiting from them.      Stop procrastinating . We all have a ton of things to do and I got into the habit of delaying simple ones like dropping off clothes for dry cleaning or selling some old electronics on eBay. Lately I’ve started going through my list of todos and it feels great. Each small achievement is something that I can cross off and encourages me to move on to the next one. For most of these priority doesn’t even matter since the important stuff happens anyway; it’s the small items that tend to get stuck in a state of never done.    Focus on one thing at a time . This is the hardest one for me but I realized that I tend to get distracted too easily. Whether it’s checking my email or browsing Hacker News it’s a huge inefficiency. I’ve also found myself trying to do work passively while watching a football game and it’s noticeable how little I get done that there’s no point in even attempting to work. It’s not possible to achieve  flow  without being able to concentrate on one thing and quality suffers. It’s not even a good use of time since you’ll just end up taking a longer time to do two things poorly. It’s better off to get the stuff that requires your attention out of the way and then spend the rest of the time relaxing.    Take distraction free breaks . Given how useful going for a aimless walk was I decided to make this a habit and try to go for at least three a week. So far I tried going before work but it wasn’t the same. Either the subconscious knowledge that I needed to track my time in order to get to the office or being in a highly trafficked city intersection made it harder to get lost in my thoughts. In any case I might end up making this a once-a-week activity and do it on the weekends in a quieter place where I can be distraction free.     I’d love to know of other resolutions that people have embraced to make them feel happier, inspired, and productive so if you have any ideas definitely let me know.",1,1,2014-10-08,4,"inspiration, productivity, happiness, depression",534,Clearing my head
19,0,"More often than not web application performance problems can be solved by digging into, and optimizing, the queries.",#meta,"{% include setup %} The title is clearly tongue in cheek but a very large number of web application issues can be solved through better SQL. Web applications performance problems are either UI or API related. And if the performance is on the API side it's likely due to slow queries. The majority of endpoints are doing some form of database interaction and many are making multiple database calls. It only takes one of these to be slow to make the entire application feel sluggish - especially if you have lots of concurrent requests to this endpoint. This often happens when you rely on an ORM to manage your database reads and writes without understanding the queries that are actually being made. An ORM makes it much easier to get started but to get the best performance you need to go lower level and write your own queries. The way to dig into these is to look at the slow query log which gives you (at least in MySQL) both the query, the duration, and the amount of rows examined and snet. Sorting by the query duration gives you a the worst performing queries and then it’s up to you to address them. Sometimes it’s adding a few indices to a table. Other times it’s rewriting the query to be more efficient. Even more rarely it highlights a weakness in the application logic that requires rethinking a particular approach.",0,1,2018-12-24,2,"sql, web performnace",239,Solve all web app performance problems with SQL
36,0,"One behavior I always found frustrating on smartphones is the way clicking in app links opens up a new window. I recently discovered a mobile browser, Javelin, that has a great solution to this problem.",#product,{% include setup %} Something that’s bothered me ever since I started using a smartphone is the link opening behavior. Whenever I’m in an app and click on a web link it would immediately open up that page in a browser window. And when I’m already in a mobile browser and click on a link it would open that page up in a new tab. Compare this with the desktop environment. Clicking on a link within an app does open up a new browser window immediately but since there are shortcuts to quickly switch between programs it’s not a huge deal. And when I’m already looking at a webpage and want to open a new link it’s possible to open it in the background using command+click.  The most common reason I click on a link is as a bookmark so that I can go through it after I’m done with what I’m currently doing. The default behavior is the opposite of that - it turns something that I want to consume asynchronously into a synchronous process. Sure there are times where I do want to switch gears but the vast majority of the time opening a new window is a distraction. That’s why it’s so surprising that mobile web browsers have adopted this behavior. Smartphones are both slower than desktops and make it more difficult to switch between programs. Why couldn’t the default behavior be to open all new links in the background?  I finally found a browser that lets me do just that. It’s called  Javelin  and has a feature called “Stacks” that lets you click links while allowing you to continue using the phone. These links are loaded in the background and are ready to be consumed whenever you want to go through them by clicking on a little overlaid icon. This is quickly becoming one of my favorite new apps since switching to Android and I’m hopeful we’ll see other desktop behaviors translated into a mobile-friendly versions. I think this is where Android has an edge by providing a more open environment for developers to work with. With that openness you do end up with more crap but also more gold.,1,1,2014-09-27,4,"browsers, mobile, javelin, android",368,Mobile web browsing and Javelin
17,0,I finally got AMP images working in RSS by not using GitHub Pages for Jekyll generation.,#meta,"{% include setup %} Last week’s post highlighting my victory getting AMP working in RSS was a bit premature since it turns out my solution only worked locally. While being powered by Jekyll, GitHub Pages doesn’t support custom plugins which I was using to replace the “amp-img” tags with “img” when generating the XML feed. So while my approached worked when generating the blog locally it silently ignored my custom template tags when pushed to GitHub.  GitHub support was incredibly helpful here and provided me the exact steps I needed to get it working. Turns out that you can configure GitHub Pages to disable the Jekyll build and just serve content that’s within the “docs” folder of your repo. After adding the built directory to the repo and updating the settings everything’s working great.  Ironically, after reading Alex Kras’s [post on disabling AMP](https://www.alexkras.com/i-decided-to-disable-amp-on-my-site/) I’m tempted to do the same. The blog is pretty minimal and there’s no real reason for me to use AMP other than my curiosity. It’s significantly faster than what I had before but I’m sure if I take the time I can replace AMP with something much better and easier to maintain. But then all the wonderful hacks I had to do to get AMP working disappear.",1,1,2017-06-26,4,"github, amp, github pages, rss",222,RSS finally fixed
20,0,Unsubscribing from a mailing list and getting a message that it may take some time makes me cry inside.,#meta,{% include setup %}      We live in a world of miraculous technology and yet it “may take up to 10 days” for me to unsubscribe from a mailing list. I have no clue what needs to happen to remove my email from a mailing list but it feels as if they’re using the [Pony Express](https://en.wikipedia.org/wiki/Pony_Express). If someone built a system that takes 10 days to deactivate an email address whoever built that system should not be writing any code. Modern systems are able to deal with billions of events a second and support millions of concurrent users but somehow updating a field in a database takes up to 10 days?  Of course it’s not just the code and there are business reasons for this but I can’t get over the fact that unsubscribing from a mailing list takes that long. I suspect the actual reasons are that they want to open up the tiny possibility that I may get an email over the next few days that get me resubscribe and that they may have a variety of systems that have my email address that would need to be purged. In any case it’s a bit ridiculous that it takes that long. Every time I unsubscribe and see a message that indicates it’s not immediate I cry inside.,1,1,2017-06-24,3,"unsubscribe, spam, email",238,Unsubscribe and wait 10 days
31,0,To innovate companies need to make short term decisions that increase their optionality in the future. Making the right decision now can set you up for huge success later on.,"#product,#meta","{% include setup %} Last weekend I finished the [The Everything Store](http://www.amazon.com/The-Everything-Store-Bezos-Amazon-ebook/dp/B00BWQW73E), which details the rise of Amazon from a fledgling online book retailer to its current form. One pattern that stood out for me was how Amazon was able to continuously push into new business areas due to the infrastructure that they had in place based on previous decisions and commitments.  They started with books but were able to grow into other smaller products once they figured out the logistics behind shipping smaller items. Once Amazon had that in place they kept tweaking their distribution system to expand the variety of products offered while improving the speed of delivery. This allowed them to keep amassing a list of products which they used to open up their platform to third party sellers. And as Amazon improved their infrastructure they were able to open that up to these third party sellers as well. In parallel, they built AWS to provide computer services to internal Amazon teams but were able to turn it into a brand new line of business that powers the majority of new startups. And now their are rumors of Amazon building out a shipping service to bypass FedEx and UPS.  Ben Thompson coined this the ""[ladder-up strategy](https://stratechery.com/2016/snapchats-ladder/)"" and I’d argue it’s the only way companies can keep consistently growing. Relentlessly focusing on a few things and then using them to attack adjacent markets is how you grow from a struggling startup to a powerhouse. The challenge is making the short term decisions that set you up for success in the future as well as knowing when to leverage that infrastructure to move into the next thing. The former is incredibly difficult - it requires thinking beyond the immediate step and understanding the opportunities that become available after a successful execution of the initial step. Then follow up by thinking of what the next strategic step will be and what doors that will open up. After a few iterations of this exercise you may get a glimpse of your company’s future. But the ideas are the easy part - the execution is an order of magnitude more difficult. Scenarios will consistently come up that necessitate changing your approach but each change will pull you further and further away from your initial vision. You then need to either steer the company back towards the original direction or adapt your plan based on these new directions.  It’s incredible seeing this successfully execute though. Companies that are able to do this consistently increase the size of their moat and become nearly impossible to dislodge. Each of their activities complements and reinforces the others which when coupled with their benefits of scale grant them monopoly-like powers.",2,2,2016-04-03,4,"laddering up, business strategy, ben thompson, innovation",463,Ben Thompson's “laddering up” and building bigger moats
27,0,I have a simple command that looks at my history and gives me the most frequent commands. It's interesting to compare this data against previous years.,#dataviz,{% include setup %} In what has become an annual tradition I have a very simple shell script that generates a frequency of my most commonly run shell commands. This year saw a pretty big change from 2017. The most obvious difference is that I use “git” more frequently than in the past. This is a tough one to analyze by looking at the data since my usage of [oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh) skews the data. It provides a variety of git aliases - for example gp for git push and gco for git checkout - that appear elsewhere in the results so my pure use of “git” is almost isolated to the cases where I do a commit.  The other big difference is my aggressive use of the AWS CLI. I have seen my role shift to less of a coder and more in support of DevOps so this is reasonable. I’ve also made an effort to minimize my use of the AWS Console in favor of having quick and handy commands that give me quicker access to the information I need.  This was also the year I finally embraced Docker. Before this year I had never used docker I’ve finally embraced docker. Before 2018 I had never used Docker and now use it on a semi-regular basis.  A few other interesting observations come from looking at the commands that only appeared in 2018. The first is “siege” which was a small load testing program I was messing around with earlier this year. The second was “[rg](https://github.com/BurntSushi/ripgrep)” which while less powerful than grep does the simple searches much much faster.  This is an interesting exercise and I’d encourage others to start doing this as well. It really does give you a sense of how your terminal usage has changed and how it aligns with industry trends.      Command  2018 Count  2017 Count  2014 Count  2018 Pct  2017 Pct  2014 Pct      git  1403  581  347  16.39%  7.55%  41.26%    aws  907  325  0  10.60%  4.22%  0.00%    curl  525  139  0  6.13%  1.81%  0.00%    cd  466  608  49  5.44%  7.90%  5.83%    gco  447  75  0  5.22%  0.97%  0.00%    ls  316  415  103  3.69%  5.39%  12.25%    grep  278  87  4  3.25%  1.13%  0.48%    ssh  247  62  28  2.89%  0.81%  3.33%    python  242  354  89  2.83%  4.60%  10.58%    cat  226  74  28  2.64%  0.96%  3.33%    rm  203  61  15  2.37%  0.79%  1.78%    history  165  35  5  1.93%  0.45%  0.59%    less  159  85  0  1.86%  1.10%  0.00%    docker  151  0  0  1.76%  0.00%  0.00%    mv  139  49  4  1.62%  0.64%  0.48%    rake  131  52  15  1.53%  0.68%  1.78%    mediumify  125  41  0  1.46%  0.53%  0.00%    brew  117  49  5  1.37%  0.64%  0.59%    emacs  116  101  22  1.36%  1.31%  2.62%    sudo  107  61  9  1.25%  0.79%  1.07%    find  102  80  3  1.19%  1.04%  0.36%    pip  93  68  14  1.09%  0.88%  1.66%    mkdir  85  35  2  0.99%  0.45%  0.24%    docker-compose  80  65  0  0.93%  0.84%  0.00%    cp  78  37  2  0.91%  0.48%  0.24%    siege  72  0  0  0.84%  0.00%  0.00%    npm  66  241  0  0.77%  3.13%  0.00%    gd  59  421  0  0.69%  5.47%  0.00%    pbpaste  54  26  0  0.63%  0.34%  0.00%    gb  53  32  0  0.62%  0.42%  0.00%    rg  46  0  0  0.54%  0.00%  0.00%    ping  46  16  23  0.54%  0.21%  2.73%    open  45  42  3  0.53%  0.55%  0.36%    pwd  41  438  12  0.48%  5.69%  1.43%    export  39  4  0  0.46%  0.05%  0.00%    python3  38  0  0  0.44%  0.00%  0.00%    gst  36  795  0  0.42%  10.32%  0.00%    mkvirtualenv  33  16  3  0.39%  0.21%  0.36%    alias  32  16  0  0.37%  0.21%  0.00%    mongo  31  0  0  0.36%  0.00%  0.00%    echo  30  15  2  0.35%  0.19%  0.24%    gl  29  525  0  0.34%  6.82%  0.00%    yarn  28  0  0  0.33%  0.00%  0.00%    wc  27  9  7  0.32%  0.12%  0.83%    touch  27  11  0  0.32%  0.14%  0.00%    jekyll  27  112  12  0.32%  1.45%  1.43%    ./tf-wrapper  25  0  0  0.29%  0.00%  0.00%    gp  24  370  0  0.28%  4.81%  0.00%    code  24  297  0  0.28%  3.86%  0.00%    man  23  12  0  0.27%  0.16%  0.00%    join  23  7  0  0.27%  0.09%  0.00%    sh  22  0  4  0.26%  0.00%  0.48%    gbda  21  304  0  0.25%  3.95%  0.00%    ffmpeg  20  7  0  0.23%  0.09%  0.00%    workon  19  54  7  0.22%  0.70%  0.83%    go  19  47  0  0.22%  0.61%  0.00%    du  19  18  0  0.22%  0.23%  0.00%    wrk  18  0  0  0.21%  0.00%  0.00%    scp  18  13  0  0.21%  0.17%  0.00%    gradle  18  46  0  0.21%  0.60%  0.00%    diff  18  17  0  0.21%  0.22%  0.00%    ci  18  0  0  0.21%  0.00%  0.00%    ps  17  7  0  0.20%  0.09%  0.00%    php  17  3  0  0.20%  0.04%  0.00%    kubectl  16  0  0  0.19%  0.00%  0.00%    cut  16  7  0  0.19%  0.09%  0.00%    chmod  16  5  0  0.19%  0.06%  0.00%    ab  16  0  0  0.19%  0.00%  0.00%    ./gradlew  16  0  0  0.19%  0.00%  0.00%    g_pass  15  22  0  0.18%  0.29%  0.00%    flask  15  0  0  0.18%  0.00%  0.00%    tar  14  0  0  0.16%  0.00%  0.00%    airflow  14  5  0  0.16%  0.06%  0.00%    nvm  13  0  0  0.15%  0.00%  0.00%    kill  12  0  0  0.14%  0.00%  0.00%    code-insiders  12  0  0  0.14%  0.00%  0.00%    which  11  11  0  0.13%  0.14%  0.00%    n  11  13  0  0.13%  0.17%  0.00%    yolk  10  0  0  0.12%  0.00%  0.00%    route  10  0  0  0.12%  0.00%  0.00%    netstat  10  0  0  0.12%  0.00%  0.00%    nslookup  9  0  0  0.11%  0.00%  0.00%    wordcloud_cli.py  8  0  0  0.09%  0.00%  0.00%    unzip  8  6  0  0.09%  0.08%  0.00%    mail  8  14  0  0.09%  0.18%  0.00%    hub  8  42  0  0.09%  0.55%  0.00%    head  8  0  5  0.09%  0.00%  0.59%    datalab  8  0  0  0.09%  0.00%  0.00%    cdblog  8  62  14  0.09%  0.81%  1.66%    traceroute  7  4  0  0.08%  0.05%  0.00%    telnet  7  0  0  0.08%  0.00%  0.00%    split  7  0  0  0.08%  0.00%  0.00%    s3  7  0  0  0.08%  0.00%  0.00%    gzip  7  0  0  0.08%  0.00%  0.00%    gcloud  7  0  0  0.08%  0.00%  0.00%    date  7  0  0  0.08%  0.00%  0.00%    zkCli  6  0  0  0.07%  0.00%  0.00%    source  6  0  2  0.07%  0.00%  0.24%    sendEmail  6  6  0  0.07%  0.08%  0.00%    node  6  25  0  0.07%  0.32%  0.00%    jupyter  6  3  0  0.07%  0.04%  0.00%    gunzip  6  0  0  0.07%  0.00%  0.00%    ansible-vault  6  0  0  0.07%  0.00%  0.00%    ./ngrok  6  0  0  0.07%  0.00%  0.00%    terraform  5  0  0  0.06%  0.00%  0.00%    protoc  5  7  0  0.06%  0.09%  0.00%    mvn  5  0  0  0.06%  0.00%  0.00%    heptio-authenticator-aws  5  0  0  0.06%  0.00%  0.00%    ansible-playbook  5  0  0  0.06%  0.00%  0.00%    wget  4  0  0  0.05%  0.00%  0.00%    serverless  4  0  0  0.05%  0.00%  0.00%    sass  4  0  3  0.05%  0.00%  0.36%    ruby  4  8  0  0.05%  0.10%  0.00%,2,1,2018-11-28,2,"shell history, terminal",667,Shell history: 2018 edition
29,0,The Kindle is great but it doesn't sync highlights for ebooks outside of Amazon's store. This is frustrating when trying to help leave notes for a friend's book.,#meta,"{% include setup %} I love to read so it took me a surprisingly long time to get a Kindle. Before then I felt fine either just grabbing a physical book or using a tablet or a phone. LCD displays never bothered me so I figured I might as well get the responsiveness and additional functionality of a tablet rather than a single-use device. But earlier this year I spent some time using my wife’s Kindle and loved the form factor as well as the battery life. I also started to buy a lot more ebooks so finally took the plunge and got myself a Kindle.  If you’re not an avid reader you can get away with a tablet or phone. But if you enjoy reading the Kindle is great and makes it easy to fall into a reading addiction. I’m a bit odd in that I will refuse to write in a book; I remember having a half dozen SAT practice books and rather than circling the choices I would do it in a separate notebook. I don’t know how this habit stemmed but I won’t do any mutilation of the book, including folding a corner to save a spot. I like to think I value the sanctity of a book but I’m sure it’s due to a habit I picked up as a kid.  With a Kindle I don’t have this aversion and get a kick out of highlighting interesting passages or just words I don’t know. I’m also reviewing the draft of a friend’s book and this made it incredibly easy to take notes. Unfortunately, since the book was a draft and was not purchased through Amazon my highlights aren’t accessible through the website which makes it difficult to actually go through and flesh them out.  This seems like an opportunity for Amazon to improve the experience. It may be an anti-piracy decision which offers an inferior experience to pirated books but this seems misguided and ruins the experience for the majority in order to penalize the minority. Amazon is dominant in ebooks and digital publishing and has already won the space; they should be doing everything they can to encourage authors to write and a big part of that is giving them an easy way to get feedback on their drafts.",0,1,2016-11-06,3,"amazon kindle, writing, ebooks",388,Restricted highlighting on the Amazon Kindle
23,0,"It's ridiculous that we have homeless people in the US when it costs $25,000 to keep someone in prison for a year.",#meta,"I find it absurd that the average prisoner costs $25,000 a year to keep in prison ($75,000 for death row) and yet we still have homelessness in the United States. Each one of the homeless may start committing some type of crime in order to get into prison and at least not worry about where their next meal comes from yet they are staying on the streets as free men. Isn't there something we can do to encourage good behavior instead of encouraging bad behavior?  Furthermore, a large percentage of the homeless are veterans and should not be ignored.",0,1,2008-05-04,2,"homelessness, prison",100,The homeless in the US
38,0,I have a shirt that's taken a beating over the years but I love the fit. If only there was a service I could send it to in order to get more shirts with the same fit.,#product,"{% include setup %} Despite being an early tech adopter I avoid buying clothes online. At first the justification was that I didn’t want to deal with the cost of returns but these days nearly every retailer offers free returns. At this point it’s more habit than anything else and given how infrequently I buy clothes I’d rather just do it in person when necessary.  The one service that I wish existed, and I’m sure it does somewhere, is a place where I can send some existing clothes that I know fit me perfectly and then have them used as the basis for new, custom made clothes. I have a great dress shirt that I’ve had for many years and it shows - there’s a giant rip on the elbow and the colors are starting to fade yet I love the fit. I refuse to throw it out and instead keep rolling up the sleeves to hide the tear. If I could get the same shirt in a variety of styles and colors but with the same perfect fit I’d be glad to throw it out.  This must be possible somewhere and I know there’s a market for custom made shirts but this feels like something that can be done much cheaper and at a much grander scale. Rather than try to measure yourself or try on a handful of shirts it’s much easier to send a shirt that fits and receive others that have the same fit. If there are any companies doing this I’d love to give them a shot.",0,1,2017-12-25,3,"clothes, ecommerce, buying clothes",262,"Send a shirt, get a shirt"
38,0,A Lambda architecture consists of both a batch and real time component yet they're implemented differently. The logic is the same and it should be moved further upstream in order to be kept in a single place.,#code,"{% include setup %} Lately I’ve been thinking about the [Lambda architecture](https://en.wikipedia.org/wiki/Lambda_architecture) used in modern data pipelines. Lambda architectures are designed for systems that contain massive amounts of streaming data that needs to be processed and exposed quickly. The architecture consists of two different systems. One is  a real time pipeline that’s not perfectly accurate but is able to handle large volumes while providing a solid estimate quickly. The other is a batch process that is accurate but runs on a delay. By combining the two you get the best of both worlds - accurate historical data and reasonably correct recent data that will be corrected by the batch job when it runs.  A simple adtech example is to think of the events that are generated during a real time bidding auction. We start with an ad request which consists of everything an ad buyer would need to know before buying an ad - including the time, user agent, and location of the user. The buyer then submits a bid containing the ad they want to display along with the price they are willing to pay. If they win, the ad is rendered and there may be some follow up engagement events by the user - a mouseover and maybe even a click.  We can think of these 4 events as a sort of funnel - we have an ad request which may have a win event which may then have a mouse and then finally a click. The challenge is that there may be hundreds of these events being generated each second and it’s extremely rare that we would have all four events to join together. The likely case is that there was either a single ad request or an ad request followed by a win event. How do you build a system that’s able to handle non matched events that may arrive in random order?  In a batch system it’s straightforward - conceptually you’re doing a series of left joins while increasing the time window to make sure you capture events that may have trickled in after a cutoff. So in the case of us processing an hour’s worth of data we may want to pull in more than an hours worth of wins, mouseovers, and clicks to make sure we capture everything. The real time approach is similar but subtly different. First, we need to use a much smaller window since we can’t keep hundreds of millions of event in memory. Second, we need to build in logic to take into account the fact that the events may arrive in different order.  As engineers it’s our jobs to write code and logic that’s as reusable as possible and the Lambda architecture provides an interesting example of how difficult this can be. The batch and real time systems are doing very similar things yet the code to do each ends up being different. Something I’d love to see is some way to move the logic itself further upstream that defines the way these events should fit together and then the relevant code is generated for each subsystem. This would allow the actual join logic to be kept in a single place which would make it incredibly easy to add new events and fields as necessary.",1,1,2017-08-25,5,"lambda architecture, adtech, real time data, batch data, streaming data",545,A unified Lambda architecture
36,0,An idea to reduce costs and improve uptime is to use S3 for all static content and have a quick client side JS call to determine what to use when. Dynamic generation isn't always necessary.,#devops,{% include setup %} An approach to scaling sites that I haven’t seen used much is using S3 as much as possible and falling back to it in case the dynamic elements are either not needed or unavailable. Many sites will host their static assets on S3 but there’s a lot more that can be pushed that way.  Reddit gives logged out users  cached content  rather than dynamically generating a page. That way logged in users get the full experience but logged out users may see a slightly out of date site. Content rich sites would benefit significantly from this approach - it would reduce cost and ensure uptime. If it turns out that the site does go down you can flip a switch and serve the cached/static content to everyone while the site is brought back up.  Current frameworks allow you to cache various elements of a page so they don’t need to be regenerated every time but they’re still dependent on the web server. If that goes down the page won’t be generated. An interesting idea might be to use client side JS to make a quick request to a server to see if it’s up and if not fall back to an HTML file on S3. I don’t know any sites that take this approach and would love to see some examples.,1,1,2014-05-10,3,"s3, scalability, javascript",238,Site down? Fall back to S3
19,0,It's important to load test our applications but it's surprising how difficult it is to do it right.,#devops,"{% include setup %} I started writing this post about using Siege to do load testing but got carried away and ended up discovering how much I don’t know. In particular, I ended up stumbling unto Gil Tene’s talk on measuring latency and how nearly every tool gets it wrong due to the bias in the tools themselves. The general idea is that most tools measure service time rather than request time. Service time is how long it takes your application to handle a request while request time is the time it takes for the user to receive a response. The former is from the perspective of the application but the latter is from the perspective of the caller.  A good way to think about it is to imagine your application can handle 10 requests a second but also comes with an infinite request queue. If the queue is empty 10 requests will be handled immediately, otherwise the queue will start to fill up. Service time will measure how long it takes your application to handle a task once it’s off the queue but the request time is measured from the time the task was put on the queue - with no load they’re the same but as soon as load increases the request time blows up.  It’s a fascinating talk and if you’re at all interested in load testing your applications it’s well worth watching:",0,1,2018-04-12,3,"load testing, coordinated ommission, gil tene",243,Load testing
21,0,I used to have a ton of cynicism when I was younger but am now feeling a lot more idealistic.,#meta,"{% include setup %} As I've gotten older and most likely more mature, I've become far less cynical. I used to be dismissive of people trying to improve things and believed that they were just wasting their time and nothing would change. Yet as a I've gotten older I've come to appreciate this effort even if it doesn't lead to noticeable progress.  The fact that someone is working for their beliefs should be applauded. The waste is dismissing others’ work while sitting in front of a computer or a TV. We all want to see progress and yet we exert effort belittling others that are actually committed to making things better. If we applied this effort into our own passions we’d be all be much better off.  Teddy Roosevelt said this better than I ever could:   It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.",0,1,2013-10-12,1,,277,Decline of cynicism
18,0,Fab raised over $330M but is selling for $15. Just a reminder of how difficult startups are.,#meta,"{% include setup %} The recent news that PCH is set to acquire Fab reiterates how difficult startups are. So many startups strive to get an investment and believe that once they raise a round everything will get easier. That’s when things get difficult. Instead of focusing on achieving product market fit you start worrying about market share, competition, company culture, recruiting, process, which require a completely different skillset than what you started with. And to add to that you’re now accountable to a growing list of employees, shareholders, and customers. When I was working on my first startup I really thought that being able to get funding was the measure of success. Now I realize how naive that view was and how much more there actually is. Fab raised over $330M and wasn’t able to grow into a successful business despite undertaking massive pivots. There  are probably thousands of founders claiming that they’d be able to succeed with that kind of money without realizing how difficult it actually is.",0,1,2014-11-22,2,"fab, startups",171,Fab's fall from grace and the difficulty of startups
24,0,"Most guides on deploying Django only cover the individual packages. I wanted to share the way I deploy Django using Nginx, Virtualenv, and Supervisor","#python,#code","{% include setup %} After yet another attempt to deploy a  Django  application I decided to document the steps required to get everything up and running. The tutorials I’ve seen tend to focus on individual pieces rather than on the way all these packages work together which always led to me a lot of dead ends and StackOverflow so this will hopefully address some of those issues.  In particular, I want to focus on the configuration rather than the installation of the various packages since that’s covered in the package documentation.  I don't know if this is the best way to deploy Django but it's the approach I've been able to come up with by stumbling around and getting help from the docs, Google, and StackOverflow. If there are better ways out there please let me know.    	  		  Gunicorn - /home/ubuntu/project/scripts/start.sh  		 The nice thing here is that we define the port to serve our application on so we can serve multiple projects on a single server with each one using a different port. Note that the settings approach used here is from  Two Scoops of Django . 		  {% highlight bash %} #!/bin/bash set -e DJANGODIR=/home/ubuntu/project DJANGO_SETTINGS_MODULE=project.settings.prod  LOGFILE=/var/log/gunicorn/guni-project.log LOGDIR=$(dirname $LOGFILE) NUM_WORKERS=3 # user/group to run as USER=ubuntu GROUP=ubuntu cd /home/ubuntu/project source /home/ubuntu/project/venv/bin/activate  export DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE export PYTHONPATH=$DJANGODIR:$PYTHONPATH  test -d $LOGDIR || mkdir -p $LOGDIR exec /home/ubuntu/project/venv/bin/gunicorn_django -w $NUM_WORKERS \   --user=$USER --group=$GROUP --log-level=debug \   --log-file=$LOGFILE -b 0.0.0.0:8001 2>>$LOGFILE{% endhighlight %}      	  Nginx  - /etc/nginx/sites-enabled/project  	 The key parts here are that we're redirecting all www.project.com requests to project.com, serving the static files using Nginx rather than rely on Gunicorn, and passing other requests to the Gunicorn server running on the port defined in the Gunicorn start script above. 	  {% highlight nginx %} server {     # Redirect all www.project.com requests to project.com     listen 80;     server_name www.project.com;     return 301 http://project.com$request_uri; }  server {     listen   80;     server_name project.com;     # no security problem here, since / is alway passed to upstream     root /home/ubuntu/project/;     # serve directly - analogous for static/staticfiles     location /media/ {         # if asset versioning is used         if ($query_string) {             expires max;         }     }     location /admin/media/ {         # this changes depending on your python version         root /home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib;     }     location /static/admin {         autoindex on;         root   /home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib/admin/;     }     location /static/ {         autoindex on;         alias   /home/ubuntu/project/assets/;     }     location / {     # This section is to redirect all http traffic to https if desired     # if ($http_x_forwarded_proto != 'https') {     #   rewrite ^ https://$host$request_uri? permanent;     # }          client_max_body_size 5M;         client_body_buffer_size 128k;         proxy_pass_header Server;         proxy_set_header Host $http_host;         proxy_redirect off;         proxy_set_header X-Real-IP $remote_addr;         proxy_set_header X-Scheme $scheme;         proxy_connect_timeout 300;         proxy_read_timeout 300;         proxy_pass http://127.0.0.1:8001/;     }     # what to serve if upstream is not available or crashes     error_page 500 502 503 504 /media/50x.html;{% endhighlight %}      	  Supervisord  - /etc/supervisord/gunicorn-project.conf  	 Here we just specify the location of the Gunicorn start script so Supervisor can manage it.  {% highlight ini %} [program:gunicorn-project] directory = /home/ubuntu/project user = ubuntu command = /home/ubuntu/project/scripts/start.sh stdout_logfile = /var/log/gunicorn/project-std.log stderr_logfile = /var/log/gunicorn/project-err.log{% endhighlight %}",2,2,2013-07-30,4,"django, nginx, virtualenv, supervisor",545,"Run Django under Nginx, Virtualenv and Supervisor"
22,0,I'm a big fan of the new trend that puts a ton of content on one page. Especially on news sites.,"#product,#design","{% include setup %} A recent trend in website design I’ve been seeing is the long single page. My first distinct memory of seeing it is from  Karma  but I’ve started noticing it everywhere. It runs the gamut from non-profit  causes  to video game  PR firms . In fact, we’re even using this approach for the Makers Alley  homepage .  Surprisingly, it’s starting to make inroads on news sites as well. Whereas before news sites would have an article spread across 20 pages (looking at you Business Insider) in order to increase page views and show more ads, some news outlets are actually improving the user experience. Both  NPR  and the  Washington Post  have posted pieces that leverage this approach and it creates a significantly more engaging read. It’s a pleasure reading long form content this way, richer media adds to the experience and minimizing mouse clicks avoids the disruption of a page load.  I hope it stays.",6,2,2013-03-26,5,"website design, karma, single webpage, news, single page sites",184,"Welcoming the long, single webpage"
15,0,"As developers, we need to learn to overcome the bias to want to build everything","#product,#meta","An issue I’ve been trying to overcome is what I like to call the “build bias.” Whenever I’d run into a technical problem, I’d want to solve it on my own - whether it’s by writing some code or by installing and configuring various libraries and packages. I remember the time I needed to collect feedback for a website but instead of just using an off the shelf product like  GetSatisfaction , I decided to create my own. Although I was able to get it working, it took me longer than expected to get it into a usable state and distracted me from the other improvements I wanted to make.  As a developer, it’s very easy to convince yourself to build from scratch every time you need something rather than using an existing solution. It’s exciting to work on something new and it’s annoying integrating someone else’s code. It’s even worse when they’re charging a few dollars a month for something that you can build in a few hours.  More often than not we underestimate the cost of building something of sufficient quality and don’t include the ongoing maintenance cost we’ll most likely be doing. More importantly, we are no longer focusing on the highest leverage activity. As they teach in business schools, you shouldn’t outsource your core competency but everything else is fair game. This is also supported by the lean startup approach which encourages getting your product to market as soon as possible so you can validate your market hypotheses. Why spend time building features when you don’t even know you have a marketable product? If it does turn out that you have a successful product you can always go back and develop your own solution then.  My new process is to first make sure that the feature is even needed. If it is, I check out the open source alternatives to see if anything can be used. If not, I look at the available paid solutions. For many small projects, it turns out that you can ride the trial/basic version enough to validate your idea. This approach has led  Glossi  to use  MongoHQ  to host my database,  SendGrid  as my email system, and  GetSatisfaction  as a feedback widget in addition to ton of open source libraries. With every new project, I’m offloading more and more of my auxiliary features to cloud based services and feel much more productive. Makes me wonder how many other services there are out there that can be leveraged.",5,2,2012-03-24,2,"startups, entrepreneurship",437,Overcoming the Build Bias
21,0,It's surprising how poor some of the Google News recommendations are. It seems the prediction is optimizing towards click bait.,#society,"{% include setup %} Last week, I wanted to get a bit more information about the vote in Alabama and have always had good success by starting with a Google search so did a simple search for ""alabama vote."" I saw three suggested results but surprisingly the first one was a link to Breitbart.      Search for alabama vote on 2017-12-05   I ran the same search a few days ago and saw that the first link was to yet another Breitbart article.      Search for alabama vote on 2017-12-09   Finally when I repeated the search the past two days Breitbart was out and I saw legitimate news publishers.      Search for alabama vote on 2017-12-10       Search for alabama vote on 2017-12-11   Now I realize that it’s tough to do recommendations right but it’s still shocking that Google wouldn’t even prioritize the most recent news results ahead of something clearly misleading. The sad truth is that the linkbaity articles are more engaging than real news but it’s disappointing that Google is prioritizing user engagement and clicks over quality. I’ve been trying to make myself less and less reliant on Google and examples like this give me more and more reason to do so.",0,1,2017-12-10,2,"google news, politics",269,Google's news problem
18,0,I recently had my Pinterest account hacked which led to a bunch of spam tweets on Twitter.,#meta,"{% include setup %} Earlier today a I got a message on Twitter letting know that my Twitter account was hacked. Sure enough when I looked at my tweet history I saw a slew of weight loss tweets linking to a Pinterest pin. Turns out that my Pinterest account was compromised and since it was connected to Twitter every time someone pinned a weight loss link it got shared on Twitter.  The fix was simple - block the Pinterest app from within Twitter, disconnect Twitter from within Pinterest, and reset my Pinterest password. Unfortunately, none of these can be done via the apps nor the mobile sites. Instead, both provide a minimal settings page with no clear way of accessing the complete settings. Since I wasn't near a computer, I had to use the Twitter app to delete the spam tweets that were being posted a few times each hour.  This is a frustrating design pattern. Sites and apps should default to a mobile optimized experience but if any functionality is missing they should allow users to fall back to the web view. Arguably, that option should always be available since a user may have a specific flow in mind and shouldn't be forced to learn a new approach - especially if something's urgent. In my case, this wasn't that huge of a deal but I can see how it could have been.  The other lesson is that in this world of interconnected apps you're only as strong as the weakest app. Twitter followers don't care why they're seeing spam and having TFA enabled on Twitter won't help you if another account is compromised.",0,1,2014-06-15,3,"pinterest, twitter, security",273,On having my Pinterest account hacked
27,0,Augment reality will change the way we see and interact with the world and it's interesting to ponder what an AR based adblocker would look like.,#society,"{% include setup %} Augmented reality is still in its infancy but when it grows up it will change modern life. We’ll have a world of information at our fingertips and will end up with tools we can’t even imagine. An idea I’ve been thinking about is an AR based adblock. The way I envision these AR headsets working is that everyone will be constantly wearing them, akin to glasses, and the headsets are always processing, monitoring and altering what we see. Now imagine that there’s an adblocking application that intercepts everything visible and replaces it with blanks. Web adblockers work by blocking ad requests as well as removing ad HTML elements. This would work at an entirely different level by being in between the world and our eyes, acting as a sensory input filter.  This is just scratching the surface and it’s interesting to think how the world would change with the full adoption of AR - able to change everything we see and sense. We already have trouble with fake news and people choosing to see what interests them and AR has the potential to do this an entirely different level; arguably a much more dangerous one. I don’t know where we’ll end up but I’m hopeful we think through the implications of these technologies and their potential impact on society.  Disclosure: I work at TripleLift, an adtech company that’s better than most by focusing on higher quality, non-disruptive native ads.",0,1,2017-12-28,2,"augmented reality, adblocking",244,AR Adblocking
10,0,List of my 5 goals for 2013 to improve myself,#meta,"{% include setup %} Now’s the time people are making resolutions for 2013 so I’m going to join the club. I’m publishing them publicly since that should help my motivation. I’m also calling them goals since a goal seems harder to abandon than a resolution. I’m hoping that having these goals be specific, however arbitrary, will also help me in achieving them.  Here goes:    	  	 Run 1000 miles in 2013   	I’ve gotten out of shape over the past couple of months and that’s a bad place to be in the late 20s. I think it’s important to get good habits now since that will help me maintain my health as I get older. Not to mention that being in better physical health will improve my acuity. 	  	  	 Write at least 2 blog posts a week   	The more I work with various people the more I realize the importance of communication. Writing doesn’t come easily to me and I spend the majority of time editing but I’m hoping that it’ll be easier by the end of 2013. And although writing is just one aspect of communication, improving that will lay a solid foundation for the others. 	  	  	 Meet up with 2 old acquaintances each month   	Going from a company with hundreds of employees to working with a cofounder reduces the number of people you have contact with. By restoring my old relationships I’ll be able to connect with old friends and strengthen my network. Something I’ve learned over the past year of meeting with various folk in the NYC startup community is to end every meeting with an offer to help and I’m going to adopt that attitude as well. 	  	  	 Develop 6 side projects   	This one’s here for a few reasons. One, I want to keep on improving and starting a project from scratch is a great way to work with new technologies and explore different approaches. There are many times that I want to go back to my existing code and rewrite it but why fix something that isn’t broken? Isn’t it better to put that energy into something new? Two, I want to give back to the community and putting these projects on GitHub will hopefully help someone. Three, this will just be a good outlet for when I need a break from the main gig. Four, I want to build my personal brand and having more more of my work publicly available will hopefully help. 	  	  	 Start a hands-on hobby   	This one stems from a personal belief that I just need to do something with my hands since I spend so much time in front of the computer. This may end up being drawing, painting or woodworking but the goal is to find something that allows me create something physical and not digital. I’ve already dug up some colored pencils and drawing paper and am in the process of signing up for a woodworking class at a hands-on coworking space in Brooklyn called  3rd Ward .",1,1,2013-01-02,1,goals resolutions self-improvement,510,2013 Goals
24,0,More important than style and syntax is the philosophy of code. Strong teams have a shared understanding and it makes them more productive.,#code,"{% include setup %} After writing my post on the code review pyramid I realized that I had many more thoughts about the highest level, code philosophy, and wanted to dedicate a full post to dig into it. The general idea is that a highly functional engineering team is way past the point of arguing over style and syntax and has reached the point where they share the same code philosophy. At this point all members of the team have an instinctive sense of how and where new code should be written - even if they can’t necessarily explain it.  A concrete example is to think about a data pipeline project. We have a ton of events coming in that need to be aggregated, stored in a database, and then exposed via an API in a UI dashboard. Now imagine having to add some additional fields. Depending on the field you may be able to add it explicitly at the ingestion level, or potentially derive it during the agg, or maybe even calculate it at the API or UI level. Each of these may be entirely reasonable but a good team will have a strong option on how it should be done. This means that every new feature and functionality is implemented similarly and keeps the code base clean and consistent.  This also extends into the way teams approach their class hierarchies and inheritance. Do they go all in and have a ton of factories with complex class hierarchies or do they prefer flatter hierarchies with classes that are able to do more? It also ties into design patterns and which ones are preferred in the code base as well as how to treat nulls and exceptions.  A programming language can't prescribe every single approach to every type of problem so there's always some freedom by design. Unfortunately, this can get dangerous on larger projects but strong teams are able to identify the components they want to embrace and the components they want to ban. Frameworks exist because they make these decisions for us. They are purposefully opinionated in order to encourage a particular approach. By having this sort of imposed shared knowledge, every developer using that framework becomes more productive despite the fact that the framework itself limits the flexibility of the language.  The point isn't to find the one right approach but to realize that it’s more about agreeing on an approach. The fact that engineers still debate the pros and cons of nearly everything to do with software engineering indicates that there’s likely no single answer and we’re better off finding something that’s consistent and right for the team.",0,1,2017-10-29,3,"code, software engineering, software development",442,Philosophy of code
15,0,Tools are incredibly important to be productive so here are the ones I use.,#meta,"{% include setup %} Great tools have the potential to make us significantly more productive and I wanted to share my existing setup. A huge part of that productivity is our comfort with our tools since over time we learn the shortcuts, understand the capabilities better, and develop processes to solve common problems. The challenge is that there is always a tool that might be better but the learning curve is too steep to warrant a time investment. Here's what I have so far.  - Google Chrome Canary + FirefoxDeveloperEdition: I like being on the bleeding edge so use the nightly builds of both browsers. My preference would be to use Firefox for everything but I'm more familiar and comfortable with the Chrome dev tools.  - Google Calendar: I'll use this for both scheduling meetings as well jotting down deadlines and todos. It's been working great and I haven't felt a need to use anything else. I tried using a few apps but wanted something that had a better integration with the rest of the Google suite.  - Fastmail + Gmail: We use Gmail at work but my preference is for Fastmail. It's cleaner, simpler, and faster than Gmail and reminds me of what Gmail was when it first launched. Within both Fastmail and Gmail I strive to achieve ""inbox zero"" with varying success.  - Google Docs: Whenever I need to write anything non code I'll reach for Google Docs. The interface is simple to use and I like the cross device sync. The only time I'll move away is when I don't have internet access or I'm taking random notes.  - Excel: I tried using Google Spreadsheets but Excel is significantly better for larger scale projects and I'm too comfortable with the keyboard shortcuts I picked up during my maangement consulting and finance days. My ideal solution would be a spreadsheet interface built on top of a language such as R. Then I'd be able to use the spreadsheet component for the quick and dirty work, pasting data into it, doing simple transforms, etc and dive into the R for the more serious quantitative work.  - Terminal with zshell and ohmyzsh: Being comfortable with the terminal is vital for developers. It's the primary way to interact with external servers and knowing the various commands and scripts allow us to quickly diagnose and fix problems. The add on I use is zshell with the ohmyzsh configuration since it comes with a nice set of bells and whistles - git integration, useful highlighting, ..  - Sublime Text 3 + SFTP: For scripting progrmas my editor of choice is Sublime Text. It's surprisingly snappy and lightweight while providing a lot of flexibility for third party plugins. One of these plugins is SFTP which allows me to sync local files over to a remote server. I do a lot of my development work on an EC2 instance so being able to save them remotely is a huge productivity boost. I used to use Evernote for note taking but have switched to using text files in Dropbox. This allows me to organize them the way I want and leverage the command line to find exactly what I'm looking for.  - EC2: At TripleLift, each developer gets their own EC2 instance to be used for development. This both mirrors the production environment better than OS X would and allows us to make our sites publicly accessible to other developers. The other nice piece is that it interfaces nicely with the other AWS products, namely S3. Transferring files from S3 to EC2 is much quicker than going from S3 to a local computer. The two major constraints are that it's command line only and tends to be less performant than our local machines.  - Eclipse + IntelliJ: For Java I'm using Eclipse and for Scala I'm using IntelliJ. I've been coding Java for a lot longer and am much more familiar with Eclipse. It's possible that I'll move to IntelliJ at some point but for now my projects allow me to keep them separate.  - Git on GitHub with Hub: No surprise here. GitHub makes it easy to collobarate with others and I'm a big fan of the interface. The only annoyance I had was being unable to open pull requests from the command line but I've since found Hub which provides a command line interface to GitHub.  Would love to hear of other tools that people find useful.",0,1,2015-04-03,3,"productivity, tools, development",732,My tool setup
22,0,Excel has a bad reputation but it's great at doing quick analysis and should be a part of everyone's tool set.,#datascience,"{% include setup %}            Excel has developed a reputation of being bloated, slow, error prone and used primarily by ""business people"" who don't have real quantitative skills. Just like anything else, Excel is a tool that can be misused but is significantly more useful than people give it credit for.  The most important benefit Excel provides is making data approachable and fun. By making it approachable Excel opens up data analysis to a ton of new people that come into it with their own experience and knowledge. Sure they may not have data scientist skills but they're still able to run some neat analyses and derive useful insights.  The fun makes it very easy to experiment and try a lot of different ideas by making the cost of failure so low by providing quick feedback and visuals. The value of writing a formula and then dragging it down, quickly seeing the calculations is massive. This gives the quick feedback that encourages people to keep on driving their analysis. And although Excel's visualizations are simple, they provide a fast way to visualize the data and hopefully lead to more analysis. Similar to the way we use Python for a quick project instead of Java, it's much easier to run a quick analysis in Excel than in a ""real"" language such as R.  My typical approach to quantitative problem is to write a query to retrieve the data I want and then immediately dump it into Excel for a quick analysis. This lets me apply some pretty basic formulae and visualizations to to see if there's anything worth pursuing in more depth. Only then will I move to R or Python to do a deeper analysis. Even then, I most likely rewrite the code to make it ready for production. This approach forces me to focus on the data and dimensions I want to analyze. Excel only serves as a way to quickly explore the data before deciding whether there’s anything worth pursuing.  The only tool I can think of that comes close is  Tableau  but my experience has been that it has a somewhat steeper learning curve and doesn’t support the flexibility to quickly add and adjust various calculations. Replacing Excel is tough. I use Google Docs for working with documents and yet for my data I use Excel rather than Google Spreadsheets.",1,1,2013-09-20,4,"excel, data science, quantitative engineering, tableau",415,In defense of Excel
32,0,It's too easy to find answers online to our coding problems but it's much better to take the time to solve it on your own locally before going online for help.,#code,"{% include setup %} Whenever I need some coding help my first step is to do an online search which usually leads me to either the library documentation or a StackOverflow page. This is a poor habit and something I’m trying to move away from. While I’m almost always online it’s dangerous to rely on the internet to code - both because there will be times you may not have internet access but also because you lose the ability to do your own investigation, discovery, and critical thinking.  There will always be cases where I’ll have to do a search but even then I first make it an effort to solve the problem locally. The following are some tools that have become indispensable for me when doing my local discovery. Note that Python is primary programming language so the last 3 tools are Python-specific but other modern languages should have equivalent tools.  - history. While not specific to Python I find the history command incredibly useful. I make my history retention as long as I can and then pipe it into grep to see the way I used various commands and their arguments. I may not even know the command itself but by grepping for filenames or directories I’m usually able to figure out what I was looking for. - man. Short for manual and works just like one. Type man in front of any Linux command and you’ll get an exhaustive, but dense, explanation of the command with a description of the options. The documentation is dense and I have to attempt an option a few times to make sure it’s working as expected but it’s powerful having all the information in a single place. Even better is making it a habit to use man on commands you already use since you will inevitable discover some new functionality. - dir (Python). Now we’re beyond the shell but dir works similarly to the command line and lists the attributes of an object. I find this really helpful when using an open source library that is sparse on documentation since I can easily see the methods that the objects I’m using have. It’s often enough that just seeing the name gives me what I need. - [__doc__](https://docs.python.org/3/library/functions.html#dir) (Python). Combined with the above typing __doc__ after an object gives you the docstring of that attribute. This is usually a quick blurb documenting the function - both what it does as well as the argument(s) it takes. This combined with the dir function above makes it pretty easy to write a quick script to dump all the attributes of an object along with their documentation. For example you can do this for the “set” module: print(""\n"".join([a + "": "" + str(getattr(set, a).__doc__) for a in dir(set)])) - [inspect](https://docs.python.org/3/library/inspect.html) module (Python). While researching the above I came across the inspect module which provides some advanced object inspection functionality - including a cleaner for of the __doc__ approach above. I’m still getting a feel for it but just by reading the linked docs you can get a feel for how it can be used to inspect Python objects.  There must be a ton of other tools I didn’t cover and I’d love to learn them. I’m a big believer in being as independent of a developer as one can be. A good way is to push one’s craft without relying on outside support. It’s too easy to do a search and get what you want but then it’s one ear out the other. Instead, the way to become stronger is by struggling and doing things the hard way with the hope that they stick over time.",2,1,2018-11-24,4,"python, coding, software development, inspect module",613,Code without online help
21,0,I ran the famous movie quote auto complete experiment using Android after seeing it done on XKCD using iOS 8.,#meta,"{% include setup %} Earlier this week XKCD featured a  comic  where oft-quoted movie quotes are autocompleted by iOS keyboard predictions. I decided to do replicate the exercise using Android and Swype. Some are similar while others are completely different. I suspect a big part of the difference is that Swype uses my history when offering the suggestions and since I’ve been travelling recently many of them tend to be airport related.       Say hello to my little  brother and sister and the other hand.        Toto, I've a feeling we're not  going to be a good time.        Bond, James Bond  with the Eagles to the airport.        I'm a leaf on the wind. Watch the video game console and I will get there early.        Goonies never say never been to the airport.        You have my sword. And my bow. And my wife.        Hello, my name is Inigo Montoya. You can send you a call to discuss the details.        Revenge is a dish best served from the other side of the terminals.        They may take our lives, but they'll never take our word for it.",1,1,2014-10-04,4,"ios, android, autosuggestion, xkcd",198,XKCD movie quotes by Android
25,0,There are a ton of messaging apps out there but the fragmentation doesn't bother me at all. I treat them all as a utility.,#product,"{% include setup %} Apparently there’s a “messaging war” going on among the dozens of apps and social networks, all competing to be the dominant messaging app. There really are a ton of these. I have a folder on my phone dedicated to messaging and it contains Apple Messages, Google Hangouts, WhatsApp, WeChat, Telegram, Facebook Messenger, and Signal. In addition, Twitter, Instagram, and Snapchat all have messaging functionality. Adding these up I have 10 messaging apps on my phone.  It’s clear that there’s fragmentation but it also doesn’t affect me at all. Each of the apps provides basically the same functionality and if there’s another messaging app introduced I’ll just download that one too. I go wherever my contact is and I keep my notification settings the same for every app. The market may be fragmented but my workflow is so app agnostic it doesn’t actually matter. All I really need to keep track of is who uses what app but given my list of contacts is both small with strong recency properties I’m able to choose the appropriate app subconsciously. Some of my friends are on WhatsApp, others on Messages, others on Hangouts, and one on Singal. But the point is that doesn’t matter. It’s simple enough for me to have and use Signal even if it’s just for one person. Maybe others are more passionate about messaging and have their favorite app but I view them as purely utilitarian. They’re no different to me than a grocery store - I just go to whichever one is more convenient at that time.",0,1,2018-11-14,11,"messaging apps, messages, hangouts, whatsapp, wechat, telegraph, signal, messenger, twitter, intagram, snapchat",263,What messaging war?
27,0,Building Makers Alley we ran into a wide set of features we needed to implement that make marketplaces pretty hard from a technology standpoint as well.,#product,"{% include setup %} There are countless posts discussing the business and marketing challenges when building a marketplace but I wanted to discuss the issues on the tech side. While we ran into technical challenges building  Pressi  they were mostly issues with scaling and dealing with the various social network APIs. With  Makers Alley , we didn't run into scaling or API issues but had to deal with a ton of functionality in order to be seen as a credible marketplace. Individually, the features are simple for an intermediate developer to build but there are a lot of them with varying degrees of nuance and logic that need to be worked out.  Note that some of these issues are only applicable to ""maker"" marketplaces where the merchants make the pieces to order. In those cases, I refer to them as makers rather than merchants.  In no particular order:    	  Payments :  Stripe  and  Balanced  have made this significantly simpler but one thing to watch out for is that you will need to have a merchant signup process to collect the required regulatory information if you want to automate disbursements.  	  Shipping and Tracking : Makers take a different amount of time to make each piece. Buyers should know this information before placing an order and makers need to be able to change it depending on their schedule and order load. Merchants need the ability to mark an order as shipped and possibly provide a tracking number to the buyer. You should notify buyers when their order has shipped.  	  Logistics : While you're not holding inventory, you're charging customers and expect the merchants to fulfill their end of the agreement. How do you deal with a merchant sending an order late or not being able to fulfill an order? Do you want to have merchants approve every order they receive or do you assume that they'll be able to fulfill it? How about a single order containing items from many merchants?  	  Returns : No matter how good the products are there will always be someone who's unhappy with an item and you need to have a return/refund policy. For small items it's simple to figure out the logistics but how do you deal with someone wanting to return a dining table to a merchant a few states away? Where should the item be sent and who's responsible for paying the shipping and handling cost? What happens to the returned item?  	  Messaging : We discovered this a bit late but customers really want a way to talk to the merchant. Some buyers will want to customize an order and may want to both send exchange photos with the merchant to make sure they're getting what they want.  	  Shopping Carts : There are a bunch of existing solutions out there but we weren't able to find one that fit the needs of a two sided marketplace that supported customizable product options. there are a lot of things we take for granted when using a full fledged site like Amazon - making changes to your shopping cart, buying from multiple merchants, applying rebates and discounts, and getting recommendations. A possible edge case is merchants running out of inventory while someone is going through the checkout process.  	  Orders : Both merchants and buyers need to see a history of their orders. The implication is that once an order is placed it needs to be immutable and timestamped so that changes to the items are only reflected going forward. In addition, orders can get messy since a single order may be spread out across multiple merchants and items. What happens if one merchant can fulfill their half of the order while the one can't? Do you issue a refund for part of the order? What if the customer only wanted the items as a package deal?  	  Taxes : At some point you need to start dealing with taxes with each state having their own regulations and rates. We haven't implemented the details here yet but I suspect interstate commerce can get complicated quickly.  	  Reviews : Before a purchase, buyers want to see reviews of an item. After a purchase, buyers may want to rate and review the items. Should merchants have the ability to respond or challenge a review?  	  Search : This is a big one. Buyers need to be able to quickly find what they're looking for or they'll give up and go somewhere else. What criteria can users search for? Are you going to deal with typos and misspellings? Should you support faceting? How should you tier your prices? Do you need to support geospatial search? This is probably the biggest piece that requires understanding your audience and tailoring the search experience to them.  	  Images : It's rare that someone will buy a physical item without at least seeing a picture of it first. Makers need a simple way to upload multiple images and change the order in which they are displayed. The code should also be smart about generating thumbnails that can be used on different pages - search/listing, product view, shopping cart, etc.  	  Changing Inventory : Makers will need to be able to modify and remove what they're selling. At the same time, you need to have a record of the history so that buyers and makers can look at prior sales.     Most of these issues can (and should) be handled manually at the beginning either through the backend or through email but this approach won't scale. The goal is to be able to support the various use cases even if they have to be done manually. This will make you look credible to your customers and also give you a sense of which cases are the costliest and need to be automated. Technology shouldn't be the primary focus for a marketplace business and you will most likely fail due to a lack of users on one of the sides. At the same time, the technology behind a marketplace isn't simple since you're basically smashing together an ecommerce site with a social network. If you have any additional thoughts or questions definitely let me know and I'll try to help.",4,1,2013-07-13,3,"marketplaces, ecommerce, startups",1040,Marketplaces are hard
33,0,While upgrading our Kafka to 0.10 we ran into issues getting secor to scale. We did a ton of optimization but the final culprit was the version of our Kafka consumer.,"#devops,#data","{% include setup %} Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that [secor](https://github.com/pinterest/secor), a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly. I fastidiously tweaked the Kafka configuration to get as much out of it as I could to no avail. I spent hours experiment with the various secor options to see whether there was a simple solution I was missing. No matter what I tried I was unable to consume more than 50mb/min - despite the fact that both the Kafka cluster and the instance running secor could support an order of magnitude more than that. I confirmed that there was something fishy by running the same exact code on a massive c3.8xlarge instance to see how much better it would fare. And sure enough I still couldn’t get past 50mb/min.      The blue is an c4.xlarge and the orange is a c4.8xlarge. Clearly they should not both be consuming at the same rate. Also, the large spike in the middle is when the offsets start dropping off and secor keeps attempting to catch up.       The flip side is that the uploads to S3 are throttled and drop of when we're behind Kafka.   At this point I was extremely frustrated and figured I might as well revisted the Kafka docs and found this [wonderful gem](http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact):  > The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.  The light immediately went off and sure enough, secor was configured to use a Kafka 0.8 client. As soon as I [upgraded secor](https://github.com/pinterest/secor/pull/262) to use Kafka 0.10 the consumption rate shot up to over 2.5gb/min. Despite feeling incredibly stupid it felt good to finally get to the bottom of it and only wish I read the docs more thoroughly before diving in. The benefit to all this is that I have a much better understanding of how  Kafka, ZooKeeper, and secor need to be configured and the value of actually reading the documentation, something that I still haven’t internalized.      After the upgrade we see a healthy spike of data going in as we're trying to catch up.       Similarly we see us writing it all out to S3.",3,2,2016-10-10,5,"secor, kafka, 0.10, big data, streaming",770,Setting up secor for Kafka 0.10
29,0,Chinatown produce vendors are able to sell their produce much cheaper than the alternative despite smaller scale. They've set upa system designed for high turnover and low prices.,#product,"{% include setup %} The Wall Street Journal had a [great piece](http://www.wsj.com/amp/articles/why-fruits-and-veggies-are-so-crazy-cheap-in-chinatown-1466762400) on why produce is so cheap in Chinatown. The conclusion:       Her discovery: Chinatown’s 80-plus produce markets are cheap because they are connected to a web of small farms and wholesalers that operate independently of the network supplying most mainstream supermarkets.     Most of the city’s fruits and vegetables come from wholesalers at the Hunts Point Produce Market, the South Bronx distribution hub boasting all the color and accessibility of La Guardia Airport. Chinatown’s green grocers, in contrast, buy their stock from a handful of small wholesalers operating from tiny warehouses right in the neighborhood.     Because the wholesalers are in Chinatown, they can deliver fresh produce several times a day, eliminating the need for retailers to maintain storage space or refrigeration, said Ms. Imbruce.     I love this. It runs counter to the common belief that cheaper prices can also be achieved through massive scale. Yet in what I suspect is one of the hardest industries, food distribution in NYC, small scale seems to be doing the better job. Produce has an extremely short shelf life and combined with the cost of real estate in NYC it must require some incredible management to be able to sell it for the half the price of produce found at the supermarket. And everyone involved ends up winning - consumers get cheap prices and a great selection, the stands are able to turn around a ton of inventory due to the low prices, and the farms benefit from the variety of crops they’re able to grow.  This is a perfect example of being able to build a successful business by focusing on activities that complement each other (à la [Michael Porter](https://hbr.org/1996/11/what-is-strategy)): they have their own local wholesalers that get constant replenishment that can then be priced incredibly cheaply which encourages high turnover and feeds back into the need for quick replenishment. This also allows them to focus on produce that doesn’t need to be kept on the shelf as long and is expected to be sold and eaten within a short amount of time. They embraced the idea of “making it up in volume” by setting up every activity to drive that goal.",2,1,2016-06-26,5,"chinatown, startups, produce, michael porter, five forces",390,Low cost at small scale
15,0,I'm excited by the Javascript visualization libraries that encourage interactive graphics and story telling.,"#dataviz,#javascript","{% include setup %}      Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.  At first, I’d just dump the data into Excel to generate various charts but moved on to using Perl and Python to generate charts when I learned the value of reusable code. While at  Yodle , I picked up R which gave me more power than what I knew to do with and introduced me to a whole new set of visualizations and models. Recently, I’ve been having a blast using  D3  and  Vega . The biggest appeal is that they’re in Javascript so they can run in all modern browsers and make it very easy to support interactive behavior. The best analyses always tell a story and allowing users to interact with the data is a great way for them to craft their own story. I’m hopeful that such tools will improve data accessibility and get people excited about gleaning their own insights.",3,2,2013-07-09,5,"d3js, d3, vega, data visualization, javascript",252,D3 and Vega
31,0,Instead of focusing on making an AI act as a human it might be interesting to try a form of the Turing test where a human acts as an AI.,#meta,"{% include setup %} A friend sent me  an article  where the author discusses the recent news of an AI finally beating the Turing test and how he himself was clearly able to determine that the AI was not a human. The most common explanation of the Turing test is where someone communicates with both a human and an AI and is not able to tell which is the machine and which is the human. It’s almost always phrased in the way that a human will act normally and the AI will try to act as a human, mistakes, typos, and imperfect information.  Regardless of whether modern AIs can beat the Turing test I think it’s inevitable that an AI will conclusively beat the Turing test in the coming years. A more interesting question is whether a human can trick another human into thinking he or she is an AI. It’s similar to the Turing test in that it’s supposed to make the AI and human indistinguishable to a judge but instead of making the AI smarter we’re dumbing down the human.  The nice thing about this approach is that historically it was very easy for a human to act as an AI by making dumb mistakes and responding with non-sequiturs. I suspect it’s currently quite difficult to respond in a way that would convince someone you’re an AI, even after enough time speaking with one, and I’d love to see this attempted. In the end both of these approaches converge to the same goal of making AIs and humans indistinguishable and this is just another way of looking at it.",1,1,2014-12-29,2,"turing test, ai",283,A new Turing test
21,0,In what has become an annual tradition I updated my Yahoo Fantasy Football scraping bot for the 2017-2018 season.,#code,"{% include setup %} In what has become an annual tradition I updated my Yahoo Fantasy Football scraping bot for the 2017-2018 season. Every year Yahoo makes a few changes to their page and this year was no different. It’s always fun to cross my fingers, run the script, and see what breaks. This year the changes were surprisingly minor. For some reason Yahoo changed the name attribute of the password field from “passwd” to “password” and made a few tweaks to the table structure which required updating the XPath selectors. Other than that everything worked as expected and the 2017-2018 data is available [here](/assets/static/data/stats-2018.csv) with the code up on [GitHub](https://github.com/dangoldin/yahoo-ffl).",2,1,2017-07-08,2,"yahoo fantasy football stats, scraping",118,Yahoo fantasy football stats: 2017-2018 edition
52,0,A neat coding puzzle I heard is to find the shortest path from one word to another where you're only allowed to change a single letter at every step and each step needs to be a valid word. I spent some time this morning writing the code to make it happen.,#code,"{% include setup %} A fun engineering puzzle I heard this week was to write an algorithm that finds the shortest path between two words of the same length where you’re only allowed to change a single letter each step and every word needs to be valid. This morning I decided to have some fun with it and wanted to jot down my thought process going through the exercise in the hope that it provides a bit of perspective on how I approach code.  The first step was to just do an example in my head to visualize the problem. I started with two short words, dog and cat, and went through the manual transition. The optimal solution is where each letter changed is the final letter - in the case of dog to cat it was simply dog -> dot -> cot -> cat. Now that I had a baseline (and a test), I decided to dive into the actual code.  The immediate realization was that since this was asking for the shortest path I’d need to do a breadth first search, something I haven’t had to touch since some early job interviews. The other realization was that the graph would need to be constructed on the fly. With these two in mind I dove right in.  I broke the problem down into three parts - one was loading the dictionary, two was writing a function that would get the “adjacent” words, and three was doing the search itself. The first function was straightforward since I just loaded in the built in OS X dictionary:  {% highlight python %} def load_dictionary(path = '/usr/share/dict/words'):   dictionary = set()   with open('/usr/share/dict/words', 'r') as f:     for line in f:       dictionary.add(line.strip().lower())   return dictionary {% endhighlight python %}  While thinking about the adjacent word function I thought back to [Peter Norvig’s spell checker](http://norvig.com/spell-correct.html) and remembered how simple yet powerful it was (if you haven’t seen it yet you should take a look - one of the most elegant code examples I’ve seen). All his code needed was a tiny tweak to filter the list of generated words to those in the dictionary.  {% highlight python %} def adjacent_words(word, alphabet):   splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]   replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]   return [r for r in replaces if r in dictionary] {% endhighlight python %}  Now it was time to do the actual search which took me a bit of time. I knew the theory but it took me a bit of time to translate it into code. And even then I wasn’t happy with how it looked so ended up finding a pretty simple [Python implementation](http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/).  {% highlight python %} def bfs_paths(source, target, dictionary, alphabet):   queue = deque(((source, [source]),))   while queue:     v, path = queue.popleft()     for n in [w for w in adjacent_words(v, alphabet) if w not in set(path)]:       if n == target:         yield path + [n]       else:         queue.append((n, path + [n])) {% endhighlight python %}  The last part was cleaning up the code and improving its efficiency. The key parts here were using string.lowercase as the universe of letters, replacing a standard list with a collections.dequeue to significantly speed up the “pop” operation, and making the dictionary and alphabet variables locally scoped. As a final test I ran through the dog to cat example and got two additional transformations: dog->cog->cag->cat and dog->cog->cot->cat. The complete code is below but note that I left it open-ended so it will print every path it finds rather than just the shortest one.  {% highlight python %} #!/usr/bin/env python  import string from collections import deque  def load_dictionary(path = '/usr/share/dict/words'):   dictionary = set()   with open('/usr/share/dict/words', 'r') as f:     for line in f:       dictionary.add(line.strip().lower())   return dictionary  # Peter Norvig's spellcheck code is amazing: # http://norvig.com/spell-correct.html # Just use the replace part of it def adjacent_words(word, alphabet):   splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]   replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]   return [r for r in replaces if r in dictionary]  # Had to remember how to get this working again # Took a bunch from http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/ def bfs_paths(source, target, dictionary, alphabet):   queue = deque(((source, [source]),))   while queue:     v, path = queue.popleft()     for n in [w for w in adjacent_words(v, alphabet) if w not in set(path)]:       if n == target:         yield path + [n]       else:         queue.append((n, path + [n]))  if __name__ == '__main__':   alphabet = string.lowercase   dictionary = load_dictionary()    for x in bfs_paths('dog', 'cat', dictionary, alphabet):     print x {% endhighlight python %}",2,1,2016-07-17,3,"word transformation, coding puzzle, breadth first search",812,Coding puzzle: Word transformation through valid words
18,0,I'm putting together a MySQL class and would love to get some feedback on the proposed structure.,"#data,#meta","{% include setup %} I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with  TEALS , a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the  Coalition 4 Queens  program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it. The general idea is that it will consist of 3 or 4 sessions with each session lasting a couple of hours. The class will be opt-in and the students should have some technology background.  Session I  - Overview of MySQL and relational databases. What are they? How are they used? What are the alternatives? - Provide a quick overview of the normal forms and what they mean. What impact does it have when they’re violated and go over what well designed databases have. - Introduce the dataset we will be working with. This will mostly likely be a dataset I’ll pull from some of my side projects that will hopefully be relevant. Currently, I’m thinking of using a database containing some fantasy football data that I’ve scraped. - Make sure everyone has MySQL installed or can get it installed.  Session II  - Revist the dataset we’re working with and explain the relationships between the various tables and columns. - Go over the basic syntax of a query: SELECT, FROM, and WHERE. - Go over the basic INSERT statement.  Session III  - Review the basic syntax of a query and introduce the JOIN operations. Use joins to answer some simple questions from our dataset. - Introduce the GROUP BY functionality and the ways it can be used to summarize data. Use this in conjunction with joins to explore our dataset. - Develop some complicated and slow queries and introduce the idea of INDICES so everyone is aware of why they are useful.  Session IV  - Go over table creation and have the students come up with some interesting aggregate tables. - Provide a quick overview of how to diagnose a query for performance and how to test a query to make sure it was written correctly. - Discuss the various system tables (information_schema schema) and the various system commands that can be used to get a better understanding of MySQL",2,2,2014-08-17,3,"mysql, databases, tech classes",412,A MySQL class proposal
22,0,I've fallen into the bad habit of reading blog posts and not books and this is something I plan on fixing.,#meta,"{% include setup %} Right now I have three tabs open for books I plan on reading: [Designing Data-Intensive Applications](http://dataintensive.net/), Google’s [Site Reliability Engineering](https://landing.google.com/sre/book.html), and [Deep Learning](http://www.deeplearningbook.org/). Unfortunately these tabs have been open for over a week and yet I haven’t deeply committed to any one of them. Yet during this time I spent a bunch of time reading a variety of blog posts and articles that provide bite size information. This is terrible. I could have instead spent the same amount of time actually diving deeper and gaining a much better understanding of a new topic but instead I distracted myself and resorted to the easy reward.  I suspect many people fall into this trap. It feels as if we’re learning and given the massive amount of information that’s constantly being produced there’s always something to read. Most of the posts we read go in one ear and out the other but committing and reading a book makes the content much more sticky and valuable. As a child I was able to read books for hours at a time but now find myself constantly distracted - whether that’s looking at my phone for notifications, checking my email, or catching up on Twitter - but as an adult I’m less focused than a child. This needs to be fixed and I’m making a concerted effort to focus my time on books rather than blog posts and am going to push others to do the same. I also realize the hypocrisy in me preaching to read books in a short blog post but it is what it is.",3,1,2017-02-16,3,"reading, books, blog posts",273,"Read books, not blog posts"
24,0,A CEO was fined by a system that saw her face on a bus ad. Will digital adblocking bleed into the real world?,#society,{% include setup %} In a bit of dystopian news a [CEO was charged for jaywalking](https://www.caixinglobal.com/2018-11-22/ai-mistakes-bus-side-ad-for-famous-ceo-charges-her-with-jaywalkingdo-101350772.html) when a face detection algorithm saw her face on a bus ad. While simultaneously amusing and dystopian it does make one think of a world in which everyone is constantly monitored. At some level we’re already in this world digitally - our browsing behavior is constantly tracked and we all have ad behavior profiles that are constantly being tweaked and updated. Some get around this by using adblocking while a small fraction take the opposite approach and have their browser search and navigate to random pages in order to clutter and confuse their digital footprints.  The real life equivalent of the adblocking approach would be wearing a disguise in public while the equivalent of the confusion approach would be to get your likeness into as many place as possible and overwhelm the face detection system. The difference is that in the real world you’re out in public rather than browsing the web in private. It does make me wonder whether we’ll end up in a world where people stratify themselves into those that value their privacy enough to wear masks while others subscribe to the “I’m not doing anything wrong so what do I have to hide” approach. Given my posts you can probably tell which camp I fall into.,1,1,2018-11-27,4,"digital dystopia, society, face detection, computer vision",243,Privacy in a face detection world
19,0,We need to start designing classes for the web from scratch rather than adapting classes from the classroom.,#meta,"{% include setup %} I’m making an effort to freshen up and improve my data skills so when I found out that two of my friends were going to take an  R class  on Coursera, I joined them. The class is pretty typical for an online programming class: each week there are a set of lectures to watch, a quiz to take, and a programming exercise to do. In addition to this, we also have a weekly Google Hangout to discuss the lectures, go over our programs, and share our R questions.  I realize we’re still at the dawn of online education but it feels as if the class has simply been moved from the classroom to the web, without any thought as to how the class can be structured to make the best use of the web. So far, the Google Hangout paired with the programming exercises is the most valuable. The programming exercises provide the structure and the Google Hangouts help us absorb the material better. We are able to share our solutions, analyze the pros and cons of the different approaches, and by explaining why we solved them a certain way we end up understanding the material better ourselves. Why can’t online classes be designed to take advantage of this? I understand that not everyone has someone to take a class with and yet having a partner provides a big benefit.  These online programming classes should take a lesson from  Project Euler . Project Euler is a series of puzzles that require both a mathematical and programming insight to solve. The brilliance is that they have forums for each problem that you can only access after solving the problem. But once you gain access, you can see other solutions, learn from them, and pick up tricks and approaches that you’ll need to solve future problems.  Pairing the structure provided by a class with this Project Euler community would create better online programming classes than we have now. My ideal online programming class would have the following structure:      No video lectures . The content can be better presented through text and visuals and helps people work on it at their own pace.    Each lesson would be focused on a particular problem . Start by describing a problem and then spend time going over different ways of thinking about it as well as the different tools available. This should be as interactive as possible where students can follow along by running the code on their own computer.    Limited time spent on defining terms . People are sitting in front of the computer and can do a search on Google or Stackoverflow to get more and better definitions than can be covered in a class.    Leveraging the volume of people taking the class . When you have thousands of people taking a single class you can do things that you just wouldn’t be able to do normally. For example, you can analyze people’s programming solutions to see which solutions are the most common, which run the quickest, or which are the shortest. Having that information available to students would be much better than just getting a numerical score.     This approach isn’t for everyone but that’s the point. Moving online will give us the ability to have classes custom tailored for each student but we need to start by thinking about building classes for the web from scratch rather than copying them from the classroom.",2,1,2013-02-01,4,"online education, online classes, coursera, udacity",582,Improving online programming classes
28,0,Ideally we can write code that's both efficient and expressive but they're often at odds with one another. I wish there was a language that offered both.,#code,| |Array generation | 63.96| |Naive simple | 78.74| |Naive smart | 71.13| |Filter single | 82.19| |Filter multiple | 81.86| |Filter lambda single | 109.44|,0,1,2016-12-06,7,"code, software engineering, efficiency, pandas, for loops, iteration, expressiveness",32,Efficiency vs expressiveness
24,0,Given the hubbub around bots I decided to write a super simple one that works with Telegram and responds with random blog posts.,#code,"{% include setup %} A combination of bots being in vogue and Telegram offering $1M in [bot prizes](https://telegram.org/blog/botprize) got me to spend a little bit of time writing a bot last week. To get my feet wet I created a simple, self-serving bot that would reply with a random blog post when sent a /blogme command. The code itself is extremely straightforward and most of the time was spent going through the Telegram bot docs and getting the deployment and HTTPS setup. A nice feature that Telegram has is the ability to write a bot that can respond to both polling and webhooks. The polling approach is a much trivial to get started with since you don’t need to worry about any of the devops work and can work on the core interaction. The cons are that it won’t respond immediately and you need a way to track messages your bot has already replied to. Changing it to a webhook provided real time responses but made it a bit more difficult to test and wrapping everything inside a minimal web framework. The biggest hiccup was the requirement of HTTPS for a webhook integration but [Let’s Encrypt](https://letsencrypt.org/) made it simple to get up and running. A year ago I wouldn’t have bothered prototyping anything that required HTTPS but these days it’s incredibly easy to set up. The [code is up](https://github.com/dangoldin/bots) on GitHub and if you’re interested in bots definitely take a look. And if you have Telegram installed try messaging “danblog” with /blogme to get a random blog post.",3,1,2016-04-23,4,"telegram, bot, messaging, let's encrypt",263,A Telegram blog bot
41,0,Our location tells a lot about us and is actively used in advertising but rather than being treated as a single point in time it should be treated as something dynamic that grows and keeps providing more information about us.,#product,{% include setup %} Targeting is one of the best ways to improve the return on an advertising campaign. By identifying potential customers you're able to focus your advertising on them rather than someone random. And one of the best simplest ways is to set up your advertising campaigns to focus on a specific geography. Maybe your product is only sold in the United States and advertising it elsewhere is a waste. Or maybe your product is sold everywhere but the messaging and copy needs to vary by region. Or maybe it's sold everywhere with the same exact copy but the price varies by region. Being able to change your campaigns by geography is a simple way to improve the performance of any campaign.            Yet most geographic targeting is dumb. Earlier today I was on Twitter and noticed an ad for a Dodge Ram sponsored by Ram Trucks Canada. It's true that I'm on vacation in Canada but it's definitely not the case that I'll be buying a car in Canada. The solution to this isn't complicated. Every social network should have a good idea of my patterns and where home and work are. And if I'm outside those areas it should be easy to determine whether it's a quick trip out of town or a longer vacation. For all I know these platforms have this information but they should be exposing it to advertisers. Of course these inaccurate mismatches are a tiny percentage of the total advertising spend but it adds up and more importantly having more fleshed out profiles will improve the ad optimization.  Imagine being able to determine whether someone drives to work or takes the train. Every heavily used social network has the data to derive this but I suspect few have. We're already placed in various customer segments based on our behavioral and consumption history yet geography is still assumed to be the current location. I suspect the more advanced companies are using geographic information to craft better profiles but I'd love to see this opened up to advertisers.,0,1,2016-07-26,4,"advertising, targeting, geo targeting, adtech",364,Smarter geographic ad targeting
25,0,I'm launching the beta of Better 404 - a tool used to help improve 404 pages by offering suggestions to visitors and metrics to owners.,#product,"{% include setup %} I don’t understand why websites try to compete on having the cleverest 404 page. The fact that someone ended up on a 404 page is a sign that something is broken but instead of trying to fix the problem they try to distract their visitors by making them laugh. It’s equivalent to getting to a restaurant and seeing an amazing menu only to discover that it’s closed.  We can’t always control which URLs our visitors will type in or click on but we can control what they see when they get there. Instead of trying to distract them with humor why not offer suggestions for what they may have wanted to see? The majority of 404 visits are the result of typos which could be fixed with a simple spell check and the remainder are due to moved pages which can be solved by notifying the linker or providing a redirect.  I’ve had some free time over the past month and put together the basics of a simple tool to help sites improve their 404 pages. Appropriately, it’s called  Better 404  and I’m currently running a beta period to get feedback and work out any kinks. If you manage a site and are interested in trying this out, let me know and I’ll help you get started.",1,1,2013-08-07,3,"404 pages, better 404, improved 404",225,Introducing Better 404
13,0,I wanted to highlight my favorite books of 2016 with a quick summary,#meta,"{% include setup %} I have a longer post planned taking a quantified self approach to my 2016 but I wanted to share my favorite books of 2016. Looking at this list the primary themes were rediscovering my love for science fiction, digging deeper into society and culture, getting back into history - both focused on technology but also the world.  - [The Righteous Mind: Why Good People Are Divided by Politics and Religion](https://www.amazon.com/gp/product/B0052FF7YM/ref=oh_aui_d_detailpage_o05_?ie=UTF8&psc=1) (Jonathan Haidt): Heidt highlights morality framework that explains why there's such a disconnect between the right and the left in politics. I ended up reading alongside the election since it just seemed that the world is getting more and more polarized. - [The Death and Life of Great American Cities](https://www.amazon.com/gp/product/B01HWKSBDI/ref=oh_aui_d_detailpage_o07_?ie=UTF8&psc=1) (Jane Jacobs): This has been on my list for a while and finally got the chance to dig into it. I love cities and this is foundational to understand what makes cities, and neighborhoods, great. - [Between the World and Me](https://www.amazon.com/Between-World-Me-Ta-Nehisi-Coates-ebook/dp/B00SEFAIRI/ref=sr_1_1?s=digital-text&ie=UTF8&qid=1483245443&sr=1-1&keywords=between+the+world+and+me) (Ta-Nehisi Coates): Really personal book on what it's like to be black in the United States and having to deal with the never-ending struggles. - [Hillbilly Elegy: A Memoir of a Family and Culture in Crisis](https://www.amazon.com/gp/product/B0166ISAS8/ref=oh_aui_d_detailpage_o02_?ie=UTF8&psc=1) (J. D. Vance): After reading Between the World and Me this takes a very similar approach but focuses on the story of growing up in the rural Midwest and making his way out. - [Deep South: Four Seasons on Back Roads](https://www.amazon.com/gp/product/B00QPHKR0K/ref=oh_aui_d_detailpage_o00_?ie=UTF8&psc=1) (Paul Theroux): Just an engaging read of the author driving through the rural South. I was inspired to read this after Hillbilly Elegy since I just wanted to understand the world outside of the coasts better. - [How Paris Became Paris: The Invention of the Modern City](https://www.amazon.com/How-Paris-Became-Invention-Modern-ebook/dp/B00GC53AEA/ref=sr_1_4?s=books&ie=UTF8&qid=1483062652&sr=1-4&keywords=paris+city) (Joan DeJean): Picked this up when I was traveling in Paris and it's an extremely immersive and engaging read highlighting how Paris was the first city to introduce what we claim are modern necessities: including sidewalks, public transport, and street lighting. - [Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age](https://www.amazon.com/gp/product/B0029PBVCA/ref=oh_aui_d_detailpage_o01_?ie=UTF8&psc=1) (Michael A. Hiltzik): Just a really neat look at Xerox PARC, their growth, as well as the failure. So many key technologies were developed here and it's great to see how it all played out. - [Chaos Monkeys: Obscene Fortune and Random Failure in Silicon Valley](https://www.amazon.com/gp/product/B019MMUAAQ/ref=oh_aui_d_detailpage_o07_?ie=UTF8&psc=1) (Antonio Garcia Martinez): A very personal and open look at how Silicon Valley works with a strong focus on adtech and Facebook. - [The Everything Store: Jeff Bezos and the Age of Amazon](https://www.amazon.com/gp/product/B00BWQW73E/ref=oh_aui_d_detailpage_o00_?ie=UTF8&psc=1) (Brad Stone): Amazon is such a dominant company and this gives a history and talks about the strategy Amazon has taken to get where it is. - [Seveneves: A Novel](https://www.amazon.com/Seveneves-Novel-Neal-Stephenson-ebook/dp/B00LZWV8JO/ref=sr_1_1?s=books&ie=UTF8&qid=1483245954&sr=1-1&keywords=seveneves) (Neal Stephenson): What a great and novel plot. The moon is destroyed and this talks about humanity escaping and trying to rebuild. I don't want to spoil any more but it's a great read. - [The Nexus series](https://www.amazon.com/Nexus-Trilogy-Book-1-ebook/dp/B00TOZI7FM/ref=sr_1_1?s=books&ie=UTF8&qid=1483246162&sr=1-1&keywords=ramez+naam) (Ramez Naam): Really engaging series that focuses on bionanotechnology and both the risks and benefits it exposes. This is a three part series and each book's great.",11,1,2016-12-31,3,"books, 2016, reading",565,Favorite books of 2016
24,0,Amazon's real power comes from having a massive amount of employees that allow them to get special treatment when dealing with local governments.,#meta,"{% include setup %}      Source:  Wikipedia    Amazon’s an incredibly competitive company with a ton of defensive moats but one I haven’t seen mentioned much is also the one I think is one of the most powerful: its sheer number of employees. And rather than having them isolated to a few key offices they’re spread out across a variety of cities, states, and countries. Amazon has over 340,000 [employees](https://en.wikipedia.org/wiki/List_of_largest_employers_in_the_United_States) which is almost 5 times the size of Google’s 72,000.  The benefit of these employees is that they give Amazon significant leverage when dealing with governments. By having such a large, unified block of people Amazon is able to push for special treatment that wouldn’t be given to any other company. They can have cities competing against one another to offer the best benefits in order to win an Amazon location. Winning the location would bring thousands of jobs to the community which would improve the economy, lead to a higher tax revenue, and generally make the city more popular. Amazon is still significantly overshadowed by Walmart at 2.3M employees but it puts them in an incredible position when compared to their competitors in tech.",2,1,2017-08-31,4,"amazon, retail, government, negotiation",217,Amazon's real leverage
22,0,Instead of talking about a lack of time we should realize it's about our priorities and actually think through our choices.,#meta,"{% include setup %} I'm frustrated by the expression ""I don't have time"". As my friends and I have gotten older, I’ve been hearing it more and more frequently. I’ve even caught myself using when trying to come up with an excuse when coordinating evening or weekend plans.  The reason I dislike the phrase is that it’s equivalent to saying ""it's not a priority"" and yet we phrase it such that we convince ourselves it’s something outside our control rather than due to the choices we make. I could go out until 3 AM if I make that a priority over running 6 miles in the morning before heading to work just like I could go catch a movie  instead of working on a side project. If we expressed our choices in terms of priorities rather than time we’d be more likely to deal with them.  Life is full of constraints and it’s impossible to do everything we want. This will only get worse as we get older and deal with more “grown-up” things. Better to develop the right mindset now rather than realize it later.",0,1,2013-10-14,4,"life, time, time management, philosophy",188,But I don't have time
24,0,Ten years ago a friend and I attempted a startup to aggregate a city's events. The biggest success was how much we learned.,#product,"{% include setup %} Way back when as I was just leaving the finance world to go into tech I attempted a startup with a friend called Scenepeek. The goal was to constantly aggregate all of a city’s events and make them easily searchable and discoverable by people who were looking to do something. Since both of us were in NYC and there are always a ton of events we decided that it would be the perfect candidate. We built a ton of scrapers optimized for all sorts of different sites, created a data model that was able to support nearly any type of event, and ended up launching a pretty crude but functional site. It never succeeded but we learned a ton throughout the process.  I often think back to it given all the knowledge I’ve picked up over the past 10 years and wonder what we could have done differently and whether it would have made a difference. We launched around the time that the first iPhone was released but before apps were allowed and never even considered the mobile angle. I still believe it’s a great idea that was ahead of its time but the fact that there's still nothing equivalent makes me wonder. The closest would be something like Swarm but that’s not so much about events as it is about places. Maybe the closest equivalent is actually Facebook Events but the events on there feel more private. Scraping every event is difficult but given how much technology and tooling has improved over the past decade it seems there should be something there. The learning experience was great and introduced me to the startup world but a tiny part of me wonders what would have happened had we started a tad later and leaned into mobile.",0,1,2018-12-12,4,"statups, events, swarm, foursquare",301,Scenepeek
32,0,"Good code is written in such a way where changes only touch a few files. Bad code, on the other hand, is sprawling and seemingly minor changes touch dozens of files.",#meta,"{% include setup %} I’ve done my fair share of code reviews and one of the best indicators of great code is the locality of changes. Nearly all code is taking some data, transforming it, and passing it along somewhere else. This implies that modifications change or add to this flow. It may be passing an additional variable to a function, changing the behavior of a function, or adding another step in our execution. It turns out that if the code is poorly written a seemingly minor change may require changing a series of functions since for some reason each function in the flow needs to be modified. Great code, on the other hand, is written in such a way where making a change to a single function or behavior doesn’t cause any changes upstream or downstream of the code.  Without even knowing what the code does by looking at a diff it’s easy to see how many files were changed and how insignificant the changes were. All code gets modified over time but good code remains isolated with clear separation of concerns while seemingly minor changes to poor code may end up touching every file.  When writing new code assume it will change and try to think how your code will need to be modified for different use cases. This should help guide you to an implementation that ends up standing the test of time.",0,1,2017-04-04,3,"coding, software engineering, separation of concerns",237,Changing good code is easy
30,0,The best efficiency wins come from using the right tool for the job. This is a quick example of combining Redshift and Excel to identify a missing data issue.,#code,"{% include setup %} As part of our data pipeline, we have a Redshift agg job that takes low level data and rolls it up to an hourly aggregate. A latter job takes the hourly data and rolls it up to a daily level which is used for high level reporting and summary statistics. Earlier this week we ran into a hiccup that caused some of these aggregate jobs to fail. After fixing the issue we had to figure out what data was affected and rerun it. We wrote a simple query to count the numbers of rows per day per hour in order to spot any gaps.  {% highlight sql %}select ymd, hour, count(1) as cnt from hourly_agg_table where ymd >= '2015-04-01' group by ymd, hour order by ymd, hour; {% endhighlight %}  This gave us a dataset with three columns that we wanted to then “pivot” in order to quickly spot the gaps. Using the pivot table functionality in Excel, it was simple to put date along one dimension and hour along the other to quickly spot the missing agg periods. All that was left was rerunning the job for those hours.            This investigation reminded me how important it is to be familiar with your tools and choose the right one for the job. Redshift and Excel are antithetical - Redshift is massively parallelizable and built for terabytes of data while Excel slows to a crawl when dealing with tens of thousands of rows. But by mixing them together we’re able to use each for what it’s best for: Redshift for very quick, large scale queries and Excel for the quick and dirty investigative work. This approach is useful in all sorts of problems - from mixing command line scripts with fleshed out programs to using a script or Excel to generate commands that you can then paste into the terminal or an editor. The key point is understanding your workflow and tools well enough to come up with an optimized process.",0,1,2015-04-06,3,"redshift, excel, data analysis",355,Redshift meets Excel
16,0,Updated my Yahoo fantasy football stats script to pull data for the 2016-2017 season.,#code,{% include setup %} This is an annual tradition now but I just updated my old script that crawls and extracts the projected fantasy football data from Yahoo to work with the 2016-2017 season. The changes were incredibly minor: Yahoo broke the the login page into two steps and there was a minor change in the order of the columns. Both of these were trivial to implement and the code is up on [GitHub](https://github.com/dangoldin/yahoo-ffl). If all you care about is the raw data you can just download the [CSV](https://raw.githubusercontent.com/dangoldin/yahoo-ffl/master/stats-2017.csv).  Every year I intend to use the data to come up with a drafting algorithm yet I’ve failed to do anything with it over the past couple of years. I’m hoping this year is different.,2,1,2016-08-13,4,"fantasy football, yahoo, fantasy sports, data",133,Fantasy football stats: 2016-2017 edition
26,0,Engineering teams constantly struggle with the speed vs quality tradeoff. I think both should be achievable but if they're not then quality should be prioritized.,#management,"{% include setup %} Recently I’ve found myself have similar conversations with various members of the engineering team regarding the tradeoff between speed and quality. Every situation is different but without going into project details I've found that quality come first, speed second. Not because I think speed is unimportant but because I think quality is underrated. In the desire to push the next feature and launch the next product quality tends to be sacrificed. This is fine as long as we understand the tradeoffs but in most cases those are externalized to others. For example, if an engineering team ships a buggy feature, the engineering team only incurs the cost of fixing it, and even then only if they end up fixing it. Simultaneously, the cost is passed on to the users who are powerless to fix it. And then it goes through multiple tiers - first the end user who becomes inefficient and may lose work, then the support person responsible for dealing with these issues, the product manager who has to context switch to both understand and prioritize the issue, and finally the engineer. During each step time is lost but most importantly is the interruption of [flow](https://en.wikipedia.org/wiki/Flow_%28psychology%29) for multiple people, each of whom gets distracted from what they’re doing in order to deal with a problem that could have been prevented in the first place.  I'm also skeptical of the quality versus speed tradeoff and believe that both can be achieved. I've worked with many people who have been able to deliver both and believe it's a skill that can be developed just like any other. Some situations do force a tradeoff but I suspect these are in the minority for a good engineer. Even then I would push for a refactor after it's deployed in order to bring the quality up to par. Focusing on quality also builds better habits - you'll get quicker naturally over time but if all you do is prioritize speed your quality won't improve. By focusing on quality first your speed will improve on it's own.",1,1,2015-12-25,4,"management, code quality, speed vs quality, software engineering",347,The true cost of low quality
31,0,There have been rumors that Twitter will move to a curated model for the stream. I think it's interesting to see the impact this will have on their tech stack.,#product,{% include setup %} Apparently Twitter is considering curation user’s timelines. A perspective people haven’t really discussed is the impact on the tech side. Right now each user has a unique timeline that needs to be presented in near-real time in case they need to see it. This results in a massive storage operation using Redis where these timelines are  continuously generated and cached . By moving to a model where every user can be categorized into a group that sees a particular set of tweets Twitter can drastically reduce the amount of data they need to store per user. I’m sure Twitter already has a way of categorizing users in order to support the ad product and this approach would extend it to the “stream” product. In a way it’s akin to how compression works - find repeated patterns and replace every occurrence with something shorter. Then when you want to uncompress you just reverse the process.  I doubt this is the primary driver of the curation discussion and there are clearly more important issues at stake but this may be the proverbial “cherry on top” that will get Twitter to move to the curated model.,1,1,2014-09-08,4,"twitter, curation, social media, tech stack",212,Curated Twitter timelines and the tech stack
23,0,Corporations are getting larger and larger and entering more and more industries. Whatever they lack they're able to buy from a startup.,#meta,"{% include setup %} I’m both fascinated and extremely unqualified to discuss China. Every time I read an article about technology and business in China I discover it’s a completely different world than the one I’m used to. Just today I read an [article](https://www.theinformation.com/chinas-jd-com-fights-alibaba-with-robots-drone) about two competing e-commerce companies - JD and Alibaba - and how they’re tackling ecommerce in China via antithetical approaches. And the market is so big and so nascent that it feels as if every company is trying to be involved in every industry. In the US we have Amazon which seemingly is getting involved in every industry but in China it feels as if every major technology company is doing the same. In this case JD is allying itself with Tencent - a competitor to Alibaba. Maybe it’s the natural order to have massive corporations competing with each other in every industry via partnerships with hundreds of competing startups.  This is very similar with what’s happening in the US now. Amazon’s acquisition of Whole Foods was seen as such a threat by other grocers that they’re all [rushing to partner](https://www.forbes.com/sites/bizcarson/2017/11/08/amazon-whole-foods-deal-future-of-instacart-grocery-delivery/#1e451e7e6d5a) with Instacart - a company many thought would be hurt by the acquisition. The same thing is [happening with Deliv](https://www.theinformation.com/amazons-growth-speeds-demand-for-deliv), a same-day delivery startup that’s quickly increasing the number of partnerships it has with brick and mortar retailers. How does this play out? It seems that the largest companies are getting larger and larger and whatever functionality they lack they will either acquire or partner. And if any one of these major corporations achieves some sort of breakthrough there will always be a startup willing to provide that same service to competitors.",3,1,2017-11-11,6,"ecommerce, jd, alibaba, tencent, amazon, business",302,Modern world of massive competiting corporations
25,0,"If you send out the personal welcome emails from the CEO to every user that signs up, make sure you're responding to the replies.",#product,"{% include setup %} Recently, I’ve been receiving many startups sending out “personal emails” from the CEO or cofounder around 30 minutes after signing up. The idea is to engage the new user by showing them that there’s a real person behind the service that cares and to offer any help that they may need. There’s a great  article  on Segment.io about this tactic as well as a few other emails that can be sent to improve retention. This technique is called “drip marketing” and there are a bunch of companies offering it as a service - the ones I can immediately think of are  Vero  and  Intercom ; and Mixpanel is moving into this space as well with their  Notifications  product. There are also a variety of open source packages available, I’m familiar with  django-drip  for Django and Dan Shipper’s  Faucet  for RoR.  The twist is that you actually need to respond to the people who reply to the email. There have been numerous times where I’d reply to this email without ever receiving a response. At least I understand that the email was most likely automated; I suspect most users wouldn’t be so understanding. I’m not sure why I need to point this out but if you do decide to send out these personal emails, make sure you’re actually going to respond to each reply. Otherwise you’re better off not sending that email in the first place.",6,1,2013-05-02,2,"Drip marketing, personal email",268,Follow through on that personal welcome email
5,0,An excuse for not blogging,"#blog,#meta",I've been busy recently but have a bunch of topics that I want to write about so keep on checking.  Thanks for reading!,0,2,2009-01-20,1,blogging,23,More posts coming up
15,0,"Now that Zynga bought Draw Something, I wonder what Draw Something will turn into?",#product,"We’re aware of Zynga’s purchase of Draw Something and Zynga’s emphasis on analytics and metrics to drive product features and decisions. I’m a bit late to the party but I tried brainstorming to put together  a Zyngafied version of Draw Something:              Favor drawings that require colors that a user does not have to encourage the user to buy new colors.      Leverage the priming effect by picking words that will encourage users to spend more. For example using the words “gold”, “coin”, and “rich” would put users in a buying mood.      Charge more for the more popular color packs.      Include “limited edition” color and word packs.      Reward active users with free color packs, bombs, etc.        I, for one, am glad that I got to play Draw Something before it turns into this money extraction machine.",0,1,2012-04-12,2,"draw something, zynga",143,Draw Something Zyngafied
36,0,I attended a talk by Prof Michale Stonebraker where he shared his thoughts about the future of databases. In particular I like that many of these new databases are standardizing around a SQL-like syntax.,"#sql,#data","{% include setup %} A couple of weeks ago I attended a talk by  Professor Michael Stonebraker . For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.  This is already happening at the larger tech companies and it’s spreading downwards. Data is becoming increasingly important and having the tools available to leverage it is a critical advantage. It’s impossible to find a single database that can be used to run a transactional site, support complex yet quick analytics queries, scale to terabytes of data, and still maintain synchronization between its various nodes. Each of these use cases requires a database that’s optimized for that need and an application that knows how to leverage that database.  The neat thing is that many of these newer databases have embraced a SQL-like query syntax so it’s surprisingly easy to get started. The challenge is that this similarity is only skin deep and the implementations are drastically differently both in terms of how the data is stored as well as how the queries are executed. So although it’s simple to write a query that will execute on both Redshift and PostgreSQL it’s likely that this query isn’t as efficient as it can be on one or both of the databases.  This is the right approach for the specialized-database future. By providing a standard interface it makes us more comfortable with introducing a new database into our stack while providing very different functionality under the surface. It’s likely that the first implementations won’t be ideal but as teams become more comfortable with these new systems the implementations will evolve. I hope this pattern of standardizing around a simple interface becomes more popular. Then the backend can be designed for a variety of use cases without forcing the users to completely change the way they think.",1,2,2014-07-05,8,"databases, sql, data, mysql, postgresql, vertical, voltdb, redshift",371,The future of databases
29,0,We have a routine and goal when going to the gym and yet for something we spend 8 hours at each day we do not. This is crazy.,#meta,"{% include setup %} To get the most benefit from working out it’s important to have a plan and consistently measure yourself and keep pushing your goals and yet it’s surprising how rarely that’s done in a professional setting. We spend over 40 hours a week working but the majority of us view it as a chore and something that we just have to do. Imagine if everyone approached work the same way they approach the gym. People would have much clearer ideas of what they want to do and what challenges they face. They would be able to measure how well they’re doing and understanding what they need to start doing to get to the next level. Instead most approach it as something that they need to do rather than something they want to do.  Consistently going to the gym without a plan will definitely improve your shape and is better than not going at all. But it pales with what would happen if you went to the gym with a plan in mind. It took me a long time to realize this and I suspect most people approach work passively - they’ll just put in the time, do a good job, and see where things will go. But true success and joy come from constantly reevaluating your goals and thinking through the means to achieve them.  It’s depressing to think that most people spend a third of their day on something that they’re not actively engaged with when an attitude change can change the entire perception of work. Rather than putting in the bare minimum we should be thinking of what we want in our lives and how work can make that happen.  Many people seem to idealize retirement but I want to be so engaged in my work that I never want to stop. This is the vision we should all be working towards. I understand that this is a privileged perspective and not everyone has this choice and there are always constraints but it’s something we should all strive to attain.",0,1,2016-03-13,3,"life goals, work, success",345,Approach work like the gym
28,0,I watched Thursday night football via the Twitter app and it highlights the opportunity Twitter has. It leverages their strength in real time with a great product.,#product,"{% include setup %} In April, Twitter [announced a deal](http://www.bloomberg.com/news/articles/2016-04-05/twitter-said-to-win-nfl-deal-for-thursday-night-streaming-rights) with the NFL to broadcast Thursday night games and I gave it a shot this past Thursday via the Twitter app on my FireTV. The primary motivation was to watch the game but I was also curious to see Twitter’s implementation. I was pleasantly surprised by how smooth and clean the overall experience was: you could watch the entire game without knowing it was via Twitter but the tweets added a please, yet optional, touch. The only real difference between the Twitter app and any other FireTV streaming app was that Twitter augmented the experience with twitter content - tweets, images, and scopes.  This NFL product makes me optimistic about Twitter and does feel as if they finally stumbled unto a product that works and reinforces their strengths. By focusing on live events and building on top of them with content that’s unique to Twitter they have the potential to change the way we consume live TV. At the moment the feed seems to be chronological but if Twitter can figure out how to make it a bit more relevant it can make the feed section standard.  The feed section currently takes up close to a third of the screen which causes the video to be scaled down. The sidebar is an obvious first attempt but I can think of other ways the tweets can be shown - maybe a ticker tape or even a translucent overlay - would make it likelier that people keep the feed on throughout the game.  There’s also a ton of opportunity in opening this platform up to developers. Twitter developed the reputation of betraying the developer community during their growth but this can be a chance to redeem themselves. Imagine being able to build an app that lives within the TV app and can show you how your fantasy team is doing or just displaying a more targeted subset of the tweets or even pulling in additional stats. All Twitter would need to do is provide the platform and the community can build on top of it to deliver custom experiences. The incentive is already there - being able to have an app that’s used while people are watching TV is hugely motivating and will get developers supporting Twitter.  Disclosure: I own a small number of Twitter shares.",1,1,2016-09-25,3,"twitter, nfl, twitter thursday night football",405,NFL Thursdays on Twitter
10,0,I used some old jeans to make an art piece,#meta,"{% include setup %} At the beginning of 2013, I set a  goal  to do something with my hands to contrast with the constant life in front of the screen. I finally finished my first “art” project this past weekend and documented the result. I had a stash of old, torn jeans that were just taking up space and instead of throwing them out I decided to have some fun. Here’s the process and end result.                              1. Find a silhouette of the NYC skyline                                      2. Replicate the silhouette by cutting pieces out of old jeans and arranging them on a piece of cardboard wrapped in another shade of jeans.                                      3. Use some fabric glue to attach the pieces to the canvas                                      4. The result - now all it needs is a frame",1,1,2013-03-20,3,"jeans, jean art, dyi",220,"One art, please"
19,0,Mary Meeker does an annual report highlighting internet trends and the 2017 version was a blast to read.,#society,"{% include setup %} In what has become annual tradition, Mary Meeker has just published the [2017 Internet Trends report](http://dq756f9pzlyr3.cloudfront.net/file/Internet+Trends+2017+Report.pdf) and it’s a whopper. Over 350 pages describing the state of the modern digital world. I can’t imagine how long it must have taken to pull the data and put it together but I’m incredibly grateful that it’s been done and continues to be done every year. There’s so much great stuff in there that it’s worth going through it on your own but I wanted to highlight the slides that stood out to me.                              Digital advertising has finally surpassed TV with a much higher slope. This also indicates total ad spend is growing. I'm surprised that TV advertising hasn't started dropping.                                       While the entire online advertising industry is growing Google and Facebook are taking up the lion's share. The rich get richer.                                       Adblocking is growing but it's incredible to see how it varies by country and device type. The Western world is all about adblock on desktop but Asia is all about adblock on mobile. It's incredible that Indonesia has an adblock rate of 58% on mobile.                                       I just found this interesting and it makes sense given that we're doing so much shopping online and need more proof that we're not being scammed.                                       Another interesting slide that just highlights the secondary effects the internet is having. I can imagine building lobbies being completely swamped and needing to adapt to much higher shipping volumes.                                       Pretty cool seeing designs generated by machines based on data. I expect to see much more of this even the ability to create one off designs per customer based on their unique profile.                                       Compared to the old media companies Netflix is growing incredibly quickly. Other than the Discovery channel every other old company is dropping in minutes watched.                                       Despite that Netflix is still small compared to the major social network companies.                                       UX is becoming increasingly important and companies are starting to realize this by improving their designer to engineer ratios.                                       The next few just highlight the scale of China and India. In this case despite the average GDP of China being significantly lower than the USA it has such a large population that the total gaming revenue surpasses that of the US.                                       Similar with on demand transportation. We talk about Uber taking over the world but China has more on demand car and bike trips than the rest of the world combined.                                       India has a similar population to China but is much further behind in adoption. Fewer than 30% of the population is online and it's crazy to imagine what will happen when it gets closer to 100%.                                       Another example of how different the developing world is. The developing world is mobile first - which probably explains the significantly higher rates of mobile adblocking.                                       India has a massive opportunity here given how young its population is. In the US peak income is achieved in people's 50s while in India and China it's in the 20s and 30s. It's hard to fathom what this means for the future.",1,1,2017-06-04,2,"mary meeker, internet trends report",839,Mary Meeker's Internet Trends 2017
6,0,How to improve your luck.,#meta,"People often blame bad luck for their failures. This absolves them of responsibility and allows them to stop trying. What they should have done is admitted their failure, learned from the experience, and prepared themselves for the next opportunity. Exposing yourself to opportunities is the best way to overcome bad luck. Authors are a great example of this:  J K Rowling  and  John Grisham  had their novels rejected numerous times before they succeeded. Yet soon after publishing they became blockbusters. How many authors gave up when trying to have their work published? Imagine if they had the determination that J K Rowling and John Grisham had.  As  Seneca the Younger , a Roman philosopher, said, ""Luck is what happens when preparation meets opportunity."" In order to increase your luck you need to increase your exposure to different opportunities. In addition, you need to realize an opportunity when it presents itself. A way to view this is through the simple roll of a die. Although a die only has a 1 in 6 chance of rolling a 1 when rolled once, it has a greater chance of landing on a 1 when rolled multiple times. Most people give up after a few rolls but in order to succeed you need to keep on playing the game until you get a successful roll.  The fact that you are reading this shows that you are luckier than the majority of the world's population. You have access to the internet and the desire to improve your luck. You can leverage that to contact leaders in your field or people who can help you succeed. You may get no responses from some but you will get encouraging responses from others. You just need to be open and increase your opportunities.",3,1,2009-09-28,1,luck,349,Improving your luck
20,0,No one is talking about the data Tesla is collecting and how valuable it will be in the future.,#meta,"{% include setup %} What’s lost in the  Tesla/NY Times discussion  is how much information Tesla is collecting. Tesla collected the location, the speed, and the battery charge throughout the journey and referenced it during the rebuttal. Is Tesla collecting this data for every car sold? Do the drivers know this data is being collected? If John Broder knew Tesla had this data from his drive  his review  would have turned out differently. We’re all in favor of truth and honesty in reporting but should it be this easy to share data? What prevents Elon Musk from digging into the driving data of a politician who proposes some legislation that will adversely impact Tesla and finds likely unethical behavior?  As  software eats the world , data will be collected from more and more areas of our lives. Target is already  figuring out  whether you’re pregnant and this is just from using your purchase history. Combine that with other data sources, increased computation power, and cheaper data storage and companies end up knowing us better than we know ourselves. We need to make sure that our privacy evolves alongside the data. Currently, the concept of data privacy is too abstract to make us care. We need to see the actual data and the derived results in order to see how valuable it is. Only then will we want to protect it.  Disclosure: I love what Tesla is doing and own Tesla stock. I also realize that this data is used to offer a better, cheaper product. At the same time, I believe we need to find the right approach to privacy when it comes to our data.",4,1,2013-02-21,3,"tesla, privacy, data",300,Tesla and privacy
35,0,I had an interesting dinner at a restaraunt that allows you to pay what you want for your meal. The best part was seeing the subtle ways the restaurant gets you to pay up.,#meta,{% include setup %} This past Wednesday I had dinner at Blu - a restaurant that’s  adopted  a “pay what you want” pricing model. Customers have an incentive to underpay the final check so I was curious to see how Blu handled it throughout dinner. I noticed three tactics they used to get people to pay fairly and am sure they utilized a bunch more that I didn’t even notice:  - Anchoring: Before sitting down to eat the waitress explained that it was pay what you want and most of the dishes are estimated to be priced between $10 and $12. This sets the expectation early so if you do decide to pay less you’re making an explicit decision to underpay. - Reminder: At the end of the meal we were told how many dishes we ordered. This was also helpful but I can’t help but think that this is a way to give you an estimate of how much you should pay - especially when paired with the fact that the expectation is $10 per dish - a very easy number to multiply. - Shame: I found this the most interesting one. Instead of giving you a blank receipt and allowing you to write what you want to pay you have to tell the waitress what you want them to charge. This forces you to explicitly vocalize your payment to another person rather than quickly writing something and slinking away. And no one wants to be judged as cheap face to face so we’re encouraged to pay well.  I’m a huge fan of behavioral psychology experiments that shed some light on the way our minds work and it was a great experience to partake in one. I only wish I could have spotted more behavioral cues that I’m sure they employed.,1,1,2015-08-29,3,"dining, game theory, behavior psychology",309,Game theory dining
22,0,Travel is a great way to improve creativity and companies should be encouraging it. Instead many are trying to restricti t.,#meta,"{% include setup %} Now that I’ve started blogging I realize how important traveling is to creativity. After my trip to India I had a ton of different blog ideas. Some came from comparing the two cultures - for example cab rides and mobile phone business while others just came from realizations, such as the lack of truly global technology products. Many dismiss travel as a luxury but it’s a great way to bring a new perspective and let thoughts settle. In my case, it felt as if these connections formed subconsciously based on what I’ve been thinking about and doing actively for a year. It’s not surprising that our conscious experiences drive these subconscious connections but it’s interesting how stark this realization was. Prior to blogging, I never would have had an idea and immediately think of writing about it but it’s become a consistent thought. Travel encourages this serendipitous thought and companies should be encouraging it. Instead, many black ball employees who take a vacation and make employees feel guilty for taking some time off.",0,1,2014-02-09,3,"travel, vacation, creativity",177,Travel more
25,0,"Turo doesn't provide a way to download your ride history so I wrote a scraper that does it via Python, Chrome's WebDriver, and Selenium.",#code,"{% include setup %} I've been using [Turo](https://turo.com/) to rent our car out for the past couple of months and have been using a simple spreadsheet to track the revenue. Being a lazy engineer doing this manually became a bit tiresome so I finally automated it. Unfortunately Turo does not have a simple way of downloading the data and there’s no open API so I had to resort my usual solution: [scraping](https://github.com/dangoldin/turo-automation). Luckily for me I just came off of updating my Yahoo fantasy football scraping script and was ready to do the same for Turo.  The entire process took a few hours and the [result](https://github.com/dangoldin/turo-automation) is decent - it goes through every one of your completed trips and scrapes the receipt page for the total paid, total earned, the various reimbursements, and the start and end times. As of this writing it still doesn’t handle cancelled trips or trips that have not yet been taken. Another thing I noticed when writing the script is that Turo changed the representation of a trip - some of the older receipts had reimbursements in a different section from the newer ones so that needs a bit of tweaking. I’m sure there are some other edge cases I’m not handling properly since I could only code against the data I have; if it ends up not working for you let me know and I’ll see what I can do.  The process to build the scraper was standard: use Chrome’s source inspector to examine the structure of the page and then try using a few different selectors in an interactive Python section running [Selenium](http://www.seleniumhq.org/) to see whether they worked as expected. Once I had the various selectors and code figured it out it took a little bit of refactoring to get into a somewhat clean state.",4,1,2016-08-21,4,"turo, download history, car rental, relayrides",308,Downloading your Turo ride history
29,0,It's remarkable to think about but GPS is the foundation of so many of our modern luxuries and it was developed by the government over 40 years ago.,#society,"{% include setup %} I spent a few hours driving today and couldn’t stop but think how different driving is now compared to the pre-GPS, pre-smartphone era. Before them I would be extremely wary of deviating from the preplanned path in any way. That meant avoiding all sorts of detours and prioritizing rest stops over exits. It also meant traveling with a road atlas and planning your exact route before setting off. And always questioning whether you missed a particular exit or turn and need to turn around.  These days it’s much simpler - just get in the car, enter your destination, and go. You don’t need to plan having confidence that your phone will get you wherever you need to go. and And if you miss your turn you’ll get a new set of directions to get you back on track. This gives you the freedom to go out of your way to explore whatever looks interesting on the road or change your route to take care of some errands.  These were just the superficial thoughts; what’s really interesting to think about is how much of the current world depends on GPS. Ridesharing companies wouldn’t exist without GPS. Sure we would have had a more intelligent dispatch system but GPS commoditized drivers since you didn’t need to have any street knowledge to drive. In fact, I’ve never taken an Uber or Lyft where the driver wasn’t using a GPS.  Beyond ridesharing, delivery heavy companies would suffer. For the same reasons that GPS makes my life easier it makes life easier for delivery drivers everywhere. This leads to a lower delivery cost which increases total delivery volume - where it’s food delivery or anything purchased remotely.  What’s remarkable is that GPS was developed by the US federal government - initially for military purposes but then expanded into civilian use. A private enterprise wouldn’t have done anything of this global scale and it highlights how positive of an impact a well intentioned government can have. This decision made more than 40 years ago is the foundation for a huge part of the modern world and one can only hope we see more of these foundational innovations.",0,1,2017-10-21,4,"gps, government, technology, delivery",365,GPS: The foundation of them all
22,0,We should learn from others but realize that almost all advice is black and white whereas startups are shades of gray.,"#product,#meta","{% include setup %} When I was making the leap into the startup world I read every post I came across that talked about people’s experiences and guides in running a startup. The goal was to learn as much as I could form others and apply these hard-fought lessons my own startup. Now that I’ve been working on a startup for almost two years I realize how much startups differ from one another and how black and white these guides tend to be. You can read two posts that will promote contradictory approaches. Should you focus on revenue or growth? Should you raise money or bootstrap? Should you go with a freemium model or paid only? Should you go solo or get a cofounder? Should you focus on consumers or the enterprise?  None of these questions have a universally right answer. What worked for one startup will not necessarily work for another one. There are just too many differences; the product, market, teams are all different. Time plays a huge factor as well. In a field as quickly moving, and novelty loving, as technology what worked 6 months ago may not have a chance right now. Startups are tough. If it were as simple as just following a how-to guide the success rate of startups would be an order of magnitude higher than what it actually is.  The best we can do is be aware of the available options and try to understand why certain strategies worked for others. We shouldn’t ignore what we read but we also shouldn’t emulate an approach just because others succeeded with it. We need to be the experts of our markets and imitating others only undermines that knowledge.",0,2,2013-09-02,2,"startups, entrepreneurship",287,Startups aren't black and white
13,0,We ran into an RDS replication issue that I've never seen before.,#devops,"{% include setup %} Earlier this week we encountered an odd RDS issue that I’ve never seen before. An AWS hiccup caused a database replication query to fail which stopped the replication process. We discovered this the following day when we saw weird results during after running an analysis query. The nice thing was that this wasn't a huge deal since our production system relies on the master database but we did have to spend time dealing with this.  When we discovered this issue we did a few online searches to see how to resolve the issue and resume the replication. Turns out there's a command, ""CALL mysql.rds_skip_repl_error"", that will skip the current replication error and move on. In our case, the errors occurred when creating temporary table for a legacy job so we were able to skip it. Otherwise, we'd run the risk of breaking the sync between our master and replica databases.  Unfortunately, running this query once wasn't enough since the error keep on reappearing. After speaking with an AWS rep, we realized we could keep on running that command until we skipped past the replication errors. Another useful tip was to look at the ReplicaLag CloudWatch metric to see how far behind the replica database was from the master. In our case after going through a couple of dozen of these skip error calls replication resumed but the replica database was still more than a day behind.  While the replication caught up, we made a quick update to our scripts to point to our master database instead of replica so that our jobs would reference the correct data. After replication caught up we simply reverted this change.  To prevent this issue in the future, we're going to revisit the jobs that were using the temporary tables. We've also added a CloudWatch alert to notify us if replica gets too far behind. In a way we got lucky since these errors were recoverable. Without that we would have had to recreate the replica database which may have had a performance impact on our master database.",0,1,2014-09-20,4,"rds, aws, database, devops",349,Dealing with an RDS replication issue
21,0,It's easy to expose some functionality in a product by adding a new option but it's likely the wrong move.,#product,"{% include setup %} Building products involves making countless decisions. One of the biggest is defining the functionality and how it should be exposed to the users. In my mind, answers to this question like on a spectrum. On one extreme you have the “take what you can get” approach where the functionality is exposed with no customization and no advanced features but the experience is optimized for one specific use case. As an example of this think of the original Google search - a single search field, minimal search functionality, and two buttons. On the other extreme you have the “customize everything” approach where you think of all possible use cases and provide options to allow users to do what they want. An example of this is Microsoft Word - most people use a fraction of all the functionality yet there’s a ton hidden away behind some menu.  Consumer facing products generally fall into the first camp since you want them to be as accessible as possible while enterprise and productivity products fall into the latter camp where it’s about optimizing for power users. I understand the value of both approaches and often times you can get away with the experience of the former while providing the functionality of the latter. Google search is an example of this - it’s a single field but after doing a search you have access to the advanced search functionality to refine your search. Also, if you’re a power user you know there are a few things you can type in to the search bar to further control your search - for example using quotes for an exact match or limiting the search to a single site.  It’s rare that you find a product with fewer features in newer versions. The majority of the time products get more complex as functionality is added to make the product more appealing to a wider set of users. This makes sense when making an isolated decision but it also feels like death by a thousand cuts since each option makes the product just slightly worse for a majority of users. As product managers and engineers we should be extra thoughtful in taking the easy way out and adding an option to expose a different behavior. This option will be incredibly difficult to remove and we should take the time to understand the underlying problem and use case to determine whether the option is something that’s necessary or just us being lazy.",0,1,2018-11-24,5,"product management, software engineering, software, product design, user experience",412,Adding optionality to products
30,0,"I read a ton but can't find an ebook reader that I like. The problem is that the creation, distribution, and consumption of content are coupled into closed ecosystems.",#product,"{% include setup %} I'm an avid reader and have embraced the move to digital. An internet connection gives me access to thousands of books with a device that’s thinner than a single book. What I grapple with are the reading apps - I can’t find one that does everything I want.  On my iPad, I have iBooks, Readmill, Oyster, Kindle, and ShuBook with each having a separate use case. iBooks and the Kindle app are for books that I purchased from iTunes and Amazon, respectively. Oyster is a great ebook subscription service but I’m limited to the books available in their library. I discovered ShuBook when I wanted to host my own ebook server but have switched to Readmill due to the much nicer reading experience, a web interface to manage my library, and cross-device syncing.  Ideally, I’d be able to use Readmill for everything. I don’t mind paying for books but I do mind paying to be locked into a particular ecosystem. The creation of content should be decoupled from the distribution of content which should be decoupled from the consumption of content. Yet these days they’re tightly coupled. The only real way to overcome these restrictions is to become a digital pirate. It sucks that customers are forced to break laws in order to get the best experience.",0,1,2014-01-15,3,"ebooks, reading, DRM",222,Ebook readers
13,0,Rather than securing applications through hoops they should be secure through walls.,#devops,"{% include setup %} Two weeks ago Travis CI published a [postmortem](https://blog.travis-ci.com/2018-04-03-incident-post-mortem) describing an outage that was caused by a script that truncated all tables on a production database. The script was designed to run against a test database but instead ended up wiping the production one. The remediation steps highlighted are a great start but I’m surprised they didn’t pick the most obvious one - protect systems at the network level.  Relying on confirmation steps, user permissions, and unique credentials per environment are great steps and should be best practices but they don’t actually stop malicious or accidental behavior. They reduce the risk by adding more friction but it’s still possible to circumvent these blocks.  The way to eliminate these types of issues is to not rely on hoops but get rid of the loophole entirely. In the scenario above, the database should block traffic from all IPs that have not been whitelisted. And the whitelisted IPs should belong to production applications that need access to the system. In turn, these applications should not allow any SSH access to prevent someone from tunneling through. The way this is done in AWS is by using security groups and giving them the least allowable permissions while still allowing them to function properly. Exceptions can be made but they should be temporary and overseen by more than a single person to avoid any problems.  This sounds draconian but by investing in this approach up front you end up with a much stronger system in the long term that you don’t have to revamp to secure. And since doing this manually is a huge pain you end up investing in tools, such as [terraform](https://www.terraform.io/), that make these rules much simpler to manage.",2,1,2018-04-17,4,"security, devops, travis ci, outage",299,Secure at the network level
16,0,There's a neat iterative algorithm to generate a Sierpinski triangle that I implemented in D3/JavaScript.,"#dataviz,#javascript,#code","{% include setup %}  There's a little known algorithm for constructing a  Sierpinski triangle  that is surprisingly easy to implement.  1. Start the three vertices that form a triangle 2. Pick a random point inside the triangle 3. Pick a random vertex 4. Go halfway from a the random point to the vertex and mark that point 5. Go to step 3 using the result of 4 as the starting point  I'm trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following  file . Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the  Sierpinski carpet  can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.       {% include D3 %}  {% include custom_js %}",3,3,2014-02-19,4,"sierpinski triangle, D3, visualization, JavaScript",175,Sierpinski triangle in D3
28,0,How many languages should a codebase be? Some like the idea of choosing the perfect language for each project while others like standardizing around a few languages.,#meta,"{% include setup %} I’ve discussed the pros and cons of having a codebase out of a few languages versus having the choice made per project or application with a bunch of people and opinions differ. On one hand, having many languages provides flexibility in choosing the right language for the job and allows engineers to learn and explore new tools. Better habits are encouraged since the interface between components requires a well structured and tested structure rather than relying on code similarity. On the other, it prevents code and component reuse and makes it difficult for teams to standardize around a style and codebase. Also, engineers can’t switch projects as easily as they’d be able to under a common language and prevents the depth of knowledge one gets from working on a shared codebase with others.  I used to think the flexibility of being able to choose the right language for the job was the only thing that mattered but now prefer a limited language codebase. With modern languages, libraries, and tools it’s easy to write good code in any language and there are only a few applications that warrant a specific language, and even then only at scale.  My perfect mix is a static, strongly typed language for large, complex codebases that have high performance needs, a scripting language for one off tasks and processes and for quick experimentation, and JavaScript with a framework for the frontend. The static, strongly typed language makes it much easier to refactor code and improves performance while the scripting languages make it easy to quickly get something working or prototype an idea. My current favorites are Java and python - they’re both easy to write and complement each other’s weaknesses. Something I haven’t had a chance to explore in depth is moving to Jython or another JVM based scripting language - that would provide the benefits of the highly functional and robust Java code from a scripting context.",0,1,2015-01-09,5,"java, python, jython, limited language codebases, software engineering",325,Limited language codebases
20,0,I was the target of a scam and it should not be that easy to spoof a phone number.,#society,"{% include setup %} I use Google Voice as my voicemail due to the built in transcription which lets me tell at a glance whether a call is spam or in the rare case, worth returning. This past Friday I had my phone on silent for most of the day and was pleasantly surprised with the following transcription greeting me after I checked in:      Scam voicemail message   I immediately thought of scam and went through the process of calling the SSA and the fraud hotline but given the hour-long wait-time and the maze of “press X to do Y” I gave up. Doing a quick Google search confirmed that it, in fact, was a scam and I had nothing to worry about.  What’s shocking is how easy it is to spoof a phone number. Especially that of a government agency. It’s extremely illegal and one would think that whatever service was used to do the spoofing would prohibit this from being possible. I consider myself digitally savvy and yet I had to double check. Others may not be so lucky.",0,1,2018-11-12,3,"social security adminstration, scam, phone spoof",198,Social Security Administration spoofing scam
26,0,Our applications have gotten more and more complicated with dozens of different components and debugging those requires a different skillset than debugging single application bugs.,#devops,"{% include setup %} A skill that seems lacking is the ability to debug large scale applications. Most people are comfortable looking at exceptions or log files and working their way back to an issue in the code but given the complexity of modern applications that’s not enough. These days applications are hosted across dozens of cloud instances while utilizing a ton of cloud services. This makes it easier to ship applications but also makes it more difficult to isolate and identify issues since they’re no longer isolated to a single service or application. When there are dozens of instances and dozens of services talking to one another an issue in one system may manifest itself as a phantom issues in others which can lead to a significant amount of wasted investigative effort.  Exploring large system issues follows the same approach as investigating small issues but the toolset is different. They both start of by coming up with a hypothesis that may explain the situation we’re seeing and then a series of steps to either confirm or reject our conjecture. If it’s rejected hopefully we’ve picked up enough clues along the way to come up with another hypothesis. And this cycle repeats until we’ve figured out what the root problem was.  In both cases you’ll be much quicker at getting at the cause if your hypotheses are intelligent. This can only happen if you have a good understanding of the system architecture and the way the data flows from one service to another. Without this knowledge you’ll be guessing and exploring blindly.  Application bugs provide exceptions and logs that help us dig into problems but larger architectures require more. I’ve found high level metrics and charts to be incredibly helpful in coming up and exploring my guesses without even having to dig into the application itself. Using AWS, and surely other cloud providers, one can get high level metrics as to what’s happening with each instance or hosted service. This may be the CPU usage, the amount of data going in and out, the number of requests - you name it. Each of these provide a data point as well as a timeline that can quickly highlight the issue. But once again these are only useful if you know how the various pieces fit together.  While most companies have dedicated DevOps engineers making sure things are running smoothly I believe every engineer should have an understanding of the overall application and where their contributions fit in. This knowledge helps both in writing more intelligent code and digging into system-wide issues when they arise.",0,1,2017-06-17,3,"debugging, application issues, bugs",434,Investigating application issues
16,0,My short term memory has become optimized for interruptions and that's not a good thing.,#meta,{% include setup %} While the title is using two technical terms the post is actually about human memory - my memory. I’m not sure if it’s just me but lately my short term memory has been behaving like a stack. I’m working on something and then an interruption new comes along which suddenly gets pushed to the top of the stack. Now I’m working on this intrusion and when that’s done I hopefully remember my previous task so I can resume until it’s either done or the next task comes along and gets pushed to the top again.  The modern world has encouraged this mindset and approach with constant interruptions. It’s more difficult to stay focused on the task at hand. We take breaks to check our email or browse a social media app and we’re adopting that approach to more serious work. As I write this post I have to resist the urge to take a break but at least I’m self aware enough to catch my subconscious in the act.  Rather than treating our short term memory as a stack we should be treating it as a priority queue. Then the important pieces stick around and the interruptions get forgotten. Unfortunately we’ve trained ourselves to do the exact opposite and something we need to actively train ourselves to avoid.,0,1,2018-05-14,5,"memory, stack, queue, efficiency, productivity",222,Memory as a stack
22,0,I've looked at hundreds of resumes of coding bootcamp graduates and below is some advice on getting a software engineering job.,#meta,"{% include setup %} Coding bootcamps are increasingly popular and I’ve seen a large number of resumes come across my desk so wanted to share my perspective and offer some advice. I think it’s great that more people are learning to code. At the same time there’s a lot of volume and based on a few months it’s difficult to stand out, especially as more and more bootcamps spring up. First off, I respect the hell out of people taking the leap. It takes a lot of effort to stop what you were doing and pursue a completely different career track. It’s not easy and already sends a signal that you’re motivated and willing to grow your skills. Below are a few other ways to help improve your odds of getting hired. Most of these are relevant even if you’re not coming from a bootcamp so read on if interested.  - **Leverage your prior experience**. Everyone comes into a bootcamp with their own set of experiences and a good way to stand out from the crowd is to leverage that experience in a future role. Many companies would be willing to take on someone less experienced in coding if they make up for it with business and industry context. If your worked as an architect why apply to every other startup? Instead apply to software companies serving the architecture field or potentially technical roles at an architecture firm. No matter what your prior experience was there should be a company that would benefit from that prior knowledge and coding experience. - **Do a personal project outside of the bootcamp**. Bootcamp projects require you to do projects and are typically done with teams so it’s tough to know how much of that was done by you versus others. In addition, mentors helped so the projects aren’t an accurate barometer of your skill. A way to combat that is to do a project entirely on your own outside of the bootcamp and explicitly call it out on your resume. Bonus points for having it on GitHub with an ongoing stream of commits and even more bonus points for having it up and running at a live link. - **Commit to open source**. Another way to make up for the group project is to commit to an open source project. It shows that you’re familiar with version control tools and can code well enough to have your code accepted into an open source project. More importantly, it shows you’re serious about improving as a developer. - **Understand your application funnel**. A typical application flow is you see a job listing, submit your resume, have a quick phone screen, do a take home exercise, and then visit the office for an in-person interview. Knowing this you should understand your stats at different points in the funnel. Are you not getting to the phone screen? Work on your resume. Are you not getting invited for the in person? Focus more on your code test. By knowing these numbers you can concentrate on your weakest areas. - **Ask for feedback**. A simple and easy way to improve is to ask for feedback. Most of the time you won’t get anything but when you do it’s well worth it given it’s such little effort. Showing humility and a desire to improve can work wonders. Most people understand where you’re coming from and want to help. - **Revisit prior interview questions and exercises**. The job hunting process can be exhausting but the benefit is that you get to go through the gauntlet and collect a ton of questions and coding exercises. A good way to gauge how much you’re growing is to go over some of the prior questions and exercises and take another shot at them. If you end up repeating a take home code test and have the same implementation as you did a month ago it indicates you haven’t improved enough and need to rethink your approach. On the other hand, a cleaner and more expressive attempt is an indicator that you’re improving.  I hope these tips helped but the general idea is that having a bootcamp on its own is not enough. You need to be thinking of ways to differentiate yourself while constantly improving your skill and craft.",0,1,2017-02-21,4,"coding bootcamps, webdev bootcamps, software engineer hiring, software engineer interviews",711,Advice for coding bootcamp graduates
20,0,I think engineers are too impatient to participate in politics and give up too easily. Society even encourages this.,#meta,"{% include setup %} At Aaron Swartz’s memorial service in New York, Doc Searle said something that struck a chord: Aaron was one of the few tech people who would get involved in legal and political issues. It’s true - we hackers aren’t into it. We claim we’d be better off if there were more engineers in charge and yet we’re not making an effort to be those engineers. I’ve heard a variety of unconvincing reasons: it’s just not interesting; there’s too much bullshit; it’s more about selling than creating. I think the real reason is that we’re just too impatient.  Our roles and jobs have made us this way. Our work tends to have well structured problems that are solved through individual effort. Only when we have to rely on someone else do we become aware of how slow things move and how long things take. Even the agile methodology, for all its wonders, focuses on the short term and encourages small, easy achievable tasks. It’s no surprise that when we encounter something that takes longer than we’re used to that we dismiss it as not for us.  Impatience is also why I had difficulty as a product manager after coding for 5 years. I had a grand vision of what needed to be done but wasn’t able to execute it. I blamed it on the politics but it was really my impatience and immaturity. It was easier to work with developers to build the product features and just release them than it was to work with the actual users and get them on board. That would have required understanding their use cases, listening to everyone’s concerns, having a trial period, and all sorts of other things that would take too long.  Most of us do want to make the world better and do make an effort to contribute; we give up too soon. During the service, Roy Singham quoted Frederick Douglass:  “If there is no struggle, there is no progress.” Real progress takes time and we need to get comfortable with that if we want to see it happen.",0,1,2013-01-23,1,,352,Why aren't there more engineers in politics?
25,0,If you have a site that uses infinite scroll make sure you don't have anything clickable in the footer. It's a terrible design pattern.,#design,{% include setup %}          	    		        		         	               I’m not sure why this needs to be said but if your site offers infinite scroll make sure you don’t have anything clickable in the footer. I’d expect the occasional site to succumb to this but I was surprised to see it happening on LinkedIn. All I wanted to do was read the developer docs but unfortunately the link is located in the footer which provides a nice challenge of clicking the link before new content is loaded. I wasn’t quite able to get it and ended up just searching Google for the LinkedIn documentation link. If your site’s content is only accessible via a Google search you have a problem.,1,1,2013-09-03,5,"design, anti-pattern, footer, infinite scroll, linkedin",150,Design anti pattern: footer under infinite scroll
29,0,In the 2015 Amazon shareholder letter Jeff Bezos discusses Type 1 and Type 2 decisions and I think that's a great framework to use when writing tech specs.,#management,"{% include setup %} Every year since Amazon went public, Jeff Bezos writes an insightful and penetrating shareholder letter that covers a variety of business topics driving Amazon’s success. In 2015 he wrote about Type 1 and Type 2 decisions:  >  We want to be a large company that’s also an invention machine. We want to combine the extraordinary customer-serving capabilities that are enabled by size with the speed of movement, nimbleness, and risk-acceptance mentality normally associated with entrepreneurial start-ups.  > Can we do it? I’m optimistic. We have a good start on it, and I think our culture puts us in a position to achieve the goal. But I don’t think it’ll be easy. There are some subtle traps that even high-performing large organizations can fall into as a matter of course, and we’ll have to learn as an institution how to guard against them. One common pitfall for large organizations – one that hurts speed and inventiveness – is “one-size-fits-all” decision making.  > Some decisions are consequential and irreversible or nearly irreversible – one-way doors – and these decisions must be made methodically, carefully, slowly, with great deliberation and consultation. If you walk through and don’t like what you see on the other side, you can’t get back to where you were before. We can call these Type 1 decisions. But most decisions aren’t like that – they are changeable, reversible – they’re two-way doors. If you’ve made a suboptimal Type 2 decision, you don’t have to live with the consequences for that long. You can reopen the door and go back through. Type 2 decisions can and should be made quickly by high judgment individuals or small groups.  > As organizations get larger, there seems to be a tendency to use the heavy-weight Type 1 decision-making process on most decisions, including many Type 2 decisions. The end result of this is slowness, unthoughtful risk aversion, failure to experiment sufficiently, and consequently diminished invention.1 We’ll have to figure out how to fight that tendency.  > And one-size-fits-all thinking will turn out to be only one of the pitfalls. We’ll work hard to avoid it… and any other large organization maladies we can identify.  This has stuck with me over the years and I often com back to it. Lately I’ve been thinking about how this fits in with tech specs. As our applications become more complicated it becomes more important to take the time to think about the impact new code will have and tech specs help clarify that thinking, highlight risks, and get buying from everyone involved. At the same time it’s very easy to get carried away and spend so much time moving things around with actually pushing anything forward.  This is where the Type 1 vs Type 2 approach makes sense. Critical components warrant the extra diligence but many features would benefit from a more iterative approach. The entire Agile development process is designed around getting features built and quickly iterating to get closer to the ideal. Microservices also fit into this idea - by splitting your application into many small components you can work on each one independently without having to worry what impact it will have on the others.  The key question is determining what feature or improvement need a Type 1 spec and which can settle for a Type 2. This is where experience and context are extremely valuable. Many experienced engineers have an intuitive feel for what’s going to be risky and warrants a deeper dive and spend the extra effort there. As engineering teams grow they introduce additional process in order to protect against the edge case but that cost is incurred in every other case that would have benefited from a looser process.  Finding that sweet spot is how you find the balance between moving quickly and moving safely. Amazon’s growth has been incredible and their ability to maintain this mindset is even more impressive.",0,1,2018-06-01,4,"amazon, tech specs, startups, engineering",671,Type 1 and Type 2 tech specs
29,0,Google is launching a service called Contributor which will allow users to pay a monthly fee to avoid ads. This fee will then be distributed to participating publishers.,#product,"{% include setup %} Google recently launched a program,  Contributor , that offers an ad-free monetization model to publishers. The idea is that a user pays Google up to $3 a month and in return Google will not show that user any display ads on a website that’s a participant in the program. The monthly payment will then be distributed across the participating sites - most likely based on how many times you’ve visited that site.  I like the idea - not because most ads are terrible but because it shows that both publishers and Google are willing to experiment with another approach. Ads, as much as we dislike them, are the primary way content producers make money since web users expect free content everywhere.  The biggest audience for Contributor will be those who currently run adblock but feel guilty about it. Most people want the content free on principle and refuse to pay but the people that are using adblock but do want to support the publisher may be willing to pay the $3 a month to feel noble - in fact they might keep on running adblock and treat this as a way to reward the sites they visit.",1,1,2014-11-20,5,"google, adtech, publishers, ads, adblock",203,Paying publishers without ads
19,0,The way publishers should fight adblock is by focusing on high quality content and building an engaged audience.,#meta,"{% include setup %} Ever since the release of iOS 9 and it’s support for adblocking apps I can’t go a day without seeing some article about adblock. Some condemn it and claim it’s stealing from publishers while others make the case that ads are so intrusive that they deserve to be blocked. I don’t want to dwell into either of these but something that’s been on my mind is that publishers aren’t doing enough to differentiate themselves based on the quality of their audience.  Top tier publishers that have unique and high quality content should focus on building a community. This passionate group of users will engage with the content on the site as well as provide valuable information to the publisher. By signing up for an account users provide valuable demographic information as well as interests based on what they see and what they do. This is also something that adblock won’t be able to easily block since it will be such a core part of the experience and hosted on the publisher’s own domain.  It’s true that publishers with low quality or commodity content will suffer as users move on to something with a better experience but this will arguably make the web a better place. The fact that some people even run adblock implies they have no respect for the publisher’s effort - and pursuing a lowbrow approach just turns that into a death spiral.  Disclosure: I work at TripleLift which provides a much better advertising experience for users, publishers, and advertisers.",0,1,2015-10-15,4,"adblock, publishers, quality, content",256,"Adblock, publishers, and content quality"
26,0,It's an odd situation when a museum has a suggested donation amount as an entry fee yet you still get in via a corporate sponsorship.,#meta,"{% include setup %} Yesterday I attended a concert at the Newark Museum and ran into a fairly common situation when lining up to get in. They had a suggested donation amount, which is entirely optional, while at the same time they provided free admission to anyone with a Bank of America card due to Bank of America’s sponsorship. I’ve seen the same sort of setup at museums in New York and I suspect it’s common elsewhere in the United States as well as abroad but the entire concept strikes me as odd.  I understand that it’s a way for Bank of America to reward its customers but because the admission was a suggested amount it made me feel as as I’m neither contributing towards the museum nor as getting any value from Bank of America. Without my debit card I would have felt noble contributing when I didn’t have to but with the card it feels as if Bank of America is giving me a way to avoid feeling guilty.  I’m sure these thoughts are irrational and I’m overthinking it but the process struck a weird chord with me and I’m surprised I haven’t noticed it before.",0,1,2015-12-07,3,"museums, corporate sponsorships, economics",199,Optional museum fees and corporate sponsorships
24,0,I recently remembered the way older games used to deal with age verification and piracy protection and wanted to share them for posterity.,#meta,"{% include setup %} In the early 90s, being a kid new to the US and new to computers I developed an addiction to computer games. I’d play everything that I got my hands on and remember sharing floppy disks with school friends. Unfortunately, I was plagued by two issues that had pretty clever approaches: age verification and piracy protection.            The first manifested itself in  Leisure Suit Larry . I was as giddy as only a kid can be when I got my hands on it. Unfortunately, that went away when I was required to take an “age quiz” as soon as the game loaded. The age quiz consisted of a series of multiple questions that only an adult would be able to answer. These ranged from factual ones such as “Who recorded ‘Let it be’?” to comical ones such as “Do girls really have cooties?” I do have memories of playing it so I must have figured out some way around the verification. I must have either guesses correctly some of the time or took notes of the answers that allowed me to play.  Another game I enjoyed was a basketball game that I suspect was  Lakers vs Celtics and the NBA Playoffs . Unfortunately, I got the disk from a friend and it had a nifty way of dealing with piracy. When starting the game, it would ask to provide information that could only be found in the game manual, for example asking for the 7th word on the 15th page.  Sadly, both of these approaches disappeared as task switching became standard in the newer operating systems and internet access became common.",2,1,2013-04-29,5,"gaming, privacy, drm, leisure suit larry,",300,Some gaming nostalgia
25,0,The digital world is full of advertiser trackers and my gut is that FreshDirect is exposing some of their audience data to their competitors.,#meta,"{% include setup %}            Working in AdTech I’m slightly more aware of how modern digital advertising works compared to the average person and get curious when I see an for a brand that I have never seen before. Just yesterday I was on Twitter and saw a Whole Foods ad for what I think was the first time. This got me thinking about what I must have done to get into their targeting list. Of course it may have just been a new campaign but I know enough to suspect it had something to do with my recent browsing behavior.  I tried thinking through what I did the previous couple of hours that could have piqued Whole Foods’ interest and the only thing I that could have been remotely relevant was that I placed a FreshDirect order. If that in fact was the case it seems a pretty big error on FreshDirect’s part. I’m a potential FreshDirect customer and they are giving my data and information away to Whole Foods, a competitor. Sure it’s all bits but it’s equivalent to a company giving their leads away to their competitors.            This must have been unintentional but it highlights how complicated the modern AdTech ecosystem is. There are thousands of vendors firing trackers and sending data back and forth that is then used to create some pretty advanced audience segments. By running [Ghostery](https://www.ghostery.com/) on the FreshDirect homepage one can see the different trackers used - in my case I saw 34 trackers with 3 of them being in the audience data business: Aggregate Knowledge, BlueKai, and eXelate. I suspect it was one of these companies that classified me as a potential grocery shopper and sold this data to Whole Foods.  It’s unlikely that FreshDirect would opt in to this behavior so there must be something else going on. Either they are working with these companies to retarget these users later and the data is getting leaked or these trackers are being dropped by some of the other vendors they’re using. Both cases pose a problem to FreshDirect and is something they should address. On the other hand, it may have been an intentional move by FreshDirect and they are in fact getting paid by these data companies to drop their trackers. Maybe they have enough faith in the stickiness of their product that they don’t worry about competition and will gladly share data with them for a price.  One can get deeper into this by seeing what script is dropping what other script but it’s an incredibly intricate web with tons of unexpected outcomes. I’d love to know how many companies actually understand what’s happening behind the scenes and how they’d react to this knowledge if they found out.",1,1,2017-07-23,4,"adtech, advertising trackers, freshdirect, whole foods",496,The wild world of online trackers
29,0,I've been using Google Contributor for a couple of days now and it's interesting. The biggest effect has been making me more aware of the content I consume.,#meta,"{% include setup %} Near the end of last year, Google announced the  Contributor  program - a way to pay a monthly fee which would then be distributed across the websites you visit. In return, you’d start seeing fewer ads. Earlier this week I got off the waitlist and decided to give it a shot. The signup process was amazingly simple - choose a monthly dollar amount and you’re good to go. The effect is noticeable - on many sites I’ll see a blank spot where an ad should have been. The best part is being able to see how much I’ve contributed to the various sites I visit. Over the past couple of days I’ve spent a little over 60 cents removing 51 ads. An unforeseen effect is that I’m more aware of the content I consume and the sites I visit - seeing that some of my money is going towards shady sites makes me more conscious of my browsing behavior. I’m definitely curious to see where this approach goes.",1,1,2015-04-28,4,"google contributor, publisher, content, digital advertising",190,A few days with Google Contributor
28,0,"Despite it's shortcomings Excel is still the only Office product I use. There just hasn't been a replacement that has that combination of speed, performance, and simplicity.",#meta,"{% include setup %} Other than the usual developer tools the only desktop based app I use is Excel and every few months I try to wean myself away. I love being able to keep all my text docs and slideshows online and have them accessible and sharable anywhere. The best part is updating the content without having to worry about bombarding people with yet another email.  I tried doing the same with Google Sheets and it works for smaller tables but as soon as you get tables with thousands of rows it’s noticeably slower than Excel. It’s amazing for what it can do but it feels as if the browser just can’t handle the rendering nor the calculation that a large spreadsheet entails. Some of the time closing and reopening the table fixes the problem but this is too reminiscent of Windows in the 90s and ends up in a glacial pace after a few minutes of work. I continue to use Google Sheets for small, collaborative files but for anything larger or anything that will need heavy computation I’ll switch to Excel. I’ve also been using R for more repetitive analyses but for the quick and dirty analysis that comes from the result of a SQL query Excel is still king.",0,1,2015-07-19,3,"excel, google sheets, R",214,Excel wins
17,0,Google Calendar has been on a roll lately with the volume and quality of new functionality.,#product,{% include setup %} It’s impressive when companies constantly churn out products and features and it’s even more impressive the larger the company. In my mind this used to be Amazon but lately I’ve been surprised by how often Google Calendar is updated. It feels as if every month there’s at least one update that I accidentally discover and get a pleasant surprise.  Google Calendar underwent a major redesign earlier this year and it seemed to have been the foundation for the recent improvements. I’m sure I’m not capturing everything but just in the past few months the following features were launched:  - **Room suggestions**. By adding a bit of metadata to each room ranging from location to capacity Google Calendar suggest a room that matches the size and location of your guestlist. - **Cancellation notification**. If everyone invited to a meeting declines Google lets you know and gives you the option of releasing the booked room. - **OOO handling**. This one is great. I booked a meeting with the subject “OOO” and got the option to automatically decline all invites during that time. - **Time proposals**. This is a small addition but a nice touch that allows an invitee to propose a new time straight from the invite. - A ton more that can be found on the wonderful [GSuite update blog](https://gsuiteupdates.googleblog.com/search/label/Google%20Calendar).  The features themselves are not important and I expect them to get more and more intelligent as Google applies more and more of its AI prowess to automating more and more of our lives. I really like this as an example of investing in a proper foundation which then increases your ability to quickly ship new features and code.,1,1,2018-08-15,3,"google calendar, product, engineering",280,Google Calendar: Constantly shipping
29,0,Rather than wanting to try out the new hot thing we should become masters of a few set of tools. It is a lot more productive and efficient.,#meta,"{% include setup %}  Actually, as the artist gets more into his thing, and as he gets more successful, his number of tools tends to go down. He knows what works for him. Expending mental energy on stuff wastes time.  &nbsp;&nbsp;- Hugh MacLeod,  Ignore Everybody     This quote refers to art but it can just as easily apply to code. As developers, we’re constantly exposed to new tools and technologies and are curious to try them out. Everything new looks shiny and we imagine it will solve all the problems we’re facing. Yet almost always new tools bring their own set of problems and take time to learn. Instead of constantly chasing something new we should try to master what we’re already using - the value of that will most likely outweigh playing with a new toy. It’s better to rely on a small set of tools that we understand well rather than have a superficial knowledge of dozens of tools and technologies.  Of course it’s important to try out new tools since many of them are useful but it’s dangerous to rely on new tools exclusively and use them for a new project just because they’re the next big thing. To get some exposure to new tools, I will use them in toy projects or during hackathons so I can get a sense of how they work, what the strengths and weaknesses are, and how much I enjoy using them. Only then will I consider using them in a real project.",1,1,2014-07-13,3,"tools, productivity, engineering",256,Good artists use fewer tools
31,0,I attended two bootcamp meet and greet sessions and came away impressed by how much people can learn in 12 weeks. It got me thinking about the future of software.,#meta,"{% include setup %} I recently attended two web development workshop “meet and greet” sessions where recent graduates presented their projects and chatted with potential employers. I’m honestly surprised by how polished the projects were. Sure there were a few simple ones but most were solid; they were good ideas, well designed, and had functional backends. It’s amazing what it’s possible to do in 12 weeks.  These programs focus on a single frontend framework, such as Backbone or Angular, and a backend framework, usually Ruby on Rails. With the number of plugins and public APIs available it’s easier to get an app up and running than ever before. Of course these programs won’t provide the same level of knowledge as a degree or years of experience will but for many projects that’s not important. Being able to get something functional and private is more important than perfect and private and these bootcamps provide enough skills to do that. More importantly, they make code accessible to an entirely new group of people and provide enough skills to allow them to continue learning on their own.  It does make you think where tech skills are headed. As it becomes easier to build a larger variety of apps it will be interesting to see where software engineering will end up. Software engineering is a young industry and I suspect it’ll become increasingly specialized as the base set of tools and knowledge become widespread.",0,1,2014-09-17,3,"web dev bootcamps, coding, software engineering",241,Web development bootcamps
20,0,Based on the competitiveness of human nature I believe we'll kill each other before we end up colonizing space.,#meta,"Although it's wonderful to think that we will be able to colonize other worlds when we grow too numerous or run out of resources, it may not happen. It seems that given the level of current weapons and state of the world we will more likely try to conquer each other than try to conquer space.  In order to go into space we would need to have an advanced level of technology which could only be created through innovation. I am just worried that technological advances tend to be used as weapons first, and as humanity benefiting objects second. This may have been fine with the technology of old but we are approaching the level where a weapon can wipe us out. The non warlike use may not come to fruition if there will be no one left to develop it.  I just hope we realize that the next global war may be the last war we fight and that we need to control our competitive spirit. A quote that comes to mind is Albert Einstein: ""I know not with what weapons World War Three will be fought, but World War Four will be fought with sticks and stones."" I'd just like to add that I'm not so sure there will even be a World War Four.",0,1,2008-05-13,2,"space, colonizing space",219,Why we may never colonize space
29,0,If you’re designing systems that collect and use data from the client you need to make sure your backend code is capable of dealing with the inevitable trash.,#data,"{% include setup %} At  TripleLift , we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins. Unfortunately, we saw that the aggregate data didn’t match up with what we saw in the logs and after some investigating we discovered that we were seeing some duplicate rows generated on the client side. Taking a deeper look it turned out that there were some plugins and scripts that were making duplicate requests to our analytics server.  There may be ways to deal with this better on the client side as well as smarter backend logic to deal with potential duplicates but the easiest fix is to just assume you will have messy data and prepare accordingly. In our case it entailed writing more complicated queries that were robust enough to not require clean input. It took a little bit longer to write and design but our pipeline can now handle weird input without impacting the final results. If you’re designing systems that collect and use data from the client you need to make sure your backend code is capable of dealing with the inevitable trash.",1,1,2015-01-24,3,"analytics, data, pipeline",277,Don't trust client side data
20,0,While working on a presentation I found a subtle but neat UX gem when using the color selection tool.,"#product,#design","{% include setup %} I’m a sucker for subtle UX gems and Google continues to amaze me. While working on a presentation using the Slides product I had to modify the colors of a few objects. The UX for this is usually pretty normal - you select the object, hit the icon to modify the color, and choose a new color. Usually it’s slightly better and you get to see the existing color. What surprised me with the Google Slides experience is that I didn’t just see the existing color but also saw the previously picked color. Since I was modifying a series of objects to the same color this was a pleasant experience, especially since I was working off of a series of color shades that looked too similar. It would have been even better if I was able to bulk change the objects but a win’s a win.  I’m convinced that this sort of UX behavior only happens by users who eat their own dog food. It’s unlikely that someone who was not a regular Slides could have come up with a feature like this since it’s so subtle and baked into the power user experience. In addition, that person must have been aware enough to realize what they were doing and took a step back to think of this functionality. Or maybe it wasn’t a person at all and Google’s tracking is so sophisticated that it was able to suss out this pattern and bring it up to the relevant product manager who was able to identify the behavior that needed to be optimized. Both of these cases are extremely impressive and I can’t wait to find more of these subtle UX optimizations.      Google Slides previous color selection",0,2,2018-11-13,3,"google slides, ux, color selection",309,A UX gem in Google Slides
28,0,Working in software it's clear that there's not a strong relationship between time spent and quality yet when we evaluate other service providers that's our natural tendency.,#meta,"{% include setup %} We live in a world where it’s impractical to be a generalist so we specialize in a subset of skills and go to others for everything else. This works well but there’s still imperfect information - it’s tough to gauge someone’s skill level when you’re not an expert. In fact, when we lack awareness we end up using time spent as a proxy for skill when comparing across service providers. Imagine going to two barbers that charge the same amount for a haircut but one takes 10 minutes and the other takes 30 minutes. Even if we can’t tell the difference between the two haircuts we’d value the 30 minute one more due to the time difference. This seems backwards. The barber that was able to achieve the same result in 10 minutes is the more skilled one but instead we feel swindled when we back the price into an hourly rate. We should be willing to pay more for the 10 minute haircut since it gives us more time for our own pursuits. Yet when we lack knowledge we opt for the shortcut of equating time and skill. I’m trying to break this tendency by thinking about the end result rather than the effort and time involved. It’s interesting to compare this to software development. I know that just because someone spent more time on a project doesn’t mean it's better than someone who knocked it out yet I still view other skilled professions from a “time equals quality” perspective. It makes you wonder whether professionals in other industries have a similar mindset where they realize time spent isn’t an indicator of quality in their own profession yet view it as a sign of quality in others.",0,1,2015-05-18,3,"experts, time versus quality, estimating productivity",292,"Experts, time, and quality"
25,0,Often I need to run the same command multiple times with a different date argument. I wrote a simple script to help automate this.,#code,"{% include setup %} I know the title of the post is terrible but I found it difficult to describe the content in another way.  Lately I’ve been spending a decent amount of my time in SQL-land and running some pretty repetitive queries where only some of the arguments are changed. These run the gamut from exporting some data for a date range by day to adding a series of date partitions while messing around with [Spectrum](http://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html). Depending on the amount of these queries I needed to write I’d either just do it manually with a bunch of copy and pastes or use Excel to generate the queries I needed.  Neither of these were that efficient so I wrote a simple [script](https://github.com/dangoldin/python-tools/blob/master/date_replace.py) to simplify the entire process. It accepts the format string, the start date, and the end date and replaces the macro in the format string with each date of the date range. Then it’s as simple as either piping the results into pbcopy or copying and pasting it into the SQL client. This gets even better by aliasing the command so it can be run in any terminal window.  I often find that I get into the habit of repeating a manual process over and over instead of just taking the time to automate it. This was no exception: the script took me a few minutes to write and I suspect if I had it I would have saved myself hours given how often I had to do some form of this. Hopefully I can catch myself going down the manual path and build a tool to solve the problem instead.  {% highlight python %} #! /usr/bin/env python  import datetime import sys  if __name__ == '__main__':     string_template, start_str, end_str = sys.argv[1:]      start = datetime.datetime.strptime(start_str, '%Y-%m-%d')     end = datetime.datetime.strptime(end_str, '%Y-%m-%d')      while start < end:         print string_template.replace('{{ YMD }}', start.strftime('%Y-%m-%d')) start = start + datetime.timedelta(days=1) {% endhighlight %}",2,1,2017-10-05,4,"python, automation, date replacement, sql queries",343,Generating a series of commands covering a date range
22,0,"Best products get built when engineering, product, and design trust and respect each other. This happens when there's a skill overlap.","#product,#meta","{% include setup %} The best products are built when engineering, product, and designers work together. When things are running smoothly each brings skill, knowledge, and experience the others cannot. When things are running poorly the teams do not respect or trust each other and question the others’ approach and decision making. Inevitably this leads to a flawed product that doesn’t solve customer needs and gets replaced by one that does. It’s possible to ride previous success through inertia, marketing, and price but to grow in the modern era you need to focus on product.  One of the best ways to maintain this mutual respect and collaboration is by ensuring that there’s a skill overlap between the various groups. If an engineer has no insight in the design process she won’t be able to put herself in the designer’s shoes and understand the effort involved. Similarly, if a product manager doesn’t have some understanding of code it will be difficult to estimate complexity and potential tradeoffs without working directly with an engineer. It’s easy to dismiss others’ work as easy until you try to do it yourself and then you realize that there’s unforeseen intricacy and depth. At that point you develop an appreciation for the work others do and can follow along with the process.  Teams are most productive when everyone trusts and respects others and allows them to work uninterruptedly on their components. Ironically, this is achieved by having some experience in another’s domain since that’s how you start to truly value their contribution. Having this additional knowledge also gives you enough context to push back and collaboratively come up with a better solution than would have developed independently.",0,2,2017-05-27,3,"engineering, product, design",282,"Engineering, product, and design"
34,0,Everyone seems to be pretty shocked by the Cambridge Analytica and Facebook news but given the incentives and how easy it was to get that data I'm surprised it hadn't come out sooner.,"#society,#data","{% include setup %} The big news this weekend was that Facebook suspended Cambridge Analytica, a company that leveraged behavioral data to come up with very focused and accurate political ads, for using data that they were not supposed to have as well as not deleting it when caught. Everyone seems to be surprised by this revelation but I’m honestly surprised it took this long and I wouldn’t be surprised if there are still hundreds, or even thousands, of companies in the same situation as Cambridge Analytica, albeit at a smaller scale.  The value of this data is immense and Facebook made it incredibly easy to access. It’s no surprise that some unsavory actors pulled this data and used it for whatever they needed to. And once you have the data why delete it? It’s not as if there’s any way to get caught and if you do you can just delete it then. If you’re competitors are doing this you also have a big incentive to play along and do the same thing. It’s tough being noble while you’re competitors are running amok, especially if the risk of getting caught is low.  Facebook is right in saying it’s not a security breach - there was no hack and the data was properly fetched via the official API. It is a violation of Facebook’s terms of service but the damage has already been done. Coupled with the privacy initiatives happening in Europe it does feel as if the world is starting to take privacy seriously. Unfortunately it looks as if we’re going to need more of these fiascos before we get to where we need to be.",0,2,2018-03-18,3,"facebook, privacy, cambridge analytica",277,"Facebook's \""breach\"
26,0,Fab is moving away from flash sales into becoming a private label. This reminded me of my experience building Makers Alley and what we learned.,#product,"{% include setup %} Fab recently  laid off  a third of their staff as they transition from designer flash sales into customized goods and their own private label. The business is tough and reminded me of our experiences building  Makers Alley . We initially set out to build a place where people can buy customized, personal furniture from local designers. The idea was was consumers would benefit from being able to get items that are custom made and can be customized to fit individual styles while supporting a local business and makers would have a new avenue to sell their products and build their brand.  Unfortunately, we faced huge obstacles on the consumer side. We thought that with such a feel good story and compelling vision we’d have no trouble attracting people to buy furniture but it was extremely tough. We launched during the flash sales era where everyone was on a hunt for deals and discounts. We considered showing discounted prices by inflating the original price but that just didn’t feel right and we didn’t want to diminish the work of our makers and designers.  I recall talking to one of our woodworkers who told us about being approached by Fab which wanted to include some of his pieces but they wanted him to sell his pieces at too steep of a discount in addition to Fab taking a cut that it made no economic sense to do it. This validated our belief that we did not want to go down the discount route but that still didn’t help us attract consumers - especially when they were being inundated with expiring deals and flash sales. Fab wanted to promote good design from new designers but their flash sales model prohibited them from working with the designers and makers who needed it the most.  Fab’s new direction feels bittersweet. It sucks for any entrepreneur to realize that a business model doesn’t work and it absolutely sucks to have to lay off amazing people who actually believed in your vision but at the same time I do think that their new model is more sustainable and better for the long haul, not to mention that it’s now more similar to what we were trying to do Makers Alley.  Building a custom label should be easier than starting from scratch since Fab’s already associated with good design but their challenge will be changing the mindset of their customers from expecting great deals to be willing to pay more for curated, unique designs. This is going to be tough and there’s a lot more competition in this space. I suspect their biggest competitor is Etsy and that’s why they’re focusing on their private label in order to move away from that model and become more like the Warby Parker for design.",2,1,2014-05-24,4,"Fab, design, furniture, ecommmerce",484,On Fab's latest move
21,0,I'm a big fan of Twitter and wish they actually focused on getting their product right rather than on monetization.,#product,"{% include setup %} Eugene Wei  published a great post  on the power of networks and how Twitter hasn’t been taking advantage of their core product - a public messaging protocol. Given this thesis, Twitter should move away from the artificial 140 character limit and innovate on top of the protocol rather than be bound by it.  I’m not nearly as eloquent but I also have my gripes with Twitter that his post motivated me to write. It just feels they don’t care about the user experience. Cross device sync is still a problem - if I clear a notification on my phone why do I see it again on my computer? People are still complaining about the OS X app not being as functional as the other versions. Even on my phone the navigation feels inconsistent - sometimes I get taken out of the app and sometimes a screen is loaded inside. This causes me to hit the back button at the wrong time and randomly leave the app which resets my location. I’m a big fan of Twitter and it’s pretty much the only social network I actually use but I’m frustrated by how poor it is.  It feels as if Twitter has decided to focus purely on monetization rather than evolving the product. It’s okay to do this when you’re in a dominant market position (see LinkedIn) but Twitter should be focused on making sure they’re getting new users that stick around. The only way to do this is to make a product that’s useful, fun, and easy to use.  Disclosure: I own a bit of Twitter stock.",1,1,2015-09-03,2,"twitter, product",277,Come on Twitter
30,0,"While flying out of Lagaurdia Airport, I decided to observe the way people interacted at the Biergarten, a restaurant that required customers to order and pay using an iPad.","#product,#meta","{% include setup %}            Last week, I had a morning flight out of Laguardia Airport and being into all things tech decided to grab a coffee at a place called Biergarten since they had iPads at every seat. Turned out that the only way to order and pay was by using the provided iPad with the attached credit card reader. I had 30 minutes to kill before my flight and decided to spend it observing the interactions others had with this ordering system.  During the 30 minutes, I saw 6 people approach the bartender and every single one tried to order directly from the bartender without paying any attention to the iPads. Surprisingly, none of them gave up after being told they had to order using the iPad although two couldn’t figure out how to use the iPad and needed help. The major points of frustration were finding the app and then realizing that you needed to “check out” before submitting the order.  I suspect no one actually benefits from this sort of setup. The supposed benefits to Biergarten are that they’re able to hire fewer people and collect payments upfront but I’m not sure it’s worth it given the high usability cost to the consumer. The staff is now kept busy explaining how to operate the iPads and customers are significantly slower at ordering than a trained waitstaff would be. In addition, if an iPad is the only way to order then they can’t have more customers than there are iPads - no one can get anything to go or have a drink standing up unless there are iPads available. The only way this is a good idea is if the iPads are able to attract more customers.  Essentially, the company is trying to externalize the cost of serving customers to the customers without taking into account their experience and frustration. You want to make it as easy as possible for people to give you their money and forcing technology down your customers’ throats isn’t always the answer. As optimistic as I am about technology making things easier, it’s going to be difficult for brick and mortar places to move to a self serve model. It’s simply easier to give someone cash or a credit card and have them do the work than doing it yourself.",0,2,2013-07-01,4,"laguardia, brick and mortar, ipad, self-serve",422,Externalizing externalities in brick and mortar
30,0,Something I've been wondering about is the tradeoff between simplicity and power in product design. Is it possible to achieve both? What are some examples where it's been achieved?,#design,"{% include setup %} Although I come from a backend background, I’ve been spending more and more time on the UX side of things and have been picking up quite a bit - a combination of using Twitter Bootstrap on my projects, subscribing to the Hack Design lectures, and following a ton of designers on Twitter.  Something that’s been bothering me is this obsession with trying to make every product as intuitive and approachable as possible. That’s the right approach when focusing on mass market consumer products but if you’re building internal tools or targeting power users a simple, approachable product might be antithetical to what you actually need.  The tradeoff is between a product that people can immediately start using versus a product that takes time to learn but becomes significantly more powerful when mastered. The developer equivalent would be using a basic text editor vs vi or emacs. The text editor is easy to start using but you hit a productivity ceiling quickly; vi or emacs, on the other hand, take a while to learn but you become significantly more productive than if you were using a text editor.  The challenge is knowing your audience and building the product that will solve their problems. Sometimes it will need to be simple and other times it will need to be complex. This applies at multiple levels - the product may for the most part be simple but certain features will need to be complex in order to be useful.  Many websites and apps have adopted the approach of where it’s extremely easy to get started but provide advanced features for the users that desire and discover them. Excel provides shortcuts for the power user that make it possible to do anything without touching the mouse. Gmail, in addition to shortcuts, provides a “labs” feature that lets users enable more advanced features.  I’m interested in what happens as companies grow and try to increase their market. Some may have started with a complex product that solved a niche problem that they want to simplify in order to appeal to a bigger audience. Others may have started with a simple product that they now want to position to power users. In both cases the challenge is being able to support both use cases without negatively impacting either one. Maybe the right approach is to launch the new product under a different name but I’m curious to see creative solutions that aren’t about adding shortcuts or a settings page.",0,1,2013-08-28,4,"design, ux, startups, product management",416,Simplicity vs power in product design
12,0,PostgreSQL is surprisingly good at solving a variety of coding tests.,"#sql,#code","{% include setup %} Most developers are familiar with the FizzBuzz code test which is a quick way to filter out developers who can’t code. At Yodle, we had our own, slightly more challenging problem. The challenge was read in a text file and then print out the frequency each word appears in descending order. It’s more complicated than FizzBuzz but it assesses a variety of skills. The solution needs to do the following:       Read the file     Split the text into a list of words delimited by non-letter characters     Convert each word to lower case     Compute the frequency each word appears     Sort the results in descending order by frequency     Print this sorted list     I thought it would be fun to see if I could do it in PostgreSQL and was surprised by how quick and easy it was. The most challenging part was figuring out how to read the file - after that it was just using a few of the built in functions to clean and organize the text.  {% highlight sql %} DROP TABLE IF EXISTS temp_t; CREATE TABLE temp_t (c text);  COPY temp_t(c) FROM '/tmp/data.txt';  select lower(data.w) as word, count(1) as num_words from (   select regexp_split_to_table((select string_agg(c,' ') from temp_t), E'[^\\w]+') as w ) data where data.w <> '' group by word order by num_words desc, word; {% endhighlight %}  It also turns out to be very simple to do FizzBuzz in PostgreSQL. The nice part of the PostgreSQL solution is that it can easily scale to adding a 3rd combination. for example print Dozz if the number is divisible by 7. In the PostgreSQL solution, it would just require adding a row whereas in the standard solutions it would require a bit of work and would increase the chance of a bug.  {% highlight sql %} DROP TABLE IF EXISTS fizzbuzz; CREATE TABLE fizzbuzz (   num int,   text varchar(4),   priority smallint );  insert into fizzbuzz (num, text, priority) values (3, 'Fizz', 1), (5, 'Buzz', 2);  select coalesce(string_agg(fizzbuzz.text, '' order by fizzbuzz.priority asc), nums.num::text) as text from (   select generate_series(1,100) as num ) nums left join fizzbuzz on nums.num % fizzbuzz.num = 0 group by nums.num order by nums.num asc; {% endhighlight %}  Clearly PostgreSQL isn’t the right tool for every task but it’s surprising how powerful it can be given the right problem. It’s also a great way to think differently about a problem - even if you end up choosing a more standard solution.",0,2,2014-01-25,3,"coding tests, postgresql, fizzbuzz",421,Solving coding tests in PostgreSQL
22,0,"Fighting some writer's block I discovered a new way to write - using a bluetooth keyboard, a lap desk, and my phone.",#meta,"{% include setup %}           The past couple of weeks I’ve had a big case of writer’s block. I haven’t been able to motivate myself to write as much as I used to and when I did get to write it felt more like a chore than a joy. I didn’t know how to break out of it but this past weekend I kicked off the OS X upgrade without realizing how much time it would take.  Since I made a commitment to write two posts a week and I was computerless I had to do something. Lucky for me I have an bluetooth keyboard lying around a neat lap desk with a phone slot so I decided to give it a shot and see what I could muster.  It turned out remarkably well. The small screen made it a lot easier to focus which was magnified by the inability to easy switch to another app - something I’m prone to doing when I’m on an actual computer.  I still need the command line to commit the text and handle the image upload but it was incredibly liberating to write using a keyboard, a lap desk, and a phone. The change of environment itself may have gotten me over the writer’s block but I can also see myself using this setup whenever I travel or am outside. It’s also portable which makes it simple to write where I am.",0,1,2016-10-13,3,"blogging, bluetooth keyboard, phone",257,My new blogging setup
31,0,Due to a dumb error I locked myself out of an account and the only way back in was by entering the last 4 digits of an old credit card.,#meta,"{% include setup %} I thought I've seen every design anti-pattern out there but had the luck to run into a new one a couple of days ago. I was buying domains on  Namecheap  and ended up going through checkout without verifying the payment details. Turns out that I had an old credit card on file which led to a declined payment. I was redirected to a page that told me to update my payment methods but instead of doing that I ended up hitting back and refreshed the page which triggered another failed charge attempt. One more and I'm locked out of my account.  Ironically, other than speaking to a rep the only way to unlock my account was by entering the last 4 digits of the credit card which I no longer have. It only took a few minutes to clear that up with the rep and it was basically my  fault but it's still interesting to see security questions based on ephemeral information. Old accounts are likely to have outdated credit cards, phone numbers, and addresses. In those cases it's too easy to get locked out and be stuck with having to speak to a service rep - and I suspect most companies won't be as responsive as Namecheap.",1,1,2014-07-27,1,security questions,215,Ephemeral security questions
32,0,Something I've always disliked was the idea that if a company ran a marketing campaign the cost of that campaign would be reflected in the price. This isn't always the case.,#meta,"{% include setup %} I like to think of myself as extremely relational and efficient and something that’s always bothered me is marketing. I hated the idea that companies had to market their products since the cost of the marketing would just be absorbed into the price I would be paying for the final product. Of course it’s not that simple and marketing isn’t just a cost and can bring value to everyone involved. In fact, George Akerlof wrote a paper in 1970, [The Market for Lemons](https://en.wikipedia.org/wiki/The_Market_for_Lemons), which describes how markets with asymmetric information end up full of “lemons” and led to a Noble Prize. One of the solutions to this problem is to do marketing since that will signal that you’re a legitimate company offering a legitimate product.  As a fun weekend activity I took a stab at using my almost forgotten college economics classes and some basic algebra to dig into the effects marketing has on price. The thesis is that marketing has two potential effects - the first is increasing the price customers are willing to pay and the second is to increase demand. Depending on the type of product or service any combination of these can happen.  We can start by taking a look at one of the first thing economics students learn: the [supply and demand curve](https://en.wikipedia.org/wiki/Supply_and_demand). The idea is simple and describes the relationship between price and quantity. As the price goes up the number (quantity) of items for sale (supply) increases since there are more willing sellers but the number of willing buyers (demand) drops. By finding where these two curves intersect we get to the equilibrium price and quantity that the market should end up in. Note that it’s meant to analyze markets rather than individual companies but it’s still interesting to see what happens if we approach it from the perspective of a single company offering a product with and without marketing. Without marketing we have the initial supply and demand curve with its equilibrium point. With marketing the demand curve shifts up which increases the equilibrium price and quantity sold. This makes intuitive sense - marketing will make something more “in demand” which will increase the price but to me the bigger potential impact is actually raising awareness and letting people know that the product exists. Depending on the product this can cause a massive change to the demand curve.            Supply and demand curves. Source:  Wikipedia      The ideal situation for a consumer is a company has a great product that’s a bit too expensive to product and thus to sell. By investing in a marketing campaign the company is able to share how awesome its product is which can increase the demand so much that the cost to manufacture the item drops. This in turns makes the price go down which in turn feeds into more demand. These are the types of products where marketing is valuable.  A negative example of marketing (at least to the consumer) is where a company runs a marketing campaign which increases awareness of interest but instead of using that to sell more for a lower price uses the result to increase the price and sell to the same or fewer amount of people. In this case, fewer consumers benefit and the company may not be that much better off given the marketing expense.  As consumers we want to get a good deal and being able to think through the impact of the marketing is a nice way of seeing whether we’re getting our money’s worth. If a product is heavily commoditized with a ton of competition then marketing is solely meant to drive awareness and we’re better of buying the generic or store brand. Similarly, if a product has a high marginal cost and isn’t likely to get cheaper to make at scale (imagine a consulting service) then all the marketing will do is increase the price. On the other hand, if it’s a niche product that you’ve never heard of that you discovered through a marketing campaign that it may, in fact, be worth it and you’re arguably getting a cheaper product because of the marketing effort.",3,1,2017-03-25,3,"marketing, economics, supply and demand",719,The effects of marketing on price
34,0,It's amazing how far we've come. I remember allocating a better part of a day to setting up a new computer but now it can be easily done in less than an hour.,#meta,{% include setup %} In the past I’d be wary of setting up a new computer knowing that every time I’d need at least a couple of hours to get everything into a workable state. These days I actually look forward to setting up a new computer. Nearly every file I care about is hosted online and a large chunk of my productivity apps are online as well. The only tools I need to run locally are the various IDEs as well as a variety of open source tools and libraries that my code depends on. Even then I’d bet it takes less than an hour to get things to an 80% state at which point I’ll only discover what’s missing by just going through my day.  And this is as a developer who needs to build various applications from source and deal with potential library conflicts. For someone who doesn’t have to deal with these issues it must be incredibly quick to get a workable setup these days.  At the same time it seems as if we need to upgrade our computers less frequently since for most tasks computers from a few years ago are good enough. In fact I’m still on an early 2011 MacBook Pro with some upgraded RAM and a new SSD drive. I don’t even notice any performance difference between it and a newer MacBook Pro at the office. And for most tasks why even bother upgrading a computer when you can get nearly everything done on a tablet? Just attach a keyboard and you’re good to go. I can’t imagine us ever going back to the pre cloud days of computers and I can only imagine what kind of digital productivity tools we come up - especially with the rise of VR and the constant improvement in the performance and battery life of our existing digital devices.,0,1,2015-12-06,3,"computers, cloud, technology",313,Setting up a new computer - then and now
31,0,It's natural to gravitate to a computer when working on a new problem but it's often more useful to just go to pen and paper. Especially for creatively challenging problems.,#meta,"{% include setup %} Oftentimes when starting a new project I have a tendency to just dive in. It makes me feel immediately productive and I know I can just go back and tweak whatever needs tweaking. Yet almost always I suspect I would have been better of if I took the time to take a step back, get a sheet of paper and a pen, and think through the various steps and flows, even it was just a series of doodles.  In addition to taking a step back and slowing down it's the changing of the medium that has strongest impact. Software tends to force us into a given structure but using a pen and paper gives us the ability to think differently. We don't have to start typing or dragging elements onto a screen but are able to whatever we want. This opens a new possibility and especially for a problem requiring a creative solution provides a healthy boost.  Whiteboards have the same effect. It's amazing how much more valuable whiteboard discussions are rather than conversations around a computer screen. It's useful to start with a well structured document or presentation but the creative juices really start flowing when the whiteboard gets involved. A whiteboard removes the need for perfection and encourages ideas to keep growing on top of one another. Many will be crappy but even a crappy idea may engender a great one.  If you find yourself stuck with a problem staring at a computer I suggest changing the medium and going to pen and paper. It's surprising what that simple act will do.",0,1,2017-08-10,5,"thinking, pen, paper, whiteboards, productivity",269,Start with pen and paper
28,0,"I set up wallabag, an Instapaper alternative, earlier today and it got me thinking about open source and how it doesn't work well for cross platform projects.",#meta,{% include setup %} I’ve been a happy Instapaper user for years but the news that it was being acquired by Pinterest got me thinking about some alternatives. Not because I have anything against Pinterest; in fact I think this is a great fit and they’ll be able to complement each other but because it’s a reminder that no third party product is guaranteed to last and I wanted to see what open source alternatives are out there.  I discovered [wallabag](https://www.wallabag.org/) and got it setup earlier today. The documentation to install and get it running was incredibly straightforward and I was able to get it operational within an hour. Unfortunately it took a bit of wrestling to understand the various configuration options and I’m still unable to get it working across both the web and an iPhone. There’s a series of steps you need to do - from generating a unique RSS token to setting up an oAuth application that make it difficult to just get up and running. I understand that it’s designed for developers and offers a ton of customization but it should be simpler to get get the base installation - every user would want an extension to easily add articles and a way to access them offline on a phone and automatically generating the necessary settings would make it much easier to get started.  Trying out an open source alternatives is an eye-opening experience. You don’t realize how much polish it takes to build something usable. We love claiming that we can build anything in a day but it’s the relentless polishish that makes a successful product. I suspect this is why it’s incredibly hard to find open source products that require a cross platform approach. It’s difficult to think of successful open source applications that span across multiple environments. That requires multiple developers each agreeing on a unified vision and then making sure each of the components fits together. This is a tough combination and may be why so many popular open source projects are incredibly focused: it’s a lot easier to get multiple people working on a single product when it’s simple and they all share the same pain. But as soon as the scope expands there’s no single vision holding everything together and it shows in the final product.  The nice thing about open source is that anyone can add functionality and I’m already thinking of ways to improve wallabag. Hopefully I’ll have some time over the next few weeks.,1,1,2016-08-28,4,"instapaper, wallabag, open source, cross platform",419,Giving wallabag a shot
32,0,Be careful when using the GROUP BY in MySQL on a derived field with the same name as a column. Instead of the derived field MySQL will use the column name.,#sql,"{% include setup %} I discovered a nuance with MySQL's GROUP BY statement earlier today that I’ll share with the hope that others can learn from it. It’s fairly common to use a coalesce statement to handle null values while keeping the resulting field the same name. For example:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY user_id; {% endhighlight sql %}  The nuance is that we want the GROUP BY to apply to the entire coalesce expression but as it’s written it only applies to the user_id column from table_a. This has potential to give odd results in more complicated queries. The only fact I even discovered it was that it was causing a duplicate key constraint violation in another table. The solution is quite simple but annoying - you have to use the entire coalesce expression within the GROUP BY statement:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY coalesce(a.user_id, b.other_user_id); {% endhighlight sql %}  The reason this solution is messy is that it’s very easy to update the SELECT but forget to update the GROUP BY. This won’t throw an error and MySQL will execute the query just fine - the results just may be unexpected. What I’ve started doing is renaming the resulting column and using that within the GROUP BY:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as final_user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY final_user_id; {% endhighlight sql %}  This makes the query a bit more complicated but it’s being explicit about what we want and avoids hidden errors.",0,1,2015-06-09,3,"mysql, group by, querying",342,A MySQL “GROUP BY” nuance
27,0,We like to think in terms of black and white of large companies being evil and small companies being good but it's more complicated than that.,#meta,"{% include setup %} It's become a popular idea that big companies are evil and we should only be supporting small and local businesses. There’s some truth to it - smaller companies are much more aligned with the incentives of the community whereas larger companies may be managed from thousands of miles away via a spreadsheet. When the only goal is to make more money it’s very likely that morality and honesty will suffer.  At the same time, we should differentiate evil companies from evil actions. Even Walmart has done some good. A story that comes to mind is the reduction in the size of  detergent bottles . There was an arms race by detergent manufacturers that were diluting their detergent in order to sell large bottles for the same price. Sure, it cost more to produce the larger bottles, but then they'd be able to generate significantly more sales when people saw that you were offering “twice” the volume at the same price as your competitors. Sure enough competitors followed suit and we ended up with a  Prisoner’s Dilemma  spiral. It took Walmart, along with a few other large retailers, to reverse the trend and get the manufacturers to start producing bottles in smaller, more concentrated sizes.  And on the other extreme there’s Starbucks. Despite not being a trendy new coffee shop they have the size and resources to launch massive initiatives. One of the recent ones is a way for current employees to get a  full cost covered college degree  from Arizona State University. Despite this being isolated to a single college and only classes no local coffee shop would be able to make this happen. And the cynic may see this as a marketing ploy but if it’s still able to help more people get a degree I’m all for it.  In both of these cases the benefits came from the size of the company. Larger companies have both the resources and the clout to push change that may not have been possible by smaller company. Instead of dismissing them as permanently broken we should be focused on getting them to use their clout and money for societal gain.",3,1,2015-10-10,5,"corporate good, corporate evil, walmart, starbucks, society",374,Not every big company is evil
16,0,Just a quick review of my visit to the Museum of Math in New York.,#meta,"{% include setup %}            Although I’ve been meaning to visit the  Museum of Math  ever since it opened in December, I only got the chance to do it this Labor Day. I wanted to share my thoughts and encourage everyone who can to visit.  I love the mission. Math should not be taught in a vacuum and having various activities that each showcase different mathematical properties is a great approach to get kids (and adults) engaged while learning some math. Some of the activities that stood out to me were bikes with differently sized square wheels that can only go around a certain diameter track; a ""helix"" shape that explains multiplication by lighting up a fiber between the numbers and highlighting the resulting value; and a fractal tree generator that would use your body to create the trunk and branches. I enjoyed these since they had an interactive physical component that provided immediate feedback.  There were also a bunch of activities that were primarily software based. Two examples are a ""kaleidoscope"" drawing tool and an app that explores 3D shapes and functions by letting you tweak the parameters. These weren't very engaging and most were abandoned quickly. In addition, some of the tools either had broken sensors or were buggy which made them less fun than they should have been.  A museum should not be able to replaced with iPad apps and MoMath places too much emphasis on the software. They should move away from these apps and focus on the physical exploratory activities that cannot be recreated at home. I found that there was very little continuity between the exhibits and wish they did a better job curating so that lessons from one could be applied to another. This would limit the breadth but would make the experience more valuable. Similar to an art museum, they could have a base collection that never changes as well as exhibits that rotate every couple of months.  The museum has been open for less than a year and I’m optimistic it will only improve. I only wish there were more math museums opening up.",1,1,2013-09-11,4,"momath, museum of math, museum, math",372,MoMath visit
35,0,"I got lucky with my computer experience. I grew up with computers, saw the internet as a teen, and got a smartphone after college. I wonder whether future generations will have the same experience.",#meta,"{% include setup %} I’ve been thinking about my history with computers and the impact they’ve had on me. I grew up just as computers were becoming mainstream, the spread of the internet coincided with my teens, saw the rise of “Web 2.0” during college, and got my first smart phone a few years after college. It’s fascinating to think about how much has happened to the world since the rise of computers and the varying experiences everyone’s had.  Nearly everyone has experienced the internet but at completely different points. Some experienced it when it was just text and had to use Archie, Gopher, and telnet to discover and consume content. Others joined through the AOL floppy discs and had to get multiple land lines in order to connect over dial up. Others avoided it for as long as possible but got dragged in when joining an office. And others are only getting seeing it now due to the spread of smartphones. Age is a huge part of the experience too. First using the internet as a child is different than using it as an adult. We have our own experiences that affect our interaction and dictate the experience we’ll have.  Each experience comes with its pros and cons but I’m happy where my experience fell on the spectrum. I got to deal with the joys of DOS, floppy disks and 16 colors and was able to experience the early days of the internet with a 14.4k modem and Lycos. I do wish I could see what it was like to code in the 70s and 80s when the engineering world was much smaller and one had to deal with a ton of constraints that we currently take for granted.  I wonder whether future generations will have similar experiences to what I had or whether technological advances will either be too predictable and make change appear gradual or not significant enough to warrant attention. I believe I got lucky since so many of the advances were consumer oriented and pro-hacker. Hopefully that the future brings more of the same.",0,1,2014-04-12,3,"computers, history, technology",353,My computer experience
37,0,In a Simpsons Homer's brother decides to build a car for the average man and comes to Homer for ideas. The end is that he builds a ridiculous car that no one wants. Don't do that.,#product,"{% include setup %} Years ago, one of my projects at Yodle involved building out an automated reporting system that would consolidate all the existing reports being run via SQL queries and consolidate them into a unified application that would take care of the execution and the delivery. During the design process I spoke with existing users to see what else they’d like and it quickly morphed from a cron-job like application that just emailed CSV files based on SQL queries into a full fledged business intelligence tool that users could use to pull arbitrary data formatted in a multitude of ways. While thinking through the design of this application I spoke with the CTO and he gave me a phrase I keep going back to: “To get the expressiveness of SQL you have to write SQL.”                      While simple and glib I like how relevant this statement is to building software. When asked users will push for the most flexible and powerful system that comes with all the bells and whistles but at that point it’s equivalent to them just writing the code themselves. We have to know where to draw the line and understand what the use cases our product needs to support and not just everyone’s wishes. Otherwise we run the risk of building a [Homer](http://simpsons.wikia.com/wiki/The_Homer).",2,1,2016-02-07,3,"homer, product management, software",244,Don't build a Homer
22,0,To make it easier to connect to an AWS instance I had an old bash script that I've improved in Python.,#code,"{% include setup %} Last night I took an old bash script I wrote that simplified connecting to an EC2 instance in an AWS account and implemented the same code in Python. The old code worked by listing a set of AWS instances and then prompting to pick a single one to connect to. The problem was that it wasn’t always easy to find the index of the desired instance and the code took a bit of time to run.  The new code leverages the Python AWS library to pull down the list of instances for a given region and then filters it down based on the name, IP address, or public DNS. If it turns out there’s a match then it will only return the public IP address which makes it easy to connect using ssh. For example, to list all servers containing the name “web server” you would run the following:  {% highlight sh %}python list_hosts.py --region=us-east-1 --filter=""web server"" {% endhighlight %}  And if you know there will only be one you can connect to it directly by using ssh and running the script inside two backticks: {% highlight sh %}ssh `python list_hosts.py --region=us-east-1 --filter=""web server""` {% endhighlight %}  The code’s up on  GitHub  but at the moment there’s just this single script. I’ll keep adding more as I run into various issues working with AWS.",1,1,2014-11-09,4,"aws, ec2, python, ssh",239,Some simple AWS tools
31,0,Open Source is incredible but integrating undocumented libraries is tough. Seeing how others have used it through GitHub search is a simple way to see how it should be used.,#code,"{% include setup %} Open source is great: if you find the right library you’re able to save a ton of time and get code that’s been through the gauntlet that you can confidently incorporate into your system. Unfortunately many open source libraries are partially baked with documentation that doesn’t always accompany the rapid development of the code. This leads developers to repeatedly cross reference their code with some archaic documentation and then wonder why it’s not working as expected. This is proportional to the obscurity of the library - popular libraries will have most of their kinks worked out but esoteric ones that are likely maintained by one person won’t have the polish.  Yet it would be nice to take one of these libraries and build off of it. The simple answer is to reach out to the maintainer and ask questions. I always get excited when someone reaches out with a question about how to use one of my open source libraries; I’m not at that scale where this is burdensome and it’s encouraging that someone is actually using my code. When this doesn’t work a neat trick is to [search GitHub](https://github.com/search) for usage of that code. Most documentation provides a simple starting tutorial and assumes the user can take it from there. More often than not this doesn’t work well and you have to look at the source code to understand how the code works, what arguments the methods expects, and the order in which they should be called. By looking at actual usage of the code you can see how others have integrated these libraries in actual applications rather than toy examples. This works incredibly well for open source libraries in that middle area where they’re not popular enough to have everything figured out yet are useful enough to have had numerous developers wrangle them into their code. Many new and popular libraries fall into this bucket so if you want to use code that’s just becoming popular leveraging GitHub’s code search is a great way to start.",1,1,2016-08-14,5,"open source, documentation, free software, libraries, code",343,Integrating poorly documented Open Source libraries
28,0,We ran a Connect Four bot competition at TripleLift and wanted to share the framework and boilerplate code that made it easy for everyone to get started.,#code,"{% include setup %} Years ago when I worked at Yodle the engineering team held a Connect Four bot competition. The goal was for each person to write a Connect Four playing bot and then let them loose to determine the winner. We had either a few days or a few weeks to do this and my failed approach was to use genetic programming to evolve a bot. The best it did was beat a completely random bot 80% of the time while the winning entry leveraged Minimax with Alpha Beta Pruning.  That’s all in the past but earlier this week we held a quarterly kickoff and one of the more recent traditions is to have a short coding activity so I thought of running a mini Connect Four competition. Since we only had a bit of time I needed to make it as easy for everyone to get started as possible so I wrote up a [simple framework](https://github.com/dangoldin/connect-four) to make it easy to run a Connect Four competition. The code comes with four different applications: a server application that manages the game, a bot client in Python, a bot client in JavaScript, and a React UI to visualize the game. The first three were simple to write but the React UI involved finding [Jeff Leu’s great example](https://codepen.io/jeffleu/pen/KgbZwj) on CodePen and adapting it from an interactive game session to a game visualization.  There’s some nuance around getting the appropriate libraries and dependencies set up but it’s a neat activity that gets everyone thinking. A big thing that the code would benefit from is some utility functions around managing state and determining winners since that’s boilerplate that everyone would need to write. This code exists in the server application but didn’t make its way into the client since I wanted to keep the JavaScript and Python clients comparable.  If you end up giving this a try I’d love to know how it went.",2,1,2018-04-25,5,"connect four, ai, bots, competition, contest",326,Connect Four bot competition
24,0,You can get a ton of useful information about your web application by loading the ELB logs into Redshift and running some queries.,"#code,#sql","{% include setup %} Logging HTTP requests should be enabled for every application you run. When things go wrong, and they will, it’s often the first step to understand the problem. Unfortunately, logging isn’t always top of mind and is often forgotten. Luckily, if you use the Elastic Load Balancer (ELB) functionality within AWS you’re able to set up ELB logs that track every request and write it to an S3 bucket. The documentation is up on the [Amazon site](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html ) but there’s a surprising amount of information that’s hidden away in the logs. Since it’s Amazon and they want to make it as easy for you to use their various services together it’s simple to load the logs into Redshift and start digging into them via some basic queries.  I wanted to highlight a few I used in the past couple of weeks to dig into an issue. Despite having a centralized logging system I still find it easier to just write SQL queries - it’s significantly more expressive than any log exploration tool I’ve used and allows me to exactly what I without any magic.  {% highlight sql %} -- Load the data into Redshift copy elb_logs from 's3://logdirs/.../us-east-1/2018/02/19/' COMPUPDATE OFF CREDENTIALS 'aws_access_key_id=KEY;aws_secret_access_key=SECRET' delimiter ' ' TIMEFORMAT as 'auto' ACCEPTINVCHARS REMOVEQUOTES FILLRECORD MAXERROR as 100000;  -- Look at the distribution of ELB status codes select elbresponsecode, count(1) as cnt from elb_logs group by elbresponsecode order by elbresponsecode;  -- Look at the distribution of backend status codes select backendresponsecode, count(1) as cnt from elb_logs group by backendresponsecode order by backendresponsecode;  -- Look at distribution by both select elbresponsecode, backendresponsecode, count(1) as cnt from elb_logs group by elbresponsecode, backendresponsecode order by elbresponsecode;  -- Look at distribution by both where they're unequal -- This highlights cases where the ELB was unable to reach the hosts select elbresponsecode, backendresponsecode, count(1) as cnt from elb_logs where elbresponsecode != backendresponsecode group by elbresponsecode, backendresponsecode order by elbresponsecode;  -- A simple way to look at the request groups causing problems select split_part(httprequest, '?', 1) as httprequestbase, count(1) as cnt from elb_logs where elbresponsecode >= 500 group by httprequestbase order by cnt desc;  -- For fun, we can also look at the number of errors by minute to see if we can spot a pattern select to_char(requesttime, 'YYYY-MM-DD HH24:MI') as datetime,   sum(case when elbresponsecode = 500 then 1 else 0 end) as n500,   sum(case when elbresponsecode = 501 then 1 else 0 end) as n501,   sum(case when elbresponsecode = 502 then 1 else 0 end) as n502,   sum(case when elbresponsecode = 503 then 1 else 0 end) as n503,   sum(case when elbresponsecode = 504 then 1 else 0 end) as n504 from elb_logs where elbresponsecode >= 500 group by datetime order by datetime; {% endhighlight sql %}",0,2,2018-02-20,2,"aws elb logs, redshift",464,Analyzing AWS ELB logs
22,0,MySQL ignores case when doing the general ORDER BY. It's possible to make it case sensitive by using the BINARY keyword.,#sql,"{% include setup %} At  TripleLift , we have a migrations job that copies aggregate data from Redshift to MySQL so it can be accessed along the rest of the transactional data. As part of a test, I tried comparing that the data matched exactly but ran into an issue when exporting the data to select. Namely, to make the comparison as simple as possible I wanted to run the same select query in both tables and compare the results. Unfortunately, the sort order between MySQL and PostgreSQL (what Redshift is based on) acts differently for text fields. PostgreSQL takes case into account while MySQL does not. This has an especially weird results when you have values that contain characters with an ASCII code between the lower and upper case letters: [\]^-`. It took some research but I discovered that MySQL provides an option to do a case sensitive sort - just add a “BINARY” option before the field name.  The following   queries demonstrate this behavior - all but the BINARY one can run on both MySQL and PostgreSQL.  {% highlight sql %} CREATE TABLE test_table ( t varchar(5) );  INSERT INTO test_table (t) VALUES ('a'),('b'),('c'),('d'),('e'),('f'),('g'),('h'),('i'),('j'),('k'),('l'),('m'),('n'),('o'),('p'),('q'),('r'),('s'),('t'),('u'),('v'),('w'),('x'),('y'),('z'),('A'),('B'),('C'),('D'),('E'),('F'),('G'),('H'),('I'),('J'),('K'),('L'),('M'),('N'),('O'),('P'),('Q'),('R'),('S'),('T'),('U'),('V'),('W'),('X'),('Y'),('Z'),('0'),('1'),('2'),('3'),('4'),('5'),('6'),('7'),('8'),('9'),('['),('\\'),(']'),('^'),('_'),('`');  SELECT * FROM test_table ORDER BY t ASC;  -- MySQL only SELECT * FROM test_table ORDER BY BINARY t ASC; {% endhighlight %}",1,1,2015-02-01,2,"mysql, postgresql",219,MySQL vs PostgreSQL sort order
20,0,In what has become an annual ritual I've updated my Yahoo fantasy football script to work for the upcoming season,#code,"{% include setup %} This is much later than in previous years but hopefully that just makes the data more accurate. I updated my Yahoo fantasy football stats scraper to account for the slightly different design for the upcoming season. It still works as before and uses Selenium to open up Chrome and scrape the projected stats by week. The change this year involved shifting the columns around a tiny bit as Yahoo changed the order but other than that there were no changes. Maybe by next year I’ll update the script to be able to actually determine the column indices for each stat automatically. As usual, the script is up on [GitHub](https://github.com/dangoldin/yahoo-ffl) and the scraped data can just be downloaded [here](/assets/static/data/stats-2019.csv).",2,1,2018-08-18,3,"yahoo fantasy football stats, fantasy football, scraper",127,Yahoo fantasy football stats: 2018-2019 edition
20,0,Open source is a necessary part of modern software development and I'd love to see open data become stronger.,#data,"{% include setup %} Open source has become a critical part of modern software development that allows small teams to move quickly and do in months what used to take years. This has been driven by massive platforms, such as GitHub, that make it extremely easy to find useful code, contribute back, and provide feedback, comments, and requests.  Unfortunately, data hasn’t seen as strong of an open sourcing trend. There are a few sites - ranging from [data.gov](https://www.data.gov/developers/open-source) for government data to various [aggregators](https://github.com/awesomedata/awesome-public-datasets) that offer various datasets for download but the formats are inconsistent and some even come in PDF. There just hasn’t been a single open data standard that’s been globally adopted. Instead we have cities offering PDF and CSV files for download and companies offering throttled APIs to their proprietary data.      Things are heading in the right direction and I only wish it was quicker. A recent trend that I’ve been a big fan of is people editing and collaborating on a Google spreadsheet that’s designed to provide transparency for a topic. The most recent example I discovered is “[parental leave in tech](https://docs.google.com/spreadsheets/d/1GKWqhc3FVtSVKRZNBxyfwZ_QrB1f-i1T0-yBJ6X_YHM/edit#gid=0 ).” It’s a simple crowdsourced spreadsheet that lists the parental leave policies for various tech companies. If you wanted the information about one company I’m sure you’d be able to find it on the web but there was nothing that consolidated the information into a single document.  While contributing to open source code generally requires some coding ability none of that is required to add or modify a few cells of a spreadsheet. Because of that low barrier the formatting may end up inconsistent but that will just be fixed by someone else later. Creating a Google spreadsheet to collect data isn’t very valuable unless others are contributing and that’s gotten much easier. I discovered the parental leave spreadsheet using Twitter which massively lowers distribution costs and if something is both valuable and easy to contribute to it ends up quickly amassing a ton of data.  These spreadsheets so far have been more tech focused but inevitably they will move beyond tech and into the mainstream. I can’t wait.",2,1,2018-03-11,4,"crowdsource, open data, open source, spreadsheets",384,Crowdsourced data
22,0,"I wish where was a way to capture those fleeting thoughts, especially at times where a phone or notebook aren't options.",#meta,"{% include setup %} Many of my thoughts come serendipitously - whether it’s an errand I need to run, an idea for a blog post, or a feature I should build into one of my projects. But unless I’m able to jot it down soon after it slips my mind until I have another serendipitous thought to bring it back. It’s frustrating when I know I had something but can’t recall what the actual thought.  A smartphone makes it a lot easier since I can always jot down some words to trigger the thought later on and I’ve also started carrying a small notebook for times I'd just like some pen and paper. Unfortunately, there are still a few cases where this approach doesn’t work. One is when I’m in a group and don’t want to bust out my phone and start taking notes. The others are where a phone just isn’t practical - whether I’m out exercising and don’t want to take a break, in bed where I don’t want to stare at a bright screen, or just in the shower. In those cases I’d love something akin to mind reading where I’d be able to just back up a thought to a place that I can reference later. The only idea I’ve had is a braille-like system that lets you enter the worlds tactilely. Imagine having a small device in your pocket that you can run your fingers over to type whatever you’d like. You’d be able to do this regardless of the location - whether you’re in bed or in a crowded subway car with your hand in a pocket. I’d love to see a Kickstarter for this.",0,1,2015-05-17,3,"notes, notetaking, productivity",280,Taking spontaneous notes
28,0,The founder of Ansible has two new DevOps tools out there - Vespene and OpsMops - and I plan on giving them a shot in the next few weesk.,#devops,"{% include setup %} While catching up on some tech news today I discovered [OpsMop](https://opsmop.io) and [Vespene](https://vespene.io). Both of these are new DevOps tools from Michael DeHaan, the creator of Ansible. Before we had a DevOps team I was doing the bulk of our AWS management through the AWS console as well as a few command line scripts but as soon as we had a real DevOps that introduced the modern DevOps stack (Ansible, Terraform, Packer, Kubernetes) I was hooked. Doing the work through the AWS console is quick and easy but inevitably leads to inconsistencies that get worse and worse as your stack gets more and more complicated.  I don’t have the depth that most experienced DevOps engineers have and have been happy with Ansible for my current projects. At the same time the fact that the creator of Ansible thought there was a need for something new makes me think he’s on to something. I have very little invested in Ansible and I’m going to do the work to move it over to OpsMop. In addition, none of my personal projects have any CI/CD and actually giving them a real deployment pipeline using Vespene seems like a fun little project",2,1,2018-12-19,3,"devops, vespene, opsmop",204,New iteration of DevOps
26,0,Amazon is offering a $6 CPM to app developers who launch an app on the Fire phone. An aggressive move to build out their ecosystem.,#product,"{% include setup %} I’m a bit late to the Amazon Fire Phone party but wanted to chime in with a perspective I haven’t seen written about. Amazon is offering a  $6 CPM to mobile app developers  that launch an Amazon app during Auguster and September. Given that typical CPMs are  less than a dollar  with premium publishers like Facebook and Twitter getting  close to $6 , this is a very aggressive move by Amazon to build out their ecosystem.  A common refrain mentioned is that ecosystems drive smartphone adoption. This is why it's extremely difficult to compete against iOS and Android which combined have  72% share  of the smartphone market. Amazon is trying to jumpstart this by offering a potentially huge sum to developers if they're able to get the users. Amazon will have a difficult time getting a large market share but I suspect they will find a niche in a particular customer segment and a few app developers will be rewarded.  It's also interesting to contrast this with Microsoft's approach of paying developers a flat sum to create a Windows phone version of their app. On one hand, this approach allows Microsoft to be selective since they can pay to get the apps they want. On the other hand they run the risk of paying for an app without any users. The interests between Microsoft and it's app developers are not as aligned as those between Amazon and it's app developers.  One thing that may have influenced this decision is that Amazon's OS is Android based so it's significantly less work to port an app to work on Amazon's phone compared to a Windows Phone. The pitch to developers is akin to saying make a few changes to your app and potentially earn a bunch of money while Microsoft's would be write a new app in a new language and we'll pay you for your efforts.  Almost all Amazon phone coverage has been bearish and I wonder whether this move will have an impact. Apps to drive smartphone adoption but with so many good options already available replicating an ecosystem won't be enough.",4,1,2014-07-10,2,"amazon fire phone, smartphones",415,Amazon courting app developers for the Fire phone
21,0,Since last May I've been cross posting my posts to Medium to see whether it would drive any additional performance.,#meta,"{% include setup %} Last May I [decided](/2016/05/15/experimenting-with-medium/) to start cross posting to Medium to see what impact that would have on pageviews. My approach was to publish first on this blog and then post the same piece a few days later to Medium to get some additional views. Once concern was that the duplicate content would hurt SEO but luckily Medium allowed me to specify the [canonical URL](https://support.google.com/webmasters/answer/139066?hl=en) which I pointed back to the original post.  So how did this work? According to the Medium stats, I published 89 stories that drove 650 direct views, 587 “additional” views, and 502 total reads. The majority of these came from two posts - [one](https://medium.com/@dangoldin/lessons-learned-from-todays-s3-failure-e308012ebf89) that had 310 direct views and [another](https://medium.com/@dangoldin/optical-illusions-and-self-driving-cars-49e906f5f328) that had no direct views but 450 additional views. This blog, on the other hand, had just over 59,000 pageviews over the same time period. It’s a bit unfair to compare them against one another since the blog does have a lot of existing content driving some of the views but the difference is substantial. I’m still going to continue cross posting to Medium since it looks as if there’s no SEO hit, it is driving some additional views, and the additional work is nominal due to a simple script I wrote that generates Medium drafts from my markdown posts. At the same time I am a bit disappointed since I did expect Medium to do more of its magic - although given how popular it is it's not surprising that my posts are not the ones being surfaced.",4,1,2017-12-22,3,"medium, blogging, blogging stats",277,My Medium experiment
44,0,There's a ton of discussion as to how Airbnb should be regulated with many people for it and many people against. I propose a new zoning category and to rely on the market to come up with an elegant solution to the problem.,#meta,"{% include setup %} I’ve been meaning to share some thoughts on regulating Airbnb for a couple of months now but kept putting it off. The  recent news  was motivating enough for me to finish it off.  I’m a huge fan of Airbnb and it’s my first step whenever I’m traveling. Nearly all my experiences have been great and I’m contemplating getting rid of my Starwood card since it’s just not as useful anymore given that I gravitate towards Airbnb first. At the same time I understand the impact renting a place Airbnb has on the neighbors and can imagine myself hating it if my neighbors were listing their places.  The challenge is that the host is benefiting while passing the cost to someone else. The host is able to get above market rent while the neighbors have to deal with the potential noise and the risk, albeit a low one, of a stranger. The obvious solution seems to come up with a new zone category between residential and hotel commercial that Airbnb as well as other home rental places would be able to fit in. These locations can then be rented out with the city’s blessing as well as contribute to the tax revenue of the city. This relies on the market to come up with a fair price for the location. If you’re interested in renting a place on Airbnb you should be willing to pay more for the property and if you decide you’ll never be renting your place on Airbnb you should have neighbors that share the same belief.  I don’t have much knowledge on zoning laws and what goes into it but this feels like a solution that should work once in place. Getting there is the hard part - what happens to buildings where half the tenants want their units to be Airbnb eligible and half don’t? Who ends up having the final say? Maybe the solution would be to keep existing places the way they are and make sure new construction goes through this zoning process. This will ensure that over time more and more buildings have a clear definition of rental eligibility.",1,1,2015-11-05,4,"airbnb, sharing economy, zoning, real estate",365,Zoning Airbnb
9,0,Some crazy ideas on a new voting system.,#meta,"It seems that there is a recent emphasis on ""character"" in the election. Unfortunately, character can be faked with some acting and campaign management. To understand how the candidates will perform in office we need to look at their past accomplishments and failures. Public records show the  vote history  for the candidates and all it takes is a little bit of research to see how the candidates have voted.    But people aren't interested in looking at data; they are more interested in how much candidates spend on their haircut or who looks more ""confident."" I imagine that before radio or television existed and the only news source was the newspaper, candidates would have stood on their issues alone. The vast majority of the population would not have seen the candidates in person and would have to have focused on the issues each candidate presented. Maybe it's time we go back to those days with a ballot only containing issues.    In addition, why not throw in an intelligence test geared towards the issues and use the score to weigh the vote. If companies can use intelligence in their hiring decisions, why can't the government use it in the voting process? The impact of a wrong decision is much greater.",1,1,2008-09-12,2,"voting, politics",217,Voting system proposal
21,0,MoviePass sells subscriptions for less than $10 a month but pays for unlimited movies. How will they make it work?,#product,"{% include setup %} Given my lack of cultural knowledge the fact that I’ve heard of MoviePass should be a sign that it’s a big deal. I came across a [fascinating article](https://www.wired.com/story/moviepass-second-act/) today about MoviePass’s revenue model - they charge just under $10/month and in return you get unlimited access to movies in theaters. What’s remarkable is that it seems MoviePass doesn’t actually have a subsidized relationship with theaters and just pays theaters the full ticket price.  That seems like an odd combination - having a customer see one movie a month puts you in the red. At the same time I understand where they’re coming from. The major advantage of this approach is that they can start signing up customers without having any relationship with the theater. And once they have the customers they have [enough leverage to become dangerous](https://stratechery.com/2015/aggregation-theory/). They may get theaters to give them lower pricing for tickets or a fraction of the concession sales. They may be able to go beyond theaters themselves and into studios, as the aforementioned Wired article suggests. In any case it’s having the army of consumers that’s the source of power. I look forward to seeing how this plays out.",2,1,2018-01-28,3,"moviepass, business models, aggregation theory",205,MoviePass: A fascinating business model
26,0,I made my firstr contribution to an open source project and want to start doing it to every project that I use and find useful.,#meta,"{% include setup %} Despite being a huge proponent of open source I’ve never made a contribution to a  third party project  until this weekend. The project was a simple scraper that downloads each Jeopardy game from  j-archive.com , parses the data, and loads into a SQLite database. The project had one issue open that was to make the download code threaded in order to reduce the time of downloading nearly 4700 games from over 7 hours to less than 30 minutes. I gave this a stab on Saturday and submitted a pull request that was merged in by the author on Sunday.  The process was surprisingly simple and it felt good making an improvement to an already useful project. An idea I’ve been toying with is making a contribution to every open source project I use. Some would be simple and may not even be code while others might be significant improvements. The idea is to continue continue the cycle of improvement to projects that have made me more productive. I can only imagine what would happen if everyone who uses open source adopted this approach.",2,1,2014-10-19,1,open source,197,My first open source contribution
30,0,I recently caught myself using Foursquare as a contact book when I couldn't recall someone's name or company but did remember that they like using Foursquare to check in.,#product,"{% include setup %} A few days ago I discovered a new use case for Foursquare when I was meeting up with a friend. We were catching up and during the course of the conversation I realized I needed to introduce him to someone I had met earlier. Unfortunately, I completely blanked on his name and company. All I recalled was that he frequently checked into his company on Foursquare. Sure enough, when I opened up Foursquare I saw that he had checked in there that morning.  Clearly Foursquare isn’t a contact book app but it provided enough adjacent information and triggered enough thoughts to come in handy. It’s not surprising that we use apps and products in unintended ways but it’s always great being able to catch ourselves in the act.",0,1,2014-02-13,3,"foursquare, product design, product management",133,Foursquare as a contact book
31,0,If you run a server that's open to the world. Take some time to make it secure. It doesn't take long and makes it significantly less likely you'll get hacked.,"#devops,#code","{% include setup %} I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.  {% highlight bash %}grep ""Invalid user "" /var/log/auth.log | cut -d' ' -f8 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r | head -n 100{% endhighlight %}      Username  Count      test  141    postgres  116    oracle  88    web  75    test2  74    admin  59    jboss  49    ubuntu  45    webmaster  43    user  42    tech  40    debian  40    testuser  39    server  38    penguin  38    shoutcast  36    rdp  36    www  35    radio  35    ftp  33    test3  30    student  29    guest  29    toor  21    public  19    testing  15    tester  15    students  15    var  13    gov  9    adm  9    x  8    nagios  8    zabbix  7    z  7    y  7    w  7    vyatta  7    u  7    t  7    shell  7    s  7    r  7    q  7    p  7    o  7    n  7    michael  7    m  7    l  7    k  7    j  7    i  7    h  7    g  7    f  7    e  7    dup  7    d  7    ch  7    c  7    b  7    a  7    sales  6    office  6    home  6    data  6    bash  6    apache  6    administrator  6    v  5    test1  5    teamspeak  5    ssh  5    plesk  5    master  5    linux  5    ircd  5    http  5    walid  4    vnc  4    ust  4    ts  4    temp  4    telnet  4    smmsp  4    smart  4    samba  4    org  4    operator  4    net  4    named  4    mike  4    library  4    info  4    hacker  4    git  4    ftpuser  4    dan  4    cc  4      The usernames were all over the place - from generic ones (such as test, admin, ubuntu, guest) to the names used by various services (postgres, oracle, nagios) to letters of the alphabet. There was also a slew of common English first names. In total, there were ~1500 unique usernames that attempted to access my box.  The auth.log file also contains the IP address of each attempt and we can easily summarize by that.  {% highlight bash %}grep ""Invalid user "" /var/log/auth.log | cut -d' ' -f10 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r | head -n 100{% endhighlight %}      IP  Count      162.13.41.12  874    176.31.244.7  733    216.127.160.146  572    195.50.80.169  382    66.219.106.164  359    199.33.127.35  220    112.167.161.194  98    128.199.226.160  66    198.50.120.178  60    189.85.66.234  37    14.18.145.82  29    166.78.243.86  23    222.190.114.98  22    130.126.141.74  18    178.208.77.133  17    61.160.213.171  8    49.213.20.249  8    23.253.51.76  7    178.254.8.177  7    193.107.128.10  5    121.167.232.196  2    107.182.134.51  2    82.221.106.233  1    74.3.121.10  1    72.225.239.90  1    111.74.134.216  1      In this case, the total number of IP addresses is significantly smaller with only 26 unique IP addresses trying to connect. I took a look at a few and some of them look to be legitimate sites that may have been compromised.  If you have a box open to the world, you should make sure it’s secure. A small program that makes this easy is fail2ban - it scans log files and bans IPs that have had too many failed attempts. Two other quick fixes are to disable password authentication entirely and rely solely on public key authentication which is significantly harder to crack and change the default SSH port from 22 to something else. These should be enough to eliminate the bulk of attempts and keep your box secure.",0,2,2014-05-16,4,"security, ssh, ubuntu, server",420,Examining ssh login requests
17,0,Code is not done when a pull request is submitted but when it's deployed to production.,#meta,"{% include setup %} As a developer, it feels wonderful to commit some code and knock an item off of the ever growing to do list. Unfortunately, until that code is deployed it’s not delivering any actual benefit. It’s easy to open a pull request and move on to the next task but to create high quality products we need to only consider our code complete when it’s deployed and running issue free. So many things need to happen between writing the code and deploying it - handling conflicts with other database changes, updating database schemas, and monitoring the actual code to make sure it’s working as expected on a production system. Calling something done before it’s deployed is a lazy shortcut.  This approach also encourages developers to care more about their code and take a big picture view of the product. By taking an active role in the deployment we’re forced to think through the dependencies and design a release process that avoids downtime and occurs in the right order. For simple features it’s straightforward but larger, coupled ones require an approach that may even end up in rewriting code in order to simplify or stage a complicated deployment process. And if you know your features aren’t complete until they’re deployed you’ll make an effort to actually get them deployed. This is a huge risk reduction since the code and ideas are still fresh in our minds and can deploy code in small batches rather than massive monoliths.  The holy grail is continuous deployment which couples code commits and deployments but it requires significant effort to get it working smoothly that may not be worth it for early stage companies who need to focus on building their product. For them iterating is crucial and every developer needs to take ownership of getting their code into production.",0,1,2015-08-01,4,"engineering management, deployment, quality, engineering teams",307,It's not done until it's deployed
16,0,Crawling the Oyster books web API and having some fun with the resulting data set.,"#datascience,#dataviz","{% include setup %} I’m an avid reader and signed up for  Oyster  as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.  One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.  These API endpoints turned out to be publicly accessible and it only took a short Python  script  to retrieve the books and dump them into a CSV file. This got me a little less than 3,000 books - turns out that the publicly accessible data is only a fraction of the entire collection and my endeavour wasn’t as fruitful as hoped.  I did manage to get a set of over 4,000 books and decided to have fun with it.                                                          Num of authors by num of books written.  Very few others appear more than once in the data set. This may be due to the limited data set or Oyster's job in editing the publicly accessible collections, maybe both.                                                                          Distribution of ratings.  Ratings are clustered around 4 with very few ratings under 3. This is most likely a biased set since the Oyster editors would have chosen the highest rated books to be featured in their sets.                                                                          Num of books by author.  Kurt Vonnegut has over 20 books available on Oyster with Shakespeare in the number 2 spot.                                                                          Ratings by author box plot.  Just a quick box plot to see the rating distribution by author.                                                                          Rating vs # of books.  Doesn't look as if the # of books an author has written on Oyster has any relationship with their rating. I thought maybe authors with higher average ratings would appear more frequently.                                                                          Rating over time by author.  This was a reach but I wanted to see whether an author was most likely to have better ratings earlier or later in his or her career. In this case it looks as if the publish date isn't the original authorship date so not a very useful analysis.                                                                          Publisher ratings.  Similar to authors, we can take a look to see whether some publishers have significantl higher ratings than others. This is a bit more useful since there's a lot more data per publisher than there is per author. I couldn't make much sense of the results here.                                                                          Avg number of pages by decade.  I wanted to see whether books were getting longer or shorter so did a quick plot of the average number of published pages by decade since the year was too fine. The publish dates aren't entirely accurate so I wouldn't read too much into this.                                                                          Avg rating by decade.  Similar to the previous plot but looking at the average rating rather than the number of pages. Seems to be pretty steady to me although this may be due to the dataset being a curated list of top books.                                                                          Rating vs date.  Another way to look at the previous plot but plotting each book rather than the average by decade. Not much going on here although this may be due to the biased dataset and flawed publish dates.                                                                          Rating vs number of pages.  This is an interesting one - are longer books more popular? Most of the books are clustered around a couple of hundred pages but longer books do tend to have a higher average rating. I'm not sure why this would be the case but would guess that only someone who's already interested in a long book would read it or stick with it enough to leave a review.                      As usual, the code's up on  GitHub .",14,2,2014-03-16,3,"oyster, dataviz, api",1128,Fun with the Oyster books API
15,0,Today's Pi Day so this is a quick post highlighting it's interest and appeal.,#meta,{% include setup %} Since it’s Pi Day (at least in the US)I decided to jump on the bandwagon and contribute my own thoughts. Pi is fascinating. It’s such a simple definition - the ratio of a circle’s circumference to it’s diameter - yet it’s both irrational and transcendental and impossible to actually express as a simple number. People have been trying to get more accurate estimates for multiple millennia with multiple great mathematicians trying to derive their own approximation.            Srinivasa Ramanujan's Pi approximation     Looking at a Wikipedia article for [Pi approximations](http://en.wikipedia.org/wiki/Approximations_of_%CF%80) is itself overwhelming. How Ramanujan was able to come up with his approximations is tough to understand - they seem so ridiculously esoteric that it’s hard to imagine someone was able to come up with such a formulation. Since then there have been improved approximations and Pi’s been calculated to 12.1 trillion digits. I can’t think of any real reason why we’re spending countless computer cycles to get better approximations but that’s the allure of Pi: incredibly simple to explain while being infinitely expressive.,1,1,2015-03-14,2,"Pi, math",192,An ode to Pi
36,0,The biggest thing an engineering team can do to be productive is start imposing standards and conventions on everything they do. This will allow them to write less code that does more and increases quality.,#management,"{% include setup %} When it comes to productive coding, one of the most important things to do is to impose a set of standards and conventions. As long as you stick with them your code becomes significantly easier to write and maintain. Conventions range from having a standard way of declaring variables to the way files are organized within a project to the field names in database tables. The obvious benefit is that your code becomes significantly easier to navigate, both to you as well as to others on the team, since you don’t have to run through a series of searches trying to figure out whether a variable is called myVariable, MyVariable, or my_variable. The bigger impact is how much simpler your code becomes. By using a standard structure it’s possible to write code that’s further up in the abstraction hierarchy. This is a huge win for productivity and quality since  more code leads to more errors  and the best code is code that’s not written in the first place.  Two examples of how we’ve adopted conventions include:  - Making sure that every database table in our “log” schema has a timestamp column containing timestamps and every table in our “agg” schema has a ymd column containing dates. This allowed us to write an abstract job that aggregate the data from a log table to an agg table without having to worry about the underlying structure. All we had to do was specify the columns that were the keys and which ones needed to be aggregated - the job itself took care of the scheduling, the query construction, and the reporting. In addition, we were able to quickly write up a simple job that archived old log data. The job doesn’t care what table it takes as long as it has a timestamp column. - We use RabbitMQ for some of our asynchronous tasks and we’ve developed a standardized format that a majority of tasks share. These tasks take a name, a date, and an hour and then run a query for that hour. By imposing this structure, we were able to write a single block of code that would take tasks with a start and end date and republish them as a series of hourly tasks in the date/hour format. Since each task takes the same arguments, we’re also able to use reflection to automatically create an instance of the appropriate class for each task. For example, the task {“task”: “do_an_agg”, “ymd”: “2015-03-17”, “hour”: 10} automatically gets translated into new DoAnAgg(“2015-03-17”, 10). All we need to do is make sure the class DoAnAgg exists, has the appropriate constructor, and exists in the proper package.  Both of these examples are straightforward but the value comes in coming up with the proper abstraction that avoids unnecessary code. Standards make it easy to spot repeated patterns which can then be refactored upstream. This improves the leverage of everyone else on the team and makes every engineer more productive. People idolize the mythical 10 or 100x engineer but there’s more value in making the entire team more productive.",1,1,2015-03-18,3,"engineering culture, standards, convention",542,Power of engineering standards
25,0,I recently got an Android phone and have been enjoying the Google Now experience. I'm hopeful that Google opens it up to third parties.,#product,"{% include setup %} As much as it pains me to admit it I’m really enjoying Google Now. I’m aware of how much information I'm sharing with Google to make it helpful but at the moment I find the tradeoff worth it.  It came in especially useful as I've been traveling over the past couple of weeks:  - Show the official exchange rate when traveling. This may not be perfect, especially in the case of ""blue markets,"" but it's nice having a rough idea of how much a US dollar is worth. - Flight information. Since my flight details get sent to my Gmail account, I can quickly tell whether my flight's delayed and what terminal and gate it's scheduled to depart from and arrive to. This is useful to have when I need to make a transfer since I can quickly see where my next flight departs from. - Flight boarding passes. In addition to the flight information Google Now also shows the boarding passes for my checked in flights. I didn't have to do anything to board a plane other than activate my home screen and place it against a scanner. - Hotel information. Similar to flight information, I get a card telling me where my hotel is and how to get there. - Google calendar integration. This is an obvious one but I run my life through Google calendar. This gives me constant notifications of what I have to do when and as long as I enter an address for my events I also get an estimate for when I should leave.  A concern is that to actually make it useful I have to integrate more and more of my world with Google and I expect this to get worse as more Google Now cards are developed. The optimist in me hopes that Google Now will be opened up to third party developers in a future version of Android but the cynic suspects it's not going to happen.",0,1,2014-07-09,2,"google now, android",327,On Google Now
11,0,To fight patent trolls we need to go on the offensive,#meta,"{% include setup %}            A Patent Troll        Newegg recently defended  itself against a patent troll that sued them over a shopping cart patent. As a result, the patent was invalidated and Soverain Software will lose $2.5M from this and the $18M they won in 2011 from Victoria’s Secret and Avon. Unfortunately, they’ll still keep the tens of millions of dollars they “earned” in earlier years. Since virtually every ecommerce site has a shopping cart feature you’d think that this patent would have been invalidated sooner.   The reason it takes this long is that most companies settle when faced with a lawsuit and only a few fight back. Over time, companies that have a reputation for fighting back are sued less frequently and companies that do settle just pass the cost onto the consumer. It’s no surprise that these patents end up sticking around. Unfortunately, it’s a shitty situation for smaller businesses: they can’t afford a lawsuit and can’t afford to raise their prices.  What can we do to change these incentives around? Right now, a big advantage patent trolls have is that they make the first move and can choose who to sue and who to avoid. Why not bring the fight to them? A simple approach would be to find these these flawed patents and file for a  reexamination  with the USPTO.  Ask Patents  has already started collecting a database for prior art to challenge patent applications but this information can also be leveraged to challenge already granted patents. Another option would be to sue the patent troll directly, as Microsoft and Google  have done , which has an added benefit of a jurisdiction other than  East Texas . An extreme approach would be to create shell companies that intentionally violate these patents in order to challenge them. Imagine a hackathon whose sole purpose is to create sites and companies that violate these patents in order to troll the troll. Why not bring the fight to them?",6,1,2013-01-29,3,"patent trolls, law, newegg",391,The Patent troll troll
22,0,I added another tool to my JavaScript tools page that lets you generate a date range from a series of inputs.,#code,"{% include setup %} I finally had the chance to go back and add  another quick tool  to my JavaScript arsenal. This one lets you specify a start date, an end date, a step size and interval, along with a desired date format and it will generate the dates in between. This is a surprisingly common activity for me. Every time I need to split a query into multiple date ranges or come up with a series of arguments for various jobs I end up using Excel to come up with the appropriate date ranges. By having it available via the web it makes it a lot easier to generate exactly what I need as well as provides the flexibility to keep on improving. If there are any improvements you’d like to see or if anything is unclear definitely let me know.",1,1,2015-05-30,4,"javascript, date range, automate date calculation, date format",171,Date range generation
21,0,My first experience with Java left a bitter taste in my mouth but recent work has made it fun again.,"#devops,#product","{% include setup %} It’s amazing the impact tools have on productivity and enjoyment. I remember my first foray into Java using a combination of text editors and Ant. Setting up and configuring a simple project was a nightmare and without the internet I don’t know how I would have figured it out. This initial experience made me associate Java with an unnecessarily complicated approach that I wanted to avoid.  After Java, Python felt like a breath of fresh air. The code was simpler, more compact, and I was able to just dive in. Discovering pip and virtualenv made me enjoy it even more. But no language is perfect and with enough you uncover the imperfections. Performance became a bottleneck when I started working on serious code and I missed the benefits of static typing - especially when refactoring a large projects.  Recently, I started using Java again and it’s a completely different experience. I’m not sure whether it’s due to hardware or software improvements but Eclipse feels faster and more responsive. It makes Java nearly as fun as Python. The static, strong typing makes it easy to do large scale refactorings, Gradle and the open source ecosystem make it trivial to leverage all sorts of libraries, and the performance/coding ease is great - especially when dealing with concurrency. Good tools can make a world of difference to the accessibility and joy of writing a language. I read a while ago that Facebook has the strongest people  working on internal tools  and I’m not surprised; it may be one of the most effective way to make everyone happier and more productive.",1,2,2014-09-25,3,"java, python, developer tools",280,Dev tools matter
32,0,When doing Java development a huge win comes from using log4j effectively. Done right it can separate the signal from the noise and make it significantly easier to navigte your code.,"#code,#java","{% include setup %} Something that’s incredibly helpful when writing Java code is customizing  log4j . There are a variety of configuration options and learning just a little bit about them can make you notably more productive. I’ve found two features that have sped up my development cycles.  One was updating my PatternLayout to include the filename and line of each message. With Eclipse, this allows me to quickly jump to the relevant code block whenever anything looks odd rather than having to first open the file and then search for that particular message.  {% highlight properties %} log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d %p (%t) [%c] (%F:%L) - %m%n {% endhighlight properties %}  The other was to pick the appropriate log level at the package level. If I’m working on a single package I'll reduce the logging level of other packages to make the relevant messages stand out. This is especially handy when you incorporate eager third party packages that drown out your own messages with their own.  {% highlight properties %} log4j.logger.com.dan.package.one.logging=WARN log4j.logger.com.dan.package.one.logging.ClassName=INFO log4j.logger.com.dan.package.two=DEBUG log4j.logger.com.dan.package.two.working_on=TRACE {% endhighlight properties %}  My style of development is to rely on logs more than the debugger so these two have made my life a lot easier. In general. logging is an important tool for all developers and yet few tend to tweak the default settings. By understanding the available configuration options you’re able to tweak them for whichever problem you’re solving. This may not seem like a huge win but when you’re running the same program hundreds of times a day the small efficiencies add up.",1,2,2015-02-28,3,"java, log4j, logging",299,Getting the most out of log4j
34,0,Software is being jammed into every single product which also allows products to move towards a subscription pricing model. I wouldn't be surprised if everything we own is a subscription within 20 years.,#society,"{% include setup %} Reading modern technology and business news it seems that every single thing is moving to the subscription model. It’s no longer just the obvious stuff that’s being turned into a subscription with the help of technology. Software is becoming a critical part of many systems - ranging from powering cars to tractors to juicers - and would be hilarious if it weren’t so real. Software has transformed the world and now we’re trying to find the remaining places that software can be jammed into. And once the software sets in everything can be turned into a subscription model.  Just last month there was a [piece in Motherboard](https://motherboard.vice.com/en_us/article/why-american-farmers-are-hacking-their-tractors-with-ukrainian-firmware) describing how John Deere is building software protection into their tractors that farmers have been bypassing with Ukrainian firmware. The software is incredibly sophisticated and allows tractors to become more and more automated. Unfortunately this also allows John Deere to protect their code in such a way that only authorized technicians can make the necessary repairs - something that farmers may not be willing to wait for during a busy season.  In lighter news, this past week it was [discovered that Juicero](https://www.bloomberg.com/news/features/2017-04-19/silicon-valley-s-400-juicer-may-be-feeling-the-squeeze), a new age wifi-enabled juicer, serves no actual purpose and the juice packets can be squeezed just as effectively with hands instead of the machine. But of course the machine can scan the packet QR codes and let you know if they’re expired. All this for a $400 machine and up to $8 for a packet.  In our race to push software into everything and turn everything into a subscription it seems we’ve lost a bit of common sense as well as consumer power. The subscription model is short term cheaper and appealing but comes at the loss of power and control. Unfortunately given human behavior and psychology it seems this approach is here to stay. We’re much more wired for short term thinking and getting something at an immediate discount sounds much better than buying something for a huge upfront cost. Even if we were to compare the two options against one another the subscription model would come out cheaper: businesses would be able to price the subscriptions lower since they’d expect to reduce the unit cost over time and they can monetize in a variety of other ways, possibly by selling user data and information. With these incentives I wouldn’t be surprised if in 20 years everything we consume is sold via a subscription.",2,1,2017-04-21,4,"subscription, saas, john deere, juicero",432,Subscription all the things
9,0,Companies should strive to get into address bar autocomplete,"#product,#meta","{% include setup %}      Over the past few days, I’ve been thinking about habits. How do they form? How do they change? And the selfish one - how can you build a product that is habit forming? My cofunder sent me a great Nir &amp; Far  blog post  that goes into detail about generating desire which is a great read to anyone building a consumer product.  Along these lines, I decided to be a bit introspective and see which products and sites are a part of my habit. A simple way was to type each letter of the alphabet into the Google Chrome address bar and see what site autocompletes. Here goes:       analytics.google.com     bankofamerica.com     cad-comic.com/cad     docs.google.com     eventbrite.com     facebook.com     glos.si     heroku.com     instapaper.com     joinblended.com     klout.com     linkedin.com     maps.google.com     news.ycombinator.com     optimum.com     plus.google.com     questionablecontent.net     reader.google.com     startupmullings.com     twitter.com     udacity.com     voice.google.com     wixlounge.com     xkcd.com     youtube.com     zerply.com      After excluding my sites (glos.si and startupmullings.com), we can organize them into the following categories:        Entertainment (the comic sites - xkcd, QC, CAD; Youtube; Google Reader)     Social Networks (Facebook, LinkedIn, Google Plus, Twitter)     Utilities (Google analytics/docs/voice, Bank of America, Instapaper, Eventbrite, Optimum, Heroku)     The rare letters (Zerply, Udacity, Wix Lounge). I’d like to include Klout on this list rather than admit to browsing it but I don’t know if that will be believable.     Every consumer site should strive to get to browser autocomplete status for some users rather than being semi-popular to more users. Being useful to a few passionate users and growing with their help is a much better approach than trying to immediately appeal to the mass market.  And although this exercise may be embarrassing, I’d love to see what others have as their 26 sites.",1,2,2012-06-07,2,"startups, marketing",334,Achieving browser autocomplete
32,0,Despite being bullish on tech I tend to manage as much as I can through text files. They're extremely powerful while providing the flexibility to change to suit my evolving needs.,#meta,"{% include setup %} As many people know despite being bullish on tech I’m spartan and utilitarian with my technology usage. This expresses itself as a strong bias for text above nearly another format. There are tons of apps that try to help me organize my tasks and todos but I prefer simple text files and an intelligent folder structure. This is true when it comes to blogging as well - rather than using a fancy CMS or hosted application I rely on Jekyll which exposes my content in Markdown.  On the surface this seems inefficient - why build your own tools when perfectly good apps exist that will be maintained and improved over time? Unless I spend a ton of time there’s no way I’m going to be able to build a blogging platform that competes with Medium or Wordpress nor will I ever make a to do application that is better than Todoist, Wunderlist, or Google Calendar.  For me it’s less about the tool and more about the problem. Sure, a tool helps with that but I’m more about figuring out a process that works for me. Despite how great an app is it’s extremely unlikely that it will change to accommodate my evolving needs. Having my own process optimized around text gives me the flexibility to do things my way as well as easily change both the process and the underlying data.  Just last week I realized that I forgot to add metadata to a few of my blog posts. Had the content been squirrelled away in a web app there’s no way I would have been able to easily find which posts were affected other than writing a crawler and examining the DOM. But having everything in simple structured Jekyll text files made it as simple as writing a simple command line regular expression to identify these posts. And this can easily scale to any other blog maintenance task I have - whether it’s adding some additional information to a subset of posts or just searching for various words or phrases.  The success of this system depends on building out and committing to a structured approach when dealing with text. Text is innately extremely flexible but by imposing a semi-structured system of tags and folder structures it makes it extremely easy to navigate and manage. And if anything does change it only requires a small script to update everything to fit the new format. Replication is also simple - I can either keep it in a version control system or have it synced via Dropbox. If you’re undisciplined or have a static workflow definitely leverage an existing tool but if you’re constantly trying to improve your system and want the ability to go back and analyze content you produced there’s not much better than text. It unlocks the power of the command line while giving you the option to write whatever esoteric script you need to solve your own problem. And if you do want to export your data anywhere else it can be as simple as turning your simple, semi-structured text into an API request to whatever service is in vogue at the moment.",0,1,2016-04-30,5,"text, todo list, calendar, blogging, utilities",528,Text is king
32,0,Over 2015 I've been a lot more self aware and have been thinking of life lessons to adopt. This post is a summary of the key lessons I've discovered and adopted.,#meta,"{% include setup %} Part of my 2015 goals was to have a weekly retrospective where I’d be distraction free and force myself to just sit and think. I usually did this on a Sunday morning by going outside and sitting on a bench overlooking the river or inside a quiet park. At the end of each of the retrospectives I’d sit down and jot down my thoughts in order to consistently revisit the list in order to keep improving. Below are the lessons of 2015 that I’m adopting going into 2016.  - Scheduling time for a task rather than just a goal. In the past I’d add tasks as a day event to my calendar. The better approach is to block specific time for a task - this ensures I’ll at least get something done and makes it more difficult to push things back. - Minimize the amount of physical things I own and focus quality over quantity. Maybe this was due to the apartment move but I’ve come to the conclusion that I would rather have fewer things of higher quality. This is a bit tough for me to act on since I tend to like getting deals and am pretty cynical towards trends and fashions - I can’t tell what’s actually high quality and what’s just marketing. - Sleeping more and better. Some people can get away with little sleep but I’m unfortunately not one of them. I need to get at least 7 hours to be productive. - Tracking my time usage better. It’s amazing how much time we actually have and how much of it we waste. For me it’s due to a variety of distractions and I need to be better at understanding how I spend my time in order to improve my behavior. - Don't procrastinate. A simple lesson here but I need to stop pushing things to tomorrow that I can do today. Especially when delaying something ends up snowballing and delaying a bunch of other things. - Focus on one thing at a time. Multitasking doesn’t actually work and I end up doing multiple things poorly and slowly rather than one thing well. I typically fall into this habit when watching some TV while doing some work - in those cases I’m almost always slower at my work and it would have just been better to finish the work and reward myself with some leisure time. - Distraction free walks to think. This is a rephrasing of the introductory paragraph but it’s important to get away from distractions and just force your mind to wander and think. It’s difficult at first with the desire to look at a phone or a random website but it’s worth it. - Knowing at every point why I'm doing something. Another lesson here in understanding how I use my time better. If I’m doing something I should know exactly why I’m doing it since everything comes with an opportunity cost. This doesn’t mean that I need to be productive at all times and can never relax but I should understand the tradeoffs I’m making. - Having and evaluating short and long term goals. I wish I did this when I was younger but it’s important to have goals we’re constantly working towards since it provides direction and allows us to measure our progress. - Watching less TV. A no brainer here but TV is a pretty big waste of time and I should watch less of it. I already don’t have cable but still find myself wasting time watching Netflix or some football games. - Focus on making versus consuming. This is all about productivity but I need to get into the habit of not consuming as much (TV, blogs, games, etc) and instead using that time to create. I’m already decent at this but need to get to the point where creating actually gives me more pleasure and relaxation than consuming. - Focusing and dedicating time to finance/investing/routine/research. As I’ve gotten older I can’t help but think about my later life and a big part of is figuring out how to invest my savings now to prepare for the future. I need to be more active in my investments and make sure the money I have isn’t just sitting around depreciating. - It’s okay to not have any new insights. During one of my walks I just wasn’t able to think of anything new and that’s perfectly okay. Not everything is about productivity and novelty and it’s fine to just relax and enjoy the moment. - Having a behavior consistent with views. A philosophy of life one here but if there are certain things you feel strongly about you need to make sure you act in alignment with it. It’s tough to do given outside constraints but something I’ve been more keen on. This sounds a bit abstract but an example is fighting peer pressure - sometimes it’s better to just skip an event and focus on what you want to do. - Just get started with something small, work your way up. Oftentimes embarking on something new feels like a gargantuan undertaking but it’s better to just start and take it one step at a time. The point above on scheduling time for tasks rather than goals helps address this. - Identifying bad habits and working on eliminating them. This is all about self-awareness but we all have bad habits and if we acknowledge them and work on eliminating them we’ll all be better off. - Abstinence versus moderation. I don’t recall where I read this but it rung true to me. The point was that we’re all wired differently and that some people have a hard time doing moderation and for them abstinence is necessary. A lot of my bad behaviors fall into this territory and I’d be better off completely abstaining rather than walk the fine line of moderation. - Thinking about personal brand. I’m not sure this is relevant to everything but I think it’s important to think about the personal brands we have and fostering it. Who knows how the world will look in the future but it’s important to have a good reputation and understand how you’re seen and perceived. - Having constant list of todos. I maintain an ever-growing list of todos that I will try to knock out when I have some spare time. It helps take care of a few items while keeping me productive. - Finding entertainment from within, not outside sources. Rather than rely on the outside world to entertain us we should find that within - that way we can always be entertained and don’t need to be blocked by anything. - 1% better each day. Just a thought here but if we all got 1% better each day and that compounded then at the end of a single year we’d be nearly 38 times better. This is tough to achieve but there’s just so much potential that we at least have to try. - Expectations are oftentimes better than the reality. Many times I’ll do something because i have the expectations and thought that I’ll enjoy it but after the fact I realize that it was a waste of time. The biggest example of this for me is drinking - I come in with the notion that it’ll be fun but more often than not it’s the same as any other time. It would have been better to save the money and calories and just have a fun time with friends. - When making spelling mistakes, retype the entire word. A small one here but my spelling has gotten worse with the advent of built in spellcheckers and my way of fighting it is to retype the entire word without using the spellchecker whenever I make a spelling mistake. This at least gets me into the habit of spelling words properly. - Investing time and value into things that compound. Similar to many earlier points but we should be focused on investing our time into things that matter and help lay the foundation for the long term. In my case these are knowledge and health - investing in both of them now provides compounding effects for nearly everything later. - Taking care of the small things. These days it’s easy to get inundated with tons of small things that all eat up small amounts of time. It’s easy to dismiss these but I still strive to take care of the small details. - Figure out habits and rituals. Rather than trying to do too much at once it’s better to focus on a few things and do them until they become habits and rituals. Only then should we pick up new habits to adopt. - Running in the morning changes mood the rest of the day. It may be tough to wake up early in order to go for a run but it sets the tone for the entire day so I need to just do it. - Exceptions are never exceptions. It’s easy to skip something you don’t want to do by writing it off as an exception but it never is. It’s just a rational trick to make us feel better but it’s easy to destroy a habit by constantly thinking of exceptions.",0,1,2016-01-01,3,"2015 lessons, productivity, self-awareness",1522,Lessons from 2015
32,0,A couple hundred years ago Napoleon or Frederick the great stated that an army marches on its stomach. The modern day equivalent would be that a company marches on it's data.,#meta,"{% include setup %} A couple of hundred years ago nearly every European country was engaged in some sort of military conflict which led either Napoleon or Frederick the Great to state that “ an army marches on its stomach .” The point being that logistics are the most important when it comes to having a successful army. These days the corporate equivalent would be that a company marches on its data.  Every company claims to be data driven and there’s a slew of data collected about us each day. The most successful companies are able to leverage this data and use it to derive insights that drive direction. Unsuccessful companies may collect the same data but don't leverage in an impactful way. It’s easy to collect information but it’s a huge challenge to turn into action. There are many options just for storing the data: one approach may make it easy to store tons of data while making it hard to run large scale analyses while another allows for a distributed computation approach that's too slow. Beyond data storage there’s the actual analysis piece: what’s the appropriate model to use that can represent the relationships between the variables while being true to life? All these are questions that will become increasingly critical and separate the winners from the losers. Data itself has potential for massive  monopoly feedback loops  - companies that succeed are able to collect more and more which improves their product which collects more data. Right now it may only seem as if larger companies should care but I suspect within the next 10 years we’ll see more and more small and local businesses adopt a truly data-driven approach, whether through internal tools or through external services",2,1,2015-11-08,3,"data, strategy, business",300,A company marches on its data
18,0,I'm running an experiment and will cross post to Medium in order to compare the engagement numbers.,#meta,"{% include setup %} Despite my [aversion](http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog/) to walled gardens and platforms I’ve seen a ton of people make the switch to [Medium](https://medium.com/). Within the past month I’ve seen a variety of bloggers move over to Medium, both big and small: [Mark Suster](https://bothsidesofthetable.com/finding-a-new-medium-aa0f882815d#.s4y1c45ky), [Semil Shah](http://blog.semilshah.com/2016/04/30/medium-rare/), [Andrew Parker](http://thegongshow.tumblr.com/post/143602596745/corporate-governance-dictatorships-vs-democracy), and a former coworker, [Dillon Forrest](https://medium.com/@dillonforrest). I’m still not convinced that Medium is for me but it definitely feels as if it’s at that inflection point with more and more people moving to Medium. And from what I’ve heard it does wonders for reach and promotion - something that I’ve been relying on Google search and Twitter for.  To that end I’m going to try an experiment and start publishing on Medium ([https://medium.com/@dangoldin](https://medium.com/@dangoldin)) as well as on my primary blog. The goal is to experiment with Medium and see how much engagement it can actually drive. To start I’m going to copy some of my posts over to Medium and see how they fare.  So far, one of the nice things about Medium is that it comes with a simple API that allows you to take either Markdown or a subset of HTML and turn into a Medium post via a quick API call. In fact, earlier today I wrote a [small script](https://github.com/dangoldin/medium-tools) that that takes the raw Jekyll markdown and posts it as a draft to Medium. It won’t work on every single post yet but for the ones that are pure markdown it works perfectly (example: the [original](http://dangoldin.com/2016/05/11/identifying-unused-database-tables/) vs on [Medium](https://medium.com/@dangoldin/identifying-unused-database-tables-f1e969039f6c#.1n6p1g1jw)).",10,1,2016-05-15,2,"medium, blogging. content",287,Experimenting with Medium
22,0,Politics is akin to economic bundling so we need to see how those were disrupted in order to revamp modern politics.,#meta,"{% include setup %} Yesterday I made the case that the current political system consists of a [series of product bundles](/2016/03/26/political-parties-are-product-bundles/) and I’ve been thinking of ways unbundling would work. And what better way than to look at existing products and industries that have been unbundled.  As numerous people have pointed out, the music industry is a clear example. Initially music was sold on CDs and there was no idea of buying solo songs. But with the launch of the iPod, iTunes, and internet proliferation it became possible to buy individual songs. Lately we’ve been back in the bundling phase with the various monthly music subscription services, such as Spotify and Apple Music.  Music debundling was driven by technological changes. It made no sense to package individual songs for sale when they required physical packaging. But as soon as the majority of households got reasonably fast internet it became possible to start selling individual songs.  But how does this apply to the political system? It’s not really a technology problem. We have the ability to share and disseminate information to anyone with an internet connection. We have the ability to allow everyone to vote through a smartphone. We have the ability for anyone to start a cause and share it with millions of people. Unfortunately, having the ability doesn’t mean much without follow through.  In the case of politics there’s so much entrenchment (think recording studios) that change occurs at a glacial pace. The reason the recording studios signed with Apple was because of the rampant piracy - not due to their desire to improve the consumer experience. We need the political equivalent of piracy to spur this unbundling. Ideally it comes dressed as a white knight ready to save the system but leads to unexpected secondary effects that lead to significant changes in the system.  My gut is that we need a few small changes that open to the door to these unintended effects. Something akin to allowing people to request or report services via an app which leads to people asking what else? That will open the door to voting for issues and politics from our phones and maybe even filing taxes.",1,1,2016-03-27,2,"politics, bundling",368,Unbundling politics
20,0,It's incredible that I have a site that's been running for the past 4 years with virtually no maintenance.,#code,{% include setup %} There’s something spectacular in checking in on a project you worked on years ago and discovering it's still running years later. This past Friday I got an HTTPS alert from [Let’s Encrypt](https://letsencrypt.org/) reminding me that my SSL certificate for [https://yahnr.dangoldin.com](https://yahnr.dangoldin.com/) was set to expire. I checked it out and remarkably it's still up and running. I built that in March of 2013 as a proof of concept of what I termed a [“pseudo-static site.”](http://dangoldin.com/2013/03/12/mmmm-pseudo-static-sites/) The idea was to have a site hosted on statically on S3 but powered by a dynamic job that refreshes the underlying data.  It’s incredible that the code is still functioning as expected. Based on the [commit history](https://github.com/dangoldin/yahnr/commits/master) I did make minor tweaks in 2014 and 2015 but they didn’t alter any of the core functionality. What’s more impressive is that this was based on scraping Hacker News which implies that Hacker News itself didn’t go through a significant enough redesign that broke the page parsing. It does look as if the comment count isn’t being properly fetched but other than that everything looks perfect.  Four human years isn’t very long but in internet years it’s ancient. The fact that some throwaway code is still running four years later is a pleasant surprise and highlights the resiliency of a good design. No matter what code we’re writing it’s valuable to think about how it will stand the test of time and whether there’s a way of improving the resiliency - not for the project itself but to get into the habit of writing robust and durable code.,4,1,2017-03-12,1,,277,In praise of long running code
32,0,"It's getting easier and easier to launch a new brand and we're moving toward a world where there will be a unique brand per person, all owned by a few companies.","#product,#society","{% include setup %}      While browsing Instagram the other day I saw an ad for Optimum. Despite it being for Optimum the ad showed a sports clip that could have just as easily been on a highlight real or a a trailer for a sports movie. In fact, if it weren’t associated with Optimum maybe I would have even clicked on it.  Having a strong brand is incredibly valuable - think Coca Cola. A weak brand (think amy ISP), on the other hand, is incredibly damaging. If a company has a strong brand they should definitely flaunt it but I’m surprised that in this day and age poorly branded companies still operate under their primary brand. There’s nothing stopping them from launching dozens of unique brands each designed to appeal to a particular audience.  They’d still be able to leverage the underlying infrastructure but the differences would be on the way it’s presented to customers. This would range from the way the product is marketed and advertised to the way the website is designed.  We’ve already seen something similar in retail with major brands having a variety of subsidiaries. The best example is [Gap](https://en.wikipedia.org/wiki/Gap_Inc.). In addition to Gap, it also owns Old Navy, Banana Republic, Athleta, and Intermix. Gap could have marketed each of them under its own mantle but given the audience it made a lot more sense to let each brand manage itself. I don’t know how much of the Gap infrastructure each of the subsidiaries leverages and Gap may very well be just a holding company but in that case efficiens are lost. Gap should leverage its size to get discounted rates on everything it uses - ranging from the raw materials used for clothing to improved shipping rates to improved advertising rates.  Similarly, Anheuser-Busch InBev owns more than [500 beer brands](https://en.wikipedia.org/wiki/Anheuser-Busch_InBev). Budweiser is the ultimate Anheuser-Busch beer but so are Hoegaarden, Shock Top, and Blue Point. They’ve each managed to maintain their own brand and reputation while taking advantage of the Anheuser-Busch InBev infrastructure.  For modern internet brands it is easier than ever to push towards a world where every customer has their own brand. It is extreme and unlikely but it’s inevitable that we see an increase in these microbrands that are designed to go after specific customer segments. I’m constantly surprised by how many startups there are [selling mattresses](https://www.buzzfeed.com/iknowkayleen/slumber-party?utm_term=.fogE7AvJRd#.mwRBN9MJvO) and this is only the start.",3,2,2018-04-28,3,"commerce, marketing, product",430,Rise of microbrands
19,0,United and Bank of America are charging false fees with the hope that consumers don't notice. It's disgusting.,#meta,"{% include setup %} Over the past month I had two experiences that seem too coincidental to be true and highlight how desperate some companies are to chase every penny.  These are massive corporations that seem intent on sneaking in false fees and charges hoping that their customers don't notice.  The first involved United Airlines. The airline industry already suffers from poor reputation and it seems United is doubling down on it. I had a flight back from a vacation and thought I'd treat ourselves to an upgrade. United had an option to use miles and money which would charge you immediately, add you to the upgrade list, and refund both if you weren't picked. We didn't get the upgrade but we miraculously received a refund of the miles but only half of the money. It took a few phone calls but in the end I got the refund.  The second was a much smaller amount but just as frustrating. I requested a new debit card from Bank of America and got a wonderful notice telling me it was free of charge, this time. Lo and behold I check my statement in a week and see a debit for a ""card replacement fee."" Sure enough a phone called took care of this as well.  It's possible I'm overreacting and these attempts were accidental but    it's incredibly frustrating to have to waste time on a phone dealing with these errors. I'm more annoyed than most since I code for a living and know how easy these would be to fix. If not malicious then it's negligent. There's no consumer protection here and I'm sure tons of people fall privy to this scam. I wish we could do more but for now all we can do is always review our billing statements.",0,1,2017-01-26,5,"united, bank of america, scam, billing statements, consumer protection",302,Shame on United and Bank of America
32,0,There are very few global tech products - those that work wherever you are in the world. A big reason is due to infrastructure differences and I don't see this changing soon.,#product,"{% include setup %} The trip to India got me thinking about “global products” that work the same wherever they are. It’s surprisingly difficult to find tech products that fit this description. Cell phones will almost always work internationally but roaming charges make it impractical. Having an unlocked phone helps but you still need to get a SIM card which is a  hassle  in many countries.  Even something as standard as a laptop isn’t as easy to use as it should be. Wifi connectivity varies depending where you are with most cities being great fickle elsewhere. Dealing with voltage conversion and plug adapters is something that always comes up. I’ve learned to travel with an adapter kit that includes enough combinations to be able to charge my laptop wherever I go.  The one product that actually worked as expected was the GPS running watch my wife got me for our anniversary. No matter where I was it was able to lock on to a satellite within a few minutes and accurately track my run. The only issue was charging which I was able to do via a USB cable connected to my laptop. Even that shows a weakness since if I didn’t have a laptop I wouldn’t have been able to charge the watch.  The funny thing is, each of these products was designed to work globally - it’s just that the infrastructure differences prevent that from being a reality. Whether it’s having a different set of of plugs or a particular way of getting a SIM card it’s not the product that’s the problem. As powerful as our products are they’re still operating within an infrastructure. And since products evolve faster than infrastructure we’ll continue to see this inconsistent product behavior around the world. Maybe by the time we colonize space we’ll have a consistent global infrastructure.",1,1,2013-12-25,3,"global products, infrastructure, technology",315,Global products
26,0,I'm having my phone repaired and am feeling what I expect are withdrawal symptoms. It's scary to thinh how addicted we've become to our smartphones.,#meta,"{% include setup %} As they say you don't appreciate something until it's gone and I got to experience first hand when I cracked the screen on my phone and dropped it off for a quick repair. Unfortunately, the repair wasn't so quick due to a screw up and I'm still phoneless more than a day later.  I find myself reaching for it despite knowing it's gone and even feel it vibrating in my pocket without it being there. It's both amazing and frightening how significant my phone has become in my life and I'm am actually glad that it's missing. In many ways I feel like an addict that missed a fix and it's a rude awakening. For the first time in years I had to ask a stranger for the time and had to find an open barber shop  without a map or an online search. I also went to bed without my usual habit of checking up on Twitter or catching up on some blog posts and I woke up without immediately reaching for my phone.  It's shocking how hooked we've become - nearly every person at my train station is staring at their phone while waiting for the train, on the train itself, and when they leave. Of course phones make our lives better but we have to realize the price we're paying and sacrifices we're making. I plan on being more mindful when I do get my phone back and will try to keep some of my days phone free - who knows what sorts of adventures I'll have.",0,1,2015-08-16,2,"smartphone, phone addiction",263,A phoneless day
34,0,I ended up stripped the head of a screw when assembling a piece of furniture and needed a way to finish the job. Luckily I stumbled on an easy fix using the driver.,#meta,{% include setup %} Note that this is straying a bit far from my usual posts but I thought it would be helpful for anyone that’s had to deal with a stripped screw or a broken screw head. In my haste I used the wrong driver bit and completely stripped the screw head. It was deep enough that I wasn’t able to extract it using pliers while being so stripped that none of my screwdrivers had enough grip to finish screwing it in. After a bunch of failed ideas I finally stumbled unto a solution that worked and could have helped me over the years. The idea is to use a drill/driver but instead of using a bit in the head you tighten it around the stripped screw. Then when it’s tight around the screw you drive it in until it’s where you want it to be. The other option is to use this approach to get the screw out and replace it with a brand new one to make sure it’s able to removable in the future.,0,1,2015-05-26,3,"screw, stripped screw, hardware",179,Dealing with a stripped screw
33,0,I've stumbled unto a way of doing recursion using a series of redirects. I can't think of a real use case but it's one of those fun hacks that's interesting to see.,"#code,#javascript","{% include setup %} I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.  {% highlight javascript %} var express  = require('express'),     port = 4000;  var app = express();  app.get('/sum', function(req, res) {   var n = parseInt(req.param('n'),10) || 0,       a = parseInt(req.param('a'),10) || 0;   if (n === 0) {       res.status(200).send('Sum: ' + a);   } else {       var url = ""/sum?n="" + (n-1) + ""&a="" + (a+n);       res.redirect(url);   } });  app.get('/fact', function(req, res) {   var n = parseInt(req.param('n'),10) || 1,           a = parseInt(req.param('a'),10) || 1;   if (n === 1) {       res.status(200).send('Factorial: ' + a);   } else {       var url = ""/fact?n="" + (n-1) + ""&a="" + (a*n);       res.redirect(url);   } });  app.get('/isPrime', function(req, res) {   var n = parseInt(req.param('n'),10),       f = parseInt(req.param('f'),10) || 2;   if (f > Math.sqrt(n)) {       res.status(200).send('Prime');   } else if (n % f === 0) {       res.status(200).send('Composite');   } else {       res.redirect('/isPrime?n=' + n + '&f=' + (f+1));   } });  app.listen(port); console.log('Server started on port ' + port); {% endhighlight %}  The only cases I can think of where it’s even remotely useful is if your servers are behind a CDN and you want to cache every intermediate result without having to write the application logic to do it or you need to reduce the amount of work done by a single HTTP request. It’s just not an efficient approach otherwise - the overhead of making new HTTP connections and handling arguments for every recursive step is usually more expensive than doing the actual logic within a single request.  The other use case I can think of is purely educational - it forces you to write your recursive code in a tail recursive style and forces you to think about the state you need to share between redirect requests. And if you’re ever told to solve a problem without using for loops or recursion you can violate the spirit of the request by using a series of HTTP redirects.  I’m genuinely curious if there’s an actual use case for this and whether anyone’s had to do this.",0,2,2014-12-31,6,"javascript, redirects, recursion, fun hacks, code, programming",443,Redirect recursion
39,0,Amazon announced that they'll be offering Prime as a monthly service and this contributed to a price drop in Netflix. I find this irrational - it's a tiny decision that was expected and should have been taken into account.,"#product,#meta","{% include setup %}      Last night, Amazon [announced](http://www.nytimes.com/2016/04/18/business/amazon-challenges-netflix-by-opening-prime-to-monthly-subscribers.html) that in addition to the annual plan they’re going to start offering Prime as a monthly service. Sure enough, investors interpreted this as good move by Amazon (up 1.51% at end of day) while hurting Netflix (down 2.79% at end of day and even more post earnings). These percentages translate into a $1.34B decrease to the Netflix valuation and a $4.49B increase in valuation for Amazon. As a shareholder of both I find this behavior interesting for its irrationality.  Companies are constantly innovating and have a constant stream of ongoing initiatives and experiments. I’m surprised such a simple move can impact the markets so much - it seems like an obvious move that would have happened at some point and should have been baked into the current price. The fact that there was such a sudden stock price move attributed to the news strikes as proof in the irrationality of the markets - trivial decisions shouldn’t be moving the needle and people should be investing in long term plans and visions.  I’m bullish on both and view them both as compelling replacements to cable and legacy TV consumption. Netflix has better content portfolio and is worth the $7.99 I pay each month. Amazon provides some new shows but I’m a Prime member for the free 2 day shipping. I’d love to see the numbers but I suspect there’s a large overlap between households that have Netflix and those that have Prime. The real competition is existing cable networks that are going to get punished as the younger cord-cutter generations move out of their parents’ homes.",1,2,2016-04-18,6,"amazon, netflix, stock market, media, tv, cable",304,"Irrationality of the markets: Amazon up, Netflix down"
18,0,I did a few months of consulting work and wanted to share some of the lessons learned.,#meta,"{% include setup %} To supplement my income while working on a startup, I took on a few consulting projects and wanted to share some lessons learned. It seems that everyone’s consulting experience is different so consider mine experience as just another data point.       I was able to get more work from my existing network than anything else I tried. As soon as I told people I was looking to take on some consulting projects I was able to get interest and referrals. If I didn’t have that I’m not sure how I would have gotten my first few projects.     It took longer than I expected to agree on a project’s scope and get the contract signed. My approach was to do a call or meeting to understand the goals of the project and then break it down into components with an estimated time and cost for each piece. I liked this approach since we were able to discuss the priorities of various pieces and talk about the risks associated with each.     It took longer to get paid than I expected. I was confident that I’d get paid but it took a few emails and meetings to get the payments made. The part that helped was getting an initial deposit before starting the work.     The biggest benefit was the flexibility to choose when and how to work on the projects.  Unfortunately, this flexibility is better in the abstract. I didn’t find the flexibility that valuable since almost everyone I know is working at a full time job which causes me to also follow a pretty standard schedule.     Most of the knowledge I gained was on the business/marketing side rather than on the tech side. I wasn’t doing challenging work and for the most part didn’t get a chance to work closely with others. The projects I did were also pretty independent so I had to resort to Google and Stack Overflow to help me deal with various questions that came up.     The projects I had were not critical to the company and were mostly “nice to haves.” This had the effect of me not feeling very aligned with the company vision which made the projects less interesting than they should have been. I’m not sure if this was due to the way I positioned myself for consulting work or due to the companies not wanting to outsource their critical projects.     A shared GitHub account worked amazingly well. The client was able to track the progress and provide feedback at various stages. This required me to commit well documented, working code but it definitely made communication easier. I also had a staging environment set up for my projects which let the clients see the code in action.     I wrote a  post  last month on pricing smaller consulting projects and wanted to highlight that again. I would come up a time estimate for a project that would be billed at my usual rate. Any work that spilled over would be billed at a discounted rate. This gave clients confidence that my estimate was reasonable and gave them a sense of the total project cost.     This was my first time doing serious consulting work and it’s a mixed bag. I enjoyed the flexibility but didn’t find it being a huge deal. I was also taking on projects that paid the bills but weren’t the most exciting. My biggest gripe was that I felt I wasn’t learning as much as I would have had I been working as part of a team. This gave the illusion that I was falling behind on my skills and not improving as much as others were.If I were to do it again, I’d want to specialize in a particular field and only do projects that fit in with my passions and interests. I’d also want to get it to the state where I’d be working alongside others rather than being entirely independent.",1,1,2013-10-27,3,"consulting, contracting, engineering",673,Lessons from consulting
21,0,I spoke at DataEngConf 2018 about the scaling TripleLift's data pipeline and wanted to share the slides and lessons learned.,#data,"{% include setup %} On November 9th I had the privilege of speaking at DataEngConf under the “Hero Engineering” track. My talk was titled “The Highs and Lows of Building an AdTech Data Pipeline” and I covered our evolution from a dead simple, sampled approach that had nothing to do with big data to the latest version which is leveraging a variety of modern open source data technologies.  I spoke about the motivation, challenges, and lessons learned during each iteration and ended the talk with the top 3 lessons learned across the various iterations of the pipeline. If you’re interested in the details you can grab the slides as either [Google Slides](https://docs.google.com/presentation/d/1XmOPgsbxoah2Pulw3eRvzjClOM5A5Dq-B2MWfot0guo/edit#slide=id.p) or as PowerPoint from the [DataEngConf site](https://www.dataengconf.com/speaker/the-highs-and-lows-of-building-an-adtech-data-pipeline ). Note that there was also a recording made but I’m still waiting for it to be processed and uploaded it to YouTube and will share that when it’s available.",1,1,2018-11-20,2,"dataengconf, scaling a data pipeline",165,My DataEngConf 2018 talk
30,0,If you do the math you discover that an iPhone 6s has the same battery capacity as a strawberry. One day we'll be powering our devices using our bodies.,#society,"{% include setup %} Lately I’ve been thinking of how we’re moving more and more to a world where humans will start merging with technology. We’re already carrying our phones around nearly every minute of every day but at least we keep them in our pockets. Watches are always on our wrists and with augmented reality the permanently attached technologies will only grow.  At some point they’ll just become part of our bodies and I was curious what additional energy we’d need to consume in order to power these devices. A naive calculation makes it seem that our devices are incredibly cheap to power. An iPhone 6S [comes with](https://www.apple.com/legal/more-resources/docs/apple-product-information-sheet.pdf) 6.61 Watt hour battery which is 23,796 Watt seconds (6.61 * 60 * 60). And since a single calorie is equivalent to 4.1868 Watt seconds with some simple math we get that an iPhone 6s battery is equivalent to 5683 calories. But these are true calories and not what’s actually listed on food labels. Food labels list kilocalories so remarkably the capacity of an iPhone 6s is fewer than 6 “food” calories. For context 6 calories is what one gets from a large strawberry. I charge my phone once a day and I’m shocked that the energy in a strawberry would be able power my phone for a single day.  I’m sure the math is not that simple and there’s a lot of factors I’m not taking into account but it’s amazing to think how incredibly efficient smartphones are given all they do. I can imagine a future where instead of having separate chargers for our devices we can power them through our bodies. And while this vision is reminiscent of The Matrix and is a bit dystopian it does have its appeal and I suspect we’re getting closer and closer.",1,1,2017-01-16,5,"technology, batteries, powering technology through food, future, science fiction",311,Powering our devices using the human body
31,0,When writing code it's important to think about how it will be deployed. That leads to higher quality code and improves the rate at which teams can deploy new code.,"#management,#devops","{% include setup %} The goal of every bit of code should be to make it to production. Code that’s not deployed is wasted effort as well as a loss to the business. And a big part of making sure code is deployed is thinking through the deployment plan as we write the code. Some code is deployed simply by pushing the new application while other code may require updating the database schema. More complex code may depend on other applications which will need to be tweaked and deployed beforehand. Large companies and teams have dedicated ops teams that handle deployments but small teams need to do this on their own.  Thinking through the deployment also leads to better code. By going through the steps of how the deploy will work you end up breaking your code down into a series of changes that end up being significantly safer and reduce the risk of a large failure. For example, we may want to write an update that will add an additional feature to our application based on a flag in a database. A safe way of doing it is to create a new column first that will have no effect on the existing application and then roll out our the new code that starts using this column. Future releases can then remove the code that uses legacy columns with latter releases dropping those columns entirely. None of this is shocking news but it’s surprising how rarely we think about deployments when we set out to write code. Especially as a team grows it’s important for everyone to be thinking about the way their code will work and the way it needs to be deployed.",0,2,2015-10-18,3,"deployment, coding, dev ops",285,Writing code? Think about the deployment
14,0,Let's Encrypt made it surprisingly simple to support HTTPS on my old projects.,#devops,"{% include setup %} I’ve been meaning to mess around with [Let’s Encrypt](https://letsencrypt.org/) since they launched their public beta but haven’t had the chance until earlier today. As an proof of concept I had a bunch of old projects running on a Digital Ocean instance and decided to try converting them to HTTPS using the Let’s Encrypt project.  Despite the usual complexity of getting and integrating an SSL certificate Let’s Encrypt made it extremely easy. It was smart enough to go through each of my Apache configuration files and prompted me to see which domains I wanted to switch over to HTTPS. After selecting a few and continuing to the next step it generated new configuration files with the appropriate setting to enable SSL support.  The only issue I ran into was handling a WSGI configuration properly. Let’s Encrypt works by copying an existing configuration file and adding a few lines to specify the SSL certificate. This works great for simple configurations but can lead to an issue when you have the same WSGI configuration across two files. The fix was straightforward - temporarily comment out the conflict lines, run the Let’s Encrypt script, and then uncomment the lines in the new SSL version of the file.  Overall an extremely simple way to enable HTTPS on your projects. In the past I would have never set SSL up on toy projects due to both the cost of buying one as well as the cost of dealing with a bunch of esoteric commands to set it up. Let’s Encrypt makes it incredibly easy - especially if you’re running Apache.",1,1,2016-02-20,3,"ssl, https, Let's Encrypt",268,Let's Encrypt
14,0,I wish calendars had a SQL-like language that let me run queries.,#meta,"{% include setup %} I’m a power user of Google Calendar and use it to organize meetings, tasks, and important dates. The one thing I wish it had was a more powerful query language. I often wish I could run SQL-like queries on top of my calendar. For example being able to get a count of the number of people taking a vacation by day by team or looking at the intersection between multiple calendars. The goal would be to automate much of the work I’m doing now when looking at managing my calendar but also uncover insights that are currently constrained by a lack of easy access.  Given that calendars are reasonably well structured this should be doable by just dumping the data into a relational database and writing the appropriate queries. I’m positive there are some subtle nuances that I’d have to work around - ranging from missing fields, to recurring events, to overly complicated metadata - but it feels like a solvable problem. As I write this an analogy that comes to mind is [JQL](https://www.atlassian.com/blog/jira-software/jql-the-most-flexible-way-to-search-jira-14), or JIRA Query Language, a bastardized SQL that was designed to work on top of JIRA. It supports SELECT statement on top of the JIRA fields but neglects joins and aggregates. JQL is designed for a single purpose and a calendar query language can follow this model. I think I gave myself a project for the next couple of weeks.",1,1,2018-01-13,4,"calendars, query language, sql, jql",249,Calendar query language
13,0,Modern browsers support incognito browsing but Chrome and Safari implement it differently.,#meta,"{% include setup %} To maintain some semblance of privacy I’ve been doing the majority of my searches in incognito mode. I normally use Chrome but a few days ago I had to do some browser testing in Safari and discovered that the two browsers handle incognito mode differently. Chrome’s incognito mode will share cookies across all incognito tabs - equivalent to running another instance of Chrome. Safari, on the other hand, will give each tab as its own cookie store.  This is pure speculation but Apple has been pushing the narrative that they care more about privacy than Google and this reinforces that view. Depending on the scenario either one can be considered less user friendly - Chrome if you truly do want each tab to be unique or Safari if you want the entire incognito session to act as a single browser - so it does seem as if it was just a product decision made by the respective teams. The behavior difference may even be explained by the name - it’s “incognito” in Chrome but “private” in Safari - and they actually do suggest different behaviors. I wonder if we’ll see a browser with support for both types.",0,1,2018-08-21,4,"chrome, safari, incognito, privacy",197,Incognito mode: Chrome vs Safari
30,0,A neat idea is to write every scraper you code as an API. This provides a nice separation of concerns and turns boring scraping problems into interesting engineering challenges.,"#code,#meta","{% include setup %} While building the [Turo scraper](http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history/) I became annoyed that there was no API to make my job significantly easier. Then I wouldn’t have had to go through a variety of hoops and iterations to get the data I needed and would also not have to worry about changes to their page design breaking the script. This got me thinking about an idea to write my scraper in such a way that it’s exposed as an API. In that case I can architect the code so that the retrieval and manipulation of the ride data is completely separate from the scraping code. Then if and when Turo does decide to release an official API all I’d need to do is swap my unofficial implementation out for the official one.  This chain of thought led to me to the challenges of building this on the engineering side. There’s something neat about being able to specify a bit of data through a series of steps. For example, to get the details for a ride the steps may be: 1) login to Turo, 2) navigate to that ride’s receipt page, 3) parse the details, 4) return them as JSON. Another API endpoint may be to retrieve all the rides. This one would be 1) login to Turo, 2) navigate to the first page, 3) fetch all the rides, 4) if there’s a next page, go to it and repeat step 3, otherwise 5) return the list of rides as JSON. For almost every request the first and last steps will be the same but the intermediate step will vary. This becomes even more interesting since we can now start to think about caching the results at the intermediate levels so you can avoid the steps if you’ve already done them in the past. This way we’re incrementally building a “shadow” version of the site and use that for everything we need but keep augmenting it when needed.  Pushing this further we can imagine a scraping specific language that represents the steps involved during a scraping session. The goal here is to replace the code that does the DOM traversal and instead come up with a cleaner and more expressive way that can be applied through code. Sometimes the application will be going to our cache but other times it will require actually navigating to the appropriate page.  I’m excited to try this approach out since it turns a rote scraping exercise into a higher order solution that can scale to other scraping jobs. I only wish I thought of it sooner since by the time I went down this rabbit hole I was mostly done with the actual code so I’ll have to give this a shot on the next scraping job.",1,2,2016-08-24,6,"scraping, apis, domain specific language, dsl, dom manipulation, meta programming",468,Writing scrapers as APIs
14,0,I just updated my Yahoo fantasy football stats scraper for the 2014 season.,"#data,#code,#python","{% include setup %} This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and  updated it to work  for the 2014 season. The old version used the great  Scrapy  framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses  Selenium  to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.  Note that the code was designed around my league’s settings and that the column order in Yahoo will depend on the scoring categories of your league. If that's the case you need to make sure to update the code (primarily the xpath expressions) to map to the columns in your view. Definitely feel free to submit a pull request that makes the code a bit more flexible since my goal was to get something out quick in time for a draft later this week.  And if all you care about is the data, here’s the  projected 2014 data  as of August 25, 2014.",4,3,2014-08-26,3,"yahoo, fantasy football stats, scraper",215,Yahoo fantasy football stats - 2014 edition
22,0,Despite being extremely similar on a technical level - streaming a digital product - Netflix and Spotify have completely different profiles and performance.,#product,{% include setup %} The news that Adele was not going to put her new album on the streaming services got me thinking about the differences between the way music and video are consumed. Just last week Rdio announced that it’s selling its assets to Pandora which is a reminder of how hard it is to start a music company - music labels wield all the control and are able to dictate the terms they want. Even Spotify is not yet profitable despite having millions of subscribers.  On the flip side we have Netflix which on the surface provides a very similar product - streaming video rather than streaming audio. Yet they’re profitable and are quickly expanding internationally and even developing critically acclaimed shows such as Narcos and Master of None.  I find it fascinating that although the two industries are so similar on a technical level they’ve played out so differently. Part of it is that audio consumption is just drastically different than video. Most people will stream music throughout the entire day at work and not mind repeats of a favorite song while shows and movies are watched in shorter bursts and I like to think that most people want to avoid repeats. I’m not a huge music listener and the music I enjoy tends to be available on every service but I suspect most people who are passionate about music want access to a band’s entire catalog as well as having immediate access to new releases - this is something that Spotify needs to provide that Netflix doesn’t have to worry much about. Netflix can survive on the back catalog alone while Spotify needs to bend over backwards to make sure they have the most recent releases.  Netflix has moved into producing their own shows which is allowing them to get ahead of the back catalog problem and move into the HBO model while still having access to a slew of old shows and movies.These are divorced from their creators and can stand on their own while music has extremely strong ties to the artist. This makes it extremely difficult for Spotify to apply a Netflix model and start producing albums - the only way they’d be able to make it work is by becoming a music label. Netflix on the other hand can pay top directors and actors to develop a show that can succeed or fail - but in either case it’s only loosely coupled with the creators.  I can’t find the blog post now but I read something a few days ago about how hard it is to build a successful music startup. The root cause is that the music labels have so much control and power that they’re charging a license fee that prevents startups from having any money to spend on innovation or product. Instead they’re transferring money from venture capitalists into the hands of the labels. The labels are basically the rentiers of the music industry and prevent innovation by sucking up investment that can be used to launch new products. My gut is that this won’t last since there’s just too much happening in adjacent industries but I’m crossing my fingers.,0,1,2015-11-22,3,"netflix, spotify, streaming",528,Why are Netflix and Spotify so different?
10,0,What I've learned about writing so far from this blog,#meta,"I recently made an effort to improve my writing and this blog gives me a great way to practice. I force myself to write at least two posts a week, even if it’s just a paragraph. Writing hasn’t come easy to me and I spent more than 20 years returning the favor. In high school, I rarely edited and a quick spell and grammar check was good enough for me. In college, I avoided the writing-heavy classes and the ones I did take I just followed my high school approach. Something changed when I started working. Although initially driven by my desire to perform, I started seeing writing as a challenging, creative process. I remember spending 30 minutes on a paragraph-long email before being comfortable enough to send it out. Even after only a few weeks, I feel that my writing has gotten better - both in terms of speed and clarity. I still have trouble writing long posts since I tend to go on tangents and lose focus.      An issue I’m currently dealing with is deciding when something is “done.” I could always spend more time editing and rewriting but should I? How much editing is a good use of my time? Jack Kerouac wrote the first draft of On The Road in three weeks and the final draft in 20 days. On the other hand, T. S. Eliot wrote The Waste Land over a few years, with the drafts being almost twice as long. I fall somewhere in between. I realize that I learn better through struggle and forcing myself to edit and rewrite helps me in the long term. At the same time, I realize that I have a ton of other things to do and rewriting the same paragraph a dozen times is not the best use of my time. At the moment, I write and rewrite until I’m proud of what I have and hope that it will get easier in the future. As frustrating as it can get, it’s significantly easier than what people were doing only 20 years ago before computers. This thought helps me focus and slowly work my way up to the fabled 10,000 hours.",0,1,2012-04-04,1,writing,367,On Writing
22,0,There are a variety of software development methodologies and I’d like to throw another one into the fray - refactor driven development.,"#management,#code","{% include setup %} There are a variety of software development methodologies and I’d like to throw another one into the fray - refactor driven development. Rather than focusing on tests or models or functions the focus should be on expressive and maintainable code. Imagine spending 80% of your development time on refactoring old code and laying a solid foundation for all future work. Then the remaining 20% of the time can be spent on writing new features and functionality that drive the product forward. Once this work is done it may lead to more refactoring work to get the code back into a pristine state.  The intuition behind this is that a 10x developer is not just writing 10 times more code but is making decisions and designs that allow future changes to be done 10 times quicker. This only comes from building a system that can be easily extended and morph into something else. A few years ago I read [Where Good Ideas Come From](https://www.amazon.com/Where-Good-Ideas-Come-Innovation/dp/1594485380) which introduced me to the concept of “[the adjacent possible](http://www.practicallyefficient.com/2010/09/28/the-adjacent-possible.html)” which is this concept that we may not always see two steps ahead but once we take a step in a direction we’re able to see a whole new set of options. This translates beautifully into good code - we may not always see the benefits but once it’s written we suddenly see all this potential. Great code has a high degree of optionality which allows it to easily mutate to support a whole new world.  This emphasis on refactoring is risky since you may very well end up with something that’s too rigid and doesn’t provide any of the expected functionality. To make it work the team needs to have enough knowledge about the business to understand how the product will need to evolve as well as a strong understanding of design patterns and tradeoffs between various implementations. It’s not for the faint of heart and requires a team committed to improving the code quality and having the confidence and ability to hustle when an urgent business need arises.  The value of this approach is that business requirements and features can be done in hours or days instead of weeks. That’s incredibly powerful since so much of the time we are writing code with the goal to deliver something by a target date. Yet oftentimes we reach that date and discover that only a small bit of the whole is being used or even worse the code we wrote only handles a fraction of the desired use cases. Both of these indicate wasted development effort and while the agile process is meant to address this by having frequent iterations that are each meant to deliver value and raise potential problems earlier. At the same time the agile approach encourages us to think more tactically which prevents us from constantly thinking about the big picture and what can be done to increase our long term optionality.  Imagine being able to wait till the last minute before knocking a feature out. This gives you the luxury of waiting until you know something is a necessity rather than building something due to risk aversion. It’s definitely not easy and carries a world of risk but if you have a strong foundation and confidence that you can get it to work this ability is priceless.  This is of course a simplification of how development works and the reality is not as black and white. At the same time, I believe as an industry we do gloss over the business value a well maintained and clean code base provides. It’s difficult to prove and make the case that spending the majority of your time refactoring is actually going to be more valuable to the business  but in many cases this is true.",2,2,2017-03-19,4,"software engineering methodologies, software development, driven development, coding",645,Refactor driven development
26,0,We've all heard of the the 10x developers but it's not about the code. It's about the decisions made that provide leverage in the future.,#meta,{% include setup %} If you’re in the software engineering world you’ve probably heard of the 10x developer. They’re an order of magnitude more productive than everyone else and can make all sorts of problems go away. The 10x number is completely arbitrary but I’ve worked with numerous developers who were notably more productive than others. A big part of it is just being able to write more code - a combination of knowing the right tools for the job and moving quickly while avoiding mistakes. But a bigger part in the productivity comes from making the appropriate decisions that are able to stand the test of time. If your code needs rewriting every time a new feature comes out it’s going to be tough to be as productive as someone whose code can be easily expanded and maintained as the product evolves. Great developers make design decisions that are able to solve the immediate problem but also leave a clear path for the improvements that will inevitably come. If you know what’s coming in a couple of months or in a year it’s simple to account for it in the current design but the real skill comes in being able to think of the unanticipated cases and be able to support them with minimal effort. Beyond that some choices end up unlocking opportunities that would have been difficult to fathom in the first place. Imagine coming up with an elegant implementation that solves an urgent problem and a couple of months later you realize that with minimal tweaking that implementation can turn into something that is transformative to the product. It’s impossible to think through every decision since you’ll end up stuck in a world of “analysis paralysis” but great engineers either have a gut feel or enough experience to make these high leverage decisions more frequently than others.,0,1,2016-01-24,3,"software engineering, software development, productivity",310,The famed 10x developer
20,0,One metric I've started looking at to analyze Scrum efficiency is average number of sprints to complete a story.,#management,"{% include setup %}  Most startups employ some form of [Agile software development](https://en.wikipedia.org/wiki/Agile_software_development) and one of the most common approaches is [Scrum](https://en.wikipedia.org/wiki/Scrum_(software_development)) which breaks down units of work into time based periods. I don’t want to spend too much time describing either Agile or Scrum since I suspect most are familiar with the concepts but a big challenge is measuring productivity. There are tons of different ways of doing this but the vast majority are different ways of looking at the relationship between story points and sprints. Most of these are focused on some form of velocity - measuring the amount of story points per sprint but an interesting metric I’ve started tracking is average number of sprints per story point. On the surface this is similar to taking the inverse but it turns out it tells different story.  By taking a look at the average number of sprints it takes to do a story you can both get a sense of the team’s velocity but also highlight that some stories keep getting deprioritized. As an example imagine a team doing every story within the sprint it’s assigned - the team’s average sprints per story value is 1. If any stories end up rolling over into the next sprint then this value keeps growing past 1. A value of 2 would indicate that it takes two sprints to complete an average story - something that indicates that the team’s not able to do what they’re setting out to do each sprint.  This metric can also be weighted by our favorite value - story points. The idea here is to give more weight to larger stories and see what the resulting sprints per story value looks like. If it’s above the unweighted one it indicates that more complex stories are taking longer to do and there may be an issue in the point estimation. If it’s lower than the unweighted average it may indicate that the story points are inflated.  No single metric will tell you everything you need to do know and it’s dangerous to rely solely on numbers. At the same time, each new metric is a data point and can help paint a picture of what’s going on. Combined with a more qualitative approach they provide a better understanding than either one alone.",2,1,2017-12-01,5,"management, agile, scrum, sprint, efficiency",386,Measuring sprint efficiency
17,0,I got a Raspberry Pi and was able to turn it into a media center using Raspbmc,#code,"{% include setup %}            I’ve been interested in the Raspberry Pi ever since I first saw it mentioned in the tech news and finally got to play with it over the past few days when my brother (thanks  Simon !) lent me an extra one he had. I’ve been in need of a better media center setup ever since my DisplayPort cable stopped working so I decided to try out Raspbmc, a Raspberry Pi based media center.  I scavenged an SD card from my camera and a microUSB AC adapter from my old Droid phone which I somehow still had lying around. With those two, I was able to install Raspbmc but couldn’t get any farther without a wifi adapter. It took the wifi adapter a few days to get delivered but it worked right out of the box and I had a functional media center. Surprisingly, I didn’t need a keyboard at all and was able to run through the entire setup using SSH and a downloadable iPhone app that acts as a remote. The most time-consuming part was setting up a Samba shared folder under Mountain Lion and adding it to Raspbmc using the onscreen UI.            It works well. It solves my “must-have” problem of using my TV to play videos that are on my computer and also has a bunch of “nice-to-haves”. The two big ones are AirPlay support which allows streaming of audio and video from iOS devices and the ability to use my iPhone as a remote. Only thing left is getting an enclosure so it’s not just lying on the floor.  Here’s the setup:          Raspberry Pi  - $25 or $35 model        Wifi Adapter  - $10 on Newegg       microUSB AC Adapter - I found one but should be around $5       SD Card - I had one but can find one for around $6 on  Amazon        HDMI Cable - $2 and up on  Monoprice",5,1,2013-04-10,4,"raspberry pi, raspbmc, media center, airplay",371,Raspbmc
31,0,Snapchat announced earnings today and if you thought they were likely to have a large move in either direction stock options provided a neat way to bet on that outcome.,#finance,"{% include setup %} I rarely write about finance but a decade ago I did a stint in finance and picked up a few things. One of these was the idea of options which are an interesting and powerful way to participate in the market. There’s a ton of information online describing how they work but a simple explanation is that they give you the “option,” or the right, to buy or sell shares of the underlying stock at a particular price by a future expiration date. This particular price, referred to as the strike price, and the expiration date, are the significant drivers of the price of the option. But generally, buying options on unlikely scenarios (ie far away from the current value) in the very short term ends up being extremely cheap while buying very likely scenarios with a long horizon can get very expensive. I’m not the best equipped to get into the specifics of pricing options but it’s incredibly intricate and involves some deep math that one can get lost in and is worth exploring for the mathematically minded or curious.  That aside, I started thinking about options yesterday after discovering that Snapchat was going to announce earnings today. I don’t have any shares in Snapchat and wrote a [post in February](http://dangoldin.com/2017/02/26/my-snapchat-investment-strategy/) describing my Snapchat investment - summarized as it’s going to volatile since there’s so much uncertainty and I should just wait to see what happens since if it does start growing it will keep growing for years, very similar to what happened with Facebook.  But this is passive and there are some interesting things we can do with options. One of my theses around Snapchat was that this earnings call would lead to either a massive decrease or increase and it’s unlikely that Snapchat would be stuck in the middle. If they showed significant progress in Q1 they would discredit the idea that Facebook was beating them and if they failed it would indicate that they will, in fact, lose to Facebook. Options are a great way to implement this idea. The way one can play this is to buy options that are out of the money on both sides - meaning we buy a put option for significantly below the current price and a call option for significantly above the current price. If the stock stays within the range we lose our investment but if it swings too much in either direction and becomes “in the money” we end up profiting. Earlier today I took a look at the [Snapchat option chain](https://finance.yahoo.com/quote/SNAP/options?p=SNAP&date=1494547200) for options expiring in 2 days, on May 12. Snapchat closed at just under $23 today so if go a few dollars, I chose $4 arbitrarily, and go in either direction to find the appropriate options we get a $19 put and $27 call. While the markets were open each of these cost roughly 30 cents. Then if Snapchat ended up dropping below $19 or increased to be above $27 the options would have some value. And that value is simply the difference between the stock’s current price and the option’s strike price. If Snapchat increased to $29 then the $19 put options would be worthless while the $27 call options would be worth $2 each ($29 - $27) - especially since our options would expire in 2 days so there wouldn’t be a lot of room for movement. Of course we would still have to pay for the options themselves but in this case we would come out ahead - the two options would cost us around 30 cents each but the gross return would be $2 leading to a profit of $1.40. Of course just as easily the options could have expired worthless and we would be out the 60 cents for the two options.  Unfortunately, I submitted my trades too late in the day and the markets had unfortunately closed before my orders went through so I have to take solace in the idea alone. It was also an extremely risky trade and I wasn’t going to put too much into it - small enough to make me not feel too bad about losing the cost of the options if they ended up being worthless. And while the tone of this post is very bullish on options they are incredibly risky and I’d only recommend taking a stab if you know what you’re doing and are comfortable losing the entire investment.",2,1,2017-05-10,4,"snapchat, stock trading, options, investing",746,Using options to play Snapchat's quarterly results
24,0,Snapchat has the opportunity to be the next Facebook but there are a lot of warning signs. What should one's investment strategy be?,#finance,{% include setup %}            Facebook stock price since IPO     Snapchat is expected to IPO [March 2nd](http://fortune.com/2017/02/17/snapchat-ipo-what-time-when-stock/) and I’m torn as to whether to invest. I think it’s an innovative product that provides a compelling experience but there’s a series of red flags - from the weird ownership structure to the potential slowdown in user growth to the growing per user costs. At the same time it may be the next social network. Facebook IPOed at just over $100B and is now worth almost $400B. I hate to compare Snapchat to Facebook but the bull case is that it can be the next Facebook. There’s been a history of startups growing to surpass the prior generation of companies and outside of Snapchat there’s nothing in the social media space. Snapchat has its work cut out but they do offer a unique product with a strong user experience that’s constantly improving.  So what should one do in this case? On one hand the product seems great with a ton of potential but on the other there are quite a few questions. My gut is that Snapchat is unlikely to be stuck in the middle - it will either be a massive success or a flop. If this is the case it’s not necessary to get in at the very beginning and I can see how it goes for a few quarters before deciding to invest. I’m cautiously optimistic that they can figure out their costs but worry that their growth stalled due to Facebook’s push of stories in Instagram and WhatsApp. From the S1 it’s impossible to know if there was an impact but after a few quarters it should be clear what’s happening. If at that point it looks as if they’ve taken care of their growth and cost problems it will be an obvious investment but at the IPO I’ll likely invest a token amount just to have a stake.  It took Facebook over a year to get back to their IPO price but since then they’ve nearly quadrupled. There’s no need to jump into Snapchat at the first possible moment. Instead it’s better to wait a few quarters and see how it evolves before making a significant investment decision.,1,1,2017-02-26,3,"snapchat ipo, facebook ipo, social media",394,My Snapchat investment strategy
26,0,I've been blogging for a few years now and have enough blog posts to actually analyze the content and see if there are any paterns.,"#dataviz,#code","{% include setup %} I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple [Python script](https://github.com/dangoldin/blog-analytics/blob/master/analyze.py) that went through each post and dumped it into a CSV file with a series of columns that would be easy to [analyze via R](https://github.com/dangoldin/blog-analytics/blob/master/analyze.R). The columns ranged from numeric stats - such as how many words, tags, images, and links - to the actual text of the post itself. The goal was to put in a structured enough shape that the rest of the analysis could be handled in R. I started by collecting some summary statistics and looking at them over time but got carried away and ended up digging deeper into my evolution as a blogger.  Some high level stats to start it off:  - 412 total posts with 54 of them before 2013 - 725 total links - 537 total tags - 1,379 total keywords - 9,705 total words in the meta descriptions - 145,499 total words of content                              As mentioned I started actively blogging in 2013 so there's no surprise here.                                       Given that I've written the same number of posts in 2013, 2014, and 2015 it looks as if my posts have gotten shorter and shorter.                                       Similar to the point above - I'm sharing fewer and fewer links.                                       Yet I'm still tagging the posts at roughly the same rate. This makes sense since I'll do anywhere from 1 to 3 tags per post.                                       By month there's a bit more noise due to vacations but am keeping pace with 2 a week.                                       Nothing obvious here.                                       Just for fun but this is the total number of words by week. I also did this by day but it was even noisier.                                       Clearly I write more during the weekend. Note that I had to prepend a number to the day of week to get the sort working.                                       Similarly, the number of words is also higher on weekends.                                       Another way to look at it is to see the distribution by year. In 2013 I was actually pretty on-point with my Tuesday/Friday writing schedule but since then have regressed to mostly writing on the weekends.                                       The same information as above but switching the X and Y axes. I find this one not as easy to interpret as the previous one.                                       This examines the various companies I mentioned over time. Google's dominant and it looks as if I haven't written about microsoft since 2014. You can also see the rise of Uber and Snapchat.                                       Looks as if 2015 was the year of languages with Python and JavaScript dominating the others.                                       Word cloud of the various tags I used on my posts. Clearly I like engineering and startups.                                       Tag wordcloud for 2013. All about startups and design here.                                       Tag wordcloud for 2014. This gets deeper into technology with strong representation by AWS, devops, coding, as well as a variety of programming languages.                                       Tag wordcloud for 2015. Welcome to engineering management. In 2015 I developed into a manager and start writing about the various lessons I've learned on the journey.                                       Tag wordcloud for 2016. Nothing significant yet and looks like a pretty healthy mix of the prior years. We'll see how this looks after the year is over.",2,2,2016-06-12,2,"data visualization, blog statistics",964,Analyzing my blog
28,0,I've only been managing an engineering team for 6 months now but something that I've been trying to adopt is the idea of a full stack developer.,#management,"{% include setup %} I’m a pretty new engineering manager but a philosophy I’ve adopted is to try to have everyone on the team be as full stack as possible. Everyone has their strengths and weaknesses but being able to grasp the entire stack improves code quality and reduces disruption. And it goes beyond technology and into the business and user world too. Understanding how these various components fit together allow you to make smarter decisions and provide the tools to test and verify your code. The other big benefit is that you’re not waiting on anyone and avoid having your flow disrupted by others.  As an example, imagine having an ecommerce website when you get the idea that you want to start tracking the amount of time people are spending mousing over your product images. The goal is to see whether this behavior is correlated with sales which will give you more data to drive an upcoming redesign. Clearly there will be front-end JavaScript involved since that will be triggering the event but there’s also a lot going on behind the scenes. Depending on the number of events you expect to see you can have a wildly different implementation. How do you want to handle multiple mouseovers over the same image? What data do you want to capture? What kind of analysis will you want to run? How will this data be tied back to the sales data? Where will this data be stored? Will there need to be any additional processing to make the data usable? How will you test the data flow? What needs to happen for you to deploy it? How much additional load will this put on the production system?  These questions can all be answered by looping in various people but understanding the business case and the full tech stack makes you more independent and increases the likelihood that the first version will be the final version. In addition to having the necessary language skills, I’d love to see every web engineer know how to set up a VPS from scratch, be comfortable with the command line, have a basic understanding of SQL and databases, and understand the various components of the web and how they fit together.",0,1,2015-02-23,3,"full stack developer, software engineering, engineering management",376,In praise of the full stack developer
27,0,Excel on a Mac has a completely different set of shortcuts that it makes me wonder if it was intentionablly hobbled to keep people on Windows.,"#datascience,#product","{% include setup %} The longer I’ve been involved in tech the fewer Windows laptops I’ve been seeing. It seems that to even be considered a startup you need to be giving your employees MacBooks. My conversion came years ago when I made the move from Linux in order to be able to run Excel since neither OpenOffice nor Google Spreadsheet were cutting it. Unfortunately, even after years of effort, I still can’t get to the same level of productivity as I had when using Windows during my consulting days. It’s entirely due to the shortcuts. Some of the shortcuts just changed while others simply disappeared.  The difference is most likely due to a different keyboard layout on a Mac but the cynic in me can’t help but think that it’s also a way to keep the Excel power users on Windows. No one in the finance or consulting industry will switch to Macs until the actual workflow of using applications is the same between Mac and Windows. The more power user focused an app is the more difficult it is to convince its users to switch from one OS to another. People already get annoyed when a minor change is introduced by a new version; I can’t imagine the reaction a new workflow would produce.  This is why the web apps are so intriguing, they’re able to maintain their look, feel, and functionality no matter where they’re accessed giving users the ability to choose the hardware that fits them. I’m already replacing Word with Google Docs but it’s going to take quite a bit of effort to get spreadsheet apps to the same level. In the mean time, I’m trying to replace Excel with R. I’m not there yet but improving every day.",0,2,2013-11-09,5,"excel, office, mac, os x, windows",295,Is Excel on a Mac intentionally hobbled?
30,0,"In addition to undertanding a database, looking at the AWS account and understanding how the various components fit together is a great way to ramp up as an engineer.",#devops,"{% include setup %} Last month I  wrote  that one of the best ways to ramp us a new engineer is to start going through the database schema and understand how the various tables fit together and what the various values mean. That provides a great view around the engineering product - the various fields indicate the options and functionality available and the tables indicate how the components work together as well as what and how data is collected.  The flip side is that this doesn’t actually provide any view into the application architecture - what’s the hardware used? What are the applications and how do they fit together? How do the applications work? What’s the load on the various components and what’s done to address it?  If you’re on AWS or another cloud provider a neat way to answer these questions is to look at the relationship between the various components and the appropriate stats. For example you can start with Route 53 to see the subdomains used and what they’re mapped to. Some may be mapped to EC2 instances while others may be mapped to a ELB, S3 buckets, or Cloudfront. Each of these provides a view of how the application is used - if it’s on S3 then the application is going to be static HTML, CSS, and JavaScript but if it’s hitting a load balancer then you can expect the application to be under heavy load and be supported by multiple EC2 instances. Beyond this you can look at the amount of requests being made and the volume of data going in and out as well as whether there’s any pattern throughout a day or week. There’s a ton of monitoring tools in AWS and each provides an additional data point that provides insight into the application architecture. The various options and dashboards available highlight how important devops is for every engineer - and how valuable it is for every engineer to have at least read-only access to AWS. It’s tough to write good, scalable good unless you understand how it will be used and how it will fit in with the rest of the stack.",1,1,2015-08-23,4,"aws, devops, learning, application architecture",363,Learn the application architecture through AWS
11,0,The Instagram acquisition shows what the future of startups is.,#product,"The big news today was that Facebook acquired Instagram for $1B in cash and stock. I don’t want to debate whether that was a good price but I am amazed that Instagram was able to get to over 30 million users with 13 employees, of which 3 are engineers. I see a few factors combining to make this an ideal model for the future tech startup.          Open source tools and the cloud have made starting easier than ever and a few motivated, talented people can build a marketable product over a weekend.    Social networks simplify distribution and allow a good product to stand out and succeed without heavy marketing.    People are comfortable with technology and can start using a product without any dedicated support.       They are converging to provide a massive increase in leverage. A small team is able to quickly and cheaply build a product that can spread organically to millions of users. The enterprise space will also be impacted as people start expecting their personal tools in their corporate environments. It’s definitely an exciting time to be building a tech startup.",0,1,2012-04-10,3,"startups, instagram, facebook",191,"Future of Startups - Small Teams, Big Profit"
18,0,Recent trends have made consumers more comfortable with technology which makes it easier for startups to succeed.,#meta,"In a  previous post , I discussed the factors that allow small teams to create products that can be exposed to millions of users within a few months. In this post, I want to take a deeper look into why consumers are so much more comfortable with technology now compared to 20 years ago and try to see where this leads. Since customers are what cause our businesses to grow, we need to be cognizant of what drives their behavior in order to plan for the future. Wayne Gretzky’s father famously said “A good hockey player plays where the puck is. A great hockey player plays where the puck is going to be” and I’m hopeful that we’ll be able to see where the consumer puck is going to be.  To me, the major driver is  Moore’s Law . We’ve seen computation speeds double every 18 months for the past 50 years. This has obviously led to faster computers but has also led to exponentially reducing costs. This has been a huge economic driver and is allowing computers to be more accessible than ever. Our cellphones are more powerful than what was used to land on the moon. These increases in computation also led to the rise of the modern web. It went from being a military/academic project that dealt with text data to something that’s distributing pictures and videos to whoever is interested.  More importantly, improvements in computation led to improvements in usability. Even if we had modern browser standards like CSS3 and HTML5 in the 1990s our computers would be too weak to handle them. We would not have any of the modern innovations (AJAX, DOM manipulation) and our web pages would be static without any rich media content. If we never got past the command line, how many people would have computers in their home? How many smartphones would exist? I’d argue that the usability improvements are what led to the massive consumer adoption of tech products. Of course, computation, cost, and usability are all intertwined but computation and cost alone would not have led to the consumer adoption we’ve seen.  What does this mean for the future? I see usability becoming even more native with us not realizing that we’re even using a computer. We’re already seeing this emerging with Siri and Google Glasses. As long as our computation speeds continue to improve these technologies will become better and better and will recede more and more into the background. Of course, this is all dependent on Moore’s Law holding, with many saying the pace will decrease by 2020. I’m optimistic that we’ll come up with something but even if we don’t, as long as we computing costs keep on dropping, via  Koomey’s Law , we should still see the benefits as we move more and more computation to the ever cheaper cloud. It’s difficult to imagine what would happen if our computation speeds stop increasing the way they have been over the past 50 years.",3,1,2012-04-25,3,"startups, entrepreneurship, technology",525,Growth of Consumer Comfort with Technology
25,0,It's possible to generate data using either a generative or filtering approach. Both are useful and the skill is knowing when to use each.,#meta,"{% include setup %} While playing around with Scala I rediscovered streams - a list-like structure that's lazily evaluated - meaning that only when you access a particular value is it evaluated. This makes it possible to create infinite streams since all you need is a function that's able to compute the next value. In such a way we can create a stream of all numbers, just the positive even numbers, or just the prime numbers. Calculating each successive prime number will become more difficult but it is possible.  In the case of the positive even numbers it's possible to generate the stream in two simple ways - one is to take each positive integer and double it while the other is to take every positive integer and filter them down to those that are divisible by two. Both of these will generate the exact same numbers in the same order but do it in opposite ways. One generates the numbers from a base list and the other filters a larger list down.  In this example the efficiency of the two approaches is similar: the first goes through each element once and does a bit shift while the second goes through two elements and does a bit comparison. But on real code the differences between the two approaches can be significant. It's also likely that one of the approaches may not even be possible or be too arduous - imagine generating a list of prime numbers.  Both are useful depending on the problem and the skill is figuring out when to use each. The generative approach feels as if it should be the more efficient one but there are many cases where filtering is easier and quicker.",0,1,2015-08-15,5,"scala, streams, lazy evaluation, generating, filtering",285,Generating vs filtering
30,0,Retargeters work by showing you products you've already seen with the hope that you buy it. But why should I see an ad for a product I've already bought?,#meta,"{% include setup %} Retargeting ads work by checking to see a product you’ve looked at and then showing you that product over and over again with the hope that at some point you buy it. There are entire companies dedicated to this with extremely sophisticated algorithms so I’m surprised when I see inefficient behavior. In my case it was an Amazon ad that kept following me around even after I already purchased the product, a precision cooker. Given that Amazon knows my purchase history and sees that I’ve already bought the cooker it makes no sense to keep showing it to me. It seems that their algorithm figured this out as well and started showing me the same product in different packages and at different price points. The fact that they have logic that’s smart enough to show me different variations of the same product but not take into account my purchase history shocks me. What makes this even worse is that I own some Amazon stock and realize that this inefficiency has an impact, albeit a tiny one, on my shares.",0,1,2016-02-10,4,"retargeting, advertising, adtech, amazon",184,Retargeting gone wrong
27,0,I need to write 62 posts by the end of the year and am significantly behind. I'm going to try my hardest to do it though.,#meta,"{% include setup %} Since 2013 I’ve been writing 2 blog posts a week. This stemmed from a conscious effort to improve my writing, clarify my thought process, and grow my brand. It’s been quite a ride and while difficult I’ve been able to do keep it going for 5 consecutive years. This year I’m significantly behind but am still committed to catching up and hitting 104 posts. As I write this I’m only at 42 posts for 2018; I have 42 days to write 62 posts which means I need to write nearly 1.5 posts a day to make up for my prior sloth. I’m going to do as much as I can to hit that goal and will be extremely disappointed if I’m unable to do it. That may mean that some of the posts will be on the shorter side and half baked but there will be some that do offer something valuable and insightful. My blogging history tells me that it’s impossible (for me at least) to actually tell which posts will be popular and the best I can do is keep writing and share more of my thoughts. The one element I will not compromise on is editing - poor writing is a pet peeve and I’ll continue to edit my posts after they’re written.",0,1,2018-11-18,3,"blogging, goals, commitments",234,Falling behind my 2018 blogging goal
38,0,Code deployment is something all tech companies need to do and there's been a lot of progress over the past decade in managing it. I was thinking of how my process has evolved and what I've learned.,#devops,"{% include setup %} I’ve been working on various tech related projects for over a decade now and have gone through a variety of approaches to deploying code. I’m far from an expert but though it would be helpful to jot down what I’ve seen and where I'm hoping to get.  - FTP upload, no version control: I developed my first few sites locally and then just copied them over to the host server via FTP. This worked well for simple projects where I was the only contributor.  - Version control, single branch: Once I discovered version control I immediately found it helpful. Version control made it easy to work with others but our deployment was still manual. When we were ready to deploy we would log in to our server, run the necessary commands to update the database schema, and then do pull/update to get a new version of our code base.  - Version control, single branch, more automated deployment: Logging in every time to do a deployment was a pain so we started using  Fabric  to automate deployments. Fabric allowed us to execute scripts on multiple machines without having to manually log in to each one. Since each box had a set of roles we were able to set up Fabric to deploy by role (ie deploy this change to the DB server, deploy this change to all webservers).  - Version control, multiple branches, more automated deployment: Another improvement was following git best practices and setting up a production branch with everyone working on development branches that would then be merged into master. When the deployment was ready to go out it would be merged into production. The value here was that when we ran into a bug on production, we were able to fix it without having to merge in a bunch of new features.  - Version control, multiple branches, automated testing, automated deployment: This is the ideal state. Each of our repositories is tested enough that code changes are automatically tested, merged, and deployed to production. The process should also be smart enough to handle db migrations and would be to revert changes if any problems arise. In addition, each box may have a different set of required systems libraries and packages and an automated deployment should be able to automatically configure a server with the necessary packages. I know  Chef  and  Puppet  are used for this but I’m only exploring them now.  Something to add is that there’s a huge incentive to make your stack as stateless as possible - for example having multiple web servers behind a load balancer that don’t need to share any state with other webservers directly. This makes it simple to spin up new servers when there’s more demand and improves scalability. Unfortunately, it’s not always possible and complicated deployments end up having coupling - especially when high performance is required. In that case adopting a declarative approach when configuring your instances helps bring some sort of statelessness - for example using AWS tags to declare an instance to be of a particular type and using the region information to dictate what other instances it needs to connect to. Otherwise you’re stuck trying to define a complicated topology via config files. I’d love to know how massive companies manage their deployments - I know Facebook has a  custom process  that will deploy new code to a set of boxes and then use BitTorrent to share it to others but I’d love to be able to compare that with those of others, for example Google and Amazon.",4,1,2014-08-09,4,"aws, code deployment, release management, devops",611,Evolution of code deployment
30,0,There was no easy way to see the parking zones of Jersey City on a map so I decided to use the PDF they provided to generate a map.,"#code,#dataviz,#python,#javascript","{% include setup %} A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there's a  PDF  that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.  The first step was extracting the address ranges from the PDF into something more digestible by a program. I tried using a PDF converter but that ended up mashing up the addresses together so I took a step back and came up with a very simple script that took copy and pasted text from the PDF and cleaned it up into a list of addresses.  To do a quick proof of concept I started with a single zone for now and see whether I could get it visualized the way I wanted to using Google Maps. After converting the address range into a starting and end address I attempted to use the Google Maps API to do the geocoding (going from an address to a latitude/longitude). Unfortunately, due to the volume of addresses I wanted geocoded I quickly hit the rate limit cap. I introduced a throttle between calls but that ended up causing the page to take too long to load.  Even then, the geocoding wasn't 100% accurate and I needed to figure out how to visualize the resulting zones from a set of coordinates. The first visualization atttempt was to just connect the coordinates with a series of lines but as expected that led to just a jumbling of lines. As a quick fix I sorted the coordinates clockwise by figuring out the center of the coordinates, converting each coordinate as an angle from the center, and then sorting the resulting points by angle. This led to a ""starburst"" shape that was neater but still didn't represent the actual zone.  It's not done just yet and I'm working on two improvements - one is moving the actual geocoding work to an offline script so I don't have to deal with the rate limiting issue and two is using a convex hull algorithm to come up with a polygon that encapsulates each of the addresses in a zone that should improve the visualization. Feel free to follow along on  GitHub  and offer any feedback, suggestions, or even a pull request.  Writing good code on the first try is tough and part of the process is attempting an approach that may require backtracking. The challenge is realizing when something isn't working and being able to take a step back and revisit the actual goals and understand the constraints. Some projects do end up perfect on the first try but the vast majority require multiple iterations to get right. Experience helps us understand the constraints and tools we're working with but as the popular saying goes: ""Wisdom comes from experience. Experience comes from bad judgement.""",2,4,2015-09-12,3,"jersey city, parking zones, 07302",548,Mapping the Jersey City parking zones
29,0,The results of analyzing the IMDB data in order to compare actors and actresses. The metrics include looking at age and height and how they change over time.,"#data,#code,#dataviz","{% include setup %} After getting the [IMDB data loaded](http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/) it was time to dive in and start looking at the data. In 2012, I did an [analysis](http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/) to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the [help of CuriousGnu](https://www.curiousgnu.com/imdb-age-distribution), I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on [GitHub](https://github.com/dangoldin/imdb) and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.                              A replication of  CuriousGnu's chart  as a sanity check to make sure the data was loaded correctly. As one can guess, actresses skew younger compared to actors with an average of 34.6 compared with 41 for actors.                                       The previous chart examined the distribution across the entire date range but we can see how this shift occurred over time. Before the 1940's actors and actresses were much closer in age. Another interesting point is that both actors and actresses have been getting older on average. One theory is that this is a function of the movie industry being new at the beginning of the 20th century with very few actors and actresses at the start that have aged along with the industry. Another reason may be lack of accurate data prior to the 1940s in the IMDB dataset which skews the results toward more recently-born actors and actresses.                                       Similar to the above but focused on actors and actresses that have appeared in at least 100 movies. The goal here was eliminate some of the noise and focus on the high volume actors and actress. This tells a similar story to the previous chart.                                       Combining both the distribution and trend over time we can look at the distribution changes over time. This also highlights the bias in the early years - in the 1920s it looks as if no one was older than 30 according to the IMDB data. After some digging around it's due to the lack of birth dates for many of the early 20th century actors and actresses. For example, for movies produced in 1920 we have close to 19,770 actor/actress movie combinations but only 1,060 (~5%) with a birth date. For 2010 the respective numbers are 269,645 and 52,262 (~19%). This causes our distribution to look heavily truncated but ends up correcting itself once we get into the 30s and 40s. In this case the average ages are inaccurate until the 1940s but I suspect the relationship between the genders still holds.                                                        This is just a timelapse of the data above that makes it much easier to see the shift of the average actor getting older at a faster pace than the average actress.                                       In addition to birth date the data also contained the height so I decided to have some fun and see how that looked. This is just a plot of actor and actress height by year of production. My takeaway is that actor heights stayed roughly flat while actress heights have been increasing. Note that since I only had a single height for each person this wouldn't be able to accurately represent children growing up but I imagine those are a small fraction and wouldn't influence the results.                                       This is an interesting one. Instead of looking at the heights by movie production year this examines heights by birth date of the actor and actresses. In this case we see that actors have stayed roughly the same height while actresses have increased in height over time. There's also a huge looking drop at the end - going from a bit over 70 inches to less than 65 for actors and from 65 inches to less than 63 for actresses. This drop off is in the late 90s which also indicates these are teenagers just growing up.                                       For the last one I wanted to get a sense of whether actors are more likely to be in more movies than actresses. The chart here is a bit tough to read but it looks at the distribution of actors and actresses by the number of movies made. in this case the scale was massive since there were tons of people who've only been in a few movies so I had to normalize by taking the log. The effect is subtle but the fact that the tail for actors goes wider than the tail for actresses indicates that an average actor is more likely to appear in multiple movies than the average actress.",5,3,2016-05-22,5,"data analysis, data visualization, actors, actresss, imdb",1047,Analyzing IMDB data: Actors vs actresses
40,0,There's a new design pattern I've seen in B2B startups that allow you to sign up with a company email address and do all the setup work in the background. This is great and everyone should be doing it.,"#product,#design","{% include setup %} One of the latest trends I’ve noticed is B2B companies is allowing you to sign up with a company email address and automatically linking you with the rest of the organization. This is a definite no-brainer and a really simple way of getting new users setup without having to be bottlenecked by a burdensome administrative process. No one on the HR team has to enter employees into the system nor send anyone their username or account info. Instead they just provide a link to the service and have people sign up with their company email address. Once this is done they immediately have access to whatever the base employee account should have. Only later one does an admin need to grant additional permissions and privileges.  The companies off the top of my mind that have done this are [Slack](https://slack.com/), [Greenhouse](https://www.greenhouse.io/), and [Tallie](https://tallie.com/) but there are countless others. If you’re building a B2B product that’s designed around teams working together this should be at the top of the product queue. It’s a great way to get on the good side of the HR team while getting your users onboarded quicker.",3,2,2016-04-13,7,"b2b, company signups, corporate email address, company email address, slack, greenhouse, tallie",199,The corporate email signup design pattern
25,0,I've been noticing various ecommerce sites offering discounts via popup just to enter an email. This is a compilation of the ones I've seen.,#product,{% include setup %}   	  		  A couple of months ago I started noticing popups on various ecommerce sites offering a first purchase discounts in exchange for entering an email address. Every time I noticed this happening I took a screenshot to track the offer and compile a list of the retailers using this approach. I’m still collecting examples and would more of them but so far the going rate seems to be anywhere from 10 to 25% off the first order. The pitch is pretty compelling and I think most people would gladly give up their email for the possibility of a discount. I’d also love to know what impact the magnitude of the discount has on the sign up rates; I suspect it’s minimal but definitely better than gaining entry to a sweepstakes or a lottery. 		  	  	      Company   Offer      Ann Taylor  Lottery    Blue Nile  Lottery    Bonobos  20%    Gap  25%    Wayfair  10%    West Elm  10%    Williams-Sonoma  10%     	             	                  Ann Taylor offers the chance to win a vacation                  	                  Blue Nile offers the chance to win a diamond                  	                  Bonobos offers 20% off your first purchase                  	                  Gap offers 25% off your first purchase                  	                  Wayfair offers 10% off your first purchase                  	                  West Elm offers 10% off your first purchase                  	                  Williams-Sonoma offers 10% off your first purchase,0,1,2013-09-30,5,"ecommerce, marketing, discount, rebates, coupons",344,Going rate for an email address
28,0,Good code is just like a physical system with a high potential energy - it can quickly turn into kinetic energy to solve a variety of problems quickly.,"#management,#code","{% include setup %}        Potential energy : the energy of a body or a system with respect to the position of the body or the arrangement of the particles of the system.     Dictionary.com              Kinetic energy : the energy of a body or a system with respect to the motion of the body or of the particles in the system.     Dictionary.com       I’m constantly striving to discover new ways of thinking about code and my latest is thinking about it through what many of us learned in high school physics - potential and kinetic energy. The definitions are above but a simple way to think about it that potential energy is what your system is capable of while kinetic is exercising that option. One can look at code the same way. Code that has a high potential energy can be turned into a vast amount of kinetic energy that can deliver new products and features at an amazing pace. This is code that is well architected and tested and is designed in such a way that it can be easily modified to handle whatever it comes its way. Code with low potential energy, on the other hand, is poorly designed with small changes leading to unintended side effects such that most of the time is spent fixing the code up. The comparison here between a rocket and an old, rickety car is appropriate. The rocket expends the bulk of its energy in minutes and travels hundreds of miles. The car breaks down every couple of miles and requires a skilled mechanic just to keep it going for another few miles.  But in physics we have the law of the [Conservation of Energy](https://en.wikipedia.org/wiki/Conservation_of_energy) stating that energy can’t be created nor destroyed. This also applies to code! Taking code with a high potential energy and quickly modifying to solve a need may reduce its potential energy. In this case it’s up to us as developers to exert effort to bring it back up its high potential energy state.  This metaphor is an obvious exaggeration but it does strike at what makes for good code and something we should all strive to write. It’s not about being brilliant or elegant or simple but about being flexible enough to support whatever the world throws at it and it’s up to us to keep it at that level. To keep pushing the physics - every new feature adds [entropy](https://en.wikipedia.org/wiki/Entropy_(order_and_disorder)) to our codebase and unless we actively clean it up it only gets worse.",4,2,2016-07-04,4,"coding, software engineering, design, architecture",437,Maximize the potential energy of your code
16,0,Video games should be treated as art the same way other forms of entertainment are.,#meta,It's about time video games are considered an art. They are creative endeavors that take as much and as long to make as some movies. There is also a huge distinction between the great and the poor games. Games these days can be considered movies and they should be treated in the same way.  Maybe then will video games no longer be the black sheep of the media and entertainment business. It's ridiculous that some guy throwing feces at a canvas is considered higher than the developers of video games.  Video games are no longer the simple things of 20 years past but have evolved into their own worlds and stories and should be given the credit they deserve.  It's a shame people are blaming video games for the violence instead of their own parenting ability. And if they actually cared about how their kids were being raised maybe they should stop the filth on TV from being shown and replace the TV babysitter with themselves.  No one can argue that TV has been getting more and more sexual and violent yet the finger is always pointed at video games - it's time this stops.,0,1,2008-04-29,3,"video games, entertainment, art",188,Video games as art
31,0,Blue Apron's IPO highlights the difficulty of meal-kit delivery and I'm a big fan of them trying to get as much reach as possible by parnering with complementary companies.,#product,"{% include setup %} Earlier today I read a neat [article](https://www.theinformation.com/blue-apron-competitors-explore-sales-to-grocers-food-manufacturers) covering the meal-kit delivery space and how give Blue Apron’s lackluster performance on the public market the still-private competitors are exploring alternative options, including partnering with grocery stores and general CPG companies. I hadn’t thought much about the space but it’s a great idea.  Fresh food is a difficult business that benefits heavily from scale. Buying in bulk gives you significant price discounts and being able to have high throughput reduces the amount of food that spoils. One of the simplest ways to increase your scale is to increase the amount of distribution channels you have. These meal-kit delivery companies started by doing delivery to the home but there’s nothing stopping them from offering the same meal kits at grocery stores. In fact, maybe it makes sense to not even have an exclusive partnership with any single chain but try to get them into as many stores as possible. They’d have to do an accurate job modeling the demand but they’d likely be able to drop the price enough to make it attractive to customers. The biggest price reduction would come from not having to ship individual orders but instead deliver them in bulk to a single store. A more interesting version of this would be to actually allow the grocery store to contribute some of the ingredients - benefiting both.  This train of thought reminds me of Ben Thompson’s [analysis](https://stratechery.com/2017/amazons-new-customer/) of why Amazon acquired Whole Foods: Amazon was buying itself a customer that could give it the scale it needed to run the Amazon playbook. It’s getting tougher and tougher to compete against Amazon and partnering up with complementary companies is a strong move by smaller competitors.",2,1,2017-09-06,4,"meal-kit delivery, blue apron, amazon, whole foods",305,Scaling meal kit distribution
15,0,Some thoughts on the changes in privacy given the rise of the social networks.,#meta,"With so many people joining social networks like Facebook, MySpace, and LinkedIn, it's becoming harder and harder to protect your personal information. If one of your friends happens to add a host of different facebook apps, those apps will have access to his friends' (your) information. There is nothing you can do to stop this unless you either remove all your friends or create very limited profiles.    In addition, people have come to expect to be able to add you as a friend after they've met you and rejecting them may be construed as anti-social. Imagine a recruiter not being able to look at your information on LinkedIn or a potential date not being able to look at your interests or photos on facebook - you will be missing out on opportunities.    How is one supposed to play this game where you want your information both hidden and shared? My solution is to embrace this lack of privacy: integrate yourself into as many social networks as you can, start a blog, post on various forums, publish your photos on Flickr, and so forth. By being famous (if only on the internet) you will eliminate a lot of the adverse effects of having your information public. You will have enough of a community to support you in case anything goes wrong and you can stop worrying about your information being shared.    How often does Bill Gates worry about his identity being stolen?",0,1,2008-06-12,5,"privacy, social networks, facebook, myspace, linkedin",245,Privacy in the digital age
19,0,Fab and Groupon favor the consumer over the business and focus on short term vs long term growth.,#product,"{% include setup %} [Groupon](http://www.groupon.com/) has fascinated me since they’ve launched. It popularized an entirely new business model, encouraged the launch of hundreds of competitors, and was able to IPO three years after being founded. This sounds great until you look at the performance after the IPO: the stock is down 80% and it’s consistently missing the quarterly goals.  The daily deals space isn’t as profitable as it used to be and they’re trying to become a tool platform for small businesses. To grow beyond daily deals, they’ve been on an acquisition spree. Over the past two years they’ve acquired a [scheduling startup](http://techcrunch.com/2011/12/07/groupon-debuts-scheduler-to-streamline-online-bookings-for-merchants-consumers/), a [social shopping startup](http://techcrunch.com/2012/01/20/groupon-buys-social-shopping-platform-mertado-to-bolster-groupon-goods), [POS system](https://upserve.com/platform/restaurant-pos/), and a [restaurant reservation system](http://savored.com/). I don’t think this will be enough for them to get seen as something bigger than daily deals.  Groupon grew by providing steep discounts to consumers but sacrificed businesses in the process. It will be a hard sell trying to get a business to use your tools when a few months ago you were telling them to discount their products more than 50%. I understand that they needed to do this to grow and I’m sure they even had a choice: the space was so competitive that if they didn’t do this someone else surely would have. It just puts them in a pretty tough spot.  Recent fast growing ecommerce businesses have also favored the consumer over the business. This leads to quick initial growth but causes problems in the long term. [Fab](http://fab.com/) is taking this approach as well by providing steep discounts on designer products. Consumers are loving it but what happens when there aren’t any businesses left who are willing to agree to such a discount? Sure, using Groupon and Fab can be viewed as a marketing expense but I suspect they and their investors want to be seen as more.  It’s difficult to balance the needs of the various sides of a marketplace. You do want to [subsidize one side](https://hbr.org/2006/10/strategies-for-two-sided-markets) but it’s dangerous to favor the consumer side so much since it’s difficult to distance yourself from. Maybe it is the proper approach in the beginning but there needs to be a way to get out and I think that’s a tough problem. [Etsy](http://www.etsy.com/) and [AirBnB](https://www.airbnb.com) had slower growth but were able to align the incentives of the various sides of the market early on. It was easier for them since there’s overlap between the two sides (sellers are buyers and buyers are sellers) but I suspect this is still the right approach for long term success.",9,1,2013-01-15,1,Fab Groupon ecommerce,460,What do Fab and Groupon have in common?
15,0,Did the Mailbox app need to use a server or was it about marketing?,#product,"{% include setup %}           After a couple of weeks on the waiting list I finally got access to the Mailbox App. It’s a huge improvement over the standard mail app and my mobile email consumption habits have improved significantly. I’m still not at “inbox zero” but am making my way there.  I don’t know much about iOS development but one thing I’ve been wondering about is whether they could have written it to not use a remote server. Regarding scaling Mailbox, they  wrote :    A critical part of Mailbox scaling is its brand new infrastructure. Mailbox relies on servers in the cloud to do things like send push notifications, download email as fast as possible, and handle “snoozed” messages.    From reading developer docs, it does seem you need a server to do push notifications but I wonder if there’s a way to schedule notifications on the client side. Conceptually, there’s nothing the server needs to do that can’t be done client side via simple polling. This way, the server load becomes non existent and scaling issues are avoided. Even a hybrid approach could have worked: use the server approach when possible but fall back to polling if the servers are overwhelmed. I just can’t help but think that there must have been a way to have the same functionality client-side. The cynic in me wants to say that it was done to build up hype but it’s equally likely that I just don’t know iOS development. It seems odd that crippling an app would help with marketing. I hope that’s not what it takes to sell for $100M.",1,1,2013-03-29,2,"Mailbox, marketing",290,Mailbox: Is the server required?
27,0,People are excited to see the impact Oculus Rift will have on gaming but I'm more excited to see the impact it will have on productivity tools,#meta,"{% include setup %} The Oculus acquisition got me thinking about the impact it would have on software development. We currently have a slew of editors and IDEs that are making us more productive and I wonder whether there's a place for VR. I don't think it's going to be as extreme as  Minority Report  (at least at first) but I do expect some things to get much easier.  Typing is currently much quicker than any other form of data entry and I don't imagine VR making this any better. While writing this I took a break and tried looking at the letters making up this sentence on my keyboard and it was slower - not to mention the mistakes that will likely occur during transcription. The only thing that would make data entry faster would be a direct neural connection which isn't going to be happening any time soon.  Navigation and context switching might become easier. I currently spend a fair amount of time tabbing through windows until I find the right one - a visual approach might make this process much better as long as it's implemented well. I'm also significantly more productive with an additional monitor - if VR is able to increase my working area I suspect I'd be more productive.  Debugging should get better. Being able to quickly examine various states during the course of debugging is extremely useful and I haven't seen a tool that makes this simple. An interesting, scifi-like solution would be to somehow provide a three-dimensional view of code execution and be able to view your code from an additional dimension. Being able to quickly go back and forth through time would make tracing code significantly easier.  Everyone's expecting VR to have a huge impact on gaming but I'm more interested in seeing the unforeseen use cases emerge. These will have an impact not only on entertainment and consumption but also on creativity and productivity.",1,1,2014-05-15,3,"oculus right, productivity, software engineering",328,Coding in a VR future
21,0,The best productivity apps all offer keyboard shortcuts. Unfortunately it seems to have gotten worse as apps have shifted online.,#product,"{% include setup %} I’m a big believer of offering keyboard-only experiences to your power users. In fact, I don’t think it’s possible to build a loved productivity application without keyboard shortcuts. Productivity applications are all about productivity and keyboard shortcuts are what give your users power and speed. The mouse and menus are easier for newcomers but those that stick with the application inevitably need the speed that comes from shortcuts.  The ultimate example of this is Excel. If you watch any power user of Excel they rarely, if ever, use the mouse. Instead, they’re able to do everything they need using the keyboard and significantly quicker than if they were force the mouse. There’s some functionality that’s not even possible to do with a mouse alone - for example array formulas.  If you think about our favorite tools are almost all keyboard-shortcut based. These range from the modern IDE to the text editors of old. You see a screen with a cursor but the real power comes from the gamut of options we have from a few keyboard clicks. They offer a massive extension to our productivity and something that all productivity focused apps should embrace.  It feels as if the move to the web has made applications worse. They’re generally less snappy and generally make me feel less productive. Keyboard shortcuts are a big part of this and I’m hoping that we realize that moving to the web shouldn’t stop us from doing all the behind the scenes work to make us more productive.",0,1,2018-02-10,3,"keyboard shortcuts, productivity, web applications",259,Optimize for keyboard shortcuts
8,0,Why don't printers come with printer cables?,#meta,"Since when did printers stop coming with the cables? Does this have anything to do with printers being available in USB form and the manufacturers suddenly assuming that everyone already has USB cables? Or is it some agreement that they have with merchants that requires me to pay $20 for a 6' cable. Some quick price look ups do show that the standard printer cables cost around the same as USB cables so if they were able to afford to bundle printer cables before USB, they should be able to bundle USB cables now.  Does anyone else find this ridiculous?  Edit: My father tells me that they never came with cables and that I am misinformed. He's probably right since I wouldn't trust my childish memory with regard to such things. In any case, printers should come with cables.",0,1,2008-05-13,2,"printers, hardware",140,Printers no longer come with cables?
42,0,"Lately I've been thinking about the balance between expressive scripting languages that tend to be weakly and dynamically typed but have worse tool support. Strongly and statically typed languages, on the other hand, are less expressive but come with better tooling.",#meta,"{% include setup %} As developers we want to be as productive as possible. This encourages us to improve our tools and languages to accommodate new patterns and challenges. Many of these improvements have come due to better hardware since we're better able to cope with slight inefficiencies at the expense of higher productivity. This, coupled with the constant advancement of compilers and interpreters, has led to a massive adoption of scripting languages.  Lately I've been thinking about the trade-offs we accept when we use scripting languages. They're easier to dive into and make it easy to build a prototype. This is especially easier with the growth of open source tools and frameworks.  Unfortunately, these prototypes are difficult to scale as as they and the team working on them get larger in scope. The lack of strong and static types makes it more difficult to undertake large scale refactors and encourages type-related bugs.  Proper tools are a huge boon to productivity but their power depends on the type of language. The more rigid and standardized the language or framework the easier it is to build a higher level tool. The more flexible a language the less a tool can do. This explains why strong, statically typed languages, such as Java, have amazing tools that can automate large scale refactors, identify obvious type specific bugs, and do more advanced static code analysis to identify potential issues. It's possible to get these benefits for weak and dynamically typed languages, such as PHP, but those will only work if your code is using a particular framework or style.  The fact that we're still debating the benefits of one language over another and everyone having their favor indicates that there's no ""best"" language. The best language will depend on the problem and the constraints and all we can do is figure out what to use for a particular task. My approach is to do quick prototypes in Python but over time architect in such a way that the more performant and complex components can be easily replaced with Java, a strong and statically typed language.",0,1,2016-12-17,7,"programming languages, developer tools, software engineering, strong, weak, dynamically typed, statically typed",354,Programming languages and developer tools
36,0,Sometimes it's tough deciding whether you should use a heuristic or algorithmic approach. I tend to favor heuristic ones for quick and dirty projects but will opt for an algorithmic one for more complicated work.,#datascience,"{% include setup %} Something that’s come up frequently in my quantitative work is balancing heuristic and algorithmic approaches. It’s surprisingly difficult to get the first attempt at an algorithmic approach working properly - it’s not an academic exercise and real world issues will always appear. Over time I’ve found myself writing heuristic checks and tweaks to deal with the various edge cases the algorithmic approach encounters. For example, setting the min and max bounds on the results of a function or adjusting the slope of a curve if it ends up being set in the wrong direction.  It makes me wonder why I didn’t just start with a heuristic approach and worked on an algorithmic approach later after I’ve collected enough data and had a better understanding of the environment. The challenge is that a heuristic approach is only a temporary solution. It will be be difficult to maintain and improving it will require additional hacks and tweaks. A heuristic approach is great at setting a quick baseline but long term improvement will only come from a more rigorous approach.  An example would be writing an algorithm that bids on Google Adwords. A heuristic approach would take yesterday’s bids on a set of keywords, look at their performance, and adjust them or down based on a few simple rules. A simple heuristic might be to allocate budgets to different keywords based on their conversion rates. Unfortunately, this wouldn’t handle the case of different keywords having different costs and volume. Incorporating these would require additional rules and introduce more complexity.  An algorithmic approach would be to model the relationship between cost, impressions, and click through rates for each keyword and then optimize for total conversions. Each model would be designed to predict a dependent variable based on a set of independent variables and would require a statistical approach to make sure the results were statistically significant and safe to use. Each model would require its own research and set of tests but would lead to a more scalable system. Since the models would be independent of one another, you’d be able to improve them individually. It may turn out that our cost calculation model is great but the one that estimates impressions needs more improvement. Now you can focus on the model that needs the most work rather than trying to globally optimize the whole system.  As with anything, these are tools and their usage depends on the situation. It’s difficult to come up with a rule of when to use one over the other but I tend to favor heuristics when I need to do something quick and know it’s not going to require significant changes. If it’s a complicated problem that will require ongoing work, I’ll opt for the more rigorous, algorithmic approach. It will take more work initially but will be better in the long term.",0,1,2014-02-15,4,"data science, algorithms, heuristic, algorithms",479,Heuristic vs algorithmic approaches
26,0,Offering discounts is dangerous since consumers may start associating your brand with discounts. It can also lead to an entire industry being driven by discounts.,#pricing,"{% include setup %} Nearly every week I receive an $8 off $25 coupon from delivery.com. I’m sure the intent is to generate awareness and develop a habit but it’s having the opposite effect on me: I’m being trained to only order when I have a coupon. Couponing is tough - too little and it will have no impact but too much and you run the risk of training your customers to only react to deals which will result in you needing to have higher prices to maintain your margin.  I recall reading that the retail clothing industry is essentially sales and coupons. Since people are trained to only buy on sales, retailers will set an initial high price and use discounts to drop it to something that will appeal to consumers. No wonder clothing retailers have sales practically every week and definitely every holiday. Ron Johnson tried changing this when he became the CEO of JCPenney but wasn’t able to do it before his ouster. I suspect even if he had more time he wouldn’t have been able to do it without the support of other retailers. Even then, each would have a strong incentive to deviate, a la  Prisoner’s Dilemma , so they might just end up exactly where they started.  This begs the question of why other industries haven’t embraced the discounting model. My guess is that it would take a significant amount of effort to change consumer perception that a single company wouldn’t be able to apply and it’s just too complicated to orchestrate - especially when the payoffs are uncertain since competitors can quickly move to this model as well.",1,1,2014-03-02,4,"discounts, marketing, sales, retail",277,Trained for discounts
21,0,I played around with Apple's recently announced ARKit and it's amazing. Can't wait to mess around with it some more.,#product,"{% include setup %} During this year’s WWDC Apple announced [ARKit](https://developer.apple.com/arkit/), a development framework that simplifies the ability of incorporating augmented reality into your app. I’ve been fascinated by this for a while and took a stab at getting their code running. I’ve only dabbled in iOS development so the bulk of the effort involved getting the latest Xcode beta and then discovering to run the full example I needed to upgrade my phone to run the iOS 11 beta. After this the included ARKit example worked perfectly and left me pleasantly surprised. It worked remarkably well and highlighted how powerful ARKit can be; I've included some sample screenshots below. Snapchat had to invest the time and effort to develop their filters but ARKit will make that significantly easier for future developers. This is an extremely powerful move by Apple since it’s all about the apps and this gives app developers a tool unique to iOS. I have a lot to learn here but I’m excited to get deeper into the augmented reality world. I remember playing with VRML in the 90s and I consider this an extension of my childhood interests.",1,1,2017-06-22,2,"arkit, augmented reality",233,Apple's ARKit
24,0,Switcheroo makes it easy to test development code on a production site by rewriting the URL but unfortunately it only works on Chrome.,#code,"{% include setup %} At  TripleLift , we’re big fans of the  Switcheroo  plugin and rely on it during development to test new versions of our code. It allows us to override a production hostname with one of our development boxes so we can see how our code works on a live site. So if a production site is referencing a JavaScript file at http://production-environment/script.js we use Switcheroo to have it reference the development file at http://dev-environment/script.js. Unfortunately, it’s only available for Chrome which makes it more difficult to run browser specifics tests on other browsers.  To deal with this problem we came up with a small redirection app that runs locally and is browser agnostic. Instead of entering the desired host to redirect in the extension, you add it to the local  hosts file , mapping it to localhost. This bypasses the DNS lookup and sends all requests to that domain to the locally running server which then serves a redirect to the desired URL. The  code’s up on GitHub  with a readme that should hopefully be easy to follow.",4,1,2015-02-07,3,"switcheroo, testing, client side testing",201,URL redirection app
16,0,Some best practices I've picked up after working on Django products over the past 18 months,"#python,#code","{% include setup %} I’ve discovered that every new project lets me correct mistakes from my earlier attempts by allowing me to start from scratch. This is especially true with a web framework such as Django that has a ton of little nooks and crannies that take a while to explore and understand. It’s usually not worth it to go back and fix something that’s not broken on a functional product but starting a new project lets me do it right from the beginning. Now that I’ve developed and launched (with  Sandy  and  Marc ) two serious Django-based products as well as bunch of smaller ones, I wanted to document some personal best practices I’ve picked up. Obviously, I'm still learning and I may be completely wrong with them so let me know if you disagree. If you’re interested in a deeper look at some of the topics let me know and I can write up another post going into detail about a particular topic.         Use  virtualenv : Virtualenv lets you create a virtual environment for each project you’re working on with its own version of Python and its own libraries. I’ve also created alias commands for my major projects that make moving to and activating the virtualenv of that project a single command. Note that using a virtualenv does make a few things more difficult (such as installing  MySQL-python , setting up nginx, configuring  fabric , getting supervisor running) but they’re all surmountable via Stackoverflow and Google.        Use  South : A simpler way of handling database migrations in Django. It’s natural to be updating your database models as the app grows and South makes the migration a little bit easier. It’s not perfect and every once in a while I’ll need to revert some of the migrations and craft them by hand but it’s still better than the alternative.        Use  Fabric : Fabric gives you the ability to set up your own set of commands that can interact with a remote server. This lets you do git pulls, deployments, and run any other command on a server without needing to manually SSH. This becomes especially useful when you have your app served by multiple machines with each one having a different role.        Use  Supervisor : Supervisor monitors the running processes and can restart any that go down.        Nginx/Gunicorn vs Apache: I’ve used both and don’t have strong feelings about either one. I think there’s more information online about getting Apache running but I’ve found Nginx/Gunicorn a bit easier to configure and debug. The other benefits I’ve gotten from Nginx/Gunciron is that it’s less memory intensive out of the box than Apache and I was able to get it to play nicely with Supervisor. In full disclosure, I haven’t really tried to do the same with Apache and it may very well be possible.        Use S3 for static files: Hosting your static files as well as user-uploaded files on S3 is a nice win. You don’t have to worry about serving static content and you can also move the static elements away from your web server. Another benefit I’ve found is that once you move to multiple web servers, it’s nice having all static content on a 3rd party on S3 since that allows all web servers to remain stateless and insync. Otherwise you have to worry about a user uploading a file to one web server and then having to copy it over to the other one to make it accessible.        MySQL/PostgreSQL vs RDS: Unless you plan on monetizing immediately, I suggest using MySQL/PostgreSQL. RDS ends up getting pretty expensive and configuring it isn’t as straightforward as modifying a local installation of MySQL or PostgreSQL. If you end up running into scaling issues you can make the move to RDS relatively easily (especially with MySQL) by dumping and reimporting your database and updating your production settings file.        On Django packages: Install new packages using pip instead of just downloading them into your project folder unless you know you’ll be modifying them. Even then, the well written packages let you customize their behavior by writing your own views, templates, and middleware that can exist outside the installed package. This will keep your project much simpler and better organized, and will force you focus on your app rather than trying to hack someone else’s.     After writing this, I realize I need do another post about the Django packages I’ve found to be useful. I'll put that together in a future post.  Edit: Here's the  follow-up post  where I cover useful packages.",9,2,2013-05-07,5,"Django, hacking, coding, web development, startups",817,Eighteen months of Django
33,0,"virtualenv is great but it requires a bit of work to get it running for various services. Below are the ways I've gotten it working with Nginx, Gunicorn, Supervisor, Celery, and Fabric.","#python,#code","{% include setup %} One of my favorite things about Python is being able to use  virtualenv  to create isolated environments. It’s extremely simple to use and allows you to have different versions of Python libraries used by different projects.  The thing that's tricky is getting virtualenv set up on a production environment under different services since each one requires a slightly different configuration. I’ve gone through my projects and collected the various ways I’ve gotten it running for different services. I’m sure I could have done it differently but the following worked for me and will hopefully come in handy to others. If you have any questions or I'm not being clear enough let me know and I'll updat the post with more information.       Nginx and Gunicorn under Supervisor.      Nginx  - The configuration isn't anything different than normal except that you may need to specify some specific paths that are within your virtualenv  {% highlight nginx %}   Static files needs to point to virtualenv directory location /static/admin {   autoindex on;   root   /home/ubuntu/app/venv/lib/python2.7/site-packages/django/contrib/admin/; } {% endhighlight nginx %}      Gunicorn  - I have a shell script here that's used to set the various paths and options that configure Gunicorn  {% highlight bash %} #!/bin/bash set -e DJANGODIR=/home/ubuntu/app DJANGO_SETTINGS_MODULE=app.settings.prod  LOGFILE=/var/log/gunicorn/guni-app.log LOGDIR=$(dirname $LOGFILE) NUM_WORKERS=2 # user/group to run as USER=ubuntu GROUP=ubuntu cd /home/ubuntu/app source /home/ubuntu/app/venv/bin/activate  export DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE export PYTHONPATH=$DJANGODIR:$PYTHONPATH  test -d $LOGDIR || mkdir -p $LOGDIR exec /home/ubuntu/app/venv/bin/gunicorn_django -w $NUM_WORKERS \   --user=$USER --group=$GROUP --log-level=debug \   --log-file=$LOGFILE -b 0.0.0.0:8000 2>>$LOGFILE {% endhighlight bash %}      Supevisor  - Here we just point our configuration file to the shell script for Gunicorn  {% highlight ini %} [program:gunicorn-myapp] directory = /home/ubuntu/myapp user = ubuntu command = /home/ubuntu/myapp/scripts/start.sh stdout_logfile = /var/log/gunicorn/myapp-std.log stderr_logfile = /var/log/gunicorn/myapp-err.log {% endhighlight ini %}        Celery  under Supervisor.   In this case we just configure Supervisor to start virtualenv path for celery. A cool feature is being able to specify the environment variables - in my case to pass in the Django settings module.   {% highlight ini %} [program:celery] ; Set full path to celery program if using virtualenv command=/home/ubuntu/myapp/venv/bin/celery worker -A myapp --loglevel=INFO  directory=/home/ubuntu/myapp user=nobody numprocs=1 stdout_logfile=/var/log/celery/worker.log stderr_logfile=/var/log/celery/worker.log autostart=true autorestart=true startsecs=10  environment =   DJANGO_SETTINGS_MODULE=myapp.settings.prod {% endhighlight ini %}      Fabric .   The idea here is to make sure all our remote install commands are run after activiating the virtualenv.   {% highlight python %} from __future__ import with_statement from fabric.api import * from contextlib import contextmanager as _contextmanager  env.activate = 'source /home/ubuntu/myapp/venv/bin/activate' env.directory = '/home/ubuntu/myapp'  @_contextmanager def virtualenv():     with cd(env.directory):         with prefix(env.activate):             yield  @hosts(env.roledefs['db']) def rebuild_index():     with virtualenv():         run(""python manage.py rebuild_index"") {% endhighlight python %}",6,2,2014-02-10,7,"virtualenv, python, nginx, gunicorn, supervisor, celery, fabric",480,Using virtualenv in production
13,0,Seeing a band perform 50 years after its founding makes one think.,#meta,{% include setup %}      Last week I attended a [Yes](https://en.wikipedia.org/wiki/Yes_(band)) concert. For the poor souls that don’t know - Yes was an extremely popular progressive rock band in the 70s and 80s. The implication is that the band members are currently in their 60s and 70s yet are still going on tour and performing nearly two hour long shows. And being a rock band from the 70s they ran into the typical challenges with members leaving and rejoining throughout its history.  It’s incredible that Yes is still playing 50 years after forming. I suspect at this point the members have resolved all the conflicts they had and are just appreciate of the fact that they’re still able to play and perform. Imagine if they had this mindset and maturity when they were at their peak. And yet would they have achieved their peak and fame if the individual members didn’t have this sort of competitive conflict in their youth?  I can’t quite put my finger on it but there just seems something admirable about a band continuing to tour and perform decades after its peak. The members know they’re past their prime and yet they keep touring as often as they can. They realize they have a unique opportunity and once on stage feel the same way they used to half a century years ago.,1,1,2017-10-01,3,"yes, progressive rock, age",240,Lessons from a Yes concert
33,0,Facebook keeps making an appearance in the news due to their poor privacy and data handling. The latest NY Times expose shows just how much data Facebook shared with their strategic partners.,#society,"{% include setup %} The Facebook hits just keep on coming. The latest is a [NY Times expose](https://www.nytimes.com/2018/12/18/technology/facebook-privacy.html ) around Facebook’s lack of privacy or data protection when it came to their strategic partner integrations. Citing some examples won’t do it justice and the entire article is worth a read just to see how loose Facebook was with user data. It’s shocking how ridiculous some of these practices were. I’m not surprised by the data Facebook is collecting or how it’s being used for ad targeting but I’m amazed at how much data they allowed to leave their platform. Facebook’s entire bread and butter is user data and letting it leave the “walled garden” for some short term benefit was misguided. It’s impossible to get that data back and especially now with everyone eagerly looking for Facebook’s missteps it makes them look especially imprudent.  It really does seem Facebook doesn’t understand their data. On one hand they’re leveraging the hell out of it to advance their product but on the other hand they’re extremely cavalier with it. A while back someone made the point that Facebook sees user data as their own and not belonging to their users and therefore are not treating it with the care that it deserves. At the time that was a nice sound bite but felt wrong since Facebook’s entire business model is predicated on them taking care of user data. Unfortunately, the latest bit of news has me questioning Facebook’s ability and incentive to actually safeguard user data.",0,1,2018-12-19,3,"facebook, privacy, user data",259,Facebook's latest hit
34,0,Fullfillment by Amazon is meant to make it easier for merchants to offer Prime to their customers but it seems to have been overtaken with counterfeit items. Amazon needs to do something here.,#product,"{% include setup %} Recently I’ve come across a few articles describing the supposedly massive amount of counterfeiting happening on Amazon. The way it works is that Amazon offers a [Fulfillment by Amazon](https://services.amazon.com/fulfillment-by-amazon/how-it-works.htm/ref=asus_fba_snav_how) (called FBA) option where a merchant sends their items to Amazon’s warehouse which is then eligible for Prime shipping since it’s just going to be shipped by Amazon. The way Amazon implements this is by commingling the items - so if two merchants send Amazon the same item Amazon will treat it as the same item when it comes to consumers. A merchant is able to opt out of this commingling but only with a higher fee.  The idea itself is brilliant. By ignoring the merchants and treating the items as interchangeable Amazon is able to optimize for the consumer and come up with a much more optimal warehouse distribution strategy. Rather than having to keep a merchant’s items across every distribution center Amazon can choose to just keep a single merchant’s items in a single distribution center since the remaining merchants may have shipped their items to others. And if the items themselves were the same then it’s a perfect solution that’s better and cheaper for everyone involved.  Unfortunately, this only works when the items actually are the same. Everywhere there’s money to made someone will inevitably try to abuse the system. In this case some merchants are offering counterfeit products at a lower price. And since the items are commingled they are treated like the real goods. This means that some customers are paying a low price for a counterfeit product and getting a real product while others are paying the true cost but are getting the counterfeit. Clearly Amazon needs to do something here before it becomes a huge  issue.  The obvious way is to do a more thorough job of inspecting the merchandise and making sure it’s legitimate but I can easily see this being a difficult problem at scale. Another option is to adopt a one strike policy and if you get caught selling counterfeit goods then you get a lifetime ban. This would make it much more expensive to cheat and should reduce the fraud. At the same time if it’s easy to just start selling as a different company then it won’t do much. A way to address that is to require that every seller be a legitimate and federally registered company but that significantly hurts international sellers. Every manufacturer should know the legitimate sellers so it may also make sense to enforce a merchant whitelist for some items. I honestly don’t know enough about the industry but it does seem that for many small and niche products there should only be a few legitimate sellers that can be curated by the manufacturer. This can also expand into a “manufacturer preferred” tier to handle the edge cases where a merchant is not aware of every legitimate seller. In that case Amazon ends up with two tiers of commingling - but that itself sends a very odd message to the customers since they are then admitting they are selling counterfeit items.  There’s no easy answer, especially given Amazon’s scale and aspirations, but something should be done and as a shareholder I’m hopeful they figure it out. My gut is that it’s going to require a combination of different approaches and a look at the data to identify the dishonest sellers. Amazon so far has gotten away with this by being extremely customer focused and very open to refunds and returns but no one wants to spend time dealing with a return in order to then just get another crappy item.",1,1,2017-03-26,4,"amazon, ecommerce, fulfillment by amazon, counterfeit",615,Fulfillment by Amazon counterfeiting
13,0,Just released a new JavaScript tool that geocodes a list of addresses.,,"{% include setup %} Over the weekend I dug up an  old repository  I started to contain a running collection of  JavaScript tools  to make my life easier. Ever since I created it it had two tools - one to convert CSV/TSV text into a bootstrap table and the other to generate a “BCG style” matrix. Earlier today I coded up another script - a quick way to geocode a list of addresses. All you have to do is enter a list of address you want geocoded, one per line, and the script will use the Google Maps API to geocode each one with the resulting latitude/longitude being written to an HTML table. If you have any other suggestions for a quick tool let me know.",2,0,2014-04-21,5,"javascript, tools, csv to bootstrap table, bcg matrix, bulk geocoding",134,Bulk geocoding tool
19,0,Amazon is vertically integrating our lives and their latest moves are this century's version of the company town.,#meta,{% include setup %} A few months ago I read an article describing Amazon’s [potential move](https://www.theinformation.com/articles/amazon-considers-offering-home-insurance) to offer home insurance. The premise is that Amazon is incredibly strong operationally and both their integration of Echo into all things home and their acquisition of Ring gives them valuable signals that the typical insurer does not have.  I couldn’t help but think back to the 19th century where companies were large enough to [own](https://en.wikipedia.org/wiki/Company_town) all the property in a town and have their employees live there. As one can imagine it was extremely exploitative since you had the company that paid you also collect money from you for your housing. They died down in the 20th century but this feels like a step in that direction. We’re clearly not all going to work for Amazon but the idea of having a single company vertically integrating around our lives seems a bit dystopian.,2,1,2018-12-17,2,"amazon, company town",159,The new company town
6,0,I'm migrating my blog to Github,#blog,{% include setup %} I'm going to work on migrating my posts over from Wordpress and Tumblr on to here. Let's see how it goes.,0,1,2012-11-13,1,blogging,26,"Hello, Github!"
24,0,Bulk discounts are rational in the short term where everyone wins but in the long term the seller is encouraging a monopsonic outcome.,"#product,#society","{% include setup %} While reading yet another [article](https://www.theinformation.com/articles/how-segway-ninebot-became-the-go-to-scooter-maker-for-rental-startups) about scooter startups I came across an obvious quote by Gao Lufeng, CEO of Segway-Ninebot, the leading scooter manufacturer: “As one of the biggest battery buyers, we have the bargaining power to get the lowest price in the market.”  This is obvious and we see it everywhere: buying in bulk gets you a discount. As a consumer I can go to Amazon and look at any item and the per unit price when buying a single item is going to be higher than when buying a pack. And the more I buy the more the discount on a per unit basis. The business world is no different and it’s not surprising since both sides benefit in this situation: the buyer is able to get a cheaper product and the seller is able to get guaranteed sales while hopefully getting more efficiencies of scale and which further reduces the manufacturing cost.  While rational in the short term it may not be optimal for the seller in the long term. By treating one buyer better than the others the seller is giving that buyer a market advantage. And if that buyer is already the dominant player then they become even more dominant. In the short term it’s not a big deal since the seller is selling each item at the maximum price a buyer is willing to pay but in the long term it may drive out other companies in the market and end up in a [monopsony](https://en.wikipedia.org/wiki/Monopsony) where the seller has no other buyers.  This reminds me of a point I heard in a [Exponent.fm podcast](http://exponent.fm/episode-144-90s-alt-forever/) about Spotify: the studios want as much competition in the streaming space as possible since it prevents any single buyer from becoming dominant enough to drive the market. In this way Spotify, Apple Music, Google Music, Amazon Music, Pandora, Tidal, and all the others are competing with one another which gives the studios pricing power over each individual company. If there was a single buyer for all the music then the studios would be in a much weaker negotiating position.  Of course, this is all just speculation and the fact that this practice goes on across industries implies that this short term vs long term trade-off is worth it. In most industries there’s likely a lot more driving the success of a business than their unit costs and these discounts don’t actually influence market dynamics. At the same time there probably are industries where the margins are what determine market power and success. And in these industries it would be interesting to see sellers give everyone the same price based on their total manufactured volume. This would give each of their customers the same playing field and encourage competition which improves the suppliers leverage in the long term.",3,2,2018-07-22,4,"bulk discounts, monopsony, monopoly, economics",497,Bulk discounts hurt competition
29,0,I got an Amazon Echo yesterday and have been playing around with it. Despite the gimmicky factor it really is a new way of interacting with our devices.,#meta,"{% include setup %} After reading the positive reviews I got past the gimmick factor and jumped aboard the Amazon Echo train and got it set up yesterday. After going through the obvious examples (what’s the weather, tell me a joke, add x to my shopping list, play song y) and playing around with it I’m past the gimmick stage. The always on listening is really a different way to interact with our devices. Conceptually it’s no different than using Siri or Google Now but in practice it’s a world of difference. I don’t always have my phone with me and for some things it just feels more natural to start speaking and see an immediate effect. Whether that’s playing some specific songs or playlists, changing the volume, or adding items to a shopping list it feels more natural than having to go through a phone. One of my favorite use cases so far has been using the Echo to keep track of my shopping list. In the past I’d be in the kitchen and realize we needed something and would forget as soon as I switch tasks. With the Echo I can immediately call out what to add and have the list readily available next time I go to buy something.  To be honest, 99% of our Echo usage has been playing music and adding things to a shopping list but I can see the potential there. There are a ton of apps, that Amazon calls skills, with new ones constantly being developed and I look forward to seeing what kind of cool stuff gets developed.",0,1,2016-02-21,2,"amazon echo, connected home",268,Amazon Echo
25,0,Search engines have a conflict between sources that have authority and sources that have original content. I show an example of it going wrong.,#meta,"{% include setup %} A while ago I read Bruce Schneier’s Liars and Outliers and came across a neat passage:   There was this kid who came from a poor family. He had no good options in life so he signed up for the military. After a few years he was deployed to a conflict infested, god-forsaken desert outpost. It was the worst tour of duty he could have been assigned. It was going to be hot and dangerous. Everyday he had to live with a hostile populace who hated his presence and the very sight of his uniform. Plus, the place was swarming with insurgents and terrorists.   Anyhow, one morning the soldier goes to work and finds that he's been assigned that day to a detail that is supposed to oversee the execution of three convicted insurgents. The soldier shakes his head. He didn't sign up for this. His life just totally sucks. ""They don't pay me enough,"" he thinks, ""for the shit I have to do.""   He doesn't know he's going to be executing the Son of God that day. He's just going to work, punching the time clock, keeping his head down. He's just trying to stay alive, get through the day, and send some money back home to Rome.   Bruce mentions that he found this on the internet and  cited it appropriately  in the footnotes. But when I tried Googling for the phrase ""There was this kid who came from a poor family"" the  top links  were  people citing  Liars and Outliers, including my own  highlight on Readmill . I even came across a  page  that linked to the  original source  that Bruce cited before I found a link to the original source.  I realize this conflict between authority and originality is a challenge for search engines but it seems that they rank authority ahead of originality. This leads to the unfortunate consequence that if any major site cites your personal blog they will appear earlier in the search results. I had this occur with my  post  on the history of why cell phones don’t have dialtones; searching for “cellphones dialtone” shows the Gizmodo link ahead of my blog’s. The nice thing is that Google seems to be getting better - right now searching for “There was this kid who came from a poor family” shows the original source in the third position; a few months ago it was at the bottom of the first page. Let’s hope this trend continues.",7,1,2013-05-29,3,"seo, search engines, google",462,On SEO: Authority vs Originality
31,0,MySQL foreign keys are a great way to enforce consistency in your database but it's not always obvious how they work. I provide examples for each option and its impact.,#code,"{% include setup %} Databases are the last layer of defense against corrupt data and the more restrictive you can make them the better. No matter how much validation you may have missed in your code having a strong and restrictive database schema will protect your data. One of the best approaches to building a restrictive schema is using foreign keys which specify how fields from one table relate to the fields of another table. There are a few options here and make it possible for you to specify anything from automatically removing rows when a row they’re referencing is removed to recursively updating rows when their references have changed.  The [MySQL docs](https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html) give a nice overview of how foreign keys work but they’re light on examples and since I tend to learn best from examples I wanted to share them along with a brief description. Hopefully others find these examples useful as well. Each of the examples creates two tables, test_parent and test_child, with test_child having a different foreign key option on a field referencing the test_parent. I also insert the same data into each one to start and then do a few follow up queries describing what happens in each scenario. Also note that there is both an ""ON DELETE"" and an ""ON UPDATE"" option which, as expected, controls the respective behaviors.  ### RESTRICT  {% highlight sql %} DROP TABLE IF EXISTS test_child; DROP TABLE IF EXISTS test_parent;  CREATE TABLE test_parent (     id INT NOT NULL,     PRIMARY KEY (id) ) ENGINE=INNODB;  CREATE TABLE test_child (     id INT,     parent_id INT,     INDEX par_ind (parent_id),     FOREIGN KEY (parent_id) REFERENCES test_parent(id)         ON DELETE RESTRICT         ON UPDATE RESTRICT ) ENGINE=INNODB;  insert into test_parent (id) VALUES (1), (2), (3), (4); insert into test_child (id, parent_id) VALUES (1, 1), (2, 2), (3, 3), (4, 4);  select * from test_child; select * from test_parent;  -- This fails since one of test_child rows referneces this row. delete from test_parent where id = 1;  -- By deleting the associated test_child row first we'll be able to delete the row in test_parent. delete from test_child where parent_id = 1;  -- Now that there's no test_child row referencing this we're able to delete successfully. delete from test_parent where id = 1;  -- Similarly, we can't update the key since it's being referenced by one of the child rows. update test_parent set id = 10 where id = 2; {% endhighlight %}  ### CASCADE  {% highlight sql %} DROP TABLE IF EXISTS test_child; DROP TABLE IF EXISTS test_parent;  CREATE TABLE test_parent (     id INT NOT NULL,     PRIMARY KEY (id) ) ENGINE=INNODB;  CREATE TABLE test_child (     id INT,     parent_id INT,     INDEX par_ind (parent_id),     FOREIGN KEY (parent_id) REFERENCES test_parent(id)         ON DELETE CASCADE         ON UPDATE CASCADE ) ENGINE=INNODB;  insert into test_parent (id) VALUES (1), (2), (3), (4); insert into test_child (id, parent_id) VALUES (1, 1), (2, 2), (3, 3), (4, 4);  select * from test_child; select * from test_parent;  -- This removes both the row in test_parent as well as the associated row in test_child. delete from test_parent where id = 1;  -- This updates both the row in test_parents as well as the referenced field in test_child. update test_parent set id = 10 where id = 2; {% endhighlight %}  ### SET NULL  Note that in this case we can't even make parent_id NOT NULL in the test_child table create - MySQL rejects that statement.  {% highlight sql %} DROP TABLE IF EXISTS test_child; DROP TABLE IF EXISTS test_parent;  CREATE TABLE test_parent (     id INT NOT NULL,     PRIMARY KEY (id) ) ENGINE=INNODB;  CREATE TABLE test_child (     id INT,     parent_id INT,     INDEX par_ind (parent_id),     FOREIGN KEY (parent_id) REFERENCES test_parent(id)         ON DELETE SET NULL         ON UPDATE SET NULL ) ENGINE=INNODB;  insert into test_parent (id) VALUES (1), (2), (3), (4); insert into test_child (id, parent_id) VALUES (1, 1), (2, 2), (3, 3), (4, 4);  select * from test_parent; select * from test_child;  -- This deletes the row in test_parent and also makes the parent_id value for the associated row in test_child null. delete from test_parent where id = 1;  select * from test_child;  -- Similarly, the parent_id field in test_child that used to be 2 is now null. update test_parent set id = 10 where id = 2;  select * from test_child; {% endhighlight %}  ### NO ACTION  No example here since in MySQL this works exactly the same as the RESTRICT option above.  ### SET DEFAULT  This is not a valid option in MySQL using the INNODB engine.",1,1,2018-07-07,4,"mysql, foreign keys, database integrity, relational databases",742,MySQL foreign keys
41,0,People will either argue that AWS is more expensive than the alternative or worth it due the ability to scale. The real value is in the options available and being able to use them to build the system you want.,#aws,"{% include setup %}      Every time Amazon announces a price drop there are always people pointing out that it’s still more expensive than other cloud computing services such as Linode or Digital Ocean. The Amazon fans then respond by saying sure AWS is more expensive but the value is the ability to scale quickly when needed.  For me, the biggest value behind AWS is the ecosystem and the included optionality. When building large scale web services it’s tough to know every issue you will run into and more often than not your needs and implementation will change. AWS provides a ton of available tools that make growing and scaling easier beyond the hardware itself. You may start with using EC2 for your server and S3 for hosting your static assets but over time you may start using Cloudfront as a CDN and Redshift for your analytics and EMR to process your various logs. That’s the biggest value in AWS - not being able to launch new machines quickly but having a set of infrastructure options that can be specialized to fit your needs.  It used to be that the physical hardware was orders of magnitude more expensive than engineers but this hasn’t been true for decades now - it’s perfectly reasonable to look for ways to reduce yours costs especially if it can be done quickly but obsessing over hardware costs, especially while you’re still growing, is a red herring. Building large systems is tough and the fewer things you have to worry about the better - using AWS reduces the chance that you will run into a scenario where you’re just not able to do something without changing your host and rewriting your architecture.",0,1,2014-03-30,3,"AWS, cloud computing, infrastucture",296,AWS is about infrastructure optionality
22,0,I recently discovered how powerful the date shell utility is. Definitely an improvement over a Python script for simple date logic.,#code,"{% include setup %} The longer I code the more I appreciate the power of the shell. Getting familiar with common commands is a great way to improve your productivity and over time you amass a massive collection of scripts that allow you to do nearly everything. The most recent utility I discovered was “date”. As expected, it displays the current date and time but it can easily be adapted to display the current datetime in nearly any date format but also allows you to offset the current date in a variety of ways.  {% highlight sh %} ➜  ~  date Mon Oct 19 22:35:37 EDT 2015 ➜  ~  date +%Y-%m-%d 2015-10-19 ➜  ~  date +""'%Y-%m-%d'"" '2015-10-19' ➜  ~  date -v+3d +%Y-%m-%d 2015-10-22 ➜  ~  date -v-3d +%Y-%m-%d 2015-10-16 ➜  ~  date -v-3y +%Y-%m-%d 2012-10-19 ➜  ~  date -v+3y +%Y-%m-%d 2018-10-19 ➜  ~  date -v+3y +""%Y-%m-%dT%H:%M:%S"" 2018-10-19T22:39:18 ➜  ~  date -v+3m +""%Y-%m-%dT%H:%M:%S"" 2016-01-19T22:39:24 {% endhighlight sh %}  In the past I’d resort to a JavaScript utility or a quick Python script when I needed a simple date calculation but lately I’ve been able to do nearly everything solely by using the built in date utility. It’s still a bit cumbersome for generating date ranges or when requiring complicated logic but for the basic stuff it’s surprisingly powerful and expressive. It’s amazing how full featured the shell is and how often we avoid it and use more fleshed out languages. Instead of trying to find new languages it’s worth taking the time to actually explore and understand the shell - it’s one of the better investments an engineer can make.",0,1,2015-10-19,3,"shell scripting, date, shell date",303,Dates in the shell
18,0,Using namedtuples and the csv library make it incredibly easy and clean to read a file in Python,#code,"{% include setup %} Python’s my goto language for doing quick tasks and analyses with the majority of them being quick scripts to analyze a file or pull some data. I’m constantly looking to improve my code and lately have developed the following approach. The goal isn’t to make it as short as possible but to make it as expressive and clean as possible. They're related but not synonymous.  {% highlight python %}#!/usr/bin/python  import csv from collections import namedtuple  # Can add whatever columns you want to parse here # Can also generate this via the header (skipped in this example) Row = namedtuple('Row', ('ymd', 'state', 'size', 'count'))  with open('file.csv', 'r') as f:     r = csv.reader(f, delimiter=',')     r.next() # Skip header     rows = [Row(*l) for l in r]     # Do whatever you want with rows {% endhighlight %}  The reason I like this approach is that it’s obvious what’s happening and it’s being done in a Pythonic way. There’s no traditional for loop that spans multiple lines and it’s simple to update the loop to manipulate the values during the handling of reach row. This approach also leverages the namedtuple collection which is one of my favorite types - a class-like structure that's significantly more memory efficient but provides easy named access the fields (row.ymd, row.state). With this basic structure in place we can add all the bells and whistles that manipulate and tweak the rows. One thing to be aware of is that the namedtuple generates if immutable so you either need to manipulate the values before construction or use additional structures to transform the data.",0,1,2016-01-10,3,"python, reading a file, clean code",273,Cleanest way to read a CSV file with Python
24,0,Despite not having a functioning phone I managed to survive pretty easily without Gmail but definitely learned to handle two factor authentication better.,#meta,"{% include setup %} While my phone was being repaired I ran into a predicament. The only way I could log in to my Google accounts was by authenticating via an SMS code which I wasn’t to get without an SMS code. Additionally, I never bothered to actually write down the backup codes thinking I’d never need them so I was stuck in the envious position of being Google account free for 4 days.  Luckily, I had two things going on that made the loss easily manageable. One was that I shared my personal calendar with my work account so was able to see (and create) everything I needed through my work account. And two - I’ve been forwarding all of my email from Gmail to Fastmail  since March  of last year. The only real frustration was not being able to search through my email history nor use the chat. Otherwise it was barely noticeable.  I definitely got lucky so the lesson here is that it’s impossible to predict what’s going to happen and you should just deal with the annoyance of the backup codes. Google also provides the Authenticator app which is another way of supporting two factor authentication. None of these is as simple as just not having two factor authentication but I think it’s a must have - especially for a primary email account which is linked to every other account - including your financial and social media accounts. Losing access to your emails makes it very easy to reset the passwords on those and there’s no excuse in not enabling two factor authentication.",1,1,2015-08-31,4,"two factor authenticaton, 2fa, security, google",268,Two factor authentication hell
19,0,I've bene blogging actively since 2013 and wanted to share a few of the small highlights I've experienced.,#meta,"{% include setup %} I started working on a project to investigate my blog posts and see how my writing has evolved over time. I’m still working on it and will definitely write up the results but the entire process got me thinking about my blog and some of the highlights. I started blogging to improve my writing, improve my thinking, and grow my personal brand. Despite being a large time commitment I enjoy doing it and there have been a variety of small episodes that have made it even better:  - In 2013 I wrote a [short post](http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/) with an excerpt from a book I was reading about the lack of a dial tone in cell phones. This took off on Hacker News and ended up being covered in [Gizmodo](http://gizmodo.com/5994589/why-your-cell-phone-doesnt-have-a-dial-tone), [Mental Floss](http://mentalfloss.com/article/50185/why-don%E2%80%99t-cell-phones-have-dial-tones), and even made an appearance in the NY Times tech ticker. - I built a small community. I have a small number of repeat visitors who will comment on the occasional post and I actually ended up meeting up with a frequent contributor, [Ted](https://twitter.com/tedder42), when I visited Portland for the first time. - When Turo was called RelayRides I did an [analysis](http://dangoldin.com/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/) to figure out the optimal car to get that will generate the biggest return. This led to a few people reaching out and me doing a little bit of consulting work to analyze their market. - I’m a big fan of Citibike and came up with a [small web app](https://dangoldin.github.io/citibike-station-directions//) to that translates every New York City trip into a walk to a Citibike station, a station to station bike ride, and then a walk to the final destination. After posting this a few people reached out to ask whether a smartphone app was available as well as ideas to make it even better. I unfortunately haven’t had the chance to work on it but it’s great seeing people finding value in something I’ve done. - Cities opening up their data and I had some fun visualizing the [routes of Jersey City’s garbage trucks](http://dangoldin.com/2015/12/12/jersey-city-garbage-truck-routes/). This led to me connecting to our councliwoman who then introduced me to the head of Jersey City’s tech innovation team. - An interesting one was when a journalist from [FiveThirtyEight](http://fivethirtyeight.com/) reached out to ask about an old GItHub project I was working on. This ended up not leading anywhere but did provide a glimpse into modern journalism and the desire to highlight and surface content from the tail. - You know you’ve made it when you have “SEO experts” reaching out and either offering their site optimization services or a payment to post an article with a link to another site. I’ve received dozens of offers so far but haven’t accepted any yet!  These are just the highlights and at this point I’m happy to receive any inquiry. None of these have been massive but they’re all small highs that are a reminder that what I write is being read. Their lack wouldn’t stop me from blogging but it’s always nice to receive a surprise comment or email.",8,1,2016-06-05,3,"blogging, branding, personal brand",543,Blogging: The small wins
24,0,Follow up to my previous post around a website stealing content from my mom's site and claiming it as their own. A happy outcome!,#product,"{% include setup %} Last week I  posted  about a site my mom discovered that had copied the content from her site and has been positioning as their business. I had no clue what the motivation behind it was other than thinking it was just a sketchy way to either blackmail the real business owner or use black hat SEO tactics to increase traffic and monetize using AdSense.  After sharing my problem and getting a ton of advice, I sent a DMCA request to the host, Colocation America, and received a surprisingly quick reply. Within a few days I was on the phone with a representative from the site claiming that my mom had signed up for a service that was offering free mobile websites and that’s why my mom’s business information and content had been appearing on the other site. My mom doesn’t recall signing up for any site and I believe her - she’s been sending me nearly every offer she receives asking whether it’s legitimate and worth doing and I don’t recall ever seeing this one.  I suspect they use this as a cover for if they get caught and until then they make some money from their ads. In any case this particular issue was resolved pretty quickly and I wanted to thank everyone who helped. In particular,  Rob Adler  did a ton of research on the infringing site and  Matt Cutts  helped send this to the relevant teams at Google. The web’s not always getting worse!",3,1,2013-10-30,2,"seo, website",260,What the SEO? Followup
15,0,I got featured on the HN home page and wanted to share the results.,#dataviz,"{% include setup %} A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.         The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday.                       The post ended up being featured in the NY Times  Bits blog  which accounted for ~2,900 of the total visits; the  Gizmodo network  which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.       The way it spread is pretty interesting: I submitted to HN on Friday afternoon, it was picked up by the NY Times Bits Blog that evening and Gizmodo US on Saturday. After that, it expanded to the rest of the Gizmodo network, including the  UK  on Sunday and  France  on Tuesday.  CNET  and the  Guardian  both picked it up on Monday.       Gizmodo added an Amazon affiliate link to the book I quoted, The Idea Factory, but did me the favor of linking to my startup, Makers Alley. I suppose that makes us even.       Only 80 people ended up visiting the  Makers Alley  site, which is 1/3rd of one percent of the total visitors. These visits were pretty evenly split between the link in the Gizmodo article and the link from my blog.       I have no idea why it took off and don’t consider it one of my better posts. I basically quoted a passage from a book and added a bit of my own commentary. I suspect the topic was appealing due to nostalgia and a bit of geek lore.       It’s surprisingly hard to get on to the Hacker News home page these days but it does drive a significant amount of traffic. I joined HN five years ago and it was orders of magnitude easier to end up making it to the main page.       If you write, do it for yourself and not for the recognition. And if you don’t write, start writing. Nathan Marz has a  great post  that everyone who's interested in blogging should read.       It’s great having my blog hosted on Github pages. It’s free and I don’t have to worry about server load.",8,1,2013-04-19,3,"hacker news, writing, blogging",478,What does getting on the HN front page get you?
17,0,I share a bit about my philosophy on running and how it's developed since I started.,#running,"{% include setup %} In honor of today’s NYC marathon, I finally finished up this post that’s been sitting in my drafts folder the past few weeks.  I’ve never been into running until the beginning of this year when I decided to run 1000 miles. This led to me to three half marathons and is actually making me consider doing a full one. It’s amazing where a habit and a bit of effort can take you. Initially, I ran just to hit my goal and only signed up for races in order to keep myself motivated and on track. Now, it’s become significantly more than that. There are so many things outside my control yet running is solely about my effort and willpower. If I fail it’s my defeat and if I succeed it’s my victory. I can easily skip a run on a cold, rainy morning and yet I know I’m just deceiving myself and I’ll have to make it up later. Running is one of the simplest things to do and that’s a huge appeal. The human bodies have evolved to run and kids start running as soon as they learn to walk. As our world becomes increasingly complex it’s nice being able to escape with a quick run. Whether it’s running or something else, it’s important to have an outlet that rewards us based on our efforts. The further away this activity is from our day jobs the better.",0,1,2013-11-03,1,running,243,Why I run
25,0,Turns out not even Google is immune from the pressures of Cyber Monday and put an ad on their homepage promoting the Google store.,#product,"{% include setup %}      Turns out not even Google is immune from the pressures of Cyber Monday. As minimal as the Google search homepage is they still made the effort to have a callout, however slight, to promote their Cyber Monday deals. A while back I read an article that calculated how much additional revenue Google would be able to generate if they put ads on their homepage. I don’t recall the exact number or the details but I imagine it’s a massive amount. Yet Google never took the step of putting ads on the search homepage so it’s interesting to see them doing it for their own store. The revenue here must be significantly less than what they’d see through traditional advertising but at the same time it’s a very minimal hit to the user experience. I can only imagine how many people needed to be involved in this decision though.",0,1,2018-11-26,2,"google search, advertising",171,An ad on the Google search homepage
28,0,People worry about tech debt but database debt is worse since it's so hard to change and tweak. Spend the extra time desiging a flexibile database schema.,#devops,"{% include setup %} One of the biggest lessons I’ve learned is to spend extra effort thinking about the database when setting out to build something new. Compared to changing a database schema, changing code is trivial. The database structure defines how you think about your business and either provides the flexibility as you grow or impedes you when forced to support something it wasn’t designed to handle.  With code you can do a deploy which can replace all behavior at once while with data you’re forced to acknowledge and handle the data you have. If this is a large table you have to figure out how to migrate the data to a new schema. The simple way is to deal with the downtime and hope the migration works. The more complex way is to support two database schemas at once with your code while the migration occurs. Neither of these would be necessary if you think through the database design choices you’re making. It’s going to be impossible to address every future need but there’s incredible value in at least thinking through potential changes and how they’d be supported.  A simple question is the relationship between tables - are you ever assuming a one-to-one relationship that may be one-to-many in the future? If that’s the case you’re probably better off designing the database to support the more advanced case but having your application only support the one-to-one case. This keeps the flexibility in place if you need but doesn’t complicate the code too much.  Another question to ask is whether there’s anything redundant. It may be easier to denormalize your data a bit for the sake of improving a query but don’t. If a database can support an inconsistent state it will support an inconsistent state. Whether due to a bug, a timing issue during a deploy, or someone making a manual update you’ll end up with an inconsistent state in the database which will likely lay dormant for too long. Avoid this issue entirely by removing all redundancies and potentially conflicting fields.  Beyond the tactical questions, thinking about your business and product roadmap a year from now is a great way to influence your schema now. If you suspect you’ll need to support a particular feature or flow you should imagine what your data would need to look like. It’s important to do this when writing code but it’s more important to do this when designing your database. Code can be changed with a deploy but database changes require more.",0,1,2016-02-15,3,"database, tech debt, database design",428,Design your database for flexibility
25,0,A recent FiveThirtyEight article on the sneaker resale market and getting luxury for cheap is a good guide for how to launch successful products.,#product,"{% include setup %} I recently read a FiveThirtyEight article on the  sneaker resale market . The concept is extremely foreign to me since I tend to not collect anything other than old notes and have a tendency of grossly mistreating my shoes and clothes. Nonetheless, I found it fascinating as it discusses the incentives of the various parties involved and comparing them against standard economic theory. One passage in particular was so insightful that I had to save it:    That differential allows people to buy something on the cheap but feel like they’re wearing a luxury item.    “So even if you paid $100, you’ve got $800 on your feet. It’s like having Gucci,” Taylor said   Everyone loves getting a deal but I think this is slightly different since it’s not so much about getting the deal as it is about being able to afford luxury and show it off. Sure it’s vain and has a bit of conspicuous consumption but if it gives someone self confidence and makes them feel like a million bucks I can’t complain.  We should strive to provide this type of experience when building apps and services. It’s not just adding some cheap gamification tricks or rewarding early users as much as it is making people proud to use your product. gamification tricks or rewarding early users as much as it is making people proud to use your product.",1,1,2014-11-05,2,"product, marketing",250,Luxury for cheap
33,0,"If you adopt a technology early you end up running a ramshackled implementation that you will rarely, if ever, update. You likely won't even keep track of the improvements to the technology.",#meta,"{% include setup %} Yesterday I spent a bit of time getting Ansible setup for my various instances that host my various projects. There are a handful scattered across AWS and Digital Ocean and I go through a round of maintenance every few months where I upgrade everything that needs upgrading. This was rare enough that I never bothered automating it but had half an hour to spare and thought this was an opportunity to learn Ansible - something that the TripleLift DevOps team has been using.  Turns out it was surprisingly easy and I only wish I had done it sooner. The documentation is great and it’s mature enough that there are tons of examples for whatever scenario one may encounter. The fact that my use case was simple didn’t hurt but I suspect even a more complicated one would have some examples.  But the only reason it was so easy was because I was such a late adopter. Ansible was initially released in 2012 and it’s grown significantly since then. Had I adopted it when it launched I would have had to figure everything out the hard way. More importantly, once I got it working I doubt I’d revisit later versions to see the latest improvements. That’s the curse of the early adopter - it takes you a while to get everything working in a ramshackle way that it takes massive amounts of discipline to actually change your implementation once the software improves. It’s likely that you may not even know what the improvements were or how they can be applied.  This is akin to tech debt but rather than living within your own code that you’re intimately familiar with it’s with foreign code that’s rarely checked or referenced. Upgrading it carries risks and we subscribe to the policy of “if it ain’t broke, don’t fix it’ - which may very well be the correct choice but it’s important to know the tradeoffs which don’t happen with only half the information. The lesson here is if you use open source tools and libraries subscribe to their release notes and keep an open mind - it’s likely the improvements will benefit you and your code. Even better would be to contribute back to the community and build whatever needs building.",0,1,2018-05-12,3,"early adopter, open source, tech debt",378,Curse of the early adopter
25,0,A few years ago I wrote a genetic program that generates a Connect 4 bot. This provides a quick overview of how it works.,"#code,#python","{% include setup %} Over Thanksgiving break I was going through some old GitHub repos and found an interesting one I wanted to share. It’s a  Connect 4 bot  that’s evolved through a genetic program. The goal of the strategy is to choose a column to move to that will give the highest probability of a win given a board position. To figure out the move column, the genetic program simulates play of strategy against strategy and gives the most successful ones a greater chance of reproducing into the next generation. The idea is that over time the resulting strategy will be the most fit.  The way a typical genetic program works is represented is through a tree structure with the leaf nodes (terminals) containing the various features of the input and the non-leaf nodes containing functions to evaluate the values in the leaf nodes. This way, the program can evaluate any input and we can create new functions by taking subbranches from one tree and combining them with another.  I used the  PyEvolve framework  which took care of all the simulation code so the bulk of my work was spent in figuring out which features and functions to use as well as a way of tracking the intermediate strategies so I could store the resulting strategy for later use. The features I ended up using where the number of own and opponent’s pieces adjacent to the move, the number of own and opponent’s 3 piece segments that would be created with the move, and the height of the column. I experimented with a few functions but ended up keeping a simple set of four - add, subtract, multiply, and an “is greater than” function.  In the end, the best I could get was a genetic program that was able to beat a random move strategy a little over 70% of the time. Unfortunately, this “optimal” strategy failed to win against a real strategy, such as  minimax . I suspect the strategy would have done a bit better had I trained it against a smarter set of strategies but I doubt it would have ever been able to compete with the minimax approach. I’m mostly amazed that by starting with a few features and some simple functions it’s possible to evolve a strategy that’s actually better than random. I doubt I can use this approach in a professional project but it’s still great being exposed to it.",3,2,2013-11-30,3,"genetic programming, ai, connect 4",418,Genetic programming Connect 4
8,1,I share some thoughts I have on innovation,#product,"I stumbled unto an  article  written in 1968 that tries to predict what the world of 2008 will be like. Usually, these types of predictions are completely off and tend to predict a future far more advanced than what it actually becomes.           As expected, the article had it's exaggerations (automatic cars driving at 250 miles per hour, inter-continental rockets, average work day of 4 hours ) but what struck me the most is how accurate the predictions about computers are:    The single most important item in 2008 households is the computer. These electronic brains govern everything from meal preparation and waking up the household to assembling shopping lists and keeping track of the bank balance. Sensors in kitchen appliances, climatizing units, communicators, power supply and other household utilities warn the computer when the item is likely to fail. A repairman will show up even before any obvious breakdown occurs.  Computers also handle travel reservations, relay telephone messages, keep track of birthdays and anniversaries, compute taxes and even figure the monthly bills for electricity, water, telephone and other utilities. Not every family has its private computer. Many families reserve time on a city or regional computer to serve their needs. The machine tallies up its own services and submits a bill, just as it does with other utilities.  Money has all but disappeared. Employers deposit salary checks directly into their employees’ accounts. Credit cards are used for paying all bills. Each time you buy something, the card’s number is fed into the store’s computer station. A master computer then deducts the charge from your bank balance.  Computers not only keep track of money, they make spending it easier. TV-telephone shopping is common. To shop, you simply press the numbered code of a giant shopping center. You press another combination to zero in on the department and the merchandise in which you are interested. When you see what you want, you press a number that signifies “buy,” and the household computer takes over, places the order, notifies the store of the home address and subtracts the purchase price from your bank balance. Much of the family shopping is done this way. Instead of being jostled by crowds, shoppers electronically browse through the merchandise of any number of stores.    Compared to the rest of the predictions, this is amazingly close to what we currently have. There is still some emphasis on the server and treating computers as a utility that is not currently present but with the rise of Google Docs and other online tools, that is not such a distant notion.    This begs the question, why are the predictions so close when it comes to computers but so off when it comes to other technologies? More importantly, why does  Moore's Law  apply to transistors but not to larger technologies? I have a few ideas:     	 Infrastructure costs - it's cheaper to replace modern day computers than modern day cars. Thus, innovation can happen at a faster pace as people replace their computers. Also, computers tend to work in a much more solitary environment than cars do; being able to drive a car 300 miles per hour is useless when the roads can't take it and the laws prevent it.  	 Experimentation - it's easier for the average person to hack around on a computer than it is to hack around on a car. Thus, a lot more people are working on ideas and due to sheer numbers, more ideas are bound to stick.  	 Brand new technologies breed creativity - When computers were invented, no one knew what they were capable of and everyone had ideas as to how they could be used. Many people pursued their ideas and were able to create and improve on various technologies. Also, many teenagers and students were involved in embracing this new technology - and they didn't know what was impossible so they reached for the stars.Now, everyone is so used to what cars are that people don't even imagine what cars could be capable of. We may be approaching this same plateau with computers.      I'll try to update these when I think of any more.",3,1,2008-10-01,2,"innovation, technology",791,Some thoughts on innovation
21,0,In 2012 I documented the first autocomplete option for each letter of the alphabet and did the same in 2016.,"#meta,#society","{% include setup %} While going through and making sure each of my old posts was AMP compatible I came across a [post from 2012](/2012/06/07/achieving-browser-autocomplete/) where I tried to list the first autocomplete suggestion for each letter. This naturally made me think of what the results would be if I did the same exercise now. Comparing the information 4 years apart is an interesting way to see how my habits have changed but also provide a glimpse into the evolution of companies, products, and technology. The biggest surprise is how much of the list is work related - it’s somewhat expected given how much time we spend working and how many more cloud services there are but it’s still shocking that almost half the list is work related. The other major realization is that much of my consumption has shifted to mobile - many of the sites that are no longer on the list I actively use on my phone; I may actually use Instapaper, Google Maps, and Twitter more frequently now but it’s mostly on mobile via an app. Given how interesting this exercise was I plan on doing this annually and encourage others to do the same - it’s an extremely simple way to see how technology and our relationship to it changes over time.      2012  2016  Notes      analytics.google.com  amazon.com  I don't care as much as I used to about site metrics but to make up for it I'm shopping more frequently.    bankofamerica.com  betterworks.com  We've been using BetterWorks to manage team and personal OKRs.    cad-comic.com/cad  console.aws.amazon.com  Another work product - need to make sure everything's still up and running.    docs.google.com  drive.google.com  Just a domain change.    eventbrite.com  -  An internal Sentry installation to help us track errors.    facebook.com  football.fantasysports.yahoo.com  Start of football season but not sure what would have replaced this.    glos.si  github.com  Glossi is no longer around but I spend a ton of time on GitHub now.    heroku.com  hellofresh.com  Might be based on recency since I was cancelling my account last week.    instapaper.com  interactivebrokers.com  I'm mostly using the Instapaper app now.    joinblended.com  jira.com  Need to maintain our agility.    klout.com  kafka.apache.org  Apparently I spend a lot of time read Kafka docs.    linkedin.com  localhost:4000  This is Jekyll which powers my blog.    maps.google.com  mint.com  This is an interesting one. I use Google Mpas more frequently than Mint but it's mostly on mobile or typing an address in directly or incognito.    news.ycombinator.com  news.ycombinator.com  One of the few that stayed the same.    optimum.com  opentable.com  We no longer have Optimum and booked a dinner reservation recently.    plus.google.com  -  This is a work domain that's just not very well secured.    questionablecontent.net  questionablecontent.net  Another one that stayed the same.    reader.google.com  -  Another inernal work domain.    startupmullings.com  suntrust.com  I went from blogging about startups to paying a mortgage.    twitter.com  triplelift.atlassian.net  Our Atlassian installation.    udacity.com  usetallie.com  Another work site to submit expense reports.    voice.google.com  vettery.com  A work site to help recruiting.    wixlounge.com  wrike.com  Not many sites starting with w. We tried Wrike out before using JIRA.    xkcd.com  xkcd.com  Another one that stayed the same. I'm loyal to my comics.    youtube.com  youtube.com  Same here. I don't use YouTube much but not many other sites starting with a y.    zerply.com  zillow.com  I'm such an adult.",1,2,2016-09-10,4,"autocomplete, browser, internet, technology",547,Comparing my top sites: 2012 vs now
25,0,Living in NYC it's tough to see that retail is dying but I came across a sign this morning that drove the point home.,#society,"{% include setup %} I keep hearing that retail is dying and while I do believe that to be true it’s been difficult to see that from  living in the NYC area. The city streets are always bustling and while there are always some empty commercial spaces they seem to always get rented within a month or two. Yet this morning on my way to the office I came across a legitimate sign, literal and figurative, that brick and mortar retail is on its way out.      This was a small store that primarily sold t-shirts and they posted a sign that the physical location is closing and the only storefront that’s left is online. Beyond that, they made a point to note that they have both a store on Amazon as well as on their own domain. Remarkably, they list the Amazon url first. If this isn’t a sign that Amazon is taking over I don’t know what is.",0,1,2017-08-22,3,"death of retail, ecommerce, amazon",176,Death of retail
40,0,There are tons of tools to learn a new programming language from simple exercices to walkthroughs to large open source projects but the best way is to have your own project that you've implemented across a variety of languages.,#meta,"{% include setup %} At the beginning of the year I wanted to learn a bit of Node.js and decided the best way was to code up a simple project. The idea was  jsonify.me , a simple API only app that provided people a simple way to generate their own JSON profiles that they would then be able to map to any domain name, for example  http://json.dangoldin.com . The primary goal was to get some real experience with Node.js rather than rely on some walkthroughs and tutorials. Since then I’ve used it as the starter project to learn new languages. I’ve coded it up in Scala and have just finished up the Go version.  The project has a few nice properties that force me to gain a pretty good understanding of the language and how a typical project plays out. Despite being a pretty simple program it touches a bunch of modern web components. The code needs to be able to parse and modify HTTP requests and headers in order to support redirection and authentication. In addition, the code comes with a working LinkedIn OAuth example and gives an opportunity to incorporate an OAuth library. The other big thing is integrating the AWS S3 client library which provides a simple way to get exposure to the AWS ecosystem.  Everyone who’s learning new languages should have a “go to” project. It’s okay to go through a series of tutorials to get the basics of a language but nothing beats working on a project you’ve already done across a variety of other languages. In addition to coding up a project in a new language you get a feel for the way the program is structured and laid out. Over time you start getting an intuitive feel for how one language works compared to another and can understand the tradeoffs between them. Having your own project also allows you to optimize towards the skills you want to learn - in my case I wanted them to be focused on the web and allow me to work with the various HTTP elements as well as a few third party libraries. Tutorials and walkthroughs are great to get a feel for the language but they don’t force you to think through the design or architecture which are critical when working on larger projects. It’s amazing how much effort that takes up when learning a new language and the only way to learn it is to experience the frustration of doing it.",2,1,2015-10-11,3,"learning to code, new programming languages, software design",426,Have a 'go to' project when learning a new programming language
34,0,"I ran into a weird bug today where a database record was updated but would sometimes revert back to older values. As with most Heisenbugs, this turned out to be a timing/concurrency issue.",#code,"{% include setup %} I ran into an odd bug today where a database entry was reverting itself after a seemingly simple update. For  Better404 , a customer can change the design of their 404 page but it turns out that every once in a while a change would go through but within a minute would revert back to the previous value. At the same time, update queries run directly via the MySQL client ran fine and were not being reverted - just the ones made through the site. To see what was going on, I enabled full query logging in MySQL (SET GLOBAL general_log = 'ON') and sure enough I saw a lagging query that would update the record to the prior values. Stepping through the code I was able to figure out the cause.  As with most  Heisenbugs , it turned out to be a timing/concurrency issue. As part of a user updating their settings, we would kick off a job to crawl and index the modified domain. After running, the job would update the record with the newly crawled timestamp. Unfortunately, the other fields were updated as well and since this indexing job was kicked off before the record was updated, stale data was written to the database. After figuring out the cause, the fix is easy. One option is to kick off the indexing job after the database is updated with the new values so that the indexing job will use the new values. The second option is to modify the indexing job to only update the relevant fields. Both options were trivial to implement so I played it safe and did both. The first required changing the order of some lines and the second was just specifying an optional parameter to the Django model’s save method. Below’s a visual representation of what was happening and the two fixes.      Unfortunately, concurrency and timing bugs tend to be the most difficult ones to figure out but whenever there’s non deterministic behavior they should be at the top of the suspect list. It’s important to know the tools we’re using and their default behavior - it’s possible that the approach they take differs from how we think they work and becomes a major source of bugs and frustrated debugging efforts.",2,1,2014-06-07,3,"bugs, hacking, coding",403,Debugging a reverting database update
38,0,As we get larger platforms it's going to be increasingly important that they stay open and allow any third party to integrate. Otherwise we run the risk of helping these big companies entrench themselves more and more.,#society,"{% include setup %} I [set up](http://dangoldin.com/2016/02/21/amazon-echo) the Amazon Echo over the weekend and have been an active user of my wife’s Spotify account which comes integrated with the Echo. I would have preferred to use my Apple Music account but the Echo currently only supports Spotify. I suspect the biggest reasons are competitive - Amazon and Apple are competing for the home and it’s likely that either Amazon doesn’t want to integrate Apple or Apple is preventing Amazon from getting the integration done. At the same time Amazon has a music offering yet they specifically call out the Spotify integration. Is this because Spotify is only a competitor for music and the value of an Echo trumps this? Is it because Spotify has more reach and this is a necessary integration? I’m sure the answer is a bit of both but it’s fascinating to see how these partnerships develop.  Ideally every company would provide an open way for others to integrate their apps but we live in a competitive, capitalist world where every company wants to get an edge over their competitors. As consumers it’s up to us to push for the integrations we want and make sure these platforms stay as open as possible - otherwise we’ll end up making the rich richer and prevent new entrants from even having a chance.",1,1,2016-02-22,5,"amazon, apple, competition, monopoly, business",225,Platform partnerships
33,0,It feels as if Google Voice is still in the dark ages of the web. Both the frontend and the backend seem as if they haven't seen any improvements in a while.,,{% include setup %} Nearly all the conversations with my family is in Russian and phone calls are no different. The fun happens when I miss a call and it goes to voicemail. Turns out that despite the amazing job Google does in transcribing English calls it fails terribly at Russian. Instead of realizing that it’s not English it ends up with transcriptions such as “douche nozzle booster.”      Given Google’s expertise in machine learning and their massive data sets I’d expect them to at least be able to identify a non-English language. My guess is that Google Voice is no longer a priority and may not even be under development at all. I had a little over a hundred unread messages I needed to mark as read. With Gmail you get the option of applying an action to the entire selection - not just what’s visible - but with Google Voice you have to go through it page by page. And there’s no way to include more items per page. A tiny bit of modern web functionality did make it through though and I was able to use shortcuts to get the job done relatively quickly. I realize self driving cars are both more exciting and have more potential but I wish there was something being done to improve Google Voice - there’s a ton of us still using it.,0,0,2016-01-17,3,"google voice, google, language transcription",245,"Poor, neglected Google Voice"
16,0,A standard end of the year post to talk about my blog stats over 2013.,#blog,"{% include setup %} Now that I actually have over 100 posts for the year I can actually follow the trend and highlight the most popular ones as well as share some data from my Google Analytics account. This is the first year I’ve seriously committed to blogging and didn’t think I’d enjoy it as much as I did. I will continue to write at least twice a week in 2014 so it will be interesting to see how next year’s data compares against the data from 2013. Thanks for reading and definitely let me know if you have any topics you want me to write about.                                                A general overview of 2013 traffic. I'm honestly surprised by the number of visitors I've had but it's mostly come from a few posts that ended up getting signifcant traffic from Hacker News and Twitter. Note that the bounce rate dropped near the beginning of the year since I added a  Google Analytics event  to consider 15 seconds of being on the site as a ""read"" event.                                                         The obligatory top posts. One thing I discovered is that I am terribly poor at predicting which posts will be the most successful. We'll see if I get better at this in 2014. Here are the links in clickable form:                                                   Why don't cell phones have a dialtone?                                                                   Drowning in JavaScript                                                                   Eighteeen months of Django                                                                   What the SEO?                                                                   Fun with Prolog: Priceonomics Puzzle                                                                   In defense of Excel                                                                   Getting a SIM card in India                                                                   Eighteeen months of Django: Part 2                                                                   The power inbox                                                                   Run Django under Nginx, Virtualenv and Supervisor                                                                   Coke, Pepsi and Passover                                                                   Scraping Yahoo fantasy football stats with Scrapy                                                                                                    Definitely surprised by how significant mobile and tablet traffic was. I imagine these will only increase in the coming years.",16,1,2013-12-30,5,"blog, 2013, year in review, blog stats, blog analytics",510,2013 blog stats
16,0,The weev verdict got me thinking about email addresses and how they are no longer private,#meta,"{% include setup %} Last week, Andrew “weev” Aurenheimer, was sentenced to 41 months for going through publicly accessible AT&T URLs which exposed the email address of 114,000 iPad owners. I don’t want to talk get into the absurdity of the sentence or how AT&T should be the one held accountable for this “ security .”  I’m more interested in the fact that people still consider an email address to be private information (although I do realize that the leak also revealed iPad ownership information). This may have been the case years ago when we arrived on the internet but right now, our email addresses are everywhere. We give it to every new website we sign up for and we display it proudly on our websites. I’m sure my email address appears on dozens of spam lists for sale on the internet. Google already gives 3000 results when I search for my email address.  The definition of what is and is not private changes as a society evolves. Technology has been increasing the pace and society has yet to catch up. Most of the people in the tech world are pretty aware of the trends but the majority of people are surprised by how much information they’re sharing whenever they touch a digital device. And it’s only going to get worse. If we’re this concerned about our email addresses, how will we feel when people use Google Glass to look up our Facebook or LinkedIn profiles just by looking at us?",1,1,2013-03-24,5,"weev, privacy, email addresses, technology, society",255,Email addresses are private?
28,0,I received an email from imo.im earlier this week indicating that they will stop supporting 3rd party chat clients. Amazing what the 19B WhatsApp acquisition did.,#product,"{% include setup %}  Earlier this week I received the following email from imo.im:      It’s amazing what $19 billion can do. For years imo.im has supported third party chat clients but within a couple of weeks of the WhatsApp acquisition they’ve abandoned that support to focus on their own network and become the next WhatsApp. For a while now they’ve been building features to support this move - videos in August, stickers in January - and I wonder what would have happened if they focused on their platform earlier. Now they’re just playing catch up to WhatsApp, Kik, Line, and countless others. Timing is critical and I suspect it’s too late for imo.im to be entering the first party messaging fray.",0,1,2014-03-06,3,"imo.im, whatsapp, messaging",135,Follow the WhatsApp money
27,0,Just a quick write up of how settings files can be handled in a Django project. Things are constnatly improving so this will be outdated soon.,"#python,#code,#devops",{% include setup %} I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.  This got me thinking of the various configurations I’ve gone through over the years and what my latest approach has been. If you have additional variations and suggestions that you’ve been happy with I’d love to hear them.     My first real project had a single settings.py file that defined a different set of options based on the hostname. The benefit here was that every configuration setting was in one file and it was pretty easy to see what the differences were between development and production. The problem is that if the hostname ever changes you may break your entire application. This also makes it difficult to share you code with a team since the hostnames will be different and you end up with a massive file full of different configuration settings. {% highlight python %}import socket  if socket.gethostname() == 'ubuntu':     CONFIG_OPTION = 'prod-setting' else:     CONFIG_OPTION = 'dev-setting' {% endhighlight python %}     A simple improvement was splitting this single file into a common settings file along with separate files for each environment that were then imported based on the hostname. The common settings file would contain the shared settings while the individual environment files would contain the settings unique to each environment. This kept the files cleaner and made it clear what setting applied to which environment. {% highlight python %}import socket  if socket.gethostname() == 'ubuntu':     from settings_prod import * else:     from settings_dev import * {% endhighlight python %}     The previous options still suffered from relying on the hostname so a simple improvement was using symbolic links to point to the appropriate file. With this approach we can still have a common file as well as the individual files but the environment-specific files are importing the shared settings. The big advantage to this approach is that the the symbolic link command only needs to be run once on each server and will always point to the correct file. {% highlight python %}# settings_prod.py  from settings_common import *  # Now set the prod only options CONFIG_OPTION = 'prod-setting' {% endhighlight python %}  {% highlight sh %}ln -s settings_prod.py settings.py {% endhighlight sh %}     Another option that I started using is using the DJANGO_SETTINGS_MODULE environment variable to point to the appropriate settings file. I adopted this approach after reading  Two Scoops of Django  which has a ton of other useful tips that improved my development approach. This approach isn’t significantly different than the symbolic link one but it feels less hacky since it’s an approach supported by the official Django documentation and it’s easier to examine environment variables than looking at the symbolic links across your directory. {% highlight sh %}export DJANGO_SETTINGS_MODULE = project.settings.prod{% endhighlight sh %},1,3,2014-08-30,3,"Django, python, settings files",561,Managing settings files in Django projects
24,0,Sometimes you want to just quickly go from a query to a line chart. I built a simple script to make it easy.,"#code,#dataviz","{% include setup %} Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.  Earlier today I did yet another one of these analysis and got annoyed by how repetitive the process was and wrote a quick script to handle simplest case: a single line chart derived from two columns - each representing an axis. The script works by taking tab delimited data via stdin and then using matplotlib to do a standard line chart. There’s a ton of room for improvement but it fits my standard workflow of using [SQL Workbench/J](http://www.sql-workbench.net/) to execute the query and then quickly copy it over to my clipboard in a tab delimited format.  The [code is up on GitHub](https://github.com/dangoldin/python-tools/blob/master/plotter.py) and it can be executed from the command line by piping the raw data directly into the script. If the data is in the clipboard it’s as simple as typing in “pbpaste &#124; ./plotter.py”. Using this approach I was able to generate the image below as well as the Excel version for comparison. The major improvements are cleaning up the styling so it looks nicer as well as supporting multiple series.",2,2,2016-10-26,4,"data visuaization, command line plotting, command line visualization, command line line chart",293,Simple data visualizations from the command line
28,0,The PSEG site isn't up to date with the latest HTTPS best practices so Google Chrome prevents me from accessing it. We'll see how this plays out.,#meta,{% include setup %} Earlier today I wanted to check up on my electricity bill but ran into an issue trying to login to my PSEG account. Turns out that my nightly version of Google Chrome is preventing me from logging into their site since it has a poor HTTPS configuration. Instead of seeing the login page I get the following message: “Server has a weak ephemeral Diffie-Hellman public key”. Luckily for me this only happened in the nightly build and I was able to login using both the nightly version of Firefox and the standard version of Chrome.            I wonder whether Google’s making the right decision here. What happens when they propagate these changes down to the standard versions of Chrome and countless people start having issues paying their bills. I understand that Google wants sites to upgrade their security but there will be a ton of disruption in the interim. Especially on sites people use to manage their lives. I’m just hoping that the sites that need to upgrade their security do so before Chrome updates itself.,0,1,2015-05-25,3,"chrome, https, security",203,Google Chrome knows what's best for me
34,0,It used to be a best practices to automatically redirect users to the logged in version of a site but I've noticed two sites that are forcing the homepage to be seen first.,#product,"{% include setup %} When I started building sites one of the accepted principles was to give customers what they want as soon as you can. This manifested itself by taking users to the logged in view whenever they navigated to the site’s homepage. This makes sense - if you know a user’s logged in why waste their time by showing them a homepage that’s designed to sell the product?  Yet recently I encountered two sites, [Greenhouse](https://www.greenhouse.io/) and [Tallie](https://tallie.com/), that will default to the homepage and only load the logged in view when I click the sign in link. One argument is that they both have separate domains for the logged in experience - app.greenhouse.io rather than www.greenhouse.io and usetallie.com rather than tallie.com - but there’s nothing stopping them from redirecting to those as soon as they recognize that a user is logged in.  One explanation is that they’re using different domains for the landing page versus the app but it’s still odd. Greenhouse can set cookies at the wildcard domain (*.greenhouse.io) and Tallie can make the necessary redirect or client side check to see whether a user is logged in. In fact if you actually go to usetallie.com first it will redirect you to tallie.com which, after clicking on “Client Login”, will take you back to usetalie.com.  Another explanation is that they want to save on hosting costs and serve a purely static webpage at first. This way they don’t need any dynamic content and only need to have the dynamic logic for when a user wants to login. This seems like a reach though - these are both enterprise apps and can’t possibly have the traffic load to warrant this degradation of the user experience. Even then the cost of doing a simple login check should be enough for any modern web application to handle.  The last explanation I can think of is that there’s something on the homepage that they want every user to experience. And the only thing that would make sense is tracking and advertising. One potential reason is that the content is so sensitive that they either legally can’t or just don’t want to drop third party trackers inside the app yet still want the ability to target and track users who’ve landed on the home page. A preliminary look using Ghostery bears this out - the homepage for Tallie drops 20 trackers while the in app page drops 6, most of which are for analytics. For Greenhouse it’s not as direct with the homepage dropping 17 while the in app page drops 13, most of which are advertising related. If this is the case I’m disappointed, but not surprised, that user experience was sacrificed to drop some third party JavaScript trackers.                              Ghostery trackers: Tallie home + in app, Greenhouse home + in app               I’m searching for other explanations but can’t think of anything that else that would encourage this “anti-pattern” to make a comeback. If anyone has any ideas I’d love to hear them.",2,1,2016-07-16,6,"user experience, login, signin, homepage, greenhouse, tallie",543,Whatever happened to automatic login?
20,0,My blog analysis is taking a while but I discovered a simple way of generating word clouds in R.,"#meta,#code,#dataviz","{% include setup %} Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a [quick script](http://www.r-bloggers.com/building-wordclouds-in-r/) I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.csv and am generating two word clouds - one for keywords and the other for tags of my blog posts.  {% highlight R %} # Install and the libraries install.packages(""tm"") install.packages(""SnowballC"") install.packages(""wordcloud"")  library(tm) library(SnowballC) library(wordcloud)  # Read the file df = read.csv(""/tmp/out.csv"", header=TRUE)  # Now generate the two word clouds. # Most of the work here is removing the unncessary and common words # as well as optionally stemming each of the words. In my case since # I'm plotting the keywords and tags I ignore this step.  corpus     Word cloud of the tags I use.       Word cloud of the keywords I use.",1,3,2016-06-06,3,"word clouds, R, data visualization",311,Word clouds in R
23,0,AIM is shutting down and rather than think about what could have been we should remember how it was in its prime.,#meta,"{% include setup %} Yesterday’s [big news](https://www.theverge.com/2017/10/6/16435690/aim-shutting-down-after-20-years-aol-instant-messenger) was that AIM will be shutting down in December after 20 years of service. This is not surprising - we’ve all moved on from AIM, first to Google Chat and now to the variety of smartphone messaging apps. Yet it’s incredible to think about what AIM was at its peak. I first got an account in middle school and remember it becoming the defacto way to communicate with friends throughout high school and college.  Sure, it’s easy for us to criticize AOL for not being able to capitalize on what they had and losing the messaging market but smartphones disrupted everything. At this point I find it more enjoyable to reminisce about its peak and glory instead of speculating what could have been. It  introduced tens of millions of people to a digital social network and paved the way for every social network since.  I logged in earlier today to see how it’s changed and it was a ghost town. The only others online were people reacting to the shutdown news who were feeling similarly nostalgic. I haven’t logged in in so long that it was a fun game to see if I can identify who the different screen names belonged to - I like to think I got the majority correct but there were definitely a few that I couldn’t remember at all. Despite not having logged in to AIM for the better part of the last decade it’s definitely sad to see it go. At the same time I’m actually surprised that AIM is still around given that no I know has used it in years. It’s just one of those things that is past its prime but has significant nostalgic value. I wonder if anyone would have noticed if it was just quietly shut down.",1,1,2017-10-07,5,"aim, aol, instant messenger, chatting, messaging",314,"Farewell, AIM"
31,0,MixPanel sends an email to login to your account after a few failed login attempts. I wonder if the approach of being able to login solely through an inbox would work,"#product,#design",{% include setup %} MixPanel has a clever way of handling failed login attempts. Instead of locking the user out of the account or forcing a password reset they send an email with two links - one to log in to the account directly and another to reset the password. I don’t recall ever seeing this approach before and wish more sites started doing it. This approach also obviates the need to even have a password - a site can just send a “login link” for an entered email address and the user can login via their inbox. This is similar to the way we login via the various social networks but instead of being sent to a social network for confirmation we are sent to our inbox. The only friction is having to go to your inbox to click on the link but since most people keep their inboxes open all day I don’t see this as a huge problem. The other advantage is security - most people use the same password across multiple sites so if one is compromised the others become vulnerable. Under this approach each site will have its own security controls and it becomes impossible for one site’s shoddy security to affect another’s. This is probably too drastic of a change for most users but I’d love to see sites start embracing this model.,0,2,2014-05-20,4,"security, mixpanel, logging in, design patterns",245,Logging in through your inbox
38,0,Both AWS and GitHub are integral to the modern tech startup yet they charge completely differently. That's not a surprise since they're very different businesses with very different cost structures but it's still useful to think about.,#meta,"{% include setup %} This is a bit of an odd comparison since they offer two very different services with very different cost models but it’s just incredible how much more expensive AWS is than GitHub. It makes sense that GitHub is significantly cheaper since it’s fundamentally just git hosting and it has virtually zero marginal costs to support new customers. AWS on the other hand is bills entirely based on usage and has to allocate the additional hardware for every customer and is definitely not zero marginal cost. Yet they’re both integral in the modern tech ecosystem and are used extensively by companies and startups.  There’s nothing out of the ordinary here and it’s just useful to take a step back every once in a while and think about the tools and products we use, how they fit into our workflows, and how much we pay for each. In this case they’re both incredibly valuable to us yet one is orders of magnitude cheaper than the other.",0,1,2018-12-11,3,"github, aws, hosting",169,The price of AWS vs GitHub
17,0,Supposedly in addition to advertising Quora makes money by selling its Q&A data to AI/ML companies.,#product,"{% include setup %} While browsing a Hacker News thread about Quora I came across the following exchange:      I had always assumed Quora made money through ads and didn’t even consider that they had another revenue stream. The Hacker News thread indicates that they have a substantial revenue stream coming from selling the Q&A data to AI/ML companies but I haven’t been able to find anything else that supports this. Part of the difficulty in searching for results is that just by typing in “Quora” the results are biased towards Quora itself.  If, in fact, Quora does sell its Q&A data it’s an interesting move - it’s similar to Duolingo’s revenue model which gives users free language learning exercises and in turns sells translation services powered by those same users.",0,1,2018-12-23,2,"quora, business model",150,Quora's revenue model
22,0,Over the weekend I updated my old Yahoo fantasy football stats scraper to use Scrapy and wanted to share some thoughts.,"#python,#code,#datascience","{% include setup %} Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the  Scrapy  framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.  The full project is  available on GitHub  but I wanted to highlight a few of the components:    	  parse method : This submits a form POST to the Yahoo login page which authenticates the session. The key point here is to specify a callback function which will continue the existing session. {% highlight python %} def parse(self, response):     return [FormRequest.from_response(response,                 formdata={'login': self.settings['YAHOO_USERNAME'],                 		  'passwd': self.settings['YAHOO_PASSWORD']},                 callback=self.after_login)]{% endhighlight python %} 	  	  parse_stats method : In previous projects, I struggled with separating the crawling from the parsing since the page would have information that would relevant to both - for example I would want to extract information from a page as well as find the next page to scrape. Scrapy offers a nice solution by letting you return different types from the same method. Returing a Request will lead to another page being crawled but one can also returned the scraped structured data via an Item. In the case of the scraper, I return the fantasy football stats on each page via Items but also return a Request when I want to navigate to the next page of stats. {% highlight python %} def parse_stats(self, response):     hxs = HtmlXPathSelector(response)      # Parse the next url     next_page = hxs.select('//ul[@class=""pagingnavlist""]/li[contains(@class,""last"")]/a/@href')     next_page_url = 'http://football.fantasysports.yahoo.com' + next_page.extract()[0]     count = int(RE_CNT.findall(next_page_url)[0]) # Don't go past a certain threshold of players     current_week = int(RE_WEEK.findall(next_page_url)[0])      self.log('Next url is at count {} with week {}'.format(count, current_week))      if current_week   self.settings['MAX_STATS_PER_WEEK']:             yield Request(self.base_url.format(self.settings['YAHOO_LEAGUEID'], current_week + 1), callback=self.parse_stats)         else:             yield Request(next_page_url, callback=self.parse_stats){% endhighlight python %}} 	  	  XPath expressions : In the past, I'd use either BeautifulSoup or PyQuery to traverse the DOM but found XPath expressions to be simpler. There’s less code to write and the expressions are easier to understand and have a higher information density. {% highlight python %} stat_rows = hxs.select('//table[@id=""statTable0""]/tbody/tr') xpath_map = {     'name': 'td[contains(@class,""player"")]/div[contains(@class,""ysf-player-name"")]/a/text()',     'position': 'td[contains(@class,""player"")]/div[contains(@class,""ysf-player-detail"")]/ul/li[contains(@class,""ysf-player-team-pos"")]/span/text()',     'opp': 'td[contains(@class,""opp"")]/text()',     'passing_yds': 'td[@class=""stat""][1]/text()',     'passing_tds': 'td[@class=""stat""][2]/text()',     'passing_int': 'td[@class=""stat""][3]/text()',     'rushing_yds': 'td[@class=""stat""][4]/text()',     'rushing_tds': 'td[@class=""stat""][5]/text()',     'receiving_recs': 'td[@class=""stat""][6]/text()',     'receiving_yds': 'td[@class=""stat""][7]/text()',     'receiving_tds': 'td[@class=""stat""][8]/text()',     'return_tds': 'td[@class=""stat""][9]/text()',     'misc_twopt': 'td[@class=""stat""][10]/text()',     'fumbles': 'td[@class=""stat""][11]/text()',     'points': 'td[contains(@class,""pts"")]/text()', }{% endhighlight python %} 	     This also got me thinking about the evolution of my approach to scraping. In 2006, I was into Perl and scraped using the LWP::Simple, WWW::Mechanize and the HTML::TreeBuilder libraries. After I moved on to Python I switched to using urllib and BeautifulSoup. Most recently, I’ve started using the wonderful requests library along with PyQuery. Conceptually, these approaches are the same: first retrieve a web page and then extract the data you want by traversing the DOM. Scrapy does the same thing internally but by removing a ton of the boilerplate, it lets you focus on the key problems in scraping - figuring out what page to scrape next and figuring out how to extract the content. The rest is handled by Scrapy itself - including file storage, retries, throttling, and probably a ton more that I haven’t gotten a chance to explore yet.  This also gives me some time to work on the actual draft algorithm. My goal is to create a strategy that’s using a value based approach combined with my schedule. The idea is that I shouldn’t pick the players that will have the highest point total over the season but the ones that will have more points during my tough matchups. Of course, it’s almost all luck but I’m still looking forward to attempting this approach.",2,3,2013-07-17,4,"scrapy, scraping, yahoo fantasy football stats, fantasy football",769,Scraping Yahoo fantasy football stats with Scrapy
22,0,I played around with Node.js for the past couple of months and wanted to share some thoughts around my experience.,#javascript,"{% include setup %} I've decided to move on from  Node  after messing around with it for the past couple of months. And while the experience is still fresh I wanted to share my thoughts. I’m far from an expert so take all these with a grain of salt.  - Node’s powerful and in the right hands can make a developer extremely productive. I was able to write a few simple applications surprisingly quickly given my limited knowledge and I can see why so many opt to use it. At the same time it requires a commitment to the Node-centric way which can be tough depending on your background. JavaScript has functional scope and the benefit of Node depends on an asynchronous approach which can be difficult to write. - It’s drastically different from writing client side JavaScript. Instead of worrying about supporting multiple browsers you have to write code that’s maintainable and supports a growing number of use cases. This isn’t that much different from any other backend language but came as a surprise to me since I expected it to be somewhat similar to writing front-end code. - JavaScript is very difficult to write well. Despite (and possibly due to) JavaScript’s pervasiveness it’s tough to find good code. It’s so flexible that it’s easy to get started but that flexibility makes it critical to keep pruning and cleaning your code. Everyone has their own way of writing JavaScript which can be damaging when working as part of a large team on a large application. Many dismiss JavaScript as being an introductory language but a case can be made that it actually requires an expert to do well. Whereas other languages have rules that prevent new developers from making mistakes, JavaScript lets you do whatever you want. - Testing is paramount. Due to JavaScript’s flexible nature it’s important to test thoroughly. When writing Java I rarely have to worry about typos or scope issues since my IDE will let me know immediately but there’s no such luck with JavaScript. I discovered a ton of issues in my toy applications as soon as I started writing tests. - Lots of resources to learn about it online. After committing to working on some Node I was able to find a ton of useful examples and resources online. The community is large and there are a ton of useful libraries on npm but it’s tough to identify the best ones. There seem to be multiple versions of every library and for someone new it can be a bit overwhelming trying to pick the right one to use.  I enjoyed my experience with Node and learned a ton but it’s style and approach just don’t fit the way I work. JavaScript’s lack of structure makes it difficult for me to imagine using it on large, team-based projects. Of course there are best practices to make it work but that’s something that would need to be part of the engineering culture versus something that’s part of the language itself. Node is great for small, experienced teams who want to get an app up and running quickly but if the application has complex logic or will require a large team to maintain I would opt for a more rigid, higher performance language. I’m biased towards the JVM and have recently picked up Scala as my “experimental” language. The goal is to do a similar post on Scala once I get more experience.",1,1,2015-02-22,3,"node, javascript, engineering",579,Lessons from Node
22,0,It's easy to get so obsessed with a technical problem that by solving it you introduce a litany of new ones.,#meta,{% include setup %} A common behavior when solving a coding problem is focusing too much on the solution and not enough on the general context. If this is a software problem this may manifest itself as a very quick turnaround on a task that inadvertantly breaks an existing behavior or even something that ends up causing a headache months from now when a slightly more nuanced use case needs to be supported. Experienced developers will not only solve the task at hand but will also understand the limitations of their solution and are able to identify the areas that will be adversely affected by their solution. Nearly every software decision comes with tradeoffs and strong developers can think through this maze and pick the most appropriate one given the situation.  Part is experience and learning from our mistakes and part is knowing our applications and how the various components interact and fit together. A big driver of this is curiosity - some developers will stop as soon as they find a library that solves a particular problem and will maybe even read the docs but great developers will step through the actual code to understand how it works. Imagine two people working on the same tasks for a year - one who’s curious enough to read through the source code of open source libraries they used and one who doesn’t. I’d bet that the one who was curious enough to read through third party source code learned and retained significantly more than the one who didn’t. Another one is relentlessly thinking in abstractions - thinking at a higher level makes it much easier to spot patterns and identify code that needs to be refactored. This ends up paying massive dividends in the long term when massive scopes of work can be eliminated. One way to get better at this is to constantly reevaluate your work and identify code that’s been duplicated since it may be a sign you chose the wrong level of abstraction. Another one is thinking through what you’d have to do if you needed to make some tweaks in the future - would you be able to reuse the code in a clean way? Which part would be the easiest to modify? Which parts are too coupled to separate easily? The simplest way may be to just look and the code and see if it’s ugly - that may be a sign that you did something wrong.,0,1,2015-07-20,3,"programming, coding, software engineering",407,Tunnel vision
10,0,Just the obligatory post highlighting the top posts of 2016,#meta,"| | /2013/08/26/extract-info-from-a-web-page-using-javascript/ | 3,538     | 3,076            | 0:04:38           | 3,067     | 91.62%      | 85.81% | $0.00      | | /2016/10/02/ios-wifi-security-recommendation/              | 2,628     | 2,546            | 0:02:03           | 2,543     | 97.09%      | 96.80% | $0.00      | | /2013/12/23/getting-a-sim-card-in-india/                   | 1,513     | 1,443            | 0:01:27           | 1,442     | 94.80%      | 93.72% | $0.00      | | /2014/02/10/using-virtualenv-in-production/                | 1,097     | 1,039            | 0:05:09           | 1,035     | 92.17%      | 92.80% | $0.00      | | /2016/01/17/poor-neglected-google-voice/                   | 799       | 764              | 0:03:25           | 756       | 94.58%      | 93.87% | $0.00      | | /2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/ | 756       | 718              | 0:06:35           | 716       | 94.41%      | 94.31% | $0.00      | | /2016/05/08/googles-photo-search-is-eerily-incredible/     | 664       | 643              | 0:01:45           | 632       | 96.52%      | 95.33% | $0.00      | | /2015/09/24/mapping-the-jersey-city-parking-zones-ii/      | 659       | 592              | 0:02:44           | 582       | 90.03%      | 88.01% | $0.00      | | /2016/07/03/snapchats-massive-potential/                   | 633       | 596              | 0:02:32           | 589       | 93.04%      | 92.58% | $0.00      | | /2013/01/09/web-scraping-like-a-pro/                       | 457       | 431              | 0:07:59           | 430       | 91.40%      | 91.47% | $0.00      |",0,1,2017-01-04,2,"blogging, post retrospective",247,Top posts of 2016
26,0,I came across an old post that I was going to write after reading Twitter's API changes last August. Amazing what becomes obvious in hindsight.,#product,"{% include setup %}      I was going through my drafts and stumbled onto one that was going to criticize Twitter’s API changes that they  announced last August . In light of last week’s IPO I thought I’d finally finish it up.  The blog post described the changes Twitter planned on making with the intent of taking control of the developer ecosystem. The changes included being a lot more strict with their API by limiting the number of users a client could authenticate, reducing the volume of API calls, and requiring all Twitter content to be displayed the same way. The post also included a matrix indicating that Twitter did not want anyone developing on the consumer/engagement side but the rest being open.  At the time, many developers (me included) felt betrayed. We made Twitter successful and now we were limited in what we could do. In hindsight, these moves have been obvious. Since then, Twitter’s been rolling out a ton of changes that wouldn’t have been possible without controlling the entire experience. It also gave an indication of their monetization model. At the time, some thought that Twitter would try to monetize by selling access to the data feed or by offering premium features that large brands could use to control their pages. Turns out it was just advertising. By controlling the way every tweet is seen they’ve been able to roll out sponsored tweets and now, images.  I still think they took the easy way out by choosing an obvious business model and yet I can’t really fault them given last week’s result. It does make me wonder whether advertising is the business model of choice for an IPO.",1,1,2013-11-16,3,"Twitter, business model, IPO",302,A Twitter flashback
11,0,Analyzing and visualizing the text of Abraham Lincoln's speeches and writings,"#dataviz,#datascience","{% include setup %}  On Saturday, I finished  Team of Rivals  and while looking at my calendar noticed that it was also  Lincoln's  birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the  7 volume set  containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.  I was able to get 1,458 documents containing almost 16,500 sentences and a little over 547,000 words. I tried getting the date each letter was written or speech was given but was only able to get it for 60% of the documents. That was enough to get some insights.   Number of speeches/letters by year   I suspect a lot of his early writing and speeches and were lost since they just weren't preserved as well as his later speeches and letters        Trend of phrases   I wanted to examine the phrases that he most commonly used over time in order to see whether there were any noticeable changes and whether they meant something. Turns out there was some interesting stuff here that's highlighted in green.           Slavery  - There are references to slavery across the entire date range with the  Dred Scott decision  and the  Missouri Compromise  appearing as common phrases in the 1850s.        Civil War Generals  - You can trace the career of the generals during the Civil War based on their mentions.  General Hooker  was mentioned in 1862 and 1863;  General Meade  in 1863 and 1864; and  General Grant  in 1864 and 1865. This echoes history: General Hooker was replaced by General Meade in 1863 with General Grant being in command of the Union Army in October of 1863.        The Presidency  - When Lincoln was elected president in 1860, he started finishing his letters with the phrase ""Lincoln, President of."" During the presidency we also see mentions of his cabinet:  Stanton  and  Seward .      *The table below was generated by looking at the top 20 three word phrases used in each year range and then consolidated into a top 100 list across the entire dataset. The X indicates that the phrase was in the top 20 three word phrases for that year range. I highlighted the interesting rows in green.            phrase   1832-1845    1846-1853    1854-1859    1860     1861     1862     1863     1864     1865               the united states    X    X    X    X    X    X    X    X    X           of the united    X    X    X    X    X    X    X    X    X           i do not     X    X    X    X    X    X    X    X    X           the secretary of          X    X         X    X    X    X    X           secretary of war          X              X    X    X    X    X           in regard to     X    X    X         X    X    X    X    X           the people of    X    X    X    X    X    X    X    X    X           of the people    X    X    X         X    X    X    X    X           president of the     X    X    X    X    X    X    X    X    X           in favor of      X    X    X         X    X    X    X    X           my dear sir      X    X    X    X    X    X    X    X                as well as   X    X    X         X    X    X    X    X           so far as    X    X    X    X    X    X    X    X    X           dred scott decision                X                                         there is no      X    X    X    X    X    X    X    X    X           by the president          X    X    X    X    X    X    X    X           the supreme court    X    X    X         X         X                     united states and         X    X    X    X    X    X    X    X           of the union     X    X    X    X    X    X    X    X    X           that it is   X    X    X    X    X    X    X    X    X           it is a      X    X    X         X    X    X    X    X           that judge douglas             X                                         the dred scott             X                                         that there is    X    X    X    X    X    X    X    X    X           institution of slavery   X         X         X    X                          secretary of state        X    X    X    X    X    X    X    X           the missouri compromise                X         X                               to say that      X    X    X         X    X    X    X    X           of the state     X    X    X    X    X    X    X    X    X           the state of     X    X    X    X    X    X    X    X    X           of the government    X    X    X    X    X    X    X    X    X           major general mcclellan                          X    X    X                     of the country   X    X    X         X    X    X    X    X           secretary of the          X    X         X    X    X    X    X           of the army           X         X    X    X    X    X                it is not    X    X    X    X    X    X    X    X    X           of the potomac                       X    X    X    X    X           part of the      X    X    X         X    X    X    X    X           one of the   X    X    X    X    X    X    X    X                united states to               X    X    X    X    X    X    X           washington d c                       X    X    X    X    X           house of representatives     X    X    X    X    X    X    X    X    X           as to the    X    X    X    X    X    X    X    X    X           harper s ferry                       X    X    X    X    X           the public safety                        X    X    X    X                major general hooker                              X    X                     the gentleman from   X    X    X                                         lieutenant general grant                                        X    X           major general halleck                             X    X    X                major general meade                                    X    X                of the enemy          X              X    X    X    X    X           the union and         X    X         X    X    X    X    X           the day of   X    X    X    X    X    X    X    X    X           the president of          X    X    X    X    X    X    X    X           the rio grande        X                                              the senate and   X         X         X    X    X    X    X           to the senate              X         X    X    X    X    X           army of the           X              X    X    X    X    X           city point va                                       X    X           and house of                         X    X    X    X    X           executive mansion washington                         X    X    X    X    X           of the treasury           X              X    X    X    X    X           of the secretary                         X    X    X    X    X           of the bank      X         X                                         of the public    X    X              X    X    X    X                of the war        X    X         X    X    X    X    X           yours very truly               X    X    X    X    X    X                as may be    X    X    X         X    X    X    X    X           he did not   X    X    X              X                          lincoln president of                    X    X    X    X    X    X           m stanton secretary                               X    X    X    X           stanton secretary of                              X    X    X    X           the war department        X              X    X    X    X    X           i shall be   X    X    X    X    X    X    X    X    X           william h seward                    X    X    X    X    X    X           edwin m stanton                               X    X    X    X           for the purpose      X    X    X    X    X    X    X    X    X           general grant city                                      X    X           i have been      X    X    X         X    X    X    X    X           is to be     X    X    X         X    X    X    X                it will be   X    X    X         X    X    X    X    X           it would be      X    X    X         X    X    X    X    X           of all the   X    X    X    X    X    X    X    X    X           of the department         X              X    X    X    X                the post office      X    X              X    X    X    X                the public lands     X    X                   X    X    X                yours of the     X         X    X    X    X    X    X                at p m                            X    X    X    X           grant city point                                        X    X           h seward secretary                  X    X    X    X    X    X           i have no    X    X    X    X    X    X    X    X    X           in relation to   X    X    X         X    X    X    X    X           seward secretary of                     X    X    X    X    X    X           that i have      X    X    X    X    X    X    X    X    X           as follows to    X    X                   X    X         X           dear sir yours             X    X    X    X    X    X                sir yours of     X         X    X    X    X    X    X                dear sir i        X    X    X    X    X    X    X                ought to be      X    X    X    X    X    X    X    X    X           of the is    X    X    X    X    X    X    X    X                Phrase word clouds   I tried visualizing the table above as word clouds but in hindsight don't think it was the best way to display the data. It did give me an excuse to play around with  D3 library  though.       As usual, the code's up on  Github .",12,2,2013-02-12,7,"abraham lincoln, lincoln, speech, writing, essays, papers, natural language processing",1939,An analysis of Lincoln's words
12,0,"There should be more startups doing event recommendations, where are they?",#product,"A few years ago I worked on a startup with the goal of providing local event recommendations. Unfortunately, we were never able to make it work. We focused too much on building new features, didn’t simplify our product enough, didn’t have a focused vision, and didn’t spend time understanding the market. After reading Mark Hendrickson’s  Plancast’s postmortem , I started thinking about the problem again and what a successful approach would look like. As Mark pointed out, it’s difficult to incent people to consistently broadcast their plans. Most people will only plan major events in advance and even fewer will log into a website to note that they’re going to grab beers with a friend in a few hours. One thing people are starting to do is checking in to a venue. Knowing the present is a lot simpler than thinking about the future and smartphones have reinforced this behavior. Foursquare has been riding this wave and apps like GroupMe and Fast Society have also taken advantage.    Instead of trying to change user behavior, it’s easier to leverage existing behaviors but apply it to something else. Foursquare is best positioned to move into the event recommendation space. It has already started providing venue recommendations based on historical checkins and there’s value in knowing that 50 people have checked into a stadium within a few minutes of one another. By integrating with an event database, it’s possible to know what events people are attending. This information can initially be used to recommend venues that have similar events going on. Over time, this can transition into doing direct event recommendations.",1,1,2012-03-30,2,"startups, events",280,Where are the event recommendation startups?
29,0,I modified an old script that used a schedule in a Google spreadsheet to message users to work across multiple tabs and schedules by introducing a meta tab.,#code,"{% include setup %} Back in March I [wrote a script](http://dangoldin.com/2017/03/04/automating-admin-work-spreadsheets-to-slack/) that would go through an on-call calendar kept in a Google spreadsheet and then post the current week’s schedule to a Slack channel. This worked surprisingly well and I thought of doing something similar for the other engineering team calendars. In addition to the on call rotation, we have a dedicated time for internal tech talks as well as a session to cover the news in the industry. To make them easier to manage we keep them all in that same spreadsheet.  It would have been trivial to write a few more scripts for each type of calendar but I wanted something that could keep scaling to whatever new calendars we introduced. By looking at what’s consistent across each of the calendars - dates and people - we can come up with an approach that can scale to new calendars. My simple solution to this was to introduce a “meta” tab that contains guidance on how to interpret each of the calendar tabs. This includes specifying the date column, the column that will contain the names, and an optional message column. With this information it was straightforward to modify my existing script to accept this information as variables rather than being hardcoded to the structure of a specific spreadsheet.  Basically, [this version of the script](https://github.com/dangoldin/automating-management/blob/master/post_schedule.py) treats the Google spreadsheet as a simplified database. It’s able to read and understand what it needs to do yet the spreadsheet itself can be maintained and modified directly by people through the spreadsheet UI. I’m a fan of this hybrid approach and think there’s some really powerful things one can do with this. Spreadsheets are underrated in the tech world and can be leveraged for a variety of power-user cases. Rather than create super specific applications and UIs it might be worth exploring whether you can just build on top of a spreadsheet.",2,1,2017-10-20,6,"google spreadsheet, slack, schedule, calendar, automation, management",330,Schedule automation using Google spreadsheets and Slack
8,0,Some thoughts on using brainteasers in interviews.,#meta,"I've recently been reading some articles opposing the use of brainteasers during interviews on the grounds that they are unfair and some people have difficulty thinking on the spot. You can make the same argument for any part of the interview process and I feel that brainteasers may even attract intelligent employees.    I can come up with a few good reasons to use brainteasers during an interview. One, you are able to determine how well the interviewee thinks as well as their problem solving ability. In addition, if the interviewee does end up getting a job offer, he or she may be more likely to accept it since it was a challenging interview and getting the job feels like an accomplishment - feels better when you have to earn something than when it falls into your lap. The fact that you even asked a brain teaser shows intelligence on your part and you want to attract people who want to work with other smart people, instead of being the big fish in a small pond.    Ideally, you would want to find some brainteasers that have multiple ways of solution so you are able to identify how each of the interviewees thinks but I think a variety of brain teasers can achieve the same effect. Below are few good questions/brainteasers I enjoy.     	 What was the last book you've read? What's your favorite book? (Not a brainteaser but I believe a good question nonetheless)  	 You have a lighter and 2 ropes that are non-uniform. It takes a rope 1 hour to burn from one end to the other end. How do you measure 45 minutes?  	 You have 3 pairs of (x,y) points that determine a triangle. How can you determine if this triangle contains the origin? (from Project Euler)  	 Which is larger, 48736^95934 or 44390^96771? (Also from Project Euler)  	 How do you split a cake of nonuniform size between 2 people? Can you expand this to any number of people?      Feel free to comment or email me if you are unsure how to solve a particular problem.",0,1,2008-05-14,3,"brainteasers, interviews, project euler",350,Brainteasers and interviews
16,0,In what's become an annual tradition I'm sharing the most popular posts of the year.,"#meta,#dataviz","{% include setup %}      I use Google Analytics on my blog and now that the year is almost over it’s time for the annual tradition of sharing the top posts of the year. The total number of pageviews in 2017 was a remarkable 36,410, around the same I received in 2015 and 2016 but below those of 2013 and 2014 when I was both more lucky in the popularity of my posts while and more aggressive in promoting my writing on Hacker News. While I feel my writing has improved over the years I feel my content has atrophied and is something I’d like to correct in 2018. What’s interesting is that out of all the views this year, 17% were from posts written in 2017, 74% were from posts written in previous years, and the remaining 9% were from non post pages. One argument is that as I build up a larger stable of content it will keep growing in percentage every year. At the same time I should start seeing an overall growth in views which hasn’t happened over the past few years. A worthy goal is to keep growing the total number of views of previous years’ posts while also making sure the current year’s are driving even more views and become the evergreen content for the future.  On that note here are the most popular posts of 2017 - both written and viewed.  ## Most popular posts written in 2017     Page  Pageviews  Unique Pageviews  Avg. Time on Page  Entrances  Bounce Rate  % Exit      /2017/04/11/sql-is-the-perfect-interface/  2559  2481  169.74  2474  97.01%  96.64%    /2017/03/26/fulfillment-by-amazon-counterfeiting/  328  319  84.93  315  97.14%  95.73%    /2017/04/02/slacks-channel-exit-anti-pattern/  296  291  46.00  288  96.18%  95.61%    /2017/06/20/getting-amp-into-rss/  233  36  75.60  36  72.22%  14.59%    /2017/06/26/rss-finally-fixed/  213  19  71.31  18  55.56%  8.92%    /2017/02/21/advice-for-coding-bootcamp-graduates/  203  193  84.26  184  91.85%  88.67%    /2017/10/09/downloading-your-aim-buddy-list/  166  147  408.00  144  90.97%  84.94%    /2017/04/16/amp-and-subscription-paywalls/  155  135  67.48  133  94.74%  86.45%    /2017/05/04/security-across-multiple-aws-regions/  99  99  104.00  96  96.88%  94.95%    /2017/05/10/using-options-to-play-snapchats-quarterly-results/  99  97  64.50  97  95.88%  93.94%    /2017/08/08/google-docs-vs-confluence/  90  90  107.38  84  96.43%  91.11%    /2017/01/26/shame-on-united-and-bank-of-america/  77  75  171.25  69  95.65%  89.61%    /2017/02/28/lessons-learned-from-todays-s3-failure/  75  70  80.33  60  88.33%  84.00%    /2017/01/16/powering-our-devices-using-the-human-body/  72  67  41.25  66  83.33%  83.33%    /2017/07/04/thoughtful-code/  59  44  79.55  40  90.00%  62.71%    /2017/07/23/the-wild-world-of-online-trackers/  58  32  25.41  30  93.33%  53.45%    /2017/03/19/refactor-driven-development/  56  54  186.00  52  94.23%  94.64%    /2017/02/04/identifying-product-weaknesses-using-google-autocomplete/  51  51  2.00  49  100.00%  98.04%    /2017/02/19/math-is-incredible/  51  36  71.23  25  76.00%  49.02%    /2017/02/26/my-snapchat-investment-strategy/  45  37  192.47  26  88.46%  62.22%      ## Most popular posts viewed in 2017     Page  Pageviews  Unique Pageviews  Avg. Time on Page  Entrances  Bounce Rate  % Exit      /2013/08/26/extract-info-from-a-web-page-using-javascript/  6964  6576  379.93  6564  94.65%  94.10%    /2017/04/11/sql-is-the-perfect-interface/  2559  2481  169.74  2474  97.01%  96.64%    /2015/09/24/mapping-the-jersey-city-parking-zones-ii/  1602  1411  154.03  1377  88.96%  87.64%    /2015/05/26/dealing-with-a-stripped-screw/  1335  1287  272.61  1285  96.42%  96.33%    /2014/02/10/using-virtualenv-in-production/  968  935  442.22  932  96.46%  96.28%    /2015/04/23/adding-columns-in-postgresql-and-redshift/  964  930  255.42  929  96.56%  96.27%    /2013/01/09/web-scraping-like-a-pro/  886  854  154.84  854  95.67%  95.82%    /2014/07/15/set-up-https-on-ec2-running-nginx-without-elb/  849  819  412.23  819  96.83%  96.47%    /2013/06/21/where-are-you-on-the-sales-matrix/  797  734  251.21  732  92.08%  91.72%    /2014/02/05/visualizing-gps-data-in-r/  689  610  211.50  609  88.18%  88.10%    /2013/12/23/getting-a-sim-card-in-india/  448  400  586.48  400  94.25%  88.84%    /2017/03/26/fulfillment-by-amazon-counterfeiting/  328  319  84.93  315  97.14%  95.73%    /2015/05/26/dealing-with-a-stripped-screw/?usqp=mq331AQCCAE=  320  312  522.44  312  97.12%  97.19%    /2014/10/01/normalizing-a-csv-file-using-mysql/  318  306  480.83  302  96.36%  96.23%    /2015/04/06/redshift-meets-excel/  311  307  33.50  305  98.36%  97.43%    /2017/04/02/slacks-channel-exit-anti-pattern/  296  291  46.00  288  96.18%  95.61%    /2015/04/26/aws-service-limits/  271  265  546.78  263  97.34%  96.68%    /2015/09/24/mapping-the-jersey-city-parking-zones-ii/?usqp=mq331AQCCAE=  270  216  106.56  216  69.91%  69.63%    /2014/09/20/dealing-with-an-rds-replication-issue/  244  234  452.00  234  96.15%  95.90%    /2016/01/10/cleanest-way-to-read-a-csv-file-with-python/  238  229  384.55  228  96.49%  95.38%",0,2,2017-12-24,4,"blog pageviews, blog stats, year in review, google analytics",575,Top posts of 2017
16,0,I kept daily stats throughout 2016 and this is a visual summary of that data.,"#meta,#dataviz,#code","{% include setup %} A hallmark of blogging is to do a year in review post with every blogger having their own distinct style. Some write about their tops posts, others about the lessons learned, some focus on the books read or places seen. I’ve been keeping meticulous daily stats around the hours slept, my physical and mental states over the course of a day, as well as the food, coffee, tea, soda, and alcohol consumed and the review is an opportunity for me to summarize and visualize this data. The goal is to identify healthy and unhealthy trends over time and use that information to make changes in my life. At the moment the stats are mostly high level summaries but what I want to do is use this data in order to identify hidden relationships in order to improve my physical state and mental moods. This is a work in progress but I hope to do more of that this coming year as well as improve the way I’m gathering this data. The analysis [code is up on GitHub](https://github.com/dangoldin/annual-stats-analysis) with a guide and a sample file that can be analyzed. And now on to the data:                              Sleep duration box plot. Average of a little less than 7.5 hours but a decent range.                                       Daily consumption of coffee, tea, alcohol, and soda box plot. I'm happy with the soda consumption but disappointed with the coffee and alcohol. They are very close to last year's numbers and I wanted to reduce them both. Something I'm going to focus more on in 2017.                                       Weekly consumption of coffee, tea, alcohol, and soda box plot. Similar to the above but visualizing the data rolled up by week.                                       Breakfast wordcloud. It's not obvious how to analyze text so I figured a wordcloud was a decent attempt. I really need to avoid string cheese and move towards smoothies.                                       Lunch wordcloud. Clearly I love my Chipotle burrito bowl but need to veer away from it in 2017.                                       Dinner wordcloud. Looks as if dinner is the healthiest part of my day.                                       Snack wordcloud. This one is all over the place but string cheese is pretty dominant.                                       Alcohol wordcloud. I like my beer and wine with a pretty even split of white and red and a tiny bit of rose.                                       Alcohol (Lagged) vs Sleep Duration. This was an attempt to dig into a relationship I expected to be true: when I drink or go out I tend to sleep less. The intuition here is that I will stay up later if I'm out and will thus go to bed later. This holds pretty true although very statistically weak. Note that to do this I needed to shift the data by a day since my sleep duration is noted on the day that I woke up.",1,3,2017-01-02,3,"quantified self, year in review, data visualization",670,Year in review: 2016
38,0,It's surprising to see our users hack around our products to solve this problem. This is a great way to help us understand the limitations of our products and build new features to support new use cases.,#product,"{% include setup %} As engineers, it's easy to get focused on technical problems and lose sight of the business. We realize our code will be used externally but we have a tendency to focus on what's close to home rather than the actual real world usage. One of the biggest eye openers for me has been seeing people interact with our products.  We like to think of ourselves as ""hackers"" but it's amazing to see the length people go to ""hack"" our products to do what they want. Whether it's someone keeping multiple tabs open to be able to reference information back and forth and avoid losing data or someone registering multiple accounts to bypass a database uniquness constraint - it's a way for people to bypass the intended design and I'd argue that these ""hackers"" are a sign of a useful product. In fact, I'd argue that if people aren't hacking around a product's limitations it's not a good one. These workarounds are a sign that the product is so useful that people are willing to go through additional manual effort to use it for a different use case. If that's not a sign of new functionality to support I don't know what is.  And the best way to understand these workarounds is to talk to our customers and collect as much information as we can. Guessing people's intentions isn't helpful but being able to identify an anomalyous flow and then talking to that person is a great way to understand the intention and the workaround. This insight can then help drive product direction and innovation.",0,1,2015-09-13,2,"product management, product development",268,Workaround driven product development
25,0,With the rapid change of pace in technology we need longer government term limits to provide more stability in order to create better policies.,#meta,"{% include setup %} Many have written about society’s inability to enact laws quickly enough to deal with the current pace of technological innovation. Governments are still trying to figure out how to regulate the sharing economy with both AirBnB and Uber being reacted to rather than being effectively regulated. This leads to different treatment in different locations and causes confusion for consumers, the businesses, and regulators.  A potential way to rectify this is to actually increase term limits for people in government. With politicians focusing on reelection every few years and constantly moving in and out of office it’s tough to develop a consistent regulatory approach. This worked well a hundred years ago when new industries would take a decade to develop and you could regulate them as they grew. Now it can take a year for entirely new businesses to be created before governments can react to what’s happening. By then the new consumer behavior has become entrenched and becomes difficult to change. Rather than worrying about reelection and undoing prior policies politicians should be focused on the future and how to adapt government for an increasingly changing society.  It’s counterintuitive that to deal with rapid change you want slower government turnover but it makes sense. Imagine a football team changing coaches for every game or a company switching CEOs every year. They’d be too busy dealing with the leadership change to win games or grow as businesses. Stability is necessary in rapidly changing environments and governments need to adapt to provide that in modern times.",0,1,2016-02-13,4,"government, politics, policy, society",259,Longer terms in government?
20,0,My experience building a pseudo static site where the static site is hosted on S3 and is generated dynamically.,"#code,#aws","{% include setup %} Reading  Katie Zhu’s post  on NPR’s news app architecture got me curious about a setup where most of the content is static and can be hosted on S3 and EC2 is primarily used to generate the static content which is then uploaded to S3. The benefits were obvious:      Cost:  S3 is cheaper than EC2.    Reliable:  S3 doesn’t go down near as frequently as EC2.    Scalable:  Since it’s primarily static you don’t have to worry about additional capacity or dealing with caching, databases, and all the other fun things.    Simpler:  There are no weird server issues here. As long as you generate the right content and your rendering is good, you don’t need to worry about a web server acting up.     I’ve been meaning to write a script that would scrape Hacker News in order to show me the top content I missed while sleeping. I had some time this weekend and decided to give it a go using this “pseudo-static” approach. The result is called Yet Another Hacker News Reader ( YAHNR ) and you can take a look at the code on  GitHub . Turns out it was pretty simple to write and the most difficult part was thinking differently about the problem. Whereas I’d keep the content in a database I ended up storing them in static JSON files and instead of having the logic to generate the HTML page live on a web server I have it using Mustache templates.  I’ve become a fan of this approach and think every developer should try it out. It offers a new perspective and most apps will have some components that’ll be able to leverage this sort of setup. Right now, if you run a static blog and want comments, you can use  Disqus . You can use  Firebase  to build entire web apps that do all the work on the client side. As more and more services become available via Javascript, this approach becomes more and more practical.",5,2,2013-03-12,4,"S3, EC2, static site, web server",359,Mmmm... pseudo static sites
24,0,The Yahoo fantasy football app logs me out but doesn't force me to enter a password to log back in. What's the point?,#design,"{% include setup %} Now that fantasy football season is over for me and I have no risk of angering the fantasy football gods I can complain about an interface decision in the Yahoo Fantasy Football Android app. Every once in a while the app will sign me out, which I suspect is a security feature, but I can log back in without having to re-enter a password. The only effect this “feature” has is getting me annoyed. The app has clean and simple visual design but that shouldn’t be prioritized over actual usability. Hopping between apps is such a common task that developers should strive to make it as painless as possible. This may involve changing the views around to make them more light weight or figuring out a way to simulate behavior without having to show a loading screen but it definitely makes the app feel snappier and more responsive.",0,1,2014-12-16,3,"usability, ux, design",154,Why log me out?
18,0,A few years ago I worried about content overload but these days it's no longer a problem.,#meta,{% include setup %} While going through my list of blog post ideas I found one from a few years ago titled “Content Overload” which was meant to highlight how much content I was bombarded with across a variety of channels. I had a bunch of thoughts bemoaning how difficult it was to keep up but these days it’s just not an issue for me. I’m no longer trying to consume everything I can and instead get a lot of my news from conversations with friends and coworkers as well as Twitter. I try to check in on Hacker News every day but occasionally go on stretches where I forget about it for a few days and no one’s the wiser. I sadly gave up on my RSS feed and stopped checking into the other aggregators. Important news will inevitably make its way to me and the less important news I enjoy finding serendipitously. I wouldn’t be surprised if my content consumption actually dropped over the past few years. I’m reading many more books now and am okay with just not keeping up with everything. The one digital content channel that has gotten stronger is email: I’ll subscribe to interesting newsletters and prune these aggressively to make sure my email stays clean and high signal. I may be unique here but it seems I’ve managed to cope with the challenge of content overload.,0,1,2018-12-09,3,"digital content, social media, information overload",234,Avoiding content overload
14,0,A quick JavaScript tool to make it easy to compare two SQL schemas.,"#sql,#code,#devops","{% include setup %} During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn't necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state. In the past I would just run a simple query (select table_schema, table_name, column_name from information_schema.columns;) on each environment and then use either Excel or Google Sheets to spot the differences. This takes a bit of time so this weekend I put together a quick  JavaScript tool  to automate the process. You simply run the schema query on each of the environments and paste the resulting rows into the two text areas. The result is a JSON based diff showing the additions, deletions, and modifications to each of the tables and fields. The next step is to modify it to also identify differences in the column types.",1,3,2015-07-12,3,"sql, schema, compare sql schemas",194,Comparing SQL schemas
22,0,I share some analysis I did on investing in tech stocks now vs decades ago and share my current investment approach.,#finance,"{% include setup %} I initially set out to write a post to complain about how difficult it is for an average investor to “hit it big” these days by investing in a tech company at its IPO but ended up changing my thesis after digging into the data. It’s still possible to get the same returns as it was in the 1980s but it’s not possible by a long-term investment in a single company.  To start, I came up with a sample of large tech companies and looked at their performance since their IPO as well as their annualized return.                    Company  IPO Year  Total Return  Annualized Return                         Apple  1980  12,250%  16%             Microsoft  1986  33,800%  24%             Cisco  1990  60,800%  32%             Yahoo  1996  1,808%  19%             Amazon  1997  14,894%  37%             Ebay  1998  2,817%  25%             Netflix  2002  2,564%  34%             Google  2004  708%  24%             LinkedIn  2011  86%  -7%             Facebook  2012  36%  -64%             Tesla  2010  407%  60%            From this limited sample, it looks as if it’s still possible, but more difficult, to get the annualized returns of the 1980s and 1990s. It’s impossible to know whether the total returns will be comparable but I suspect that it’s going to be extremely difficult, if not impossible. To get a Microsoft-like return, Tesla would need to end up with a market cap of $760 trillion, excluding inflation.  In a way, this is obvious. As more and more money pours into the VC industry, companies can afford to stay private longer and just keep on raising more funding. This gives company management and investors more control, keeps the company leaner, and limits public information. Unfortunately, this leads to most of the growth occurring before the IPO with retail investors not able to capture any of the value.  While I’m optimistic that the  JOBS Act  will help, we unaccredited investors still need a way to invest right now. After leaving my finance job, I tried to replicate the traditional investment approach by doing research, analyzing statements, and reading coverage. But over the past few years I’ve been too busy and have just been investing in companies that I use and like. Unsurprisingly, this new approach has led to me invest in tech companies. Surprisingly, I’m doing better than ever before. Over the past 2 years, I’ve bought stock in three companies: Tesla, Netflix, and Yahoo with the lowest gaining 66%. I realize these returns can’t keep on going so the question becomes when to sell and invest in something else. For this, I’ve been looking at market caps compared to other companies in the same industry to estimate their potential. In my case, Tesla has a market cap of $11B while Audi’s is $26.6B and Toyota’s is $191B, and definitely has room to grow. The standard disclosure when giving financial advice is “past performance is not an indication of future results” and it’s definitely true in this case. I’m just glad I found an approach that suits me.",1,1,2013-05-24,5,"stocks, stock market, investing, IPO, venture capital",470,Investing in tech stocks
22,0,The rise of food trucks shows capitalism at work. Failed construction food trucks turned into a whole new type of cuisine.,#meta,"{% include setup %}                             Source:           EcoVeganGal.com                       Food trucks have taken over every city I've been to. A decade ago the best you could find was a taco truck but now food trucks run the gamut from the simple taco up to experimental vegan. Priceonomics has a  great piece  on the rise of the food truck as well as a fascinating look at the economics of the food truck industry. If you haven't read it yet definitely check it out.  The summary is that the decline of the housing market in 2008 led to a drop in construction which caused many food trucks to sold at a discount. These food trucks were then bought by aspiring chefs who wanted a low risk way to start a restaurant. The cheap trucks gave chefs a way to experiment with varying cuisines and locations without incurring the massive costs of starting a restaurant.  This is a perfect example of how capitalism should work. Industries that are no longer profitable make way for new ones that leverage existing infrastructure. In the future these food trucks will transform into miniature carriers that will carry drones that will deliver food throughout cities. When one door closes, another door opens.",2,1,2014-07-23,3,"food trucks, capitalism, business",250,"When one door closes, another door opens"
22,0,Code reviews are often focused on style and syntax but strong teams realize there's a lot more to it than that.,#code,"{% include setup %} Every modern software development process contains some form of code reviews. They ensure that all code is looked at by someone other than the author. This improves context, increases code quality, and generally leads to a stronger team and product. Yet there’s a world of difference in code reviews and I I’ve started to think of the different types as a pyramid. The peak is at the highest level but it’s not possible to get to that without going through the lower tiers. Another observation is that the lower levels can done by an individual but the penultimate ones require a team effort.  - **Style**: This is what I suspect most people consider a code review and ranges from feedback telling the author to add comments, fix typos, or rename variables. This is important to get right since it’s the foundation that everything else rests on but at the same time it’s not going to change any of the functionality. - **Code**: At this level the feedback is meant to change the code itself - if only ever so slightly. This includes proposing alternative implementations to methods or ways to clean up the code to make it easier to read and test. At this level an experienced engineer may also point to an existing method or library that can be reused to avoid code duplication or push for larger methods to be split into smaller, more testable ones. - **Architecture**: If done right the architecture would have been determined before any code was written but often times an issue or idea pops during or after the code has been written. Sure it’s possible to ship the code as is but if it’s a critical or key component it’s  advantageous to clean it up - even if it forces you to redo the way the app is organized and architected. - **Philosophy**: I’ve found that the highest functioning teams coalesce around a code philosophy. They have a shared and implicit understanding of the codebase and understand where and how features should be implement. Rather than focusing on the low level implementation details they’re thinking about where the functionality should exist to make sense given the rest of the code and how to keep things consistent. Teams that have spent a long time working together often start doing this organically but it’s something that all teams should strive for.  Most people associate code reviews with the first level of the pyramid but that’s just the beginning. To achieve the best code we need to push ourselves and our teams to think deeper about the code and not focus solely on the superficial factors.",0,1,2017-10-27,2,"code reviews, code style",440,The code review pyramid
