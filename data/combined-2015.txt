{% include setup %} Over the past few weeks I’ve been experimenting with Node.js and wanted to share the project I’ve been working on,  jsonify.me . It’s an “API only” product without an interface other than a  documentation page . The idea is to allow anyone to have a publicly accessible URL endpoint that can contain whatever information they want as long as it can be stored as a JSON object. In my case, I have all sorts of semi-structured data that I want to make accessible and keeping it under my domain ( json.dangoldin.com ) makes it easy to access for whoever is savvy enough to figure it out.  The code is still in the early stages and needs to be cleaned up but the core functionality is there. It works by uploading each user’s JSON object to S3 and then doing a redirect whenever a GET request is made to that user’s subdomain. In my case I registered an account with jsonify.me, got an authentication token, set json.dangoldin.com as my subdomain, updated the CNAME record of json.dangoldin.com to point to domains.jsonify.me, and then made a POST request to json.dangoldin.com with my JSON object which uploaded it to S3 under my username.  I love the idea of a semi-structured format evolving over time as more and more people standardize around common field names as well as a set of third party apps that will migrate data from other services into these JSON objects. Imagine being able to have a service that will authenticate with LinkedIn and then extract the relevant data and put it as JSON into your object or a service that fetches new posts from your blog and adds them to your JSON object. There’s still a long way to go and the code right now is very minimal but I’d love to see people give it a shot and suggest improvements.
{% include setup %} I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.                              Has hashtag         No hashtag                             Has mention         171         251                     No mention         237         181               - Avg impressions vs hashtag and mention (excluding @replies): Idea was to investigate whether tweets with hastags or mentions end up being due to the fact that they are more likely to appear in search results. The results are a bit weird since it seems as if having a hashtag only helps if there wasn't also an @ mention. Otherwise it hurts.                     @mention         # tweets         Total engagments         Total impressions         Avg engagement rate         Engagements / Impressions         Avg impressions                             No         446         2192         89714         2.7%         2.4%         201                     Yes         385         914         58112         2.9%         1.6%         151               - I suspected that sending someone an @reply would reduce total impressions but increase the engagement rate since it's directed at someone. It does reduce the average impressions and only leads to a slight increase in engagement rate - and only when looking at the average of rates, not the total engagements over total impressions. I suspect most people don't differentiate between an @reply and an @mention which doesn't lead to a significant difference in actual engagement rates.                     Has mention         # tweets         Total engagments         Total impressions         Avg engagement rate         Engagements / Impressions         Avg impressions                             No         331         1524         62850         2.6%         2.4%         190                     Yes         115         668         26864         2.8%         2.5%         234               - If we exclude @replies and compare tweets with and without mentions, the tweets with mentions have both a higher average number of impressions and a higher engagement. Nothing surprising here - @mentions are good since they draw attention to a tweet while @replies hurt since they limit total reach. Still nice to have some data to confirm.                     Has Link         # Tweets         Total engagements         Total impressions         Engagement rate         Avg impressions                             No         426         990         59385         1.7%         139.4                     Yes         405         2116         88441         2.4%         218.4               - Tweets with links have higher engagement - most likely since there's a stronger call to action. Again this isn't surprising but nice to see it backed up by a bit of data.
{% include setup %} I’ve discussed the pros and cons of having a codebase out of a few languages versus having the choice made per project or application with a bunch of people and opinions differ. On one hand, having many languages provides flexibility in choosing the right language for the job and allows engineers to learn and explore new tools. Better habits are encouraged since the interface between components requires a well structured and tested structure rather than relying on code similarity. On the other, it prevents code and component reuse and makes it difficult for teams to standardize around a style and codebase. Also, engineers can’t switch projects as easily as they’d be able to under a common language and prevents the depth of knowledge one gets from working on a shared codebase with others.  I used to think the flexibility of being able to choose the right language for the job was the only thing that mattered but now prefer a limited language codebase. With modern languages, libraries, and tools it’s easy to write good code in any language and there are only a few applications that warrant a specific language, and even then only at scale.  My perfect mix is a static, strongly typed language for large, complex codebases that have high performance needs, a scripting language for one off tasks and processes and for quick experimentation, and JavaScript with a framework for the frontend. The static, strongly typed language makes it much easier to refactor code and improves performance while the scripting languages make it easy to quickly get something working or prototype an idea. My current favorites are Java and python - they’re both easy to write and complement each other’s weaknesses. Something I haven’t had a chance to explore in depth is moving to Jython or another JVM based scripting language - that would provide the benefits of the highly functional and robust Java code from a scripting context.
{% include setup %} At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year. I tracked the data via a Google spreadsheet and exported it as a CSV in order to analyze it via a  simple Python script . For now I’ve only pulled some summary stats but will take a deeper look in the next couple of days to examine the distributions and identify any patterns.  - After removing days without data, I’m left with 354 days for the year. - Out of these days, I was in a good mood for the vast majority (89%) with the remaining days a combination of being sick, getting sick, having a sore throat, or being congested. - I slept an average of 7.15 hours each night with the most being 11 and the least being 3. - The most popular breakfast items where cheese (53 days), eggs (45), smoothies (32), bagels (25), with the rest being a various combination of fruits and various dinner leftovers. - Sandwiches were the most popular lunch item (63 days), followed by soup (42), Chipotle burrito bowls (32), salads (28), and Sophie’s (23), a nearby Cuban restaurant. The remainder were food places that hovered around the teens. - Dinner was a lot more basic. I had salad at almost a third of my meals with the main dishes being dominated by protein: chicken (43 days), fish (33), turkey (20), salmon (16). My favorite starches were potatoes (34), pasta (27), and rice (16). - I drink too much coffee averaging 1.3 cups a day. The most coffee I’ve had on a single day was 3 cups which happened on 6 days. The goal is to drop this below 1 a day in 2015. - I drink too much alcohol also averaging 1.25 drinks a day. Beer (259) and wine (143) made up 91% of my drinking with the rest being a combination of mixed drinks, cocktails, and hard liquors. Similar to coffee, I’ll try to drop the average to below 1 a day in 2015.  Despite being a simple exercise there’s a lot of interesting information that can help me improve in 2015. By looking at the data more and examining relationships between various fields (mood and drinking, drinking and sleep, etc) I’ll hopefully find even more ways to live healthier and better.  On a related note, I’m repeating the same exercise in 2015 but am modifying the template a bit in order to collect more structure data. I’m going to be tracking the times I sleep and wake up rather than the duration and decided to put all the foods under a single column rather than per meal to make it easier to include snacks. The other big change was splitting my mood columns into a physical and a mood component to measure them separately. Looking forward to seeing what I discover in 2015.
{% include setup %} Numerous times I’ve procrastinated on doing something convincing myself that it would either take too long or just wasn’t worth doing but more often than not when I finally take the first step I’m able to quickly complete the task. I don’t know why our minds encourage procrastination but I suspect it’s not just me. I’ve been combating this tendency by recognizing that it’s happening and forcing myself to just do something, as simple as it is. A small task typically turns into a series of small tasks in which I’m able to make a significant amount of progress. In fact, there have been numerous times where I’ve even been able to achieve “flow” - despite being hesitant in the first place. It doesn’t seem like much but 10 minutes here and there do add up. Whether it’s coding up a simple feature, doing a quick data analysis, or just jotting down a few ideas, it’s infinitely more valuable than staring at a phone.
{% include setup %} After discovering  GitHub's map visualization  feature I needed to give it a shot on the only GPS dataset I had available, my runs from RunKeeper. Unfortunately, the RunKeeper files were in GPX while GitHub expects either geoson or topjson. A short  Python script  later and I was able to convert the GPX data into  geojson . The other hiccup I encountered was that the generated geojson file was too large for GitHub to visualize. My 232 runs contained 162,071 latitude/longitude pairs which turned into a 4MB file - not massive but large enough for GitHub to refuse to visualize it. The simplest solution was to generate multiple files but that made it impossible to see all my runs on a single map. The other solution was to see if converting to topojson would reduce the file size. That helped but I wasn't able to find the right balance between compression and quality and ended up with a hybrid approach - two files, one per running year, each in topojson.             -->  The entire process was painless and quick. The geojson format was straightforward to generate and GitHub does a great job rendering it. The entire process took an hour and I had to read the  topojson utility docs  to figure out how simplification worked. One thing I didn't get to do was explore GitHub's map diffs but will try to in the next couple of weeks.
{% include setup %} At  TripleLift , we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins. Unfortunately, we saw that the aggregate data didn’t match up with what we saw in the logs and after some investigating we discovered that we were seeing some duplicate rows generated on the client side. Taking a deeper look it turned out that there were some plugins and scripts that were making duplicate requests to our analytics server.  There may be ways to deal with this better on the client side as well as smarter backend logic to deal with potential duplicates but the easiest fix is to just assume you will have messy data and prepare accordingly. In our case it entailed writing more complicated queries that were robust enough to not require clean input. It took a little bit longer to write and design but our pipeline can now handle weird input without impacting the final results. If you’re designing systems that collect and use data from the client you need to make sure your backend code is capable of dealing with the inevitable trash.
{% include setup %} A fun exercise is picking a random day in the past and trying to recreate it using the various tools at our disposal. In my case the most useful ones are my calendars, both personal and work, the photos I took, and Foursquare/Swarm. As long as I was vigilant in documenting the events it’s simple to figure out what I did. We lose a bit of the mystery when we document our lives and we no longer have long discussions trying to recreate events with friends. I don’t know whether this is better or worse but we’ll probably see more and more of this happening. Our phones are already collecting our location and video is becoming increasingly popular. Now we have high fidelity versions of recent events but only vague memories of our childhood. I wonder whether kids that are growing up now will have access to accurate memories of their childhood when they grow up and what the impact will be.
{% include setup %} Given the  rumor  of the massive IBM layoffs I decided to pull some others and see how it compared. Surprisingly, the next highest was also at IBM - but in 1993. On one hand, it's odd to see this pattern as if they've learned nothing. On the other, it's a sign that they acknowledge the problem and are willing to adapt. Since the 1993 layoff IBM's stock price increased over 950% and this round may provide another burst.  On a side note, the largest layoffs are available online but I wasn't able to find a non ad-ridden, non-paginated table so hopefully others find these useful.                     Number         Company         Year         Layoffs                             1         IBM         1993         60,000                     2         Sears         1993         50,000                     3         Citigroup         2008         50,000                     4         General Motors         2009         47,000                     5         AT&amp;T         1996         40,000                     6         Ford         2002         35,000                     7         Boeing         2001         31,000                     8         USPS         2010         30,000                     9         Bank of America         2011         30,000                     19         HP         2012         27,000                     11         Daimler Chrysler         2001         26,000
{% include setup %} A fun little exercise I had to do was rewrite a simple application from Node.js to Netty to fit into the rest of our stack. The rewrite took a couple of days but the deployment and testing was critical to get right so I wanted to share our approach. To provide some context, the application was an HTTP server that handled ~1,000 requests a minute with each request spawning at most three more to pull in more data.  Make it work. Make it right. Make it fast. The statement is attributed to Kent Beck but I've become a huge fan and try to approach all projects with this mindset. In our case, having a product deployment already available made it simple to test. We would just run some production requests against our new code and compare the responses. If they matched then we knew we did the right thing. Note that if you're on AWS and running an ELB, a simple way of getting HTTP requests without touching any code is through  access logs . Amazon will store each of the requests to the ELB to a file on S3 that you can then easy download and parse.  The next step was making sure it could handle the same load as the old application. The first thing was to run a series of  Apache benchmarks (ab)  so we can get a rough idea of the concurrency and performance on a single request. As part of this test we turned off the caching layer in our application to hobble it as much as possible since if it could handle that, it could handle anything. The final step was using a  script to simulate the requests in the ELB access against the new server and see how it behaved.  The deployment turned out to be the easy part. All we had to do was launch a new server behind a new ELB and swap the DNS record to point to it. We did this for a few short periods before swapping it back so we could do a few final checks before leaving the DNS record pointing to the new ELB. After a couple of days we eliminated the old ELB and instances completely. The chart below shows the transition between the two load balancers.
{% include setup %} At  TripleLift , we have a migrations job that copies aggregate data from Redshift to MySQL so it can be accessed along the rest of the transactional data. As part of a test, I tried comparing that the data matched exactly but ran into an issue when exporting the data to select. Namely, to make the comparison as simple as possible I wanted to run the same select query in both tables and compare the results. Unfortunately, the sort order between MySQL and PostgreSQL (what Redshift is based on) acts differently for text fields. PostgreSQL takes case into account while MySQL does not. This has an especially weird results when you have values that contain characters with an ASCII code between the lower and upper case letters: [\]^-`. It took some research but I discovered that MySQL provides an option to do a case sensitive sort - just add a “BINARY” option before the field name.  The following   queries demonstrate this behavior - all but the BINARY one can run on both MySQL and PostgreSQL.  {% highlight sql %} CREATE TABLE test_table ( t varchar(5) );  INSERT INTO test_table (t) VALUES ('a'),('b'),('c'),('d'),('e'),('f'),('g'),('h'),('i'),('j'),('k'),('l'),('m'),('n'),('o'),('p'),('q'),('r'),('s'),('t'),('u'),('v'),('w'),('x'),('y'),('z'),('A'),('B'),('C'),('D'),('E'),('F'),('G'),('H'),('I'),('J'),('K'),('L'),('M'),('N'),('O'),('P'),('Q'),('R'),('S'),('T'),('U'),('V'),('W'),('X'),('Y'),('Z'),('0'),('1'),('2'),('3'),('4'),('5'),('6'),('7'),('8'),('9'),('['),('\\'),(']'),('^'),('_'),('`');  SELECT * FROM test_table ORDER BY t ASC;  -- MySQL only SELECT * FROM test_table ORDER BY BINARY t ASC; {% endhighlight %}
{% include setup %} At  TripleLift , we’re big fans of the  Switcheroo  plugin and rely on it during development to test new versions of our code. It allows us to override a production hostname with one of our development boxes so we can see how our code works on a live site. So if a production site is referencing a JavaScript file at http://production-environment/script.js we use Switcheroo to have it reference the development file at http://dev-environment/script.js. Unfortunately, it’s only available for Chrome which makes it more difficult to run browser specifics tests on other browsers.  To deal with this problem we came up with a small redirection app that runs locally and is browser agnostic. Instead of entering the desired host to redirect in the extension, you add it to the local  hosts file , mapping it to localhost. This bypasses the DNS lookup and sends all requests to that domain to the locally running server which then serves a redirect to the desired URL. The  code’s up on GitHub  with a readme that should hopefully be easy to follow.
{% include setup %} Over the past week I've been learning Scala. The initial motivation was that our API code is in PHP and in dire need of a rewrite. And since we've been rewriting our other critical applications in Java we want to leverage the JVM as much as possible. At the same time, we want to keep the code simple, expressive, and maintable while being fun and easy to write. I've heard great things about Scala so decided to give it a shot.  My first step was to install the  Play framework  and play around with the examples but I quickly discovered that while I could understand and tweak it, I needed a better Scala foundation to actually work on a real project. One of my favorite ways to learn a new language is to go through the Project Euler problems in a new language. The problems are a fun balance between mathematics and computer science and gradually build up in complexity which aligns itself well with the build up in my coding skills. The other big benefit is that solving the problems gives you access to other peoples' solutions which you can use to improve your code and knowledge. Normally looking at other people's code isn't the most impactful but in this case since you've already solved the problem you have the background to actually absorb the new patterns and styles.  So far, I've found Scala fun and expressive but tend to write it in a Java-like way. I'm definitely seeing significant changes in my style though. The very first solution looks just like Java while the most recent one is definitely functional. The plan is to work on a few more problems this weekend and then take another stab at the Play framework.
{% include setup %}            Yesterday I got a reminder of how deep smartphones and tech companies have gotten into our lives. After spending a day volunteering at the [C4Q](http://www.c4q.nyc/) office, I got a text from my wife asking me where I was. When I opened the Hangouts app I saw an option to share my current location. This is the first time I’ve seen a contextual behavior in Hangouts. I’ve seen it before in other apps - a Gmail alert telling me I forgot to attach a file when the text has “please find attached” or Google Calendar defaulting to a weekly repetition if I put “weekly” in the meeting title - but this is the first time I’ve seen it happen in Hangouts. Basic contextual behavior is relatively simple to support and can just require a simple word search but it has incredible potential as more and more data gets collected. Our smartphones are with us wherever we go collecting data each step of the way. Right now the behavior is formulaic and standardized but soon enough our phones will act as personal assistants - keeping track of everything in our calendars while understanding everything we have going on. This has the potential to drastically simplify our lives but we may be making a Faustian bargain in the process.
{% include setup %} I've decided to move on from  Node  after messing around with it for the past couple of months. And while the experience is still fresh I wanted to share my thoughts. I’m far from an expert so take all these with a grain of salt.  - Node’s powerful and in the right hands can make a developer extremely productive. I was able to write a few simple applications surprisingly quickly given my limited knowledge and I can see why so many opt to use it. At the same time it requires a commitment to the Node-centric way which can be tough depending on your background. JavaScript has functional scope and the benefit of Node depends on an asynchronous approach which can be difficult to write. - It’s drastically different from writing client side JavaScript. Instead of worrying about supporting multiple browsers you have to write code that’s maintainable and supports a growing number of use cases. This isn’t that much different from any other backend language but came as a surprise to me since I expected it to be somewhat similar to writing front-end code. - JavaScript is very difficult to write well. Despite (and possibly due to) JavaScript’s pervasiveness it’s tough to find good code. It’s so flexible that it’s easy to get started but that flexibility makes it critical to keep pruning and cleaning your code. Everyone has their own way of writing JavaScript which can be damaging when working as part of a large team on a large application. Many dismiss JavaScript as being an introductory language but a case can be made that it actually requires an expert to do well. Whereas other languages have rules that prevent new developers from making mistakes, JavaScript lets you do whatever you want. - Testing is paramount. Due to JavaScript’s flexible nature it’s important to test thoroughly. When writing Java I rarely have to worry about typos or scope issues since my IDE will let me know immediately but there’s no such luck with JavaScript. I discovered a ton of issues in my toy applications as soon as I started writing tests. - Lots of resources to learn about it online. After committing to working on some Node I was able to find a ton of useful examples and resources online. The community is large and there are a ton of useful libraries on npm but it’s tough to identify the best ones. There seem to be multiple versions of every library and for someone new it can be a bit overwhelming trying to pick the right one to use.  I enjoyed my experience with Node and learned a ton but it’s style and approach just don’t fit the way I work. JavaScript’s lack of structure makes it difficult for me to imagine using it on large, team-based projects. Of course there are best practices to make it work but that’s something that would need to be part of the engineering culture versus something that’s part of the language itself. Node is great for small, experienced teams who want to get an app up and running quickly but if the application has complex logic or will require a large team to maintain I would opt for a more rigid, higher performance language. I’m biased towards the JVM and have recently picked up Scala as my “experimental” language. The goal is to do a similar post on Scala once I get more experience.
{% include setup %} I’m a pretty new engineering manager but a philosophy I’ve adopted is to try to have everyone on the team be as full stack as possible. Everyone has their strengths and weaknesses but being able to grasp the entire stack improves code quality and reduces disruption. And it goes beyond technology and into the business and user world too. Understanding how these various components fit together allow you to make smarter decisions and provide the tools to test and verify your code. The other big benefit is that you’re not waiting on anyone and avoid having your flow disrupted by others.  As an example, imagine having an ecommerce website when you get the idea that you want to start tracking the amount of time people are spending mousing over your product images. The goal is to see whether this behavior is correlated with sales which will give you more data to drive an upcoming redesign. Clearly there will be front-end JavaScript involved since that will be triggering the event but there’s also a lot going on behind the scenes. Depending on the number of events you expect to see you can have a wildly different implementation. How do you want to handle multiple mouseovers over the same image? What data do you want to capture? What kind of analysis will you want to run? How will this data be tied back to the sales data? Where will this data be stored? Will there need to be any additional processing to make the data usable? How will you test the data flow? What needs to happen for you to deploy it? How much additional load will this put on the production system?  These questions can all be answered by looping in various people but understanding the business case and the full tech stack makes you more independent and increases the likelihood that the first version will be the final version. In addition to having the necessary language skills, I’d love to see every web engineer know how to set up a VPS from scratch, be comfortable with the command line, have a basic understanding of SQL and databases, and understand the various components of the web and how they fit together.
{% include setup %} I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.  {% highlight javascript %} // Also let caller specify max size of list function addItem(k, v, limit) {   var a = getItems(k);   a.push(v);   if (!isNaN(limit)) {     while (a.length > limit) {       a.shift();     }   }   localStorage.setItem(k, JSON.stringify(a)); }  function getItems(k) {   var a = null;   try {     a = JSON.parse(localStorage.getItem(k));   } catch(e) {}   if (a && Array.isArray(a)) {     return a;   }   return []; }  // Tests/Examples localStorage.setItem('test_list', null);  addItem('test_list', {"name": "Dan"}); addItem('test_list', {"food": "pizza"}); addItem('test_list', {"beer": "Newcastle"});  var l = getItems('test_list');  console.log('Lengths match: ' + (l.length === 3)); console.log('Value 0 matches: ' + (l[0].name === 'Dan')); console.log('Value 1 matches: ' + (l[1].food === 'pizza')); console.log('Value 2 matches: ' + (l[2].beer === 'Newcastle'));  addItem('test_list', {"size": 2}, 2);  l = getItems('test_list');  console.log('List limit works: ' + (l.length === 2)); console.log('Value 0 matches: ' + (l[0].beer === 'Newcastle')); console.log('Value 1 matches: ' + (l[1].size === 2)); {% endhighlight javascript %}
{% include setup %} Something that’s incredibly helpful when writing Java code is customizing  log4j . There are a variety of configuration options and learning just a little bit about them can make you notably more productive. I’ve found two features that have sped up my development cycles.  One was updating my PatternLayout to include the filename and line of each message. With Eclipse, this allows me to quickly jump to the relevant code block whenever anything looks odd rather than having to first open the file and then search for that particular message.  {% highlight properties %} log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d %p (%t) [%c] (%F:%L) - %m%n {% endhighlight properties %}  The other was to pick the appropriate log level at the package level. If I’m working on a single package I'll reduce the logging level of other packages to make the relevant messages stand out. This is especially handy when you incorporate eager third party packages that drown out your own messages with their own.  {% highlight properties %} log4j.logger.com.dan.package.one.logging=WARN log4j.logger.com.dan.package.one.logging.ClassName=INFO log4j.logger.com.dan.package.two=DEBUG log4j.logger.com.dan.package.two.working_on=TRACE {% endhighlight properties %}  My style of development is to rely on logs more than the debugger so these two have made my life a lot easier. In general. logging is an important tool for all developers and yet few tend to tweak the default settings. By understanding the available configuration options you’re able to tweak them for whichever problem you’re solving. This may not seem like a huge win but when you’re running the same program hundreds of times a day the small efficiencies add up.
{% include setup %} Earlier today I came across another reason why basic computer literacy is a necessary skill. It’s not just about knowing to code or understanding how computers work but getting access to a slew of tools that are orders of magnitude better than what you’d find on a sketchy site.            My realization came when I was trying to download some YouTube videos that could be watched without internet access. Doing a quick Google search I found dozens of sites with each one trying to force me to download some additional software to “speed up” my experience. I’m positive most, if not all, of these would fall into the malware category so I decided instead to do a simple search on GitHub for “youtube download.” Lo and behold the first result was the wonderful  youtube-dl  library. Within two minutes I kicked off a script that proceeded to download a dozen videos.  I can only imagine what someone without access to GitHub would have done - either given up or downloaded some ridiculous malware that may have worked but would have left their computer in a sorry state. Learning to code is one thing but having a little bit of familiarity with the command line and reading technical documentation would do wonders in helping people solve their problems, avoid malware, and even encourage them to learn more.  Replacing Google with GitHub for any computer related “how to” search may be overkill but I’m going to start doing it. The side benefit of GitHub is that it even comes with a built in review system due to the star and fork system which hasn’t yet been gamed.
{% include setup %} Good tools are essential for developer productivity. Imagine how tough it would be to write code in an editor that didn’t have any of the features we use on a daily basis - syntax highlighting, smart spacing, shortcut keys, auto completion, etc. It takes time to get used to all the tools available but once we’re familiar with them we’re orders of magnitude more productive.  For the past year my primary language has been Java although I’ve gotten to do a fair amount of JavaScript, Python, and PHP as well. As great as Sublime is, I’m much more productive in Eclipse. It has nothing to do with the editor and everything to do with the language. Eclipse is able to provide a lot more functionality due to Java’s static, strongly type nature. Some examples are being able to quickly rename variables and methods, move packages, and quickly identify dumb mistakes in method signatures and typos. I suspect similar tools exist for weakly typed or dynamic languages but I can’t imagine them working as well as they do in Eclipse. Strongly typed and static languages are able to get rid of an entire class of errors that are common with scripting languages - typos, forgetting to add an argument to a method call, messing up a type - that the time saved typing gets replaced with the time spent debugging. For many tasks this tradeoff makes sense but larger projects that involve multiple developers and require higher performance would benefit from moving to a strongly typed, static language.  I’m learning Scala which seems to combine the flexibility and expressiveness of Python with the safety and performance of Java. So far I’m cautiously optimistic but we’ll see where I am in a month.
{% include setup %} Since it’s Pi Day (at least in the US)I decided to jump on the bandwagon and contribute my own thoughts. Pi is fascinating. It’s such a simple definition - the ratio of a circle’s circumference to it’s diameter - yet it’s both irrational and transcendental and impossible to actually express as a simple number. People have been trying to get more accurate estimates for multiple millennia with multiple great mathematicians trying to derive their own approximation.            Srinivasa Ramanujan's Pi approximation     Looking at a Wikipedia article for [Pi approximations](http://en.wikipedia.org/wiki/Approximations_of_%CF%80) is itself overwhelming. How Ramanujan was able to come up with his approximations is tough to understand - they seem so ridiculously esoteric that it’s hard to imagine someone was able to come up with such a formulation. Since then there have been improved approximations and Pi’s been calculated to 12.1 trillion digits. I can’t think of any real reason why we’re spending countless computer cycles to get better approximations but that’s the allure of Pi: incredibly simple to explain while being infinitely expressive.
{% include setup %} I recently discovered that I’m a much worse speller than I used to be. The culprit is obvious - computers make it too easy to correct our mistakes. In school when making mistakes we’d have to rewrite each word until it became ingrained but these days all we do is just click on the suggested fix without a second thought. There’s nothing there to help me retain the mistake so I continue making it.  This realization made me uncomfortable so I’ve adopted the hybrid strategy of retyping every word that I make a spelling mistake on. It’s never going to be as good as using pen and paper but it’s much better than picking a word from a dropdown menu. There are tons of behaviors that technology has made obsolete but that should not be the reason to abandon those skills. For the same reason that having math skills helps us in daily lives it’s important to hold on to the basic skills we learned as children.
{% include setup %} When it comes to productive coding, one of the most important things to do is to impose a set of standards and conventions. As long as you stick with them your code becomes significantly easier to write and maintain. Conventions range from having a standard way of declaring variables to the way files are organized within a project to the field names in database tables. The obvious benefit is that your code becomes significantly easier to navigate, both to you as well as to others on the team, since you don’t have to run through a series of searches trying to figure out whether a variable is called myVariable, MyVariable, or my_variable. The bigger impact is how much simpler your code becomes. By using a standard structure it’s possible to write code that’s further up in the abstraction hierarchy. This is a huge win for productivity and quality since  more code leads to more errors  and the best code is code that’s not written in the first place.  Two examples of how we’ve adopted conventions include:  - Making sure that every database table in our “log” schema has a timestamp column containing timestamps and every table in our “agg” schema has a ymd column containing dates. This allowed us to write an abstract job that aggregate the data from a log table to an agg table without having to worry about the underlying structure. All we had to do was specify the columns that were the keys and which ones needed to be aggregated - the job itself took care of the scheduling, the query construction, and the reporting. In addition, we were able to quickly write up a simple job that archived old log data. The job doesn’t care what table it takes as long as it has a timestamp column. - We use RabbitMQ for some of our asynchronous tasks and we’ve developed a standardized format that a majority of tasks share. These tasks take a name, a date, and an hour and then run a query for that hour. By imposing this structure, we were able to write a single block of code that would take tasks with a start and end date and republish them as a series of hourly tasks in the date/hour format. Since each task takes the same arguments, we’re also able to use reflection to automatically create an instance of the appropriate class for each task. For example, the task {“task”: “do_an_agg”, “ymd”: “2015-03-17”, “hour”: 10} automatically gets translated into new DoAnAgg(“2015-03-17”, 10). All we need to do is make sure the class DoAnAgg exists, has the appropriate constructor, and exists in the proper package.  Both of these examples are straightforward but the value comes in coming up with the proper abstraction that avoids unnecessary code. Standards make it easy to spot repeated patterns which can then be refactored upstream. This improves the leverage of everyone else on the team and makes every engineer more productive. People idolize the mythical 10 or 100x engineer but there’s more value in making the entire team more productive.
{% include setup %} The sharing/rental economy is getting stronger and stronger and will have a massive impact on societies - especially cities. One thing that’s been on my mind is how it will fit into human behavior and biases. We’re so used to making infrequent or one time payments and then owning something that moving to a rental or sharing model might be difficult. For example, I don’t own a car and mostly rely on a combination of public transportation and CitIBike to get around. The rare times I need a car I’ll use either Zipcar, Hertz 24/7, Lyft or Uber depending on my exact situation yet each time I make the decision I can’t help but think about the cost. I realize that in the grand scheme of things it’s much cheaper than owning a car but during the moment itself it’s draining. It’s similar to the of unbundling TV - it’s much cheaper to just pay a dollar per episode to watch a TV show than pay more than $100 each month for cable but do people actually want to be thinking about spending the dollar each time? I suspect most would rather pay the premium for the entire bundle and the option of watching anything instead of feeling as if they’re being nickeled and dimed. I have no idea whether this is innate in human behavior or something that we’ve just grown accustomed to. I suspect it’s the latter - there are countless items we pay for individually and don’t think twice about it. What will make the sharing economy universal is when we start treating the majority of our purchases as per use rather than a lifetime subscription.
{% include setup %} I can’t figure out why nearly every website forces you to login after resetting your password. It’s an extra step that adds nothing to security and introduces friction into the experience. The fact that I just entered my password into a form field should be enough to trigger the authentication flow and get me back into the app. The only reasons I can think of that it’s a way to confirm that the person actually remembers their new password or that the functionality just hasn’t been built. The former case doesn’t make sense - the fact that they forgot their password indicates they rarely use the site and will just forget it again by their next login attempt. It’s easier to just give them the immediate access and have them reset their password later. An even better approach would be to just have them enter the same password twice to make sure they match. The latter reason is just sloth - the engineering effort would be minimal and it would improve the experience and mood of the users who are already frustrated after multiple failed login attempts.
{% include setup %} Both Meerkat and Periscope launched on iOS first. That doesn’t bother me despite have an Android phone. They’re running a business and it’s up to them to decide where they want to invest the time they have. What bothers me is that I’ve been using a specific username across the various services, dangoldin, and now run the risk of losing it on these newer networks. A simple fix would be to at least allow me to preregister my username without requiring an iOS device. This would also encourage me, and a lot of other Android users, to download the app when it finally does make its way to Android.  This is a first world problem but as more and more people start building their brands it’s useful to have a single name that spans across services and networks. It makes it easier for your audience to find you and lets you transfer audiences from one to another. There’s always going to be a way to login with Twitter or Facebook but I like having my networks divorced from one another with my username serving as the only link.
{% include setup %} Great tools have the potential to make us significantly more productive and I wanted to share my existing setup. A huge part of that productivity is our comfort with our tools since over time we learn the shortcuts, understand the capabilities better, and develop processes to solve common problems. The challenge is that there is always a tool that might be better but the learning curve is too steep to warrant a time investment. Here's what I have so far.  - Google Chrome Canary + FirefoxDeveloperEdition: I like being on the bleeding edge so use the nightly builds of both browsers. My preference would be to use Firefox for everything but I'm more familiar and comfortable with the Chrome dev tools.  - Google Calendar: I'll use this for both scheduling meetings as well jotting down deadlines and todos. It's been working great and I haven't felt a need to use anything else. I tried using a few apps but wanted something that had a better integration with the rest of the Google suite.  - Fastmail + Gmail: We use Gmail at work but my preference is for Fastmail. It's cleaner, simpler, and faster than Gmail and reminds me of what Gmail was when it first launched. Within both Fastmail and Gmail I strive to achieve "inbox zero" with varying success.  - Google Docs: Whenever I need to write anything non code I'll reach for Google Docs. The interface is simple to use and I like the cross device sync. The only time I'll move away is when I don't have internet access or I'm taking random notes.  - Excel: I tried using Google Spreadsheets but Excel is significantly better for larger scale projects and I'm too comfortable with the keyboard shortcuts I picked up during my maangement consulting and finance days. My ideal solution would be a spreadsheet interface built on top of a language such as R. Then I'd be able to use the spreadsheet component for the quick and dirty work, pasting data into it, doing simple transforms, etc and dive into the R for the more serious quantitative work.  - Terminal with zshell and ohmyzsh: Being comfortable with the terminal is vital for developers. It's the primary way to interact with external servers and knowing the various commands and scripts allow us to quickly diagnose and fix problems. The add on I use is zshell with the ohmyzsh configuration since it comes with a nice set of bells and whistles - git integration, useful highlighting, ..  - Sublime Text 3 + SFTP: For scripting progrmas my editor of choice is Sublime Text. It's surprisingly snappy and lightweight while providing a lot of flexibility for third party plugins. One of these plugins is SFTP which allows me to sync local files over to a remote server. I do a lot of my development work on an EC2 instance so being able to save them remotely is a huge productivity boost. I used to use Evernote for note taking but have switched to using text files in Dropbox. This allows me to organize them the way I want and leverage the command line to find exactly what I'm looking for.  - EC2: At TripleLift, each developer gets their own EC2 instance to be used for development. This both mirrors the production environment better than OS X would and allows us to make our sites publicly accessible to other developers. The other nice piece is that it interfaces nicely with the other AWS products, namely S3. Transferring files from S3 to EC2 is much quicker than going from S3 to a local computer. The two major constraints are that it's command line only and tends to be less performant than our local machines.  - Eclipse + IntelliJ: For Java I'm using Eclipse and for Scala I'm using IntelliJ. I've been coding Java for a lot longer and am much more familiar with Eclipse. It's possible that I'll move to IntelliJ at some point but for now my projects allow me to keep them separate.  - Git on GitHub with Hub: No surprise here. GitHub makes it easy to collobarate with others and I'm a big fan of the interface. The only annoyance I had was being unable to open pull requests from the command line but I've since found Hub which provides a command line interface to GitHub.  Would love to hear of other tools that people find useful.
{% include setup %} As part of our data pipeline, we have a Redshift agg job that takes low level data and rolls it up to an hourly aggregate. A latter job takes the hourly data and rolls it up to a daily level which is used for high level reporting and summary statistics. Earlier this week we ran into a hiccup that caused some of these aggregate jobs to fail. After fixing the issue we had to figure out what data was affected and rerun it. We wrote a simple query to count the numbers of rows per day per hour in order to spot any gaps.  {% highlight sql %}select ymd, hour, count(1) as cnt from hourly_agg_table where ymd >= '2015-04-01' group by ymd, hour order by ymd, hour; {% endhighlight %}  This gave us a dataset with three columns that we wanted to then “pivot” in order to quickly spot the gaps. Using the pivot table functionality in Excel, it was simple to put date along one dimension and hour along the other to quickly spot the missing agg periods. All that was left was rerunning the job for those hours.            This investigation reminded me how important it is to be familiar with your tools and choose the right one for the job. Redshift and Excel are antithetical - Redshift is massively parallelizable and built for terabytes of data while Excel slows to a crawl when dealing with tens of thousands of rows. But by mixing them together we’re able to use each for what it’s best for: Redshift for very quick, large scale queries and Excel for the quick and dirty investigative work. This approach is useful in all sorts of problems - from mixing command line scripts with fleshed out programs to using a script or Excel to generate commands that you can then paste into the terminal or an editor. The key point is understanding your workflow and tools well enough to come up with an optimized process.
{% include setup %} I’m not sure whether this is a recent issue but earlier this week I started noticing that many HTTP requests to  Google's CDN  were taking close to a minute to complete. In particular, this blog would take almost a minute to render since it uses two fonts and an old version of jQuery both hosted by Google.            After some investigation it turned out that the issue seemed to only happen on Chrome Canary (43.0.2351.3 canary (64-bit)) and even occured when visiting the URL directly. Neither standard Chrome, Firefox, Firefox nightly, nor a simple curl requested had this issue - it seemed to be a purely Chrome Canary issue.  I didn’t spend a ton of time investigating the root cause since it seemed to be browser specific but ended up implementing two simple solutions to deal with the problem. One was self-hosting jQuery (via GitHub pages) and the other was loading the fonts asynchronously using Google’s JavaScript implementation. This allows the content to load without having to wait for the fonts or jQuery to be available. This will occasionally cause a bit of a flicker as the text gets redrawn with the newly loaded font but I prefer this to have my site not load for nearly a minute.
{% include setup %}            Old Spice ad mocking the uncanny valley (Credit:  Gizmodo )     The  uncanny valley  is this idea that although we keep getting better at depicting people through technology, a few small kinks ruin everything and make people feel repulsed compared to an obvious imitation. Another way to explain it is that we’re a lot more comfortable with cartoon characters that are obviously fake than pseudo-realistic video game characters that look real but have non-human behavior or expressions.  I think this also exists in advertising. I work at TripleLift which allows us generate ads that look and resemble a publisher's website. This means that if a publisher's site has a feed layout where each image is 300x300 with a particular font and typography we’ll use the same style for our ad. At the same time, we make it a point to include a "Sponsored" overlay, apply a brand logo, and redirect to the advertiser's website upon a click. Our goal isn't to obfuscate the ad but rather give a publisher an effective way to monetize their content without having to resort to traditional, distracting banner ads that take up the entire page and hurt the consumer experience. The goal is to complement the user experience by providing great looking ads that are relevant to the audience.  It’s likely we’d be able to increase short term performance if we start obfuscating the fact that it's an ad but in addition to being immoral it will fall into some form of uncanny valley. People aren’t stupid and will see something’s off if an ad is pretending to be content. The end result benefits no one and sets advertising back. The site visitor is pissed off by the experience, the publisher loses integrity, and the advertiser may get better vanity metrics without deriving any value. Native is a much better ad format when done right but it comes with risks that need to be properly handled by the industry or we’ll end up with a  depleted field .
{% include setup %} Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.  Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted. In addition to being over my 6 GB limit, the syncing was completely stopped and Dropbox was using up my entire CPU. I had to figure out a way to deal with this while holding on to the scraped data.  Since Dropbox was entirely unusable, I disabled it on my travelling machine and did a bit of investigation with the data I had with the hope of running it on the complete dataset when I got back home to my primary computer. When I finally got back home I saw that the scraping job was still running and had downloaded around 791 thousand files into one folder that totaled 11.7 GB.            The solution seemed straightforward - move the files out of Dropbox into a separate directory and then let Dropbox recover itself. Sadly that didn’t quite work. First, doing a “mv * targetfolder” ended up causing an issue with the globber since there were too many files for bash to handle. The fix was simple - move the entire folder and then rename it to the destination folder - but it took me a few attempts until I stumbled unto it. Second, Dropbox was in such a wretched state that it refused to do anything. The solution here was a bit more involved. I had to log in to the Dropbox site, remove the data from the UI, unlink Dropbox from my computers via the website, and then relink them via the app on the computer.  Two lesson here: do not save your results in Dropbox and when downloading hundreds of thousands of files do not save them in a single folder.
{% include setup %} A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.  Normally, this isn’t a problem in most cases since you just write a query to specify the desired column order but it makes doing a simple “SELECT *” more annoying and will break naive jobs that rely on a particular column order.  The accepted solution is to create a new table with the proper structure, migrate the data from the original table while using a default value for the new column, drop or rename the original table, and then rename the new table with the original name. It’s a lot easier to see this in code:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Migrate the data INSERT INTO test_new   SELECT id, name, 0   FROM test;  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  One issue with this approach is that if the table is very large, it will take a long time to migrate the data from the original to the new table. In this case a possible approach is to do it piecemeal - if you know you only need recent data you can migrate a subset of the data first to get the table ready, do the rename, and then migrate the rest. In code again:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Migrate some of the data INSERT INTO test_new   SELECT id, name, 0   FROM test   WHERE id > 1000000;  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- Migrate the rest of the data INSERT INTO test   SELECT id, name, 0   FROM test_old   WHERE id <= 1000000;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  If you know for a fact that it's not important to have the old data immediately available you can opt to rename the original table, create the new table, and only then migrate the data. This allows queries that need to write data to this table to complete immediately. The risk is that the legacy data will only be available after the migration completes. The last code block:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- Migrate the data INSERT INTO test   SELECT id, name, 0   FROM test_old;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  The first scenario handles having columns in the right order but the other two can be useful on MySQL as well when tables are large and performance is critical. For high load databases, transactions can be used to make sure that the renames are done atomically - this will avoid an intermittent query writing to a non existent database. It's surprising how a task as simple as adding a column can evolve into a large problem with a variety of solutions when running into a variety of constraints.
{% include setup %}            Something I haven’t seen mentioned much is that AWS has  service limits . The only way to find out that you’re hitting one is when an instance fails to launch with the error message “Your quota allows for 0 more running instance(s)” with a link to open a support ticket and request a higher limit.  This is a serious problem when you’re at the instance limit and depend on auto scaling for high loads. The instance limit prevents you from scaling to meet the demand and the only reasonable option is to file a support ticket and hope someone is able to get to it in time. The doomsday option is to temporarily shut down secondary or tertiary instances to make room for the critical ones.  I understand why Amazon chose to implement it but I wish they had better support around messaging when you’re close to the instance limit or whether your current setup can push you over. There’s nowhere in the UI where you can see the limit but luckily there’s a simple command to do it via the  command line interface  - aws ec2 describe-account-attributes --region=us-east-1.  If you’re running a quickly growing stack you should make sure to monitor your service limits so you’re not caught unaware. Also make sure to monitor the limits per region and within a VPC as they each have their own.
{% include setup %} Near the end of last year, Google announced the  Contributor  program - a way to pay a monthly fee which would then be distributed across the websites you visit. In return, you’d start seeing fewer ads. Earlier this week I got off the waitlist and decided to give it a shot. The signup process was amazingly simple - choose a monthly dollar amount and you’re good to go. The effect is noticeable - on many sites I’ll see a blank spot where an ad should have been. The best part is being able to see how much I’ve contributed to the various sites I visit. Over the past couple of days I’ve spent a little over 60 cents removing 51 ads. An unforeseen effect is that I’m more aware of the content I consume and the sites I visit - seeing that some of my money is going towards shady sites makes me more conscious of my browsing behavior. I’m definitely curious to see where this approach goes.
{% include setup %} A thought experiment I’ve had on my mind is this idea of a programming language that only has a single way of solving every programming problem. Regardless of the problem, multiple people writing code independently would end up with the same exact code. No such language actually exists (yet) but it’s fun to think about extreme cases in order to understand where we stand now. With this programming language the only differentiation between developers would be time since the end result would be the same. Beyond that, if there was always a unique solution to every problem this language would be able to write the code itself.  On the other extreme you have current languages which provide a ton of flexibility with dozens of ways to solve a simple problem. In this world developer skills are paramount. You want to make sure you find the approach that solves the current problem but is also written in a way that’s flexible enough to be easily modified for whatever the future brings. Enforcing a structure that’s based on best practices makes it easy to write code that grows with the team.  A great example of this is the rise of JavaScript frameworks. JavaScript is extremely flexible and gives the developer a wide range of paradigms to choose from. This leads to the same problem being solved hundreds of different ways depending on the style and mood of the author. The fact that it actually has a book dedicated to the “ good parts ” highlights how flexible the language is and how easy it is to go off track. Despite being close to twenty years old, only now are we seeing frameworks being developed that take a very opinionated view of how JavaScript should be written. There’s nothing in the language itself to enforce a standard so each framework takes on the responsibility. This allows large teams to collaborate on large projects without having to worry as much about individual styles and decisions.  Software engineering is a new industry and I suspect we’ll see more and more standardization as it evolves. The current approach is to use general languages for a wide range of problem domains but I think we’ll start seeing more and more languages that are specialized by problem domain. This won’t get us to the language with a single way of doing things but it will it a lot simpler to solve problems in a well defined and standard way.
{% include setup %} While doing some spring cleaning I discovered a bunch of fliers from various computer shows I attended in the mid 90s. Bsed on the Windows 95 and Windows NT promotions I suspect this must have been in 1995 or 1996. What’s striking is how much better our computers are. It’s one thing to be abstractly aware of Moore’s Law but shocking to actually see it. The top of the line model in 1995 was $2,500 and came with a 4 GB hard drive, 64 MB of RAM, a 200 MHz processor, and a 33.6 kbps modem. Adjusting for inflation, this is equivalent to $3,700 in 2015 dollars. With that budget you can a top of the line computer with an order of magnitude more of everything and still have enough leftover for a smartphone which is also an order of magnitude more powerful than a computer from the mid 1990s.
{% include setup %} I’ve been thinking about driving before GPS. I remember my family having an atlas in the backseat that we’d reference for long trips and actually map out our journey - which roads to take, which exits to get off of exits, and the distances involved. My clearest memory was constantly trying to figure out whether we missed an exit or not. The usual solution was to just pay attention for the next couple of minutes and try to use the signs along with the road atlas to figure out where you were on the map. Now, you just type in the destination on your smartphone as soon as you get the car and just start driving. Even if you make a mistake the directions automatically update to correct your course. The amount of time saved by GPS for every trip that no longer needs to be preplanned or adjusted enroute must be incredible. I suspect it’s also changed the type of trips we’re making - rather than going to the same old nearby spots that we know we can get to, we’re confident enough to go beyond that and discover something new, knowing that our phones will bail us out.
{% include setup %} Since I do a fair amount of web development having flaky internet is a big hit to my productivity; especially when I have a half dozen open SSH sessions that bulk disconnect every few minutes. After being thwarted one too many times by spotty internet at the office I decided I had enough and started looking for alternatives. One of the tools I discovered was  Mosh . Mosh allows you to open a remote session just like you would do with SSH but unlike SSH it’s robust enough to handle networking disruptions. In fact, I can start a Mosh session on Friday afternoon before leaving the office for the weekend, let my computer go to sleep, and then have it automatically resume as soon as I get back to the office on Monday and wake my computer up. I’m still amazed at how well it works and only wish I discovered it sooner.
{% include setup %} Many of my thoughts come serendipitously - whether it’s an errand I need to run, an idea for a blog post, or a feature I should build into one of my projects. But unless I’m able to jot it down soon after it slips my mind until I have another serendipitous thought to bring it back. It’s frustrating when I know I had something but can’t recall what the actual thought.  A smartphone makes it a lot easier since I can always jot down some words to trigger the thought later on and I’ve also started carrying a small notebook for times I'd just like some pen and paper. Unfortunately, there are still a few cases where this approach doesn’t work. One is when I’m in a group and don’t want to bust out my phone and start taking notes. The others are where a phone just isn’t practical - whether I’m out exercising and don’t want to take a break, in bed where I don’t want to stare at a bright screen, or just in the shower. In those cases I’d love something akin to mind reading where I’d be able to just back up a thought to a place that I can reference later. The only idea I’ve had is a braille-like system that lets you enter the worlds tactilely. Imagine having a small device in your pocket that you can run your fingers over to type whatever you’d like. You’d be able to do this regardless of the location - whether you’re in bed or in a crowded subway car with your hand in a pocket. I’d love to see a Kickstarter for this.
{% include setup %} We live in a world where it’s impractical to be a generalist so we specialize in a subset of skills and go to others for everything else. This works well but there’s still imperfect information - it’s tough to gauge someone’s skill level when you’re not an expert. In fact, when we lack awareness we end up using time spent as a proxy for skill when comparing across service providers. Imagine going to two barbers that charge the same amount for a haircut but one takes 10 minutes and the other takes 30 minutes. Even if we can’t tell the difference between the two haircuts we’d value the 30 minute one more due to the time difference. This seems backwards. The barber that was able to achieve the same result in 10 minutes is the more skilled one but instead we feel swindled when we back the price into an hourly rate. We should be willing to pay more for the 10 minute haircut since it gives us more time for our own pursuits. Yet when we lack knowledge we opt for the shortcut of equating time and skill. I’m trying to break this tendency by thinking about the end result rather than the effort and time involved. It’s interesting to compare this to software development. I know that just because someone spent more time on a project doesn’t mean it's better than someone who knocked it out yet I still view other skilled professions from a “time equals quality” perspective. It makes you wonder whether professionals in other industries have a similar mindset where they realize time spent isn’t an indicator of quality in their own profession yet view it as a sign of quality in others.
{% include setup %} I have a few sites that are “first stops” for specific use cases. I’ll go to Google Maps for directions, Foursquare for ideas of where to go, and Amazon whenever I need to buy something. They’re great most of the time but what’s interesting is what happens in the failure case. At that point my primary tool is no longer sufficient and I need to move on to secondary options. In these cases I tend to not have a well defined set of fallback options - for most of them I’ll fall back to the general case of using Google and then exploring from the search results. The only clear exception is Foursquare in which case I’ll go to Yelp before moving on to a general Google search. What’s surprising is that the fall back option usually leads to a successful outcome. Maybe I should switch my approach to start with the general search first and only move on to the specific tools when it fails. I wonder if we’re converging to a world run by fewer, smarter, and more powerful apps.  Data begets data  and as we supply more of it to the leaders we entrench their position, making it significantly harder for new companies to launch. We need regulation that enforces data mobility and allows people to export all the data that they’ve contributed and share it with whoever they’d like.
{% include setup %} Earlier today I wanted to check up on my electricity bill but ran into an issue trying to login to my PSEG account. Turns out that my nightly version of Google Chrome is preventing me from logging into their site since it has a poor HTTPS configuration. Instead of seeing the login page I get the following message: “Server has a weak ephemeral Diffie-Hellman public key”. Luckily for me this only happened in the nightly build and I was able to login using both the nightly version of Firefox and the standard version of Chrome.            I wonder whether Google’s making the right decision here. What happens when they propagate these changes down to the standard versions of Chrome and countless people start having issues paying their bills. I understand that Google wants sites to upgrade their security but there will be a ton of disruption in the interim. Especially on sites people use to manage their lives. I’m just hoping that the sites that need to upgrade their security do so before Chrome updates itself.
{% include setup %} Note that this is straying a bit far from my usual posts but I thought it would be helpful for anyone that’s had to deal with a stripped screw or a broken screw head. In my haste I used the wrong driver bit and completely stripped the screw head. It was deep enough that I wasn’t able to extract it using pliers while being so stripped that none of my screwdrivers had enough grip to finish screwing it in. After a bunch of failed ideas I finally stumbled unto a solution that worked and could have helped me over the years. The idea is to use a drill/driver but instead of using a bit in the head you tighten it around the stripped screw. Then when it’s tight around the screw you drive it in until it’s where you want it to be. The other option is to use this approach to get the screw out and replace it with a brand new one to make sure it’s able to removable in the future.
{% include setup %} I finally had the chance to go back and add  another quick tool  to my JavaScript arsenal. This one lets you specify a start date, an end date, a step size and interval, along with a desired date format and it will generate the dates in between. This is a surprisingly common activity for me. Every time I need to split a query into multiple date ranges or come up with a series of arguments for various jobs I end up using Excel to come up with the appropriate date ranges. By having it available via the web it makes it a lot easier to generate exactly what I need as well as provides the flexibility to keep on improving. If there are any improvements you’d like to see or if anything is unclear definitely let me know.
{% include setup %} After discovering and browsing [RelayRides](https://relayrides.com/) I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.  There were only a couple of factors at play here: the initial cost of the car, the price the car will rent at, and how often the car is rented. By combining these values we can come up with a ratio of car price to expected revenue per day. The challenge was in getting this data but it turned out to be surprisingly easy.  My first attempt was to simply scrape RelayRides but I ran into a variety of issues with the authentication process and not being able to evaluate JavaScript via a Python script so I switched gears. My second attempt was a lot simpler but got me what I wanted - after doing a search I opened the network tab in Google Chrome and examined the HTTP requests being made. One of these was to the /search endpoint which gave me a JSON feed of the 200 most relevant cars as well as their make, model, year, the daily rate, the listing time, as well as the total number of trips taken. All I needed to do was dump it into a file and start writing a quick script to start parsing and analyzing the data.            The next step was actually getting the price of a car. Once again I thought this would be a problem but it turns out the  Edmunds API  was perfect for this. It’s entirely free and amazingly worked in nearly all cases with the data I was able to get from RelayRides. The API was smart enough to take the make, model, and year from the RelayRides and provide an estimated price without any sort of data cleaning of transformation. The only issue I ran into was a few rate limiting errors when I decided to parallelize my script but the fix was to just introduce a delay between consecutive requests and retry if I ever encountered an issue.            Combining the RelayRides data with the Edmunds API and doing some simple math gave me the answer I was looking for - a 2008 Toyota Prius. There were a few cars that had a better expected return but they also had very few trips taken and weren’t listed for long which leads me to believe that their return won’t last. For the most part, the rental rate ends up being highly correlated with the price of the car - the most expensive in my dataset was a 2011 Mercedes G-Class which is listed at a $550/day and has an estimated cost of $80k while the cheapest was a 2003 Ford Taurus that’s listed at $32/day and has an estimated cost of $4k. In general, the market seems pretty balanced in terms of price but there’s a wide variance in how often different cars get rented out - it’s clearly proportional to price but there’s definitely something else there. Unfortunately, this approach only lets us examine the cars that are actually listed and won’t let us predict how a random car would do. In the future I might take a stab at running a regression to generalize this approach but the challenge will be in figuring out the relevant factors.            As usual, the code’s up on [GitHub](https://github.com/dangoldin/relay-rides-analysis) and I’d love to hear ideas or thoughts on how to improve the code or the analysis.
{% include setup %} I discovered a nuance with MySQL's GROUP BY statement earlier today that I’ll share with the hope that others can learn from it. It’s fairly common to use a coalesce statement to handle null values while keeping the resulting field the same name. For example:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY user_id; {% endhighlight sql %}  The nuance is that we want the GROUP BY to apply to the entire coalesce expression but as it’s written it only applies to the user_id column from table_a. This has potential to give odd results in more complicated queries. The only fact I even discovered it was that it was causing a duplicate key constraint violation in another table. The solution is quite simple but annoying - you have to use the entire coalesce expression within the GROUP BY statement:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY coalesce(a.user_id, b.other_user_id); {% endhighlight sql %}  The reason this solution is messy is that it’s very easy to update the SELECT but forget to update the GROUP BY. This won’t throw an error and MySQL will execute the query just fine - the results just may be unexpected. What I’ve started doing is renaming the resulting column and using that within the GROUP BY:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as final_user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY final_user_id; {% endhighlight sql %}  This makes the query a bit more complicated but it’s being explicit about what we want and avoids hidden errors.
{% include setup %} Smartphones are supposed to be the next big wave but I can’t get myself to be productive on them. Every action takes an order of magnitude longer than it would on a regular computer which prevents from me from starting it in the first place. The challenge is that I’m a power user on a computer able to leverage shortcuts across a variety of programs to be extremely productive. The cost is that when I switch to a phone it’s impossible for me to attain that level of speed which is extremely frustrating.  I see that others are spending a ton of time on their phones and are using it to replace a variety of activities they’d normally do on a full fledged computer but I’m not able to do the same. I wonder what impact this has. Smartphones are great for consumption but I wonder whether they actually improve people’s productivity. It’s much better than not having any digital device and is definitely helping get more people digitally connected but I can’t imagine them replacing actual computers in the short term. What will make smartphones more productive is when they start understanding our intent and predicting what we actually want to do. Something akin to Google Now but instead of showing us what we want to see allowing us to create what we want to create.
{% include setup %} As part of the  RelayRides analysis  I needed to estimate the price of a car and stumbled across the  Edmunds API . I came in with some low expectations but was pleasantly surprised by how well it worked. I thought I’d need to go through a data cleanup process to make sure I was using the correct arguments in the HTTP requests but somewhat remarkably the Edmunds API was able to properly handle nearly every request.  It’s unbelievable how happy a good API makes me. Dealing with various edge cases is a huge time suck so having an API that works as expected the first time you try it is incredibly refreshing and highlights the amount of crappy APIs we’ve all had to deal with. I’d expect this to come from a small, product focused company or at least be built in house but it turns out Edmunds partnered with  Mashery  to develop their API. It definitely makes a case against keeping development in house.
{% include setup %} The purpose of two factor authentication is to prevent unauthorized access to your accounts by requiring a device other than a password to verify that it’s actually you. Usually this is a text message to a phone or an app such as Authy or Google Authenticator. Being paranoid and despite the inconvenience I chose to do it for the vast majority of my accounts that support it but some are significantly more secure than others.  In particular, developers need to be careful when doing text message based authentication and make sure the code is not visible during a lock screen. Twitter includes the login code as the first word in the message whereas Bank of America does it right and makes sure the code is not visible without unlocking the screen. It’s a seemingly tiny difference but highlights how important it is to get security right.    	  		  			   			 Bank of America obfuscating the code  		  	  	  		  			   			 Twitter including the code on the lock screen
{% include setup %} Yesterday, Amazon  announced  a major update to their Python client, boto3. The core functionality is unchanged but they used a clever solution to make it easier to add, modify, and remove endpoints. By coming up with a  standardized representation  for each of the endpoints they’re able to write wrappers in different languages that generate the API calls programmatically. For example, I've included a subset of the  EC2 definition  below. It contains the information necessary to programatically generate the API wrapper to hit the appropriate EC2 endpoints.  {% highlight json %} {   "service": {     "actions": {       "CreateDhcpOptions": {         "request": { "operation": "CreateDhcpOptions" },         "resource": {           "type": "DhcpOptions",           "identifiers": [             { "target": "Id", "source": "response", "path": "DhcpOptions.DhcpOptionsId" }           ],           "path": "DhcpOptions"         }       },       "CreateInstances": {         "request": { "operation": "RunInstances" },         "resource": {           "type": "Instance",           "identifiers": [             { "target": "Id", "source": "response", "path": "Instances[].InstanceId" }           ],           "path": "Instances[]"         }       },       ...     }   } } {% endhighlight %}  This domain specific approach is great when working with APIs and I’m surprised more libraries don’t adopt it. The benefits include being able to keep the actual code the same and only updating the definitions as well as having definitions shared across various language implementations. An additional benefit that can be gotten is actually downloading the latest definitions at runtime. This way you’re always running against the latest version of the API and don’t have to worry about upgrading versions.  I’d love to see more companies adopt this approach and even come up with a standard API declaration language. Then a single set of scripts can be used to wrap any API. Imagine how much simpler it would be to integrate with third party APIs when all you need to do is read the docs and have everything else wired. In fact the docs themselves can be generated from the base definitions.
{% include setup %} One of the best habits to develop when working with SQL is to always refer to fields through an alias. Numerous times I decided to just take a shortcut and ended up regretting it later. Even if you’ve tested your query to make sure it works there’s no guarantee that a future change to a table schema won’t break it.  Let’s say you have the following two tables - with items.category_id corresponding to categories.id  {% highlight sql %} create table items (     id int,     name varchar(20),     category_id int,     owner_id int );  create table categories (     id int,     code varchar(4) ); {% endhighlight %}  It’s straightforward to join the two tables to get some basic info:  {% highlight sql %} select i.id as item_id, name, category_id, code from items i join categories c on i.category_id = c.id; {% endhighlight %}  Let’s say we test the code and deploy to production. It works perfectly until someone adds a “name” column to the categories table. All of a sudden our query stops working with a helpful “Column 'name' in field list is ambiguous” error. The reason is that the query doesn’t specify which source table for the name column. The solution is to simply prepend the items table alias to the name field and we’re back to a functional query.  {% highlight sql %} select i.id as item_id, i.name, category_id, code from items i join categories c on i.category_id = c.id; {% endhighlight %}  This issue is tough to check against since it requires searching your entire codebase every time you need to alter a table. A better approach is to always specify the schema and avoid the issue altogether. Especially in a quickly growing engineering team where multiple people are working on the same code base it’s very easy to run into these sorts of issues that may only get discovered in production. Although most ORM frameworks abstract this away it’s sometimes necessary to dive down into raw SQL and this is one of those small best practices that is a tiny bit of additional effort to significantly reduce a future risk. Avoid learning this lesson the hard way.
{% include setup %} I spend a fair amount of time in the command line and one of my biggest wins in productivity has come from adopting Z shell along with the wonderful  oh-my-zsh  framework. I initially installed it when looking for better git integration but have been discovering tons of new tricks and features since. In addition to the standard autocompletion for both paths as well as commands there are various plugins to support a variety of other scripts. Just a few days ago I enabled a plugin to allow for autocompletion for Python’s fabric commands. The advantage for a single command is tiny if you’re quick on the keyboard but when you’re running hundreds of commands each day it’s nice to get your typing speed to be as quick as your thought process. Zsh comes close.  If your bash environment is optimized for your flow after years of tweaking zsh is not for you. Otherwise you should give it a shot - it’s a superset of what you get in bash and you can easily migrate your bash configuration to zsh and can easily switch back. I have dozens of apps installed right now that I’m sure I’ll forget to reinstall when I get a new computer - until I realize I need them. Zsh on the other hand will be one of the first things I install - it’s that critical to my productivity.
{% include setup %} I recently finished  The Intel Trinity  which detailed the history of Intel and its rise from a small memory manufacturer to the leader in microprocessors. The entire book is worth a read if you’re interested in startups and the rise of Silicon Valley but one anecdote that immediately stood out was about the reason Apple didn’t use Intel chips  until 2005 . Before then Macs relied on  MOS Technology, Motorola and PowerPC  chips. The Intel Trinity makes the case that the reason Apple waited so long to adopt Intel chips was due to the fact that Steve Wozniak didn’t have enough money to build the Apple I prototype using Intel and had to resort to the cheaper option - a MOS 6502/Motorola 6800. And the reason Steve Wozniak didn’t have enough money was because Steve Jobs didn’t split the Atari payment fairly between them and took the lion’s share without even telling Steve Wozniak about it.  After a tiny bit of digging around the  concensus seems  to be that this is most likely a fabrication and even if Wozniak had the money he would have still gone with the Motorola chip - he was more familiar with the technology and the end goal was to make an affordable personal computer which would have been impossible with the Intel chip. It does make one wonder what the history of Apple and the computer industry would look like had they adopted Intel at the very beginning rather than in 2005.
{% include setup %} During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn't necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state. In the past I would just run a simple query (select table_schema, table_name, column_name from information_schema.columns;) on each environment and then use either Excel or Google Sheets to spot the differences. This takes a bit of time so this weekend I put together a quick  JavaScript tool  to automate the process. You simply run the schema query on each of the environments and paste the resulting rows into the two text areas. The result is a JSON based diff showing the additions, deletions, and modifications to each of the tables and fields. The next step is to modify it to also identify differences in the column types.
{% include setup %} As many at TripleLift will tell you I have a fondness for  Bloom filters  but only recently did I realize that our brains work in a similar way. We don’t always know every particular detail or have perfect recall but what we do have is the ability to realize that something is familiar and that we might have encountered it before. This triggers enough additional thoughts that we’re able to dig up the actual thought or reference. For example I can’t always recall the exact Java library I need to use for a particular problem but I know that I’ve solved similar problems before and can quickly rediscover my previous solution, whether through an online search with the appropriate keywords or even by going through some old code.  I’d even argue that it’s more important to have awareness of everything you’ve done and seen in the past than to have a perfect recollection of a smaller subset of items. Knowing that you’ve seen something before takes care of the fear of the unknown - very similar to how  George Dantzig  was able to solve an “unsolvable problem” as a student since he didn’t know it was considered unsolvable.  Unknowingly I’ve even developed an approach to take advantage of this mental model. I dump interesting notes and links into text files that I “tag” with a bunch of additional thoughts or keywords I think of at the time. Then whenever I run into an issue and realize that one of my notes might be useful it’s a simple text search to find exactly what I’m looking for. Rather than rely on a structured approach such as Evernote I rely on my own adhoc system and am rarely unable to find what I was looking for. In the extreme cases I can even resort to a regex search and some piping to deal with too many results or a very scattered document. Every once in a while I’ll even write a quick Python script to provide a semblance of order although almost always I just resort to a text search.
{% include setup %} Other than the usual developer tools the only desktop based app I use is Excel and every few months I try to wean myself away. I love being able to keep all my text docs and slideshows online and have them accessible and sharable anywhere. The best part is updating the content without having to worry about bombarding people with yet another email.  I tried doing the same with Google Sheets and it works for smaller tables but as soon as you get tables with thousands of rows it’s noticeably slower than Excel. It’s amazing for what it can do but it feels as if the browser just can’t handle the rendering nor the calculation that a large spreadsheet entails. Some of the time closing and reopening the table fixes the problem but this is too reminiscent of Windows in the 90s and ends up in a glacial pace after a few minutes of work. I continue to use Google Sheets for small, collaborative files but for anything larger or anything that will need heavy computation I’ll switch to Excel. I’ve also been using R for more repetitive analyses but for the quick and dirty analysis that comes from the result of a SQL query Excel is still king.
{% include setup %} A common behavior when solving a coding problem is focusing too much on the solution and not enough on the general context. If this is a software problem this may manifest itself as a very quick turnaround on a task that inadvertantly breaks an existing behavior or even something that ends up causing a headache months from now when a slightly more nuanced use case needs to be supported. Experienced developers will not only solve the task at hand but will also understand the limitations of their solution and are able to identify the areas that will be adversely affected by their solution. Nearly every software decision comes with tradeoffs and strong developers can think through this maze and pick the most appropriate one given the situation.  Part is experience and learning from our mistakes and part is knowing our applications and how the various components interact and fit together. A big driver of this is curiosity - some developers will stop as soon as they find a library that solves a particular problem and will maybe even read the docs but great developers will step through the actual code to understand how it works. Imagine two people working on the same tasks for a year - one who’s curious enough to read through the source code of open source libraries they used and one who doesn’t. I’d bet that the one who was curious enough to read through third party source code learned and retained significantly more than the one who didn’t. Another one is relentlessly thinking in abstractions - thinking at a higher level makes it much easier to spot patterns and identify code that needs to be refactored. This ends up paying massive dividends in the long term when massive scopes of work can be eliminated. One way to get better at this is to constantly reevaluate your work and identify code that’s been duplicated since it may be a sign you chose the wrong level of abstraction. Another one is thinking through what you’d have to do if you needed to make some tweaks in the future - would you be able to reuse the code in a clean way? Which part would be the easiest to modify? Which parts are too coupled to separate easily? The simplest way may be to just look and the code and see if it’s ugly - that may be a sign that you did something wrong.
{% include setup %} I’m convinced that the best way to ramp up as a newly hired engineer is to go through the database. Rather than relying on outdated documentation or discovering undocumented features the database is the actual source of truth and defines both the limits and the capabilities of the application. You can examine the relationships between the various objects as well as the litany of features and options that are supported. It’s definitely more difficult to get up to speed on a database rather than documentation or a demo of the UI but the knowledge gained is significantly deeper. Especially when you’re going to be working on features that depend on the database it’s incredibly useful to know how the database is laid out and set up. On its own a walk through of the UI provides a high level overview of how it works but coupling that with the database allows you to internalize the connections and actually understand how the user interactions feed the data and vice versa.  One of the most interesting benefits is seeing the progression of a company’s products and features. A typical database will contain a litany of fields and tables that stick around after the underlying feature or product is antiquated. These features have no documentation and the only way to know what they’re used for is to spend hours going through version control history or talk to someone who was actually around. Despite these fields and tables no longer being used they’re valuable for the context they provide. Seeing the evolution of a product allows you to identify what worked and what didn’t work and serve as as a springboard for new ideas.  Databases are rarely part of an engineer’s onboarding process and are mostly on a “need to know basis.” Only when you’re working on a particular feature do you have to understand the relevant schema and even then you’re not expected to go beyond what you’re working on. This is the wrong approach and there’s a ton of implicit knowledge in our databases that make the entire team more productive. Coupling this with a walkthrough of the UI and the API is a great way to learn and relate the various concepts.
{% include setup %} I’ve only been playing around with Go for a couple of weeks but one of the language design decisions I’ve really enjoyed is how interfaces are handled. Coming from a traditional object oriented background it’s typical to define an interface that defines a few method signatures and then explicitly implement that interface in a new class. Below’s a trivial example of this approach in Java:  {% highlight java %} interface Animal {   public boolean isFurry();   public String speak(); }  class Dog implements Animal {   public boolean isFurry() {     return true;   }    public String speak() {     return "Woof";   } }  public void aRandomFunction(Animal a) { ..  } // Can take anything that implements Animal {% endhighlight java %}  With this approach a compiler immediately identifies cases where you choose to implement an interface but forget (or mess up) implementing one of the underlying methods.  Go’s approach is different. In go you would define the interface as usual with the expected methods and you would write functions that accept the interface as the argument. But instead of explicitly specifying that a particular object implements an interface you just do it. Then if it turns out you’ve successfully implemented the methods you can use that object wherever the interface is expected. The compiler is still able to point out signature issues since it can tell when you’re trying to use an object with a required method but it’s done in an implicit way. Below’s the equivalent Go code:  {% highlight go %} type Animal interface {   isFurry() bool   speak string }  type Dog struct { }  func (d Dog) isFurry() bool {   return true }  func (d Dog) speak() string {   return "Woof" }  func aRandomFunction(a Animal) { .. } {% endhighlight go %}  Dynamic languages frequently use this “duck typing” approach since the variable types may only be discovered during run time so it’s neat seeing it implemented this simply in a static, strongly typed language. The simplicity and novelty of Go’s interfaces make me eager to keep digging and see what else I discover.  https://en.wikipedia.org/wiki/Duck_typing
{% include setup %} As a developer, it feels wonderful to commit some code and knock an item off of the ever growing to do list. Unfortunately, until that code is deployed it’s not delivering any actual benefit. It’s easy to open a pull request and move on to the next task but to create high quality products we need to only consider our code complete when it’s deployed and running issue free. So many things need to happen between writing the code and deploying it - handling conflicts with other database changes, updating database schemas, and monitoring the actual code to make sure it’s working as expected on a production system. Calling something done before it’s deployed is a lazy shortcut.  This approach also encourages developers to care more about their code and take a big picture view of the product. By taking an active role in the deployment we’re forced to think through the dependencies and design a release process that avoids downtime and occurs in the right order. For simple features it’s straightforward but larger, coupled ones require an approach that may even end up in rewriting code in order to simplify or stage a complicated deployment process. And if you know your features aren’t complete until they’re deployed you’ll make an effort to actually get them deployed. This is a huge risk reduction since the code and ideas are still fresh in our minds and can deploy code in small batches rather than massive monoliths.  The holy grail is continuous deployment which couples code commits and deployments but it requires significant effort to get it working smoothly that may not be worth it for early stage companies who need to focus on building their product. For them iterating is crucial and every developer needs to take ownership of getting their code into production.
{% include setup %} A critical component in communicating between various teams is knowing who has what responsibility. Especially with driven people it’s easy to have overlap between various functions - product and design; design and frontend engineering; and frontend engineering and backend engineering. This is both good - because it’s able to focus more eyes on a particular problem and provides a new perspective - and bad  - because people may feel that they can’t move quickly enough and don’t want to cede decision making power. Great teams thrive in this environment while poor teams degenerate into a Dilbert cartoon.  One approach that I’ve been preaching is to standardize on the edge points that can act as a form of “contract” between the teams. At those edges it’s great to have the debates and argue the merits of various implementations but beyond that the ownership should lie with the respective team.  An example is to image two engineering teams - one is a full-stack team responsible for the UI and the corresponding API endpoints for a customer facing application and the other is a backend team that uses this information to run the hidden part of the application - the data collection, the web server, and the various third party integrations. In this case a good intersection point would be the database - both teams leverage it and have their own thoughts on what to store and how to structure the schema. The debate should be centered around these questions rather than how each team builds their own components. Once there’s agreement on the database structure each team can go ahead and work independently of the others.  Similarly, a designer can create a series of mocks that can then be debated with the frontend team. The frontend team may push for a different design that will simplify their code and a designer may push for a certain approach that significantly increases the product’s usability. After both teams settled on an approach they can focus on what they’re great at - a designer may focus on getting the visual details perfect while the front end team can start writing the HTML, CSS, and JavaScript.  By focusing on what we actually need to do our jobs and trusting others to do the same we’re able to skip the politics and move quickly. It’s human nature to be curious and want to know everything that’s going on but it’s a massive hit to productivity. Especially at a startup when speed is critical being able to skip the unnecessary meetings, debates, and politics can make the difference between success and failure.
{% include setup %} I’ve written  previously  about the appeal of static sites and recently came up with another example of how powerful the setup can be. The gist is that the site’s content is static HTML, CSS, and JavaScript but the relevant underlying content is refreshed on a recurring basis with a separate job. This allows you to host the entire site on S3 and avoid maintaining your own web server.  Normally, implementing a site search requires a backend to accept queries, break them down into the appropriate keywords, and hit a search index to find the matching documents. A simple implementation would index the site using a server side script and store the results in a JSON file that could then be access on the client side. The client side JavaScript would need to be intelligent enough to parse the query string and reference the right index file but a simple solution is easily doable. I’m surprised more simple sites haven’t adopted this approach and I’ll give it a shot with this blog to see what issues I run into.
{% include setup %} Earlier today we had a hiccup where we had a bunch of messages piled up on a RabbitMQ queue that were not being consumed. Some of these tasks were very quick data loads while others were more involved jobs that could take multiple minutes to run. Normally these are distributed relatively evenly across the day so it’s not a problem but in this case we had hundreds of tasks in a random order and we wanted to shuffle them around such that the data load tasks executed first so that the data would be quickly accessible to other higher priority jobs.  Luckily, we remembered we had some old shell commands that helped us backup and restore a RabbitMQ queue so it only required a bit of scripting to come up with a sequence of commands to do exactly what we wanted. The script works by dumping the contents of the queue into a file, extracting the message field, filtering the messages into the desired buckets, turning them into queue addition commands, and executing the resulting files.  {% highlight sh %} # Dump the contents of the queue to a file. # To be safe requeue the messages and do a manual purge when # we confirm the data looks right. ./rabbitmqadmin get queue=data_queue requeue=true count=2000 > tasks.log  # 1 - Get the appropriate field (in our case the fifth one) # 2 - Remove the header rows # 3 - Trim the line # 4 - Prepend the publish command and turn the task message string into an argument cut -d'|' -f5 tasks.log | sed '$d' | sed '1,3d' | sed 's/^ *//;s/*$//' | sed -e "s/^/.\/rabbitmqadmin publish exchange=data_queue.task routing_key=standard payload='/" | sed -e "s/$/'/" > tasks.clean  # Split the tasks into two pieces cat tasks.clean | grep log > tasks.clean1 cat tasks.clean | grep -v log > tasks.clean2  # Queue the tasks in the appropriate order sh tasks.clean1 sh tasks.clean2 {% endhighlight %}
{% include setup %} While playing around with Scala I rediscovered streams - a list-like structure that's lazily evaluated - meaning that only when you access a particular value is it evaluated. This makes it possible to create infinite streams since all you need is a function that's able to compute the next value. In such a way we can create a stream of all numbers, just the positive even numbers, or just the prime numbers. Calculating each successive prime number will become more difficult but it is possible.  In the case of the positive even numbers it's possible to generate the stream in two simple ways - one is to take each positive integer and double it while the other is to take every positive integer and filter them down to those that are divisible by two. Both of these will generate the exact same numbers in the same order but do it in opposite ways. One generates the numbers from a base list and the other filters a larger list down.  In this example the efficiency of the two approaches is similar: the first goes through each element once and does a bit shift while the second goes through two elements and does a bit comparison. But on real code the differences between the two approaches can be significant. It's also likely that one of the approaches may not even be possible or be too arduous - imagine generating a list of prime numbers.  Both are useful depending on the problem and the skill is figuring out when to use each. The generative approach feels as if it should be the more efficient one but there are many cases where filtering is easier and quicker.
{% include setup %} As they say you don't appreciate something until it's gone and I got to experience first hand when I cracked the screen on my phone and dropped it off for a quick repair. Unfortunately, the repair wasn't so quick due to a screw up and I'm still phoneless more than a day later.  I find myself reaching for it despite knowing it's gone and even feel it vibrating in my pocket without it being there. It's both amazing and frightening how significant my phone has become in my life and I'm am actually glad that it's missing. In many ways I feel like an addict that missed a fix and it's a rude awakening. For the first time in years I had to ask a stranger for the time and had to find an open barber shop  without a map or an online search. I also went to bed without my usual habit of checking up on Twitter or catching up on some blog posts and I woke up without immediately reaching for my phone.  It's shocking how hooked we've become - nearly every person at my train station is staring at their phone while waiting for the train, on the train itself, and when they leave. Of course phones make our lives better but we have to realize the price we're paying and sacrifices we're making. I plan on being more mindful when I do get my phone back and will try to keep some of my days phone free - who knows what sorts of adventures I'll have.
{% include setup %} The most common way of making sure code works is by going through the “develop-run-test” loop. We write some code that we expect to have a certain behavior, we run the code and trigger that behavior, and then we confirm that the results are what we expected. And we keep iterating, hopefully making more progress with each new iteration.  One thing I’ve noticed is that this pattern varies drastically for me depending on the language I’m working with. I’ll cycle through iterations much quicker in Python than I will with Java. Part of it is that my Java projects are larger and take a longer amount of time to start but I suspect the bigger benefit is that Java’s strong and static type system makes it easier to take larger coding steps than I’d be able to with Python. For example, if I need to write a method to extract data from a JSON object I’ll approach it very different if I’m doing it in Python than I would if I were doing it in Java. With Python I’d jump into the REPL and walk through a few examples and make sure I handle the the various edge cases whereas with Java I’d place a lot more faith in the IDE and it’s litany of warnings.  Each language comes with it’s own pros and cons and it’s impossible to find a single language that fits every use case. The goal is to pick the appropriate language for the job at hand - and this may involve starting with one and moving to another one as the problem domain changes or the team grows. The ideal language is one that’s able to maximize the product of the iteration speed as well as the step size. Taking frequent, small steps is equivalent to taking fewer, bigger steps - the aim is to maximize the resulting speed - not the individual inputs. A great language paired with a strong development environment achieves both speed and step size.
{% include setup %} Last month I  wrote  that one of the best ways to ramp us a new engineer is to start going through the database schema and understand how the various tables fit together and what the various values mean. That provides a great view around the engineering product - the various fields indicate the options and functionality available and the tables indicate how the components work together as well as what and how data is collected.  The flip side is that this doesn’t actually provide any view into the application architecture - what’s the hardware used? What are the applications and how do they fit together? How do the applications work? What’s the load on the various components and what’s done to address it?  If you’re on AWS or another cloud provider a neat way to answer these questions is to look at the relationship between the various components and the appropriate stats. For example you can start with Route 53 to see the subdomains used and what they’re mapped to. Some may be mapped to EC2 instances while others may be mapped to a ELB, S3 buckets, or Cloudfront. Each of these provides a view of how the application is used - if it’s on S3 then the application is going to be static HTML, CSS, and JavaScript but if it’s hitting a load balancer then you can expect the application to be under heavy load and be supported by multiple EC2 instances. Beyond this you can look at the amount of requests being made and the volume of data going in and out as well as whether there’s any pattern throughout a day or week. There’s a ton of monitoring tools in AWS and each provides an additional data point that provides insight into the application architecture. The various options and dashboards available highlight how important devops is for every engineer - and how valuable it is for every engineer to have at least read-only access to AWS. It’s tough to write good, scalable good unless you understand how it will be used and how it will fit in with the rest of the stack.
{% include setup %} This past Wednesday I had dinner at Blu - a restaurant that’s  adopted  a “pay what you want” pricing model. Customers have an incentive to underpay the final check so I was curious to see how Blu handled it throughout dinner. I noticed three tactics they used to get people to pay fairly and am sure they utilized a bunch more that I didn’t even notice:  - Anchoring: Before sitting down to eat the waitress explained that it was pay what you want and most of the dishes are estimated to be priced between $10 and $12. This sets the expectation early so if you do decide to pay less you’re making an explicit decision to underpay. - Reminder: At the end of the meal we were told how many dishes we ordered. This was also helpful but I can’t help but think that this is a way to give you an estimate of how much you should pay - especially when paired with the fact that the expectation is $10 per dish - a very easy number to multiply. - Shame: I found this the most interesting one. Instead of giving you a blank receipt and allowing you to write what you want to pay you have to tell the waitress what you want them to charge. This forces you to explicitly vocalize your payment to another person rather than quickly writing something and slinking away. And no one wants to be judged as cheap face to face so we’re encouraged to pay well.  I’m a huge fan of behavioral psychology experiments that shed some light on the way our minds work and it was a great experience to partake in one. I only wish I could have spotted more behavioral cues that I’m sure they employed.
{% include setup %} While my phone was being repaired I ran into a predicament. The only way I could log in to my Google accounts was by authenticating via an SMS code which I wasn’t to get without an SMS code. Additionally, I never bothered to actually write down the backup codes thinking I’d never need them so I was stuck in the envious position of being Google account free for 4 days.  Luckily, I had two things going on that made the loss easily manageable. One was that I shared my personal calendar with my work account so was able to see (and create) everything I needed through my work account. And two - I’ve been forwarding all of my email from Gmail to Fastmail  since March  of last year. The only real frustration was not being able to search through my email history nor use the chat. Otherwise it was barely noticeable.  I definitely got lucky so the lesson here is that it’s impossible to predict what’s going to happen and you should just deal with the annoyance of the backup codes. Google also provides the Authenticator app which is another way of supporting two factor authentication. None of these is as simple as just not having two factor authentication but I think it’s a must have - especially for a primary email account which is linked to every other account - including your financial and social media accounts. Losing access to your emails makes it very easy to reset the passwords on those and there’s no excuse in not enabling two factor authentication.
{% include setup %} Eugene Wei  published a great post  on the power of networks and how Twitter hasn’t been taking advantage of their core product - a public messaging protocol. Given this thesis, Twitter should move away from the artificial 140 character limit and innovate on top of the protocol rather than be bound by it.  I’m not nearly as eloquent but I also have my gripes with Twitter that his post motivated me to write. It just feels they don’t care about the user experience. Cross device sync is still a problem - if I clear a notification on my phone why do I see it again on my computer? People are still complaining about the OS X app not being as functional as the other versions. Even on my phone the navigation feels inconsistent - sometimes I get taken out of the app and sometimes a screen is loaded inside. This causes me to hit the back button at the wrong time and randomly leave the app which resets my location. I’m a big fan of Twitter and it’s pretty much the only social network I actually use but I’m frustrated by how poor it is.  It feels as if Twitter has decided to focus purely on monetization rather than evolving the product. It’s okay to do this when you’re in a dominant market position (see LinkedIn) but Twitter should be focused on making sure they’re getting new users that stick around. The only way to do this is to make a product that’s useful, fun, and easy to use.  Disclosure: I own a bit of Twitter stock.
{% include setup %} It feels as if Google has been getting better and better at what I call search inference. I’ll oftentimes do a search for a particular place on either Google Maps or the general site and see it automatically show up in Google Now. Or I will start with a simple search query that needs to be refined with Google able to offer perfect suggestions. Given how much data they’re collecting it’s not a surprise but it’s an easy way to realize I’m not that unique.  The fact that I can even sense improvement highlights how significant the improvements have been - gradual ones wouldn’t be as noticeable. What I want is for these innovations to become part of the OS. I currently use OS X and while it’s easy to use with a lot of neat utilities and applications it’s not smart. I constantly go back and forth between apps - referencing some notes in Sublime, writing some SQL queries, messing around with Excel - and would love the OS to be smart enough to understand my intent. The simplistic version of this is a smarter auto complete that transcends apps - rather than going back and forth between Sublime and Sequel Pro copying and pasting queries it would be nice for the OS to allow me to autocomplete fields and table names that are being actively used. The more advanced version would detect patterns in my workflows and allow me to skip numerous steps. When working on most tasks we have a mental model of how we’ll proceed - first I’ll write a query to pull this data, then I’ll dump it into Excel and run these calculations to figure out some values, then I’ll use these values to make a few updates in the database. These tasks are abstract with a lot of context locked up in our heads but I can see our computers getting smart enough to help us skip the majority of these steps. Given how often we do these trivial manipulations an intelligent OS can make us strikingly more productive.  Google has a huge advantage given both the massive data they have as well as controlling the entire ecosystem. This gives them the ability to know how apps fit together on a technical and behavioral level. Modern OSes separate themselves from the applications and run in isolation to the rest of the world so they don’t have Google’s key advantages. Despite this, I think it’s inevitable we’ll see these smart OSes develop - especially if we want to be as productive on our smartphones as we are on our desktops.
{% include setup %} A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there's a  PDF  that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.  The first step was extracting the address ranges from the PDF into something more digestible by a program. I tried using a PDF converter but that ended up mashing up the addresses together so I took a step back and came up with a very simple script that took copy and pasted text from the PDF and cleaned it up into a list of addresses.  To do a quick proof of concept I started with a single zone for now and see whether I could get it visualized the way I wanted to using Google Maps. After converting the address range into a starting and end address I attempted to use the Google Maps API to do the geocoding (going from an address to a latitude/longitude). Unfortunately, due to the volume of addresses I wanted geocoded I quickly hit the rate limit cap. I introduced a throttle between calls but that ended up causing the page to take too long to load.  Even then, the geocoding wasn't 100% accurate and I needed to figure out how to visualize the resulting zones from a set of coordinates. The first visualization atttempt was to just connect the coordinates with a series of lines but as expected that led to just a jumbling of lines. As a quick fix I sorted the coordinates clockwise by figuring out the center of the coordinates, converting each coordinate as an angle from the center, and then sorting the resulting points by angle. This led to a "starburst" shape that was neater but still didn't represent the actual zone.  It's not done just yet and I'm working on two improvements - one is moving the actual geocoding work to an offline script so I don't have to deal with the rate limiting issue and two is using a convex hull algorithm to come up with a polygon that encapsulates each of the addresses in a zone that should improve the visualization. Feel free to follow along on  GitHub  and offer any feedback, suggestions, or even a pull request.  Writing good code on the first try is tough and part of the process is attempting an approach that may require backtracking. The challenge is realizing when something isn't working and being able to take a step back and revisit the actual goals and understand the constraints. Some projects do end up perfect on the first try but the vast majority require multiple iterations to get right. Experience helps us understand the constraints and tools we're working with but as the popular saying goes: "Wisdom comes from experience. Experience comes from bad judgement."
{% include setup %} As engineers, it's easy to get focused on technical problems and lose sight of the business. We realize our code will be used externally but we have a tendency to focus on what's close to home rather than the actual real world usage. One of the biggest eye openers for me has been seeing people interact with our products.  We like to think of ourselves as "hackers" but it's amazing to see the length people go to "hack" our products to do what they want. Whether it's someone keeping multiple tabs open to be able to reference information back and forth and avoid losing data or someone registering multiple accounts to bypass a database uniquness constraint - it's a way for people to bypass the intended design and I'd argue that these "hackers" are a sign of a useful product. In fact, I'd argue that if people aren't hacking around a product's limitations it's not a good one. These workarounds are a sign that the product is so useful that people are willing to go through additional manual effort to use it for a different use case. If that's not a sign of new functionality to support I don't know what is.  And the best way to understand these workarounds is to talk to our customers and collect as much information as we can. Guessing people's intentions isn't helpful but being able to identify an anomalyous flow and then talking to that person is a great way to understand the intention and the workaround. This insight can then help drive product direction and innovation.
{% include setup %} Recently I've adopted the practice of having the engineering team support other team when the core technology can support it - even if hasn't been fully built in to the product. This may require manually adding entries to a series of database tables that or manually pulling reports that aren't yet availabe via the UI. Despite being an inefficiency for the engineering team it provides a variety of benefits that outweigh this minor inconvenience.  - Engineers understand the business better. By being closer to the actual use cases engineers understand how the product is used and the problems that other teams are solving. This can have huge wins in the future when there are multiple implementation options available and the developer needs to pick one. Being able to pick the right one can significnatly change the cost of doing future development work.  - Work gets done faster. Engineers love their efficiency and as soon as they end up having to do the same thing twice they'll think of ways to automate it. This is the perfect way of shifting the problem to the person best suited to solve it. A support person may need to come up with a series of inefficient workarounds while an engineer will solve the underlying, core issue.  - Improved quality. Being closer to the actual use case will improve the quality of the code since the developer will know how their code will be used and will understand the options available. The other benefit is that it will become a lot easier to see usability issues as well as improvement opportunities that can be implemented in the future. Bugs will also get caught earlier since the developer will be more likely to monitor the behavior if they were the ones that worked on it. The fact that they used it will provide more motivation to monitor it when it's out in production.  - Prioritization makes sense. Oftentimes it's not obvious why one feature is prioritized over another. Being able to work through some business cases and experience user frustrations is a great way to get a look at the product from a different perspective.  - No workarounds that end up being "grandfathered" in. It's common to see users come up with workarounds to support a use case not officially supported. This places a big burden on the engineering team that may end up needing to support it since a particular workflow now depends on it. By exposing the problem to engineers earlier it's more likely that this situation is avoided.  Years ago I read how Kayak has their  customer support phone number get routed  to the engineering team some percentage of the time. The goal is to expose engineers to customer problems with the idea that an engineering can actually solve a problem once it bothers them enough times. I view this as "internalizing externalities" - rather than offloading engineering costs to other teams (product managers, QA, customer support) we should address the problems as soon as they arise. This ensures the quality stays high while aligning the engineering team with the rest of the company.
{% include setup %} I'm a strong believer that one needs to keep learning and to not get content with the knowledge they have. This can come in the form of new experiences or challenges but should be seen as a learning opportunity. Throughout school we have a structure in place to help us learn but after we graduate we have to take the responsibility ourselves. Unfortunately, many people don't and even take pride that they haven't read a book since college.  As more and more of the rote work becomes automated it's important to develop the creative mind set that can take topics from a variety of fields and blend them together to create something new. The irony is that our education and professional systems are becoming more and more specialized. As our knowledge of an area becomes deeper it's harder to be a generalist and we end up focused on only a few areas and skills.  My solution is to read as much as a I can across a variety of fields: fiction and non-fiction, classics and modern fiction, even the occasional textbook. The goal isn't so much to recite everything from memory as it is to plant some remnant thoughts in the back of my mind that may end up being useful in the future. It's the difference between not knowing that there's an answer to a question and knowing that an answer exists. The latter gives you the ability and confidence to find the answer whether the former gives you an exuse to give up. The other benefit of this cross-disciplinary approach is that you get great at identifying patterns. These act as a shortcut and make it easy to absorb new information as well as spot patterns in the wild.  These days it's too easy to get our knowledge bite-sized through the various social networks, articles, and headlines but that information is fleeting. The way to really absorb information is through focus and being able to spend more than a few minutes on a particular topic. Otherwise it's just wasted time and effort.
{% include setup %}                                    I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at  http://dangoldin.com/jersey-city-open-data/  and includes the zones that had enough geocodeable addresses to generate a valid polygon.  As expected, most of the work was going from the PDF to a set of valid geocoded addresses. The biggest challenge was extracting the text from the PDF and transforming them into addresses that could be accurately geocoded. Once I had that it was simply modifying the Google Maps  polygon example  to generate a list of polygon and finding a library to overlay the zone labels.  The two things I want to change are to modify the visualization to also include a street level visualization rather than relying on a polygon since it’ll make the information bit more useful as well as incorporate the street cleaning hours. If anyone has that data I’d love to get it.  As usual, the code is up on  GitHub  and pull requests are welcome.
{% include setup %} Supposedly the performance of the recently announced iPad Pro will rival that of the Macbook and encourage tons of people to buy it with productivity in mind rather than just consumption. And at a starting price of $799 it’s significantly cheaper than the Apple laptop options. Apple is known for achieving large product margins but I wonder if the iPad Pro is sold at a lower margin to get people to switch to iOS and get even more tied to Apple’s ecosystem. I currently use a Macbook Pro for my work but can switch to any Unix based environment without any hit to my productivity. I’m not sure if this is me being too cynical but I can definitely see Apple taking a long term view here and taking a much lower profit margin on the iPad Pro in order to get people to actually make the switch to the walled garden of iOS. Among the developer community Safari is already seen as the  reincarnation of IE  given Apple’s lackluster support. These may just be coincidences but for a company as detail oriented as Apple this feels like a strategic decision to shift away from the open web and into the walled app garden.  During the 90s Apple nearly failed due to the dominant Windows ecosystem and Windows’ ability to run on commodity hardware. At least for now, mobile isn’t at the commodity hardware stage and Apple has taken the lead in smartphone hardware. This position allows Apple to impose its ecosystem and software but I wonder what happens if smartphone hardware get commoditized - similar to what happened to PCs in the 80s and 90s.
{% include setup %} I’ve seen the point made across a variety of articles that media companies see media consumption as being zero sum - there’s only a fixed amount of consumption that people have and they’re allocating it between a variety of options. The typical example is people abandoning legacy television for YouTube. Thus, the rationale goes, the time people would have spent watching television has been replaced by them watching YouTube.  I understand the concern. People are watching less and less standard television with only time sensitive and restricted content (ie sports) being viewed on television but there’s a world of nuance. People may be watching less and less broadcast television but the content that’s being consumed is growing. Cordcutters pride themselves on not paying for cable television yet they’re still subscribing to Netflix and Hulu. The same shows are being watched - just not on the usual devices. In addition, there’s the rise of multi-screen consumption - we’ll be on our phones or laptops while watching TV. Whether it’s a distraction from a commercial, a way to follow an event on Twitter, or just browsing Wikipedia to get a bit more information, we’re parallelizing our media consumption and increasing the size of the media pie rather than solely switching between platforms.  I found the following graphic which highlights the shift in media consumption over the past century.                               The BrandBuilder Blog                Part has been the rise of new technology to fill in more and more of our days. Radio gave way to the rise of television which started taking up more and more of our time. The internet then came along and gave us another way to access and consume content that. The smartphone blew that out of the water and gave us the internet and media consumption wherever we are. Whereas in the past we’d be watching TV or using the internet at home or at the office with smartphones we can consume content wherever we are. The pie of “total available media consumption minutes” has been growing over time and will only keep increasing. What will people do when we’re being chauffeured around by our self-driving cars? We’ll be using our future devices to consume and entertain ourselves the exact same way we do now. That’s a whole new timeslot for us to consume content that we’ve never had the ability to before.  As more and more of our responsibilities get automated that time will become available. Some will fill it in with creative pursuits while others will view it as another timeslot for media consumption on an ever increasing number of screens and devices. It’s definitely depressing but I’m not optimistic given recent trends.
{% include setup %} It's become a popular idea that big companies are evil and we should only be supporting small and local businesses. There’s some truth to it - smaller companies are much more aligned with the incentives of the community whereas larger companies may be managed from thousands of miles away via a spreadsheet. When the only goal is to make more money it’s very likely that morality and honesty will suffer.  At the same time, we should differentiate evil companies from evil actions. Even Walmart has done some good. A story that comes to mind is the reduction in the size of  detergent bottles . There was an arms race by detergent manufacturers that were diluting their detergent in order to sell large bottles for the same price. Sure, it cost more to produce the larger bottles, but then they'd be able to generate significantly more sales when people saw that you were offering “twice” the volume at the same price as your competitors. Sure enough competitors followed suit and we ended up with a  Prisoner’s Dilemma  spiral. It took Walmart, along with a few other large retailers, to reverse the trend and get the manufacturers to start producing bottles in smaller, more concentrated sizes.  And on the other extreme there’s Starbucks. Despite not being a trendy new coffee shop they have the size and resources to launch massive initiatives. One of the recent ones is a way for current employees to get a  full cost covered college degree  from Arizona State University. Despite this being isolated to a single college and only classes no local coffee shop would be able to make this happen. And the cynic may see this as a marketing ploy but if it’s still able to help more people get a degree I’m all for it.  In both of these cases the benefits came from the size of the company. Larger companies have both the resources and the clout to push change that may not have been possible by smaller company. Instead of dismissing them as permanently broken we should be focused on getting them to use their clout and money for societal gain.
{% include setup %} At the beginning of the year I wanted to learn a bit of Node.js and decided the best way was to code up a simple project. The idea was  jsonify.me , a simple API only app that provided people a simple way to generate their own JSON profiles that they would then be able to map to any domain name, for example  http://json.dangoldin.com . The primary goal was to get some real experience with Node.js rather than rely on some walkthroughs and tutorials. Since then I’ve used it as the starter project to learn new languages. I’ve coded it up in Scala and have just finished up the Go version.  The project has a few nice properties that force me to gain a pretty good understanding of the language and how a typical project plays out. Despite being a pretty simple program it touches a bunch of modern web components. The code needs to be able to parse and modify HTTP requests and headers in order to support redirection and authentication. In addition, the code comes with a working LinkedIn OAuth example and gives an opportunity to incorporate an OAuth library. The other big thing is integrating the AWS S3 client library which provides a simple way to get exposure to the AWS ecosystem.  Everyone who’s learning new languages should have a “go to” project. It’s okay to go through a series of tutorials to get the basics of a language but nothing beats working on a project you’ve already done across a variety of other languages. In addition to coding up a project in a new language you get a feel for the way the program is structured and laid out. Over time you start getting an intuitive feel for how one language works compared to another and can understand the tradeoffs between them. Having your own project also allows you to optimize towards the skills you want to learn - in my case I wanted them to be focused on the web and allow me to work with the various HTTP elements as well as a few third party libraries. Tutorials and walkthroughs are great to get a feel for the language but they don’t force you to think through the design or architecture which are critical when working on larger projects. It’s amazing how much effort that takes up when learning a new language and the only way to learn it is to experience the frustration of doing it.
{% include setup %} Ever since the release of iOS 9 and it’s support for adblocking apps I can’t go a day without seeing some article about adblock. Some condemn it and claim it’s stealing from publishers while others make the case that ads are so intrusive that they deserve to be blocked. I don’t want to dwell into either of these but something that’s been on my mind is that publishers aren’t doing enough to differentiate themselves based on the quality of their audience.  Top tier publishers that have unique and high quality content should focus on building a community. This passionate group of users will engage with the content on the site as well as provide valuable information to the publisher. By signing up for an account users provide valuable demographic information as well as interests based on what they see and what they do. This is also something that adblock won’t be able to easily block since it will be such a core part of the experience and hosted on the publisher’s own domain.  It’s true that publishers with low quality or commodity content will suffer as users move on to something with a better experience but this will arguably make the web a better place. The fact that some people even run adblock implies they have no respect for the publisher’s effort - and pursuing a lowbrow approach just turns that into a death spiral.  Disclosure: I work at TripleLift which provides a much better advertising experience for users, publishers, and advertisers.
{% include setup %} The goal of every bit of code should be to make it to production. Code that’s not deployed is wasted effort as well as a loss to the business. And a big part of making sure code is deployed is thinking through the deployment plan as we write the code. Some code is deployed simply by pushing the new application while other code may require updating the database schema. More complex code may depend on other applications which will need to be tweaked and deployed beforehand. Large companies and teams have dedicated ops teams that handle deployments but small teams need to do this on their own.  Thinking through the deployment also leads to better code. By going through the steps of how the deploy will work you end up breaking your code down into a series of changes that end up being significantly safer and reduce the risk of a large failure. For example, we may want to write an update that will add an additional feature to our application based on a flag in a database. A safe way of doing it is to create a new column first that will have no effect on the existing application and then roll out our the new code that starts using this column. Future releases can then remove the code that uses legacy columns with latter releases dropping those columns entirely. None of this is shocking news but it’s surprising how rarely we think about deployments when we set out to write code. Especially as a team grows it’s important for everyone to be thinking about the way their code will work and the way it needs to be deployed.
{% include setup %} The longer I code the more I appreciate the power of the shell. Getting familiar with common commands is a great way to improve your productivity and over time you amass a massive collection of scripts that allow you to do nearly everything. The most recent utility I discovered was “date”. As expected, it displays the current date and time but it can easily be adapted to display the current datetime in nearly any date format but also allows you to offset the current date in a variety of ways.  {% highlight sh %} ➜  ~  date Mon Oct 19 22:35:37 EDT 2015 ➜  ~  date +%Y-%m-%d 2015-10-19 ➜  ~  date +"'%Y-%m-%d'" '2015-10-19' ➜  ~  date -v+3d +%Y-%m-%d 2015-10-22 ➜  ~  date -v-3d +%Y-%m-%d 2015-10-16 ➜  ~  date -v-3y +%Y-%m-%d 2012-10-19 ➜  ~  date -v+3y +%Y-%m-%d 2018-10-19 ➜  ~  date -v+3y +"%Y-%m-%dT%H:%M:%S" 2018-10-19T22:39:18 ➜  ~  date -v+3m +"%Y-%m-%dT%H:%M:%S" 2016-01-19T22:39:24 {% endhighlight sh %}  In the past I’d resort to a JavaScript utility or a quick Python script when I needed a simple date calculation but lately I’ve been able to do nearly everything solely by using the built in date utility. It’s still a bit cumbersome for generating date ranges or when requiring complicated logic but for the basic stuff it’s surprisingly powerful and expressive. It’s amazing how full featured the shell is and how often we avoid it and use more fleshed out languages. Instead of trying to find new languages it’s worth taking the time to actually explore and understand the shell - it’s one of the better investments an engineer can make.
{% include setup %} A couple of weeks ago I wrote about the idea of having a  “go to” project  that you use to pick up a new language and earlier this week I finished the bulk of the rewrite of  jsonify.me . It went through a Node.js phase, a Scala phase, and is currently in the go phase. The idea is to give people an open ended and simple way to generate a personal JSON object, similar to how people may have an about.me page but in JSON. This object can then be mapped to any subdomain (mine is at  json.dangoldin.com ) and be referenced by any third party code. For example, you can construct your personal jsonify.me object based on the information in your various social media profiles and then make that information accessible to a variety of sites or pages that can generate it in a variety of ways. One site can turn it into a simple resume while another one can turn into a visual timeline of your history. At the moment it’s entirely open ended with the vast majority of the functionality provided solely through an API. Over time I’ll add some more bells and whistles but I’d love to see the community come up with their own unique JSON format that can then get adopted - similar to the way the hashtag system on Twitter evolved. I suspect it’s going to be significantly more complicated since there’s no 140 character limit but am still interested to see where this goes. Play around with it and let me know what you think!
{% include setup %} The biggest development lesson I learned over the years is that production is a completely different beast from development. Code that works perfectly in a development environment can fail catastrophically in production and cause a severe impact on the business. Issues can stem from bits of inefficient codes to database schemas that just don't scale on production. Ideally your development environment  mirrors production and has the same load and hardware but that's rarely the case. For the other cases cases I’d go through the following items to make sure your code is ready for production:  - General code efficiency: Your code may pass unit tests and work fine when you’re running it on development data but you should make sure the code itself can scale to production data. Inefficient code may be fine to push to production if it’s not being hit often or you have the hardware to back it up but you need to make sure this is the case. This also extends to UI applications: if your development environment has a few rows for a customer while in production a customer will have hundreds, you need to make sure that the UI is responsive and that the design actually fits the production use case. - Query performance: This is the most frequent problem I’ve seen when new code is deployed. A query may run fine in development which can have a magnitude less data but as soon as it’s pushed to production queries that used to take milliseconds while developing start taking multiple seconds. The simplest way to deal with this is to just run your queries on production and confirm they work - especially on the datasets and filters you suspect will be problematic. The results may lead to solutions such as adding new indices to a table or generating new summary tables to speed up the code, neither of which would have been easy to discover during development. - Deployment plan: Part of writing code is thinking through the deployment and a big part of deployment is making sure you’re avoiding down time. In addition to making sure your application rolls over gracefully to the new code you should be thinking about the database migrations you’ll need to make and confirming they will run as expected on production. I’ve encountered cases where adding a column took a few seconds on a development database but multiple hours on production. If that’s the case you should rewrite the migration to avoid downtime - for example creating a new table and population it with legacy data and only then renaming it to the original name. - Rollback plan: As much as we like to think our code is perfect mistakes happen and we should write code that’s easy to rollback. Ideally it’s as simple as just pushing the older code but it may need a bit more work if it depends on database changes or other applications.  None of these should be earth shattering and over time they become a habit but until then it’s important to go through each one to ensure a successful production deployment.
{% include setup %} This past weekend I was going through some old projects and got a bit nostalgic. Some were my first foray into web programming and startups while others were just me messing around and trying to learn a new framework or language. Each of them have taught me valuable lesson and I thought it would be fun to go through each one and jot down a quick background as well as the lessons learned. I’m doing a high level pass so if any of these are interesting definitely let me know and I’ll do a deeper dive.  - scenepeek.com: I started this with a close friend back during my finance days when I really didn’t know what I was doing. The goal was scrape the web and identify various events that could then be easily surfaced and discovered. This was right before smartphones became popular so it does make one think of what could have been. This was my first real time doing “devops” and working with various instances and configuring Apache. The other big lesson learned was that we probably should have started with some framework to get our project out sooner. Instead we ended up writing raw PHP and building everything from the ground up. -  getpressi.com : Applying the lessons learned from Scenepeek I left a a full time job at Yodle to cofound a startup (initially called Glossi) that would create social media mashup pages. We were accepted into an accelerator and ended up making significant progress but were never able to figure out whether our core customers were consumers or larger companies. We couldn’t commit as a team and ended up floundering until selling to a small advertising agency. At the peak we had a dozen customers and most likely could have turned it into a lifestyle business had we had the maturity and focus. -  makersalley.com : After breaking up with Pressi went in the opposite direction and built decided to build something with a concrete business model rather than waiting for one to fall into our laps. We both liked Etsy and wanted to do something with a community element as well as having to do with physical goods. This was a two sided market play and we were never able to get people to buy expensive furniture online. We got so enamored with our vision of how awesome the furniture and designers were that we focused on getting them rather than on getting customers. -  better404.com : This was a small side project I started to help websites improve their 404 pages. I wanted something that was more passive than building a marketplace and catered to my strengths which were more on the tech side. I don’t have too much time dedicated to this but every once in a while I’ll make some updates with the idea of making it a small passive income generating product. -  jsonify.me : Scratching an itch here but I love the idea of every person having a JSON page that’s a representation of what they are and what they care about. It’s also my “go to” project when learning a new language. It’s a proof of concept more than anything else right now but I’d love to see where it goes. I’m passionate about people owning their data and lending it to third parties as needed and view this is a way to achieve it using existing methods.  On one hand I want to polish some of them off and see what I can do but on the other I’m curious about trying new things. People glorify this idea of a purely passive income but I suspect nothing is that easy and every project will require some ongoing maintenance and improvement to stay relevant.
{% include setup %} A clear pattern emerged as I was digging through my old projects. Other than the code quality and approach improving over time what stood out was the way I approached deployment. My earliest projects didn’t have a set of requirements and the configuration was all over the place. The more recent projects have a clear set of requirements as well as the command lines needed to get them running. In fact, I’m able to build and run my recent projects within a few minutes by running “pip install -r requirements.txt”, updating the configuration file, creating the database, and running the database migration script. This is a massive improvement when compared to my initial projects where there was no documentation and my setup involved a ton of adhoc, undocumented work directly on the production server that’s now lost.  I’d argue that this is one of the better habits to adopt as a developer. We do a surprising amount of duplicate work over the years and being able to reference a prior solution is immensely useful, especially when it’s easily discoverable. It’s also a great way of identifying patterns and similarities between projects and understand why some approaches worked and why some failed. This retrospective approach is an active way of improving rather than relying on the “osmosis” approach of just waiting for information to get absorbed.
{% include setup %} I’ve been meaning to share some thoughts on regulating Airbnb for a couple of months now but kept putting it off. The  recent news  was motivating enough for me to finish it off.  I’m a huge fan of Airbnb and it’s my first step whenever I’m traveling. Nearly all my experiences have been great and I’m contemplating getting rid of my Starwood card since it’s just not as useful anymore given that I gravitate towards Airbnb first. At the same time I understand the impact renting a place Airbnb has on the neighbors and can imagine myself hating it if my neighbors were listing their places.  The challenge is that the host is benefiting while passing the cost to someone else. The host is able to get above market rent while the neighbors have to deal with the potential noise and the risk, albeit a low one, of a stranger. The obvious solution seems to come up with a new zone category between residential and hotel commercial that Airbnb as well as other home rental places would be able to fit in. These locations can then be rented out with the city’s blessing as well as contribute to the tax revenue of the city. This relies on the market to come up with a fair price for the location. If you’re interested in renting a place on Airbnb you should be willing to pay more for the property and if you decide you’ll never be renting your place on Airbnb you should have neighbors that share the same belief.  I don’t have much knowledge on zoning laws and what goes into it but this feels like a solution that should work once in place. Getting there is the hard part - what happens to buildings where half the tenants want their units to be Airbnb eligible and half don’t? Who ends up having the final say? Maybe the solution would be to keep existing places the way they are and make sure new construction goes through this zoning process. This will ensure that over time more and more buildings have a clear definition of rental eligibility.
{% include setup %} A couple of hundred years ago nearly every European country was engaged in some sort of military conflict which led either Napoleon or Frederick the Great to state that “ an army marches on its stomach .” The point being that logistics are the most important when it comes to having a successful army. These days the corporate equivalent would be that a company marches on its data.  Every company claims to be data driven and there’s a slew of data collected about us each day. The most successful companies are able to leverage this data and use it to derive insights that drive direction. Unsuccessful companies may collect the same data but don't leverage in an impactful way. It’s easy to collect information but it’s a huge challenge to turn into action. There are many options just for storing the data: one approach may make it easy to store tons of data while making it hard to run large scale analyses while another allows for a distributed computation approach that's too slow. Beyond data storage there’s the actual analysis piece: what’s the appropriate model to use that can represent the relationships between the variables while being true to life? All these are questions that will become increasingly critical and separate the winners from the losers. Data itself has potential for massive  monopoly feedback loops  - companies that succeed are able to collect more and more which improves their product which collects more data. Right now it may only seem as if larger companies should care but I suspect within the next 10 years we’ll see more and more small and local businesses adopt a truly data-driven approach, whether through internal tools or through external services
{% include setup %} Lately, I’ve noticed an interesting trend with my smartphone usage. When I’m on wifi I’m much more likely to use the mobile web, click links, and read various articles whereas if I’m on LTE I’ll stick to dedicated apps. I noticed this at my apartment which has a narrow layout with my living room having wifi and my bedroom stuck on LTE. Despite me being in the same mindset regardless of which room I’m in my behavior changes dramatically.  I don’t recall making a conscious decision to change my behavior so I suspect this behavior evolved to deal with the increase in my data usage. I’m still not sure that apps are more data efficient than the mobile web but it definitely feels that way due to the improved speed and responsiveness. If you’re concerned about data usage it’s obvious that you’ll want to do as much as you can on wifi rather than go through your data plan but what’s interesting is that in my mind I’ve concluded that apps and mobile web are differentiated by my data plan. I’d love to know if others do something similar or I’m the anomaly.
{% include setup %} Writing up my old projects got me browsing through my GitHub account to see what else I've worked on. Some I'll update when I get a good idea while others I completely forgot until going through the list. I noticed two big themes when going through the list. The first is how much nicer it is to have projects that are in static HTML/CSS/JavaScript since they can be hosted publicly on GitHub and don't require any setup or configuration to start using. The other is how many third party libraries or APIs I've used and how much more difficult everything would have been had I had to build everything from scratch. If anyone is interested in forking and ressurecting some of these I'll be glad to polish it up.  - [Twitter archive analysis](https://github.com/dangoldin/twitter-archive-analysis): At some point Twitter announced that they would allow you to export your entire Tweet history and this was a quick pass at a toolkit to analyze the data and do a few simple visualizations. I wrote a blog post about it [here](/2013/01/19/making-sense-of-my-twitter-archive/).  - [Instagram download](https://github.com/dangoldin/instagram-download): A couple of years ago Instagram changed their policies so I decided to close my account. Before that I needed a way to export my photos. This was a simple app/script that spawns a very basic OAuth web application in order to authenticate you with the Instagram API which allows you to export all your photos.  - [Yahoo fantasy football download](https://github.com/dangoldin/yahoo-ffl): I've been in a fantasy football league for almost a decade now and every year I update this script to scrape the data from the Yahoo fantasy football site. The goal was to use this data to develop a statistical model to help me manage my team but I haven't gotten around to starting that yet. Maybe next year!  - [Runkeeper stats](https://github.com/dangoldin/runkeeper-stats): A pretty simple R script to analyze and map the data that can be exported from RunKeeper. I wrote a blog post about it [here](/2014/01/04/visualizing-runkeeper-data-in-r/).  - [Site analysis](https://github.com/dangoldin/site-analysis): I was frustrated by the slowness of various sites and decided to write a script to see what was taking sites so long to load. This analyzes the top Alexa sites and figures out how much data they're loading and of what types - CSS, JavaScript, images, etc. I wrote a blog post about it [here](/2014/03/09/examining-the-requests-made-by-the-top-100-sites/).  - [Relay Rides analysis](https://github.com/dangoldin/relay-rides-analysis): This script analyzes the JSON search results of Relay Rides (now Turo) and combines it with data retrieved using the Edmunds API to identify the cars that have the best financial return. The return is calculated by looking at the estimated price of the car and dividing it by average money earned per day. The obligatory blog post is [here](/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/).  - [Jersey City parking zone mapper](https://github.com/dangoldin/jersey-city-open-data): Jersey City has a ridiculous PDF that lists the streets and addresses that belong to each zone. I painstakingly extracted, cleaned, and geomapped the data in order to visualize the zones on a map.  - [JS tools](https://github.com/dangoldin/js-tools): Probably my most commonly used code. This is a series of tools hosted on ... that provide some basic utilities that help me throughout the day. The most useful lately has been a way of comparing SQL table schemas but it has a bunch of others.  - [Citibike station directions](https://github.com/dangoldin/citibike-station-directions): A web app that breaks every trip down into a walk to a Citibike station, biking from Citibike station to Citibike station, and another walk to the final destination.  - [Meerkat crawl](https://github.com/dangoldin/meerkat-crawl): To help a buddy out I started mapping out the network relationships between users on Meerkat but quickly ran into a scaling issue. I got to around 5 million connections and wasn't able to figure out how to actually visaulize it in a clean and timely way.  - [Yet another Hacker News reader](https://github.com/dangoldin/yahnr): My attempt at modifying the Hacker News experience to show the top stories over a rolling 24 hour period. This was a good exercise in messing around with pseudo static sites where the content is solely hosted on S3 with a script to push new files every few minutes.  - [Python tools](https://github.com/dangoldin/python-tools): A series of Python scripts that I've writtent to deal with various minor issues. I have a ton more that I'll add to this repo when I find them.  - [MySQL class](https://github.com/dangoldin/mysql-class): I taught a MySQL class at [Coalition for Queens](http://www.c4q.nyc/) and this is the series of slides used.  - [Redirector](https://github.com/dangoldin/redirector): A tiny Node.js app that acts similar to the "Switcheroo" Chrome browser extension but able to work across other browsers. It requires a bit of manual set up but then uses the hosts file to intercept web requests and redirect them to another host. A quick write up [here](/2015/02/07/url-redirection-app/).  - [Oyster books crawl](https://github.com/dangoldin/oyster-books-crawl): This was a series of scripts that crawled the Oyster API to pull the available books and then analyzed them to find patterns. A bit sad that this script outlived Oyster itself. I wrote a blog post about it [here](/2014/03/16/fun-with-the-oyster-books-api/).  - [Taxi pricing](https://github.com/dangoldin/taxi-pricing): The goal here was to compare the pricing of taxis across various cities. The two primary dimenisons used were cost per a minute waiting and cost per a mile of driving. Using this information one can then see how different cities and countries value labor costs. The analysis is written up [here](/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/) and [here](/2014/01/09/taxi-prices-around-the-world/).  - [Meeting place finder](https://github.com/dangoldin/meeting-place-finder): A simple script that uses the Google Maps API to come up with an ideal meeting place for a group of people that ensures everyone has the same commute time.  - [Lincoln text analysis](https://github.com/dangoldin/lincoln-text-analysis): An old project that read in the text of Abraham Lincoln's speeches and did a few visualizations of the text. I wrote a blog post about it [here](/2013/02/12/analysis-of-lincolns-words/).  - [Lawdiff](https://github.com/dangoldin/lawdiff): I participated in a journalism meets tech hackathon and this was my team's entry. We looked at proposed state laws and compared them against other states to identify laws that were most likely written by a special interest group. We had a number of false positives but were able to find a bunch of laws that were nearly identical despite being introduced in multiple states.  - [IMDB](https://github.com/dangoldin/imdb): Another early analysis project where I scraped some IMDB data in order to analyze the average age of actors and actresses over time. This came after I watched Misrepresentation and wanted to show that actors and actresses are treated differently in the movie industry. I wrote a blog post about it [here](/2012/05/23/trend-of-actor-vs-actress-age-differences/).  - [Jeopardy parser](https://github.com/dangoldin/jeopardy-parser): I found an open source crawler of Jeopardy clues and made a few updates to make the code multi threaded and able to crawl significantly faster. I then worked with my wife to turn this data into a simple web app that displayed random Jeopardy clues for us to test our knowledge.  - [Map fun](https://github.com/dangoldin/map-fun): Similar to the RunKeeper analysis above this was another pass at summarizing my running data over multiple years but this time leveraging GitHub's map tools. I wrote a blog post about it [here](/2015/01/18/fun-with-githubs-map-tools/).  - [Node toys](https://github.com/dangoldin/node-toys): This was the start of me messing around with Node.js and getting a feel for the framework. One of the fun projects I used it for was evaluating recursive functions using HTTP redirects. I did a quick write up of it [here](/2014/12/31/redirect-recursion/).  - [AWS tools](https://github.com/dangoldin/aws-tools): A super simple script that downloads a list of EC2 instances and then prints the IP, name, and address. The end goal was to make it simple to connect to an instance without going through a manual process of figuring out the appropriate address to use. I ended up not using this that much since it was easier for me to maintain a list of aliases and hosts in a text file. A very basic write up [here](/2014/11/09/some-simple-aws-tools/).  - [Wikilearn](https://github.com/dangoldin/wikilearn): This was one of my favorite projects. The goal was to analyze a Wikipedia article and come up with a visual timeline of all the dates and events that occured. I used an open source library for the visualization piece but ended up running into all sorts of issues analyzing the Wikipedia text. This is where I got a bunch of exposure to NLP but still wasn't able to make it work.  - [Mixergy mp3 download](https://github.com/dangoldin/mixergy_mp3_download): I subscribe to the Mixergy feed and this was my attempt at a script that would just download the available mp3 files and store them for future listening. I'm sure the HTML code of the page has changed since then so the code is most likely broken.  - [Geo data](https://github.com/dangoldin/geo_data): A one of script I wrote to crawl a site and generate a mapping of ZIP codes to counties. I'm not sure why I needed this but I suspect it was for some sort of data analysis project.
{% include setup %} A recent trick I’ve picked up to manage my time a bit better is to take all the adhoc tasks I have to do and scatter them into my calendar for the next few days. This allows me to actually get to working on the tasks and I can make sure none of them are forgotten. Using a calendar also forces me to think about the time I expect these tasks to take and plan around that. I’m nearly always running behind and am constantly shuffling tasks around but it’s much better than my previous system of a text file with a constantly growing list of todos. A side benefit of this approach is that I can split my day into [maker versus manager](http://www.paulgraham.com/makersschedule.html) chunks rather than be at the whim of meeting invites.
{% include setup %} I had a bit of fun with MySQL earlier this week when trying to explain a non obvious “group by” behavior. It’s fairly common to want to manipulate a field in order to transform it into something more useful. The difficulty arises when you want to keep the original name. Below is some SQL code that highlights the odd behavior.  {% highlight sql %} drop table if exists dan_test;  create table dan_test (   id int not null,   id2 int not null );  insert into dan_test (id, id2) values (1,1), (2,2), (3,3);  select * from dan_test;  select id, case when id = 1 then 2 else id end as id, id2 from dan_test;  select id, sum(id2) from dan_test group by id;  select case when id = 1 then 2 else id end as id, sum(id2) from dan_test group by id;  select case when id = 1 then 2 else id end as new_id, sum(id2) from dan_test group by new_id; {% endhighlight sql %}  With the second to last query it’s not obvious which id field the group by is referring to: the original from the table or the derived field? It turns out it’s the original field which can cause problems if you’re unaware of this subtlety. There are a few different ways to deal with this situation, including grouping by the derivation formula, but my favorite is to use a brand new field as in the last example above.
{% include setup %} The news that Adele was not going to put her new album on the streaming services got me thinking about the differences between the way music and video are consumed. Just last week Rdio announced that it’s selling its assets to Pandora which is a reminder of how hard it is to start a music company - music labels wield all the control and are able to dictate the terms they want. Even Spotify is not yet profitable despite having millions of subscribers.  On the flip side we have Netflix which on the surface provides a very similar product - streaming video rather than streaming audio. Yet they’re profitable and are quickly expanding internationally and even developing critically acclaimed shows such as Narcos and Master of None.  I find it fascinating that although the two industries are so similar on a technical level they’ve played out so differently. Part of it is that audio consumption is just drastically different than video. Most people will stream music throughout the entire day at work and not mind repeats of a favorite song while shows and movies are watched in shorter bursts and I like to think that most people want to avoid repeats. I’m not a huge music listener and the music I enjoy tends to be available on every service but I suspect most people who are passionate about music want access to a band’s entire catalog as well as having immediate access to new releases - this is something that Spotify needs to provide that Netflix doesn’t have to worry much about. Netflix can survive on the back catalog alone while Spotify needs to bend over backwards to make sure they have the most recent releases.  Netflix has moved into producing their own shows which is allowing them to get ahead of the back catalog problem and move into the HBO model while still having access to a slew of old shows and movies.These are divorced from their creators and can stand on their own while music has extremely strong ties to the artist. This makes it extremely difficult for Spotify to apply a Netflix model and start producing albums - the only way they’d be able to make it work is by becoming a music label. Netflix on the other hand can pay top directors and actors to develop a show that can succeed or fail - but in either case it’s only loosely coupled with the creators.  I can’t find the blog post now but I read something a few days ago about how hard it is to build a successful music startup. The root cause is that the music labels have so much control and power that they’re charging a license fee that prevents startups from having any money to spend on innovation or product. Instead they’re transferring money from venture capitalists into the hands of the labels. The labels are basically the rentiers of the music industry and prevent innovation by sucking up investment that can be used to launch new products. My gut is that this won’t last since there’s just too much happening in adjacent industries but I’m crossing my fingers.
{% include setup %} While going through some old repos I came across an old [project](https://github.com/dangoldin/meerkat-crawl) I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections. The challenge became actually analyzing this data to derive something useful. I tried a few tools - including [Gephi](http://gephi.github.io/), [Cytoscape](http://www.cytoscape.org/), and [NetworkX](https://networkx.github.io/) - but was unable to get anything more useful than a few simple summary stats. I was hoping to get a neat visualization of clusters to see the various cliques on the network but visualizing that data either broke the programs or took too long to even complete. I made the most progress when using a simple script to filter out the “tail” of the data which allowed the remaining data to be visualized but I felt that the filtration may have eliminated a bunch of interesting information. If anyone has some experience dealing with the analysis of large networks I’d love to hear some ideas.
{% include setup %} An idea I’ve been preaching over the past few days is to start thinking in terms of interfaces when thinking about writing code rather than the actual implementation. It’s a higher level of abstraction that leads to a higher quality and more scalable product. Rather than focusing on the details it’s better to think about the components and how they’ll interact with another - this also makes it easy to put in a crappy implementation for now while making it easy to modify and rewrite in the future. As engineers there’s a strong desire to obsess over the perfect code which can lead to a significant amount of refactors and rewrites without translating into actual business value. Thinking in terms of interfaces and components forces you to get the design and architecture right and leaving the implementation details for later. A side benefit for me has been being able to take pride in the design and flow and not worry about the code itself - allowing me to write code at a much faster place and sprinkle a series of todos for the parts of the code that I know need improving.
{% include setup %} In the past I’d be wary of setting up a new computer knowing that every time I’d need at least a couple of hours to get everything into a workable state. These days I actually look forward to setting up a new computer. Nearly every file I care about is hosted online and a large chunk of my productivity apps are online as well. The only tools I need to run locally are the various IDEs as well as a variety of open source tools and libraries that my code depends on. Even then I’d bet it takes less than an hour to get things to an 80% state at which point I’ll only discover what’s missing by just going through my day.  And this is as a developer who needs to build various applications from source and deal with potential library conflicts. For someone who doesn’t have to deal with these issues it must be incredibly quick to get a workable setup these days.  At the same time it seems as if we need to upgrade our computers less frequently since for most tasks computers from a few years ago are good enough. In fact I’m still on an early 2011 MacBook Pro with some upgraded RAM and a new SSD drive. I don’t even notice any performance difference between it and a newer MacBook Pro at the office. And for most tasks why even bother upgrading a computer when you can get nearly everything done on a tablet? Just attach a keyboard and you’re good to go. I can’t imagine us ever going back to the pre cloud days of computers and I can only imagine what kind of digital productivity tools we come up - especially with the rise of VR and the constant improvement in the performance and battery life of our existing digital devices.
{% include setup %} Yesterday I attended a concert at the Newark Museum and ran into a fairly common situation when lining up to get in. They had a suggested donation amount, which is entirely optional, while at the same time they provided free admission to anyone with a Bank of America card due to Bank of America’s sponsorship. I’ve seen the same sort of setup at museums in New York and I suspect it’s common elsewhere in the United States as well as abroad but the entire concept strikes me as odd.  I understand that it’s a way for Bank of America to reward its customers but because the admission was a suggested amount it made me feel as as I’m neither contributing towards the museum nor as getting any value from Bank of America. Without my debit card I would have felt noble contributing when I didn’t have to but with the card it feels as if Bank of America is giving me a way to avoid feeling guilty.  I’m sure these thoughts are irrational and I’m overthinking it but the process struck a weird chord with me and I’m surprised I haven’t noticed it before.
{% include setup %}                                    A couple of months ago I took a stab at plotting the Jersey City [parking zones](http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/) after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome [comment](http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530) pointing out that Jersey City has a bunch of open data available, including a near-real time feed of [garbage truck locations](http://www.jciaonline.org/gpsMap.php?view=map), a general [open data portal](http://data.jerseycitynj.gov/), as well as the ability to [request custom data](https://jerseycitynj.seamlessdocs.com/w/records_request). As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at [http://dangoldin.com/jersey-city-open-data/garbage-trucks/](http://dangoldin.com/jersey-city-open-data/garbage-trucks/) and the code is up on [GitHub](https://github.com/dangoldin/jersey-city-open-data).  The approach I took was straightforward. After going to the real time map I opened the network explorer in order to see the HTTP requests being made to update the map with the latest truck locations. It was a single URL call that was returning a pipe delimited file containing the location of each truck. By writing a simple wget script and setting it as a cronjob I was able to capture the truck locations every minute. After a day’s worth of data I combined the files and removed duplicate lines (for when the trucks stayed in a single location). After that it was simple to use the Google Maps API to draw a route for each individual truck. The neat thing here is that 90% of the work was done through simple shell commands. One command to fetch the data every minute, another to combine them into a single file, and then a few others to sort and dedup the data. By the time I got to coding all I needed to do was convert the data from a pipe delimited file into something that could be consumed by the Google Maps API.
{% include setup %} The accepted belief is that startups should move quickly and err on the side of speed rather than quality. This makes sense. Startups are so risky that they won’t fail due to making a few mistakes but will fail if they get out maneuvered and out innovated. The big advantage startups have is speed and that needs be leveraged.  The one caveat I’d make is that every company, big and small, should have mission critical elements that need to be maintained when pushing new features and updates. I was reminded of this last week when an unnamed corporate feedback startup sent out the private one-on-one notes people jotted down in preparation for their meeting to everyone within the company. This was a huge betrayal of trust and ruined the good will people had for the company and the product. If they weren’t able to get this basic piece right how are they expected to do the rest? Every company has these mission critical components that everyone needs to be aware of and great care must be taken to ensure they work before every deploying or change. In the adtech case it’s serving ads - if ads aren’t working then publishers aren’t making any money and losing money during each impression. For cloud productivity applications it’s critical that they don’t lose your data - downtime is annoying but at least you can switch to another task while they get back up. If you lose your data and documents you have to figure out exactly what you lost and decide whether it’s worth recreating. Everyone in the company should know what these these mission critical components are and it’s everyone’s job to make sure they’re working as expected since failure carries existential risk for the customer relationship. It’s unlikely that a single bad event will ruin things but as soon as it becomes a pattern it’s likely that that customer will be lost forever and never return due to the faulty first impression.
{% include setup %} A large part of modern software engineering is working with external APIs and services. Whether you want to automate a deployment on AWS, collect payments via Stripe, or track various behaviors using MixPanel, the process is the same - go through their documentation to figure out the available endpoints, the request requirements, and what the response will be. The next step is writing a simple API wrapper around the relevant endpoints that can then be accessed by the rest of the application. Given all the investment in AI research I’d love to see an application that’s able to generate API wrappers in any language for an API based solely on the documentation. Amazon has taken the first steps by [developing a data model](https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/) to represent their API which is then used to generate the actual libraries in a variety of languages. By changing something in the definition they can quickly rebuild the libraries in every language. One can also imagine using this data model to generate the actual documentation. This documentation can then be used to go back to the data model which can then be used to go back to the documentation.
{% include setup %} If you go to the gym you end up getting a lot more out of it if you approach your workout with a plan in mind. The same thing happens with work. If you go every day you will inevitably get better but if you come in with concrete goals and ways to push yourself you’ll be in a much better position. It’s not as easy to measure your performance at work compared to the gym but just taking the first step and realizing that you want to improve is already beyond how most people approach work. Just by thinking about your performance you improve your ability to identify your strengths and weaknesses. Doing this on a consistent basis gets you into the habit of being introspective and improves your self-awareness, which is necessary to improve.  I’ve approached this problem by jotting down notes and thoughts throughout the day and then having a non-disruptive, scheduled time each week for me to go through and digest it. Some weeks are slower than others but more often than not I gain valuable insight on what I did well and what I can improve. Writing these notes has become a habit and I don’t even need to focus on it anymore. The value of these is that over time you end up having a log of your improvement and can acknowledge how much you’ve grown as well as how much is still left. I’ve also discovered themes from week to week that have helped me improve as an engineer and a manager. If you haven’t been approaching work the way you approach the gym you’re losing valuable time and opportunity to improve yourself. You’re already spending at least 40 hours a week at the office so you should make it as valuable as you can.
{% include setup %} Recently I’ve found myself have similar conversations with various members of the engineering team regarding the tradeoff between speed and quality. Every situation is different but without going into project details I've found that quality come first, speed second. Not because I think speed is unimportant but because I think quality is underrated. In the desire to push the next feature and launch the next product quality tends to be sacrificed. This is fine as long as we understand the tradeoffs but in most cases those are externalized to others. For example, if an engineering team ships a buggy feature, the engineering team only incurs the cost of fixing it, and even then only if they end up fixing it. Simultaneously, the cost is passed on to the users who are powerless to fix it. And then it goes through multiple tiers - first the end user who becomes inefficient and may lose work, then the support person responsible for dealing with these issues, the product manager who has to context switch to both understand and prioritize the issue, and finally the engineer. During each step time is lost but most importantly is the interruption of [flow](https://en.wikipedia.org/wiki/Flow_%28psychology%29) for multiple people, each of whom gets distracted from what they’re doing in order to deal with a problem that could have been prevented in the first place.  I'm also skeptical of the quality versus speed tradeoff and believe that both can be achieved. I've worked with many people who have been able to deliver both and believe it's a skill that can be developed just like any other. Some situations do force a tradeoff but I suspect these are in the minority for a good engineer. Even then I would push for a refactor after it's deployed in order to bring the quality up to par. Focusing on quality also builds better habits - you'll get quicker naturally over time but if all you do is prioritize speed your quality won't improve. By focusing on quality first your speed will improve on it's own.
{% include setup %} By examining my family’s phone usage you get an interesting representations of how different generations use their phones. The five of us - parents, younger brother, and younger sister - are all on the same plan and T-Mobile breaks down the usage into phone minutes used, messages sent, and total amount of data used. I played a game with my friends to see whether they’d be able to decipher who’s who but it turned out to be surprisingly difficult and unintuitive. Turns out that my teenage sister uses least talking minutes and data but consumers average number of texts. At the same time, my mom runs a business and has close to 2000 minutes of talk time with the most number of texts sent while only using a moderate amount of data. My brother and I have a similar usage pattern - low minutes and messages but the highest data usage out of the entire family. The biggest surprise is my sister’s data usage that reinforces how much time teenagers are spending via apps and on separate social networks. That stereotype I grew up with of teenagers being constantly on their phones is still true - texting and talking have just bee replaced by siloed apps.