num_text_description,num_images,description,tags,text,num_links,num_tags,date,num_keywords,keywords,num_text_words,title
14,0,I respond to Robert Cringely's PBS post about Apple's acquisition of PA Semiconductor.,#product,"This post is a response to  Robert Cringely's PBS Post . He's giving 2 reasons for the acquisition and I wanted to add to two of his points.          The short term reason is to force Intel to give Apple price cuts for fear that Apple will make their own chips: I do not think that Intel needs to worry about Apple manufacturing their own PC chips as Apple already went through that phase and AMD already provides the necessary pressure on Intel to lower their prices.        In the future, software and OSes will not be tied down to a specific chip so Apple will start manufacturing their own processors to increase their margins: I think the author is on to something regarding the future of processors but I do not think the PC market will change that drastically. Apple will probably start making their own chips for the iPhone and their new gadgets but I doubt they will do the same for the PC market.      Edit: Just found out that Apple already makes a server (Thanks Brian). I have to start doing some research from now on.",1,1,2008-04-25,3,"apple, business, PA semiconductor",197,On Apple buying PA Semiconductor
16,0,Video games should be treated as art the same way other forms of entertainment are.,#meta,It's about time video games are considered an art. They are creative endeavors that take as much and as long to make as some movies. There is also a huge distinction between the great and the poor games. Games these days can be considered movies and they should be treated in the same way.  Maybe then will video games no longer be the black sheep of the media and entertainment business. It's ridiculous that some guy throwing feces at a canvas is considered higher than the developers of video games.  Video games are no longer the simple things of 20 years past but have evolved into their own worlds and stories and should be given the credit they deserve.  It's a shame people are blaming video games for the violence instead of their own parenting ability. And if they actually cared about how their kids were being raised maybe they should stop the filth on TV from being shown and replace the TV babysitter with themselves.  No one can argue that TV has been getting more and more sexual and violent yet the finger is always pointed at video games - it's time this stops.,0,1,2008-04-29,3,"video games, entertainment, art",188,Video games as art
9,0,Just my 2 cents on the EA/Take Two acquisition,#product,"I'm not sure why no one is pointing this out but it seems that as soon as EA acquires T2, the T2 folks would just leave to create a new studio. They seem independent and I doubt that they'd want to work for EA. And although EA would get the rights to all of the T2 games and may try to develop the series, they might not have the imagination or the guys to do it. In addition, I don't know whether EA would even want to develop such products as GTA and Bully given the violence and public relations ordeal that T2 has been going through the past few years.  What EA does want is probably the T2 sports games, then they'll have a pretty strong hold on the sports games' market.  So as long as EA does not want the T2 human capital it seems like it makes sense - but given the negative response of T2 to the offer, I doubt EA will be able to retain the developers and artists.",0,1,2008-04-29,3,"electronic arts, ea, take two",173,Why does EA want Take Two?
16,0,I just wanted to share my thoughts on the Microsoft walking away from acquiring Yahoo.,#product,"A little unexpected but I think this was the right decision for Microsoft for a couple of reasons:  1. Jerry Yang was doing everything in his power to prevent MS from acquiring Yahoo - what type of message does that send and how must the employees of Yahoo feel if their CEO is acting this way. 2. In order to compete with Google, the merger must have gotten done quickly and smoothly, this would not have been the case in the case of a hostile takeover. 3. There have been rumors that many of Yahoo's employees were just waiting for the merger to happen in order to cash in on their new accelerated vesting and compensation packages, immediately leaving Yahoo there after. 4. A lot of the Yahoo employees and MSFT employees and investors did not want the merger to go through. From what I know the largest group that wanted the merger to go through were the Yahoo investors, albeit at a higher price. 5. Yahoo will now have to deal with a variety of problems: shareholder lawsuits, talk of Jerry's management ability, sudden price drop. All these may in fact lead to another Microsoft offer in maybe a few months at a much lower price - in which case it will probably be accepted.  The only downside that just immediately to my mind is that MS and Yahoo are in deep water and do need to do something in order to compete with Google - but I do not think the merger would have necessarily helped them, given the merging difficulties and the corporate culture clash.",0,1,2008-05-04,3,"microsoft, yahoo, acquisition",266,On Microsoft walking away from Yahoo
23,0,"It's ridiculous that we have homeless people in the US when it costs $25,000 to keep someone in prison for a year.",#meta,"I find it absurd that the average prisoner costs $25,000 a year to keep in prison ($75,000 for death row) and yet we still have homelessness in the United States. Each one of the homeless may start committing some type of crime in order to get into prison and at least not worry about where their next meal comes from yet they are staying on the streets as free men. Isn't there something we can do to encourage good behavior instead of encouraging bad behavior?  Furthermore, a large percentage of the homeless are veterans and should not be ignored.",0,1,2008-05-04,2,"homelessness, prison",100,The homeless in the US
8,0,Just some observations I have about pronunciation.,#meta,"Recently I've discovered a few words where I know the definition and spelling but I don't know the pronunciation. The problem is that I think I know how to pronounce them so when I use them for the first time in conversation, or hear someone using them, some confusion arises (as well as making me look like a fool).  I am not sure if this is isolated to me or society as a whole. The world does seem to be getting more and more open so maybe this is a result of that - the spoken dictionary is getting smaller and smaller so many of the words that were commonly used in conversations decades ago are no longer being used.  In any case I think we should all try to identify such words and try to use commonly written words in conversation, we may be surprised by how wrong we are.  For me, the word I didn't know how to pronounce was ""albeit."" I was pronouncing it as all-bite when it turns out that it is pronounced all-be-it.",0,1,2008-05-08,1,pronunciation,170,Pronunciation
8,0,Why don't printers come with printer cables?,#meta,"Since when did printers stop coming with the cables? Does this have anything to do with printers being available in USB form and the manufacturers suddenly assuming that everyone already has USB cables? Or is it some agreement that they have with merchants that requires me to pay $20 for a 6' cable. Some quick price look ups do show that the standard printer cables cost around the same as USB cables so if they were able to afford to bundle printer cables before USB, they should be able to bundle USB cables now.  Does anyone else find this ridiculous?  Edit: My father tells me that they never came with cables and that I am misinformed. He's probably right since I wouldn't trust my childish memory with regard to such things. In any case, printers should come with cables.",0,1,2008-05-13,2,"printers, hardware",140,Printers no longer come with cables?
20,0,Based on the competitiveness of human nature I believe we'll kill each other before we end up colonizing space.,#meta,"Although it's wonderful to think that we will be able to colonize other worlds when we grow too numerous or run out of resources, it may not happen. It seems that given the level of current weapons and state of the world we will more likely try to conquer each other than try to conquer space.  In order to go into space we would need to have an advanced level of technology which could only be created through innovation. I am just worried that technological advances tend to be used as weapons first, and as humanity benefiting objects second. This may have been fine with the technology of old but we are approaching the level where a weapon can wipe us out. The non warlike use may not come to fruition if there will be no one left to develop it.  I just hope we realize that the next global war may be the last war we fight and that we need to control our competitive spirit. A quote that comes to mind is Albert Einstein: ""I know not with what weapons World War Three will be fought, but World War Four will be fought with sticks and stones."" I'd just like to add that I'm not so sure there will even be a World War Four.",0,1,2008-05-13,2,"space, colonizing space",219,Why we may never colonize space
8,0,Some thoughts on using brainteasers in interviews.,#meta,"I've recently been reading some articles opposing the use of brainteasers during interviews on the grounds that they are unfair and some people have difficulty thinking on the spot. You can make the same argument for any part of the interview process and I feel that brainteasers may even attract intelligent employees.    I can come up with a few good reasons to use brainteasers during an interview. One, you are able to determine how well the interviewee thinks as well as their problem solving ability. In addition, if the interviewee does end up getting a job offer, he or she may be more likely to accept it since it was a challenging interview and getting the job feels like an accomplishment - feels better when you have to earn something than when it falls into your lap. The fact that you even asked a brain teaser shows intelligence on your part and you want to attract people who want to work with other smart people, instead of being the big fish in a small pond.    Ideally, you would want to find some brainteasers that have multiple ways of solution so you are able to identify how each of the interviewees thinks but I think a variety of brain teasers can achieve the same effect. Below are few good questions/brainteasers I enjoy.     	 What was the last book you've read? What's your favorite book? (Not a brainteaser but I believe a good question nonetheless)  	 You have a lighter and 2 ropes that are non-uniform. It takes a rope 1 hour to burn from one end to the other end. How do you measure 45 minutes?  	 You have 3 pairs of (x,y) points that determine a triangle. How can you determine if this triangle contains the origin? (from Project Euler)  	 Which is larger, 48736^95934 or 44390^96771? (Also from Project Euler)  	 How do you split a cake of nonuniform size between 2 people? Can you expand this to any number of people?      Feel free to comment or email me if you are unsure how to solve a particular problem.",0,1,2008-05-14,3,"brainteasers, interviews, project euler",350,Brainteasers and interviews
6,0,Welcome to the new blog.,"#meta,#blog",So I am porting this blog over from wordpress.com to my own local hosting. Please bear with me and I'll hopefully have more things to read soon.    Edit: I did a rough job changing the dates in the database so the posts should all have the actual post date now.,0,2,2008-05-21,1,welcome,53,Welcome
7,0,Photo from a trip to Martha's Vineyard,#photo,A nice change of scenery for the long weekend. It's amazing how quiet the nights are when you are not in the city.,0,1,2008-05-24,1,martha's vineyard,44,Martha's Vineyard Lighthouse
8,0,We shouldn't just writing by the author.,#meta,"A common idiom is ""Don't judge a book by its cover"" but I think that in this modern age this needs to rehashed into ""Don't judge words by their author.""    How often do we look at the author before we read an article or blog post? And how does this impact the way we absorb it? Studies have been done[1] to show that the same words coming from two different people, one a professor and one an average Joe, are interpreted differently: the professor is trusted while the average Joe is not. This can be expanded to any source of information, anything from a book to a YouTube video. In the past, these sources of information were concentrated - not everyone could write a book, but now anyone can start a blog to spread their thoughts and opinions.    In such a world, it's becoming increasingly important to come up with our own opinions and facts and applying a ""trusted"" filter may just be the shortcut we developed to not actually have to think about what we read. We need to be aware that knowing who the author is exposes the author's biases but it also creates biases in the reader.    Try reading something before looking at who wrote it and see if changes how you read. If you can control yourself, don't even look for the author after reading the piece.    [1] I'll try to look these up and update the post.",0,1,2008-05-28,1,writing,246,Don't judge words by their author
13,0,"I found an odd, unexpected behavior in Perl that drove me crazy.","#code,#perl","I ran into this problem a while back and wanted to share it. It was a bit unintuitive but documentd so I guess I shouldn't be surprised by the results. Hopefully this will help someone else avoid this pitfall.    It looks as if declaring a variable with the ""my"" statement but then guarded with an ""if"" statement causes the scope of the variable to be global - note that the ""use strict 'vars';"" pragma does not give an error in this case.   {% highlight perl %} #!/usr/bin/perl -w use strict  'vars';  sub foo{     my $val = 0 if (0);     $val = 1 unless defined($val);     print ""Val: $val\n"";     $val = 2; }  foo(); foo();{% endhighlight %}   The output of this call gives:  Val: 1  Val: 2     Although the expected result would seem to be:  Val: 1  Val: 1     Using Google, I found the following nugget from perlsyn:    NOTE: The behaviour of a my statement modified with a statement modifier conditional or loop construct (e.g. my $x if ... ) is undefined. The value of the my variable may be undef, any previously assigned value, or possibly anything else. Don't rely on it. Future versions of perl might do something different from the version of perl you try it out on. Here be dragons.  http://perldoc.perl.org/perlsyn.html#Statement-Modifiers",1,2,2008-05-30,2,"perl, coding",226,Interesting Perl behavior
15,0,Some thoughts on the changes in privacy given the rise of the social networks.,#meta,"With so many people joining social networks like Facebook, MySpace, and LinkedIn, it's becoming harder and harder to protect your personal information. If one of your friends happens to add a host of different facebook apps, those apps will have access to his friends' (your) information. There is nothing you can do to stop this unless you either remove all your friends or create very limited profiles.    In addition, people have come to expect to be able to add you as a friend after they've met you and rejecting them may be construed as anti-social. Imagine a recruiter not being able to look at your information on LinkedIn or a potential date not being able to look at your interests or photos on facebook - you will be missing out on opportunities.    How is one supposed to play this game where you want your information both hidden and shared? My solution is to embrace this lack of privacy: integrate yourself into as many social networks as you can, start a blog, post on various forums, publish your photos on Flickr, and so forth. By being famous (if only on the internet) you will eliminate a lot of the adverse effects of having your information public. You will have enough of a community to support you in case anything goes wrong and you can stop worrying about your information being shared.    How often does Bill Gates worry about his identity being stolen?",0,1,2008-06-12,5,"privacy, social networks, facebook, myspace, linkedin",245,Privacy in the digital age
8,0,Some advice on setting up internet passwords.,#meta,"After my previous post on the lack of privacy, I feel obligated to give some advice regarding internet passwords in order to maintain the privacy that we do have.          Have at least 3 different passwords:           	 E-mail Account          This account controls all your other accounts so protect it as much as you can. All other accounts can be accessed or reset if someone has access to your email.     	 Bank/Financial Accounts          These control your money so use a different password for these than for the rest of your accounts. In addition, you may want to keep your credit card account passwords separate from your bank accounts.     	 ""Fun"" Account          These may not be vital to your survival (unless you are a facebook addict) so a password compromise here may not affect you too much. In addition, these sites may not store your password as securely as the bank accounts so you don't want this password being the same as the other accounts.            A good way to generate passwords is to contain some sort of ""base"" and add some prefixes or suffixes to it in order to come up with the password for the various sites. For example, I can have my base password be ""orange"". For financial sites my password will be ""orangeFIN22"", for my email it will be ""orangeE33"", etc. Then you don't have to remember an entirely different set of passwords yet they are distinct enough to avoid compromising all your accounts with a stolen password.              Don't trust web sites that are able to send you your password over email       If a website is able to tell you what your password is, it means it is storing it in the database as either the password itself or through a transformation that is reversible (a becomes b, b becomes c, ..). This means that the site knows what your password is and can be easily accessed by employees of the site or anyone that has access to the database.        The proper way to handle user passwords is to hash it (one way map) immediately to some obfuscated characters and store those in the database along with an additional field that ensures each row is hashed differently. Then when a user logs in, the site will do this one way map and compare the result against the value in the database; omly if they match is the user logged in.",0,1,2008-06-19,3,"security, passwords, internet",408,Advice on internet passwords
15,0,TV commercials feel as if they're a lot longer than they used to be.,#meta,"It may be my memory but it seems that TV commericals have been getting longer and longer as compared to a decade ago. It would be very interesting to see a plot of the length of the average commercial break over the past few decades - I think we'll see that the length of the average commercial break has drastically increased.    In addition, it seems as if there are no commerical breaks between consecutive TV shows anymore. Clearly this is a way to keep us from turning off the TV and doing something productive with our lives.    Does anyone have any thoughts?",0,1,2008-09-03,3,"tv, commercials, advertising",104,On TV commercials
9,0,Some crazy ideas on a new voting system.,#meta,"It seems that there is a recent emphasis on ""character"" in the election. Unfortunately, character can be faked with some acting and campaign management. To understand how the candidates will perform in office we need to look at their past accomplishments and failures. Public records show the  vote history  for the candidates and all it takes is a little bit of research to see how the candidates have voted.    But people aren't interested in looking at data; they are more interested in how much candidates spend on their haircut or who looks more ""confident."" I imagine that before radio or television existed and the only news source was the newspaper, candidates would have stood on their issues alone. The vast majority of the population would not have seen the candidates in person and would have to have focused on the issues each candidate presented. Maybe it's time we go back to those days with a ballot only containing issues.    In addition, why not throw in an intelligence test geared towards the issues and use the score to weigh the vote. If companies can use intelligence in their hiring decisions, why can't the government use it in the voting process? The impact of a wrong decision is much greater.",1,1,2008-09-12,2,"voting, politics",217,Voting system proposal
16,0,Does it make sense to apply the EU model of member countries to US states?,#meta,"I've recently been thinking about whether the US can move to a EU like model with each state having control over it's own policies but sharing a  single market  and monetary union. In addition, competition is well regulated and a shared budget exists. In addition, it looks as if this shared budget is a little over 1% of the  Gross National Income  of the individual countries (1) - imagine a Federal tax rate of 1%.    Clearly, the states would then have to handle more of the lower level administration but that may be for the best. Each state is different and must be governed differently. States with a large agricultural focus should have different policies than states with a large technology focus. States with a highly religious population should have different policies than the more atheist states. Under such a system, some states may end up doing better than others in the short term but if it becomes obvious that certain policies work, the other states would have adapt in order to compete, thus improving the US as a whole.    I believe that such a system plays on the strengths of the federal government as well as the strengths of local governments. It will still be easy to travel from state to state, use the same currency, and not deal with trade barriers but each state will have it's own social and cultural policies that reflect its population.    It just seems that the federal government cannot pass laws that will be beneficial to all states at once and so there is some form of a standstill. Maybe this focus on a more state-centered model is the approach to take.    In the future, I would like to take a look at the economic growth of the individual countries before the creation of the EU as well as after the creation of the EU adjusted for the overall growth of the world markets. I have a feeling it would show that the creation of the EU encouraged the growth of the individual markets.    Notes: (1)  http://en.wikipedia.org/wiki/European_Union#Budget",3,1,2008-09-15,3,"politics, united states, european union",370,Should we apply the EU model to the US
8,1,I share some thoughts I have on innovation,#product,"I stumbled unto an  article  written in 1968 that tries to predict what the world of 2008 will be like. Usually, these types of predictions are completely off and tend to predict a future far more advanced than what it actually becomes.           As expected, the article had it's exaggerations (automatic cars driving at 250 miles per hour, inter-continental rockets, average work day of 4 hours ) but what struck me the most is how accurate the predictions about computers are:    The single most important item in 2008 households is the computer. These electronic brains govern everything from meal preparation and waking up the household to assembling shopping lists and keeping track of the bank balance. Sensors in kitchen appliances, climatizing units, communicators, power supply and other household utilities warn the computer when the item is likely to fail. A repairman will show up even before any obvious breakdown occurs.  Computers also handle travel reservations, relay telephone messages, keep track of birthdays and anniversaries, compute taxes and even figure the monthly bills for electricity, water, telephone and other utilities. Not every family has its private computer. Many families reserve time on a city or regional computer to serve their needs. The machine tallies up its own services and submits a bill, just as it does with other utilities.  Money has all but disappeared. Employers deposit salary checks directly into their employees’ accounts. Credit cards are used for paying all bills. Each time you buy something, the card’s number is fed into the store’s computer station. A master computer then deducts the charge from your bank balance.  Computers not only keep track of money, they make spending it easier. TV-telephone shopping is common. To shop, you simply press the numbered code of a giant shopping center. You press another combination to zero in on the department and the merchandise in which you are interested. When you see what you want, you press a number that signifies “buy,” and the household computer takes over, places the order, notifies the store of the home address and subtracts the purchase price from your bank balance. Much of the family shopping is done this way. Instead of being jostled by crowds, shoppers electronically browse through the merchandise of any number of stores.    Compared to the rest of the predictions, this is amazingly close to what we currently have. There is still some emphasis on the server and treating computers as a utility that is not currently present but with the rise of Google Docs and other online tools, that is not such a distant notion.    This begs the question, why are the predictions so close when it comes to computers but so off when it comes to other technologies? More importantly, why does  Moore's Law  apply to transistors but not to larger technologies? I have a few ideas:     	 Infrastructure costs - it's cheaper to replace modern day computers than modern day cars. Thus, innovation can happen at a faster pace as people replace their computers. Also, computers tend to work in a much more solitary environment than cars do; being able to drive a car 300 miles per hour is useless when the roads can't take it and the laws prevent it.  	 Experimentation - it's easier for the average person to hack around on a computer than it is to hack around on a car. Thus, a lot more people are working on ideas and due to sheer numbers, more ideas are bound to stick.  	 Brand new technologies breed creativity - When computers were invented, no one knew what they were capable of and everyone had ideas as to how they could be used. Many people pursued their ideas and were able to create and improve on various technologies. Also, many teenagers and students were involved in embracing this new technology - and they didn't know what was impossible so they reached for the stars.Now, everyone is so used to what cars are that people don't even imagine what cars could be capable of. We may be approaching this same plateau with computers.      I'll try to update these when I think of any more.",3,1,2008-10-01,2,"innovation, technology",791,Some thoughts on innovation
13,0,A simple analysis to see whether having weekend vs weekday election days matter,#datascience,"I found an  op-ed  in the NY Times that claimed that the best way to increase voter turnout was by having election day fall on a weekend. They provide a few examples but nothing too detailed. I tried pulling in some data and seeing if I could come to the same conclusion. I used two data sets:  voter turn out by country  and  election dates by country .    Combining this data into one table, and ignoring the missing data:          Country   Election Type   Date   Day of week   Weekend   Turnout           Czech Republic   Presidential Final   Fri 2/15/08   6   No   85%       South Korea   Parliamentary      Wed 4/9/08   4   No   75%       Canada   Parliamentary      Tue 10/14/08   3   No   76%       Czech Republic   Parliamentary      Fri 10/17/08   6   No   85%       United States   Presidential      Tue 11/4/08   3   No   54%       Romania   Parliamentary      Fri 11/28/08   6   No   81%       Russia   Presidential      Sun 3/2/08   1   Yes   61%       Malta   Parliamentary      Sat 3/8/08   7   Yes   94%       Spain   Parliamentary      Sun 3/9/08   1   Yes   73%       Italy   Parliamentary      Sun 4/13/08   1   Yes   90%       Iceland   Presidential (Cancelled)      Sat 6/28/08   7   Yes   89%       Austria   Parliamentary      Sun 9/28/08   1   Yes   92%       New Zealand   Parliamentary      Sat 11/8/08   7   Yes   88%         It does seem as if they are on to something - the average turnout for weekday election days was 76% while the average turnout for weekend election days was 84%. This wasn't a very rigorous examination and I am sure there are many more issues that factor in to the voting process but it does make intuitive sense.",3,1,2008-10-24,3,"voting, elections, politics",305,On weekend voting
5,0,An excuse for not blogging,"#blog,#meta",I've been busy recently but have a bunch of topics that I want to write about so keep on checking.  Thanks for reading!,0,2,2009-01-20,1,blogging,23,More posts coming up
15,0,"The easiest way to become president is to run for a second term.""",#datascience,"Answer: Be elected for a first term, the second term will follow.  It turns out it's pretty likely that a president will be elected to a second term. If we examine all previous Presidential Elections, we will see 8 presidents who failed to get reelected:             President       Result           Benjamin Harrison   Failed to get reelected in 1892       George H. W. Bush   Failed to get reelected in 1992       Herbert Hoover   Failed to get reelected in 1932       Jimmy Carter   Failed to get reelected in 1980       John Quincy Adams   Failed to get reelected in 1828       Theodore Roosevelt   Failed to get reelected in 1912       William Henry Harrison   Failed to get elected in 1836       William Howard Taft   Failed to get reelected in 1912         On the other hand, if we look at all presidents with 2 or more terms, we only see a few Presidents who have failed to get elected. Some of these, like Andrew Jackson, failed to get elected initially but were then able to get 2 terms in office. Grover Cleveland had non consecutive terms in office. In total, there were 16 presidents who had a second term.          President   Result           Abraham Lincoln          Andrew Jackson   Failed to get elected in 1824, was elected in 1828 and 1832       Bill Clinton          Dwight D. Eisenhower          Franklin D. Roosevelt          George W. Bush          George Washington          Grover Cleveland   Failed to get reelected in 1888 (was pres in 1884 and 1892)       James Madison          James Monroe   Failed to get elected in 1808, was elected in 1816 and 1820       Richard Nixon   Failed to get elected in 1960       Ronald Reagan          Thomas Jefferson   Failed to get elected in 1796       Ulysses S. Grant          William McKinley          Woodrow Wilson            Franklin Delano Roosevelt ran for, and won, a 3rd term in 1940 using the idea that one should ""not change horses in midstream."" He did not need to do that since it seems people stick with what they are comfortable with.    Note: If I made a mistake anywhere let me know so I can correct it. The data was retrieved from Wikipedia.",0,1,2009-01-21,3,"politics, elections, presidency""",417,"What's the easiest way to be elected president?"""
6,0,Enabled Apache2 modules in Ubuntu manually,#code,"The Apache enabled modules are found in  ""/etc/apache2/mods-enabled""  as a set of .load and .conf files. If the modules you want are in the  /etc/apache2/mods-available  folder but not in  ""/etc/apache2/mods-enabled""  folder, just copy the .load and .conf files over (note that the .conf file may not exist).    If there is no file in the mods-availble folder, you will need to create a new .load file in the mods-available folder to point to a module in  ""/usr/lib/apache2/modules"" . To do this, create a .load file containing the line  ""LoadModule xxx /usr/lib/apache2/modules/yyy.so""  where xxx is the name of the module and yyy is the file name. After creating this file, you can just copy it over to the mods-enabled folder and restart apache using  ""sudo /etc/init.d/apache2 restart"" .",0,1,2009-01-23,2,"apache2, ubuntu",140,Enabling modules in Apache2 under Ubuntu
13,1,"I finally realized a use case for Twitter - getting real time info""",#product,"I may be a bit late to the party but I was finally able to see the power of  Twitter  this afternoon.    I kept on getting an ""Authentication failed"" message when trying to log in to AIM. A few years ago I would not know what to do except ask my friends if they were having any trouble. Right now, I went to searched for ""AIM"" on Twitter and discovered that other people were having the same problem. Turns out it was a systematic problem and I wasn't the only one affected. Being able to know more about this problem is a great benefit. There has been a lot of talk of the power of real time search and real time news but this was my first real glimpse into the power of Twitter.    My first action wasn't to search for ""AIM log in problem"" on Google but to search for ""AIM"" on Twitter. It's amazing to me that Twitter was able to replace a certain type of search. The majority of my searches will still be done on Google but it seems that for anything with a pulse - Twitter search is the way to go.    Twitter's character limit is a great way to take advantage of the network effect. Having a low character limit encourages a lot more users to tweet thereby making Twitter feel alive and giving everyone else more information.    There have been numerous ideas of Twitter being used for market research and to get an early customer response - I can imagine that happening now and it's mind blowing.",1,1,2009-03-08,1,"twitter""",299,"Power of Twitter"""
6,0,How to improve your luck.,#meta,"People often blame bad luck for their failures. This absolves them of responsibility and allows them to stop trying. What they should have done is admitted their failure, learned from the experience, and prepared themselves for the next opportunity. Exposing yourself to opportunities is the best way to overcome bad luck. Authors are a great example of this:  J K Rowling  and  John Grisham  had their novels rejected numerous times before they succeeded. Yet soon after publishing they became blockbusters. How many authors gave up when trying to have their work published? Imagine if they had the determination that J K Rowling and John Grisham had.  As  Seneca the Younger , a Roman philosopher, said, ""Luck is what happens when preparation meets opportunity."" In order to increase your luck you need to increase your exposure to different opportunities. In addition, you need to realize an opportunity when it presents itself. A way to view this is through the simple roll of a die. Although a die only has a 1 in 6 chance of rolling a 1 when rolled once, it has a greater chance of landing on a 1 when rolled multiple times. Most people give up after a few rolls but in order to succeed you need to keep on playing the game until you get a successful roll.  The fact that you are reading this shows that you are luckier than the majority of the world's population. You have access to the internet and the desire to improve your luck. You can leverage that to contact leaders in your field or people who can help you succeed. You may get no responses from some but you will get encouraging responses from others. You just need to be open and increase your opportunities.",3,1,2009-09-28,1,luck,349,Improving your luck
5,0,Another waste of a post,"#blog,#meta",I will try a new policy - at least one post a week. Harass me if I'm not doing it.,0,2,2010-03-10,1,blogging,20,More posts coming soon
15,1,It seems it's a conflict of interest to be giving your employees free Adwords.,#product,"According to this  blog post   Google  gives their emplo﻿yees $1 a day to advertise on Google. The intent is to give employees the perspective of an Adwords user in order to improve the product. In addition to giving employees exposure to Adwords, this also has potential to increase the competition in  Adword  auctions by causing bids to increase and leading to more revenue  for Google. Given that Google has more than 10,000 employees worldwide this can have an effect on smaller advertisers.     Other people have written about Google advertising for their own products and giving employees free money can have a similar, although smaller, effect.",3,1,2010-08-25,3,"google, adwords, employees",148,Google Giving Employees Free Adwords?
10,1,"I share some more thoughts on conducting tech interviews""",#code,"{% include setup %} When conducting interviews, I've developed the following criteria for a good interview problem:    	 Avoid brain teasers - they tend to be hit/miss and some people don't really do well under this type of problem  	 Challenging - the answer should not be immediately obvious and the should require some creativity  	 Rare - similar to above, the problem should not be a common question in order to get  	 Flexible - the problem has multiple solutions and can be modified on the fly for different skill levels     I've found that the following problem satisfies the criteria and gives a pretty good sense of a developer's skill level.  The problem starts of as a simple scenario:  You need to write a program that will accept a list of words. After the words are entered, the user will enter words and your program will need to indicate whether the entered word was in the original list. How would you design this program and what data structures would you use?  The typical answer to this question is to either store the initial word list in an array, a tree, or a hash. If it's an array or a tree, we talk about the Big O of the solutions and compare that to just using a hash. At this point you can get a sense of whether the person you're interviewing understands basic data structures and knows the use cases of each one.  To dig deeper, I add a twist:  Now imagine you were transported back in time and it turns out your program uses too much memory and you can't keep track of every word. Do you have any alternative solutions?  The creative solutions start appearing here and you can get a pretty good sense of the problem solving skills. For example, a proposed solution at this point is to use word roots or repetitive letter combinations in a tree like structure to reduce the memory usage. We then talk about the algorithm that would need to be written and try to point out possible problems and see how they would be addressed.  And a final twist:  Let's say you still do not have enough memory and but you find out that you don't need to be correct all the time. Can you think of any solutions that can achieve this?  At this point, many people will try to come up with a heuristic or machine learning technique to try to identify words that resemble the words previously entered. We can then talk about both how to construct the algorithm as well as talk about the accuracy of the approach. It turns out that for these solutions it's difficult to quantify the trade off between error rate and space requirements.     At this point the concept of a Bloom Filter is brought up, either by me or by the person I'm talking to. If it's by me I go through the basic concepts (bit array, hash functions, probabilistic data structures) and can get a good sense of whether this is understood or I need to dig deeper. It's great when you can see the moment that someone ""gets"" the value of this data structure and knows immediately how to use it. At this point we discuss the trade off between the size of the bit array and the number of hash functions. If there's time, we'll work on deriving the relationship between the two as well as talk about where they can be used in the real world.  I wish I could remember how I came up with this problem - I think it stemmed from me encountering Bloom Filters for the first time as well as reading a few articles about spell checking and dictionaries.",0,1,2011-01-08,3,"interviewing, puzzles, brainteasers""",647,"Tech Interview Question"""
7,1,Some thoughts on GroupOn launching their scheduler,#product,"I'm not surprised that someone came out with an  online scheduling tool  for SMBs. I am a bit surprised that it was GroupOn though. I suspect many smaller companies have tried doing it but found selling to the SMB much more difficult than they expected. Since GroupOn already has penetration in the SMB space they may find it a lot easier, especially if, as it looks, they will be offering it free to any business that runs a GroupOn promotion.    This brings a good amount of value to the business:        Cheaper appointment booking system since fewer people will use the phone     Reminding customers about upcoming appointments      The real value is going to GroupOn though:        GroupOn can see how busy the businesses are (not just from GroupOns)     GroupOn can start offering a finer capacity management product - Imagine being able to see a haircut for $10 if you go in the next hour but $20 if you book it for tomorrow.     GroupOn will have major visibility into the way businesses operate and will be able to relate it back to the customer      I'll be watching this to see how it turns out but I'm glad to see this space innovating. As businesses get more comfortable running their business online it will become much easier for new companies to attack this space.",1,1,2011-12-08,2,"groupon, small business tools",238,GroupOn Scheduler
5,1,Some thoughts on GoDaddy/SOPA situation,#meta,"I'm not entirely sure what to think about the GoDaddy/SOPA situation. On one hand, it's great that the online community was able to get GoDaddy to completely reverse their position on SOPA. On the other, it's disappointing that a web company would support it in the first place.    Should we still be penalizing GoDaddy for their initial SOPA support or move on the same way they did? What type of example does this set for other companies? That they will be judged based on their original position and nothing after? Why even reverse your position if the community will behave as before?    In the meantime, there are  many companies  that support SOPA that the online community is not rallying against, including the majority of television networks (ABC, CBS, Disney, ESPN, Time Warner), and yet we still continue to watch tv. Our expectations for media companies are different than web companies, but do they have to be?",1,1,2011-12-30,2,"godaddy, sopa",171,"Screwed if you do, screwed if you don't"
8,1,Some thoughts on making subways in NY better,#design,"The combination of taking the subway every day and reading design books had me thinking of ways to improve the subway user experience, other than the obvious one of making it cleaner.    One thing that struck me is the feeling you get when you see the train leaving the station. It&#8217;s annoyingly stressful and makes me wonder how long I have to wait until the next train comes. Anything that can avoid this outcome would make waiting for the train a better experience. A way to do this is to limit the sensory feedback provided by seeing and hearing it leave. To avoid seeing the train until the last minute, subway stations can be designed to have stairs that need to be climbed in order to get to the platform. This way, the train and track will be hidden until the platform is reached. Making the train quieter would reduce the noise and prevent you from being aware that a train has left.    Of course, another cheaper and simpler way to deal with this is to just have something to occupy your time when waiting for the next train - that&#8217;s why having an iPad preloaded with ebooks is great.",0,1,2012-01-12,2,"subways, nyc",211,Improving the Subway User Experience
9,0,What does Shakespeare have to do with startups?,#meta,"I was reading Ben Yagoda&#8217;s book,  When You Catch An Adjective; Kill It , when I came across the following passage: &#8220;In Shakespeare&#8217;s day, there were no fancy props, so the text had to do the work of stage settings.&#8221; Although it was referring to starting sentences with conjunctions, it got me thinking about constraints and the way they foster innovation. Startups operate the same way: you don&#8217;t always have the resources to do what you want and are forced to innovate a way out.    Here are some examples:     Google realized that scaling vertically was significantly more expensive than scaling horizontally using commodity hardware. To achieve this, they had to create the  Google File System  to deal with the thousands of computers and the frequent hardware failures.    Many computer science algorithms and data structures were created when the CPU speeds were low and memory was lacking. Low CPU speeds led to more efficient graph search algorithms like Djikstra and A*. Limited memory led to the creation of probabilistic data structures like Bloom filters and Skip lists.     The Oakland Athletics baseball team could not compete on salary. But under Billy Beane, they were able to compete by adopting an analytical approach to identify talented, undiscovered baseball players (via  Moneyball ).    Even the Lean Startup methodology, which emphasizes rapid prototyping to quickly test market hypotheses, is a way to deal with the limited financial and time constraints that plague startups.       We often complain about obstacles while ignoring their impact on innovation. If you have any other examples of innovations caused by constraints, please share.",3,1,2012-01-29,2,"shakespeare, startups",288,Shakespeare and Startups
12,0,I compare the collecting every ticket vs fining trespasser model on trains,#finance,"A few years ago, I was on vacation in Italy and spent a good amount of time on trains. Being from the US, I noticed that my ticket was not checked every single ride. At the same time, not having a ticket and being caught carried a large fine. Having the luxury of time, some assumptions, and some algebra, it’s straightforward to work out how to set the fine to make the two systems have the same expected revenue.  {%highlight txt %} N = number of passengers p = ticket price c = % of passengers that will be checked for a ticket v = % of passengers that are violators F = fine  Np = Np(1-c)(1-v) + Npc(1-v) + Ncv*F  Solving for F, we get that F = p/c. {% endhighlight %}  With these assumptions, the fine only depends on the ticket price and the check rate. For example, if the ticket price is $50 and there are 1,000 passengers, the expected amount collected is $50,000. If the conductor only checks 10% of passengers for tickets, the fine would need to be $500 to make the two systems equivalent. If the two systems are expected to generate the same revenue, but one is cheaper to implement, why is the seemingly non-optimal system chosen? I can think of a few reasons:  - Checking all tickets is easier than setting the check rate and fine - especially if they fluctuate - There are additional roles for the ticket collector other than checking tickets - There’s value in minimizing the volatility of the revenue - The cost of having conflicts between passengers and collectors over large fines is higher than checking every ticket - there may be a fear of a low-likelihood, high-cost event - Union agreements may prevent changing the structure - General cultural differences across continents  As entrepreneurs, we’re constantly thinking about the ideal business model. Noticing and comparing other business models help us refine and develop our own.  I’m always on the lookout for more examples, so if you have any please email me or post below.",0,1,2012-03-05,2,"business models, trains",347,On business models - To collect or to fine?
15,0,My thoughts on when to leave a full time job to do a startup.,#meta,A common question I get when telling my friends that I’m leaving my full time job to work on Glossi is “Why now? Why not continue working on it nights and weekends?” It’s a fair question - we’ve been working on Glossi for 6 months of nights and weekends and made significant progress. Why not keep doing that and have the best of both worlds?    Everyone has their own reasons but for me it was more of a gut feeling that it was the right time. I had the following thoughts in mind but I didn’t sit down to make a list of pros and cons:          We’re at the stage where the next steps cannot be done on nights and weekends. The coding is no longer the highest priority work and we now have to worry about business development and trying to raise funding. Neither of those can be done easily or efficiently on nights and weekends.        We’re close to finding our product-market fit and we want to get there as quickly as we can.        We felt as if the market is getting more competitive and we need to focus or be left behind. This may be especially true in the tech space where everything moves so quickly.        We realized that our todo list is growing much quicker than our progress through it. I don’t expect this to change but we want to make as much progress as we can.        I was spending more and more time thinking about Glossi that I was not as committed or focused at work. This was both a disservice to me and the company and I wanted to get out of the way as soon as I stopped being as productive as I should have been.      Please share your thoughts  - I’d love to hear other reasons.,0,1,2012-03-21,2,"startups, entrepreneurship",314,When is it time to leave the full time job?
15,0,"As developers, we need to learn to overcome the bias to want to build everything","#product,#meta","An issue I’ve been trying to overcome is what I like to call the “build bias.” Whenever I’d run into a technical problem, I’d want to solve it on my own - whether it’s by writing some code or by installing and configuring various libraries and packages. I remember the time I needed to collect feedback for a website but instead of just using an off the shelf product like  GetSatisfaction , I decided to create my own. Although I was able to get it working, it took me longer than expected to get it into a usable state and distracted me from the other improvements I wanted to make.  As a developer, it’s very easy to convince yourself to build from scratch every time you need something rather than using an existing solution. It’s exciting to work on something new and it’s annoying integrating someone else’s code. It’s even worse when they’re charging a few dollars a month for something that you can build in a few hours.  More often than not we underestimate the cost of building something of sufficient quality and don’t include the ongoing maintenance cost we’ll most likely be doing. More importantly, we are no longer focusing on the highest leverage activity. As they teach in business schools, you shouldn’t outsource your core competency but everything else is fair game. This is also supported by the lean startup approach which encourages getting your product to market as soon as possible so you can validate your market hypotheses. Why spend time building features when you don’t even know you have a marketable product? If it does turn out that you have a successful product you can always go back and develop your own solution then.  My new process is to first make sure that the feature is even needed. If it is, I check out the open source alternatives to see if anything can be used. If not, I look at the available paid solutions. For many small projects, it turns out that you can ride the trial/basic version enough to validate your idea. This approach has led  Glossi  to use  MongoHQ  to host my database,  SendGrid  as my email system, and  GetSatisfaction  as a feedback widget in addition to ton of open source libraries. With every new project, I’m offloading more and more of my auxiliary features to cloud based services and feel much more productive. Makes me wonder how many other services there are out there that can be leveraged.",5,2,2012-03-24,2,"startups, entrepreneurship",437,Overcoming the Build Bias
12,0,"There should be more startups doing event recommendations, where are they?",#product,"A few years ago I worked on a startup with the goal of providing local event recommendations. Unfortunately, we were never able to make it work. We focused too much on building new features, didn’t simplify our product enough, didn’t have a focused vision, and didn’t spend time understanding the market. After reading Mark Hendrickson’s  Plancast’s postmortem , I started thinking about the problem again and what a successful approach would look like. As Mark pointed out, it’s difficult to incent people to consistently broadcast their plans. Most people will only plan major events in advance and even fewer will log into a website to note that they’re going to grab beers with a friend in a few hours. One thing people are starting to do is checking in to a venue. Knowing the present is a lot simpler than thinking about the future and smartphones have reinforced this behavior. Foursquare has been riding this wave and apps like GroupMe and Fast Society have also taken advantage.    Instead of trying to change user behavior, it’s easier to leverage existing behaviors but apply it to something else. Foursquare is best positioned to move into the event recommendation space. It has already started providing venue recommendations based on historical checkins and there’s value in knowing that 50 people have checked into a stadium within a few minutes of one another. By integrating with an event database, it’s possible to know what events people are attending. This information can initially be used to recommend venues that have similar events going on. Over time, this can transition into doing direct event recommendations.",1,1,2012-03-30,2,"startups, events",280,Where are the event recommendation startups?
10,0,What I've learned about writing so far from this blog,#meta,"I recently made an effort to improve my writing and this blog gives me a great way to practice. I force myself to write at least two posts a week, even if it’s just a paragraph. Writing hasn’t come easy to me and I spent more than 20 years returning the favor. In high school, I rarely edited and a quick spell and grammar check was good enough for me. In college, I avoided the writing-heavy classes and the ones I did take I just followed my high school approach. Something changed when I started working. Although initially driven by my desire to perform, I started seeing writing as a challenging, creative process. I remember spending 30 minutes on a paragraph-long email before being comfortable enough to send it out. Even after only a few weeks, I feel that my writing has gotten better - both in terms of speed and clarity. I still have trouble writing long posts since I tend to go on tangents and lose focus.      An issue I’m currently dealing with is deciding when something is “done.” I could always spend more time editing and rewriting but should I? How much editing is a good use of my time? Jack Kerouac wrote the first draft of On The Road in three weeks and the final draft in 20 days. On the other hand, T. S. Eliot wrote The Waste Land over a few years, with the drafts being almost twice as long. I fall somewhere in between. I realize that I learn better through struggle and forcing myself to edit and rewrite helps me in the long term. At the same time, I realize that I have a ton of other things to do and rewriting the same paragraph a dozen times is not the best use of my time. At the moment, I write and rewrite until I’m proud of what I have and hope that it will get easier in the future. As frustrating as it can get, it’s significantly easier than what people were doing only 20 years ago before computers. This thought helps me focus and slowly work my way up to the fabled 10,000 hours.",0,1,2012-04-04,1,writing,367,On Writing
11,0,The Instagram acquisition shows what the future of startups is.,#product,"The big news today was that Facebook acquired Instagram for $1B in cash and stock. I don’t want to debate whether that was a good price but I am amazed that Instagram was able to get to over 30 million users with 13 employees, of which 3 are engineers. I see a few factors combining to make this an ideal model for the future tech startup.          Open source tools and the cloud have made starting easier than ever and a few motivated, talented people can build a marketable product over a weekend.    Social networks simplify distribution and allow a good product to stand out and succeed without heavy marketing.    People are comfortable with technology and can start using a product without any dedicated support.       They are converging to provide a massive increase in leverage. A small team is able to quickly and cheaply build a product that can spread organically to millions of users. The enterprise space will also be impacted as people start expecting their personal tools in their corporate environments. It’s definitely an exciting time to be building a tech startup.",0,1,2012-04-10,3,"startups, instagram, facebook",191,"Future of Startups - Small Teams, Big Profit"
15,0,"Now that Zynga bought Draw Something, I wonder what Draw Something will turn into?",#product,"We’re aware of Zynga’s purchase of Draw Something and Zynga’s emphasis on analytics and metrics to drive product features and decisions. I’m a bit late to the party but I tried brainstorming to put together  a Zyngafied version of Draw Something:              Favor drawings that require colors that a user does not have to encourage the user to buy new colors.      Leverage the priming effect by picking words that will encourage users to spend more. For example using the words “gold”, “coin”, and “rich” would put users in a buying mood.      Charge more for the more popular color packs.      Include “limited edition” color and word packs.      Reward active users with free color packs, bombs, etc.        I, for one, am glad that I got to play Draw Something before it turns into this money extraction machine.",0,1,2012-04-12,2,"draw something, zynga",143,Draw Something Zyngafied
18,0,Recent trends have made consumers more comfortable with technology which makes it easier for startups to succeed.,#meta,"In a  previous post , I discussed the factors that allow small teams to create products that can be exposed to millions of users within a few months. In this post, I want to take a deeper look into why consumers are so much more comfortable with technology now compared to 20 years ago and try to see where this leads. Since customers are what cause our businesses to grow, we need to be cognizant of what drives their behavior in order to plan for the future. Wayne Gretzky’s father famously said “A good hockey player plays where the puck is. A great hockey player plays where the puck is going to be” and I’m hopeful that we’ll be able to see where the consumer puck is going to be.  To me, the major driver is  Moore’s Law . We’ve seen computation speeds double every 18 months for the past 50 years. This has obviously led to faster computers but has also led to exponentially reducing costs. This has been a huge economic driver and is allowing computers to be more accessible than ever. Our cellphones are more powerful than what was used to land on the moon. These increases in computation also led to the rise of the modern web. It went from being a military/academic project that dealt with text data to something that’s distributing pictures and videos to whoever is interested.  More importantly, improvements in computation led to improvements in usability. Even if we had modern browser standards like CSS3 and HTML5 in the 1990s our computers would be too weak to handle them. We would not have any of the modern innovations (AJAX, DOM manipulation) and our web pages would be static without any rich media content. If we never got past the command line, how many people would have computers in their home? How many smartphones would exist? I’d argue that the usability improvements are what led to the massive consumer adoption of tech products. Of course, computation, cost, and usability are all intertwined but computation and cost alone would not have led to the consumer adoption we’ve seen.  What does this mean for the future? I see usability becoming even more native with us not realizing that we’re even using a computer. We’re already seeing this emerging with Siri and Google Glasses. As long as our computation speeds continue to improve these technologies will become better and better and will recede more and more into the background. Of course, this is all dependent on Moore’s Law holding, with many saying the pace will decrease by 2020. I’m optimistic that we’ll come up with something but even if we don’t, as long as we computing costs keep on dropping, via  Koomey’s Law , we should still see the benefits as we move more and more computation to the ever cheaper cloud. It’s difficult to imagine what would happen if our computation speeds stop increasing the way they have been over the past 50 years.",3,1,2012-04-25,3,"startups, entrepreneurship, technology",525,Growth of Consumer Comfort with Technology
15,0,"If you're interested in startups, check out the class Peter Thiel is teaching at Stanford",#meta,A great blog I’ve recently started following is  Blake Master’s notes  from Stanford’s CS183 class being taught by Peter Thiel. Peter provides an insightful view of the tech startup world that is valuable to anyone interested in startups and entrepreneurship.,1,1,2012-05-16,3,"peter thiel, entrepreneurship, startups",55,Peter Thiel's CS183
10,0,Target the consumer in order to sell to the enterprise,"#product,#sales","A trend I’ve been noticing more and more is enterprise sales being done bottoms up. The typical approach is to offer a free trials or have some sort of freemium product. Each sign up is then treated as an inbound lead that is assigned an account manager. Within two weeks of signing up for New Relic I was contacted by an account manager who helped answer my questions and helped me get New Relic set up for Glossi. Working with him, we were able to get a longer trial period and a discounted price for when we’re ready to upgrade.  HubSpot found  that inbound leads cost 61% less than outbound leads. If having a strong SEO and Social Media presence drops acquisition costs that much imagine the drop caused by having a usable product. Although we’re a small, scrappy startup that’s quick to try new products and services, I believe this approach will become the standard way of selling SAAS in the enterprise. It’s much easier to get a person to try something new and if you can turn him into a fan, you’re one step closer to getting the company signed up.    An extreme case of this would be to initially build a product that’s focused on the consumer and only building out enterprise features when there’s a clear demand for them. A great example would be Dropbox, they initially focused exclusively on making a kick-ass experience for the consumer and only after nailing that down did they release the “ Dropbox for Teams ” plans. I don’t recall the history of  GitHub  but they may have done something similar - initially focusing on public and private repositories and then growing into the more enterprise friendly plans. This is a great approach for a product driven startup since you can focus on building your product without getting stuck in the twisted path of custom client work. But when your product and team are more fleshed out, you can focus on the additional revenue opportunities created by going after the enterprise.",3,2,2012-05-22,4,"selling, startups, enterprise, marketing",361,Selling to the enterprise? Target the consumer
16,0,I show the differences between the ages of actors vs actresses over the past few decades,"#dataviz,#datascience,#code","I recently watched  Miss Representation  which documents how the portrayal of women in the media affects women’s roles in society. It raised many interesting points and definitely got me thinking. If you haven’t seen it already you should definitely check it out. One of the points was that there’s a huge pressure to cast female roles with young actresses whereas it doesn’t matter so much for the male. I was sure this was true but I wanted to see how big of a deal it actually was, take a coding break, and play around with some data. The goal was to replicate the results as well as provide some tools for others to do similar analyses.  I took a quick look at the IMDB site and realized that they did not have an API available. I looked at a few open source alternatives but they all seemed like overkill for what I wanted to do so I decided to just write a quick Python script to scrape the pages I needed. I started by pulling the top 50 movies for each decade (via    http://www.imdb.com/chart/1910s    -    http://www.imdb.com/chart/2010s )   and then pulling the top 5 cast members for each movie (via    http://www.imdb.com/title/tt1375666/fullcredits#cast )  . I had to actually look at the actor/actress pages as well in order to pull the birth dates as well as the sex. After loading this data into a database it was a very simple query to run the analysis and then  Google Spreadsheets  to clean it up.   Not surprisingly, it turns out that over the past 11 decades, the average actor is 41 while the average actress is 32. Interestingly, during the 1980s they were almost the same but the gap has been widening since then.        I may be a bit late to the “ Don’t learn to code ” debate but I think this illustrates that coding is a pretty useful skill to have. It’s not about being able to develop enterprise applications but more about automating some work and being able to scratch a curiosity itch. If this data were publicly available and everyone had the tools and abilities to do these types of analyses I believe we’d be in much better shape. Maybe someone can take the work I started and leverage it to discover something new.      Note: The code to scrape IMDB is posted on  github  but note that it’s definitely crude and hackish at times. My goal was to get the analysis done as quickly as possible so I didn’t spend too much time refactoring.",11,3,2012-05-23,3,"actors, actresses, hollywood",481,Trend of actor vs actress age differences
9,0,Companies should strive to get into address bar autocomplete,"#product,#meta","{% include setup %}      Over the past few days, I’ve been thinking about habits. How do they form? How do they change? And the selfish one - how can you build a product that is habit forming? My cofunder sent me a great Nir &amp; Far  blog post  that goes into detail about generating desire which is a great read to anyone building a consumer product.  Along these lines, I decided to be a bit introspective and see which products and sites are a part of my habit. A simple way was to type each letter of the alphabet into the Google Chrome address bar and see what site autocompletes. Here goes:       analytics.google.com     bankofamerica.com     cad-comic.com/cad     docs.google.com     eventbrite.com     facebook.com     glos.si     heroku.com     instapaper.com     joinblended.com     klout.com     linkedin.com     maps.google.com     news.ycombinator.com     optimum.com     plus.google.com     questionablecontent.net     reader.google.com     startupmullings.com     twitter.com     udacity.com     voice.google.com     wixlounge.com     xkcd.com     youtube.com     zerply.com      After excluding my sites (glos.si and startupmullings.com), we can organize them into the following categories:        Entertainment (the comic sites - xkcd, QC, CAD; Youtube; Google Reader)     Social Networks (Facebook, LinkedIn, Google Plus, Twitter)     Utilities (Google analytics/docs/voice, Bank of America, Instapaper, Eventbrite, Optimum, Heroku)     The rare letters (Zerply, Udacity, Wix Lounge). I’d like to include Klout on this list rather than admit to browsing it but I don’t know if that will be believable.     Every consumer site should strive to get to browser autocomplete status for some users rather than being semi-popular to more users. Being useful to a few passionate users and growing with their help is a much better approach than trying to immediately appeal to the mass market.  And although this exercise may be embarrassing, I’d love to see what others have as their 26 sites.",1,2,2012-06-07,2,"startups, marketing",334,Achieving browser autocomplete
10,0,I found a pretty potent cardboard poster near Time Square,#photo,,0,1,2012-07-09,1,startups,13,Photo taken near Times Square
9,0,Which will hit zero first? RIM or IE?,#product,"I read an  article  earlier today about how companies are preparing for a possible demise of RIM and couldn’t help but compare RIM’s decline over only a few years compared to how long it’s taking IE to disappear.  To confirm that there is in fact a difference in behavior, we can compare the RIM share among smartphones and IE share among browsers. Turns out that they are noticeably different: IE is on a linear decline with close to 70% in Q3&#160;2008 but around 36% in Q1&#160;2012 while RIM starts at 16% in Q3&#160;2008, goes up to a high of 21% in 2009 and then drops to 7% in Q1&#160;2012. Plotting their % decline since the data starting point highlights this further. If we calculate the average decline per quarter from their highest levels and try to see how long it will take to hit 0% share, IE will take almost 4 years while RIM will take less than 5 quarters.          Why are they so different? If they’re both in the enterprise why don’t we see a similar decline in both? I was able to think of a few reasons but would love to hear what others think.       RIM’s competition has been much stronger - both Apple and Android have been eating up the share at a massive rate while the browser market has been relatively stable. This is compounded by smartphones being a new, quickly evolving industry where people are upgrading phones as frequently as they can.     Guy Kawasaki says that companies should focus on making their product  10 times better  than the existing competition in order to get adoption. This may be a lot easier to accomplish with smartphones than with browsers.     Browsers are an older industry and there’s no point in even doing this comparison. We should do this analysis when the smartphone market is more mature and we can normalize the two time frames.     I tried digging in a bit further but it’s unfortunate how difficult it is to find browser market share data. I’d love to dive in and look at the trend in the browser market since the 1990s and see how that compares to the trend in smartphones. If anyone has this data please let me know.  Here's the  Google spreadsheet  if you want to play around with the data.",3,1,2012-07-10,2,"research in motion, internet explorer",442,Race to 0 - RIM vs IE
12,1,Startups can compete by focusing on the details and doing things quickly,#product,"A frustration I’ve been experiencing more and more is having to reload a webpage in order to change the date range in the options. If a company expects me to keep a site open for more than a day they should make it easy for me to update the options. The big example is Google Analytics - I open up a page, choose a date range, and get to see my charts. If I keep the tab open and want to want to run the same analysis the next day, I’m forced to reload the page to even be able to include today in the date range. It’s an unnecessary action for the user and it would be easy to correct this behavior with some simple Javascript.    Such small details don’t matter individually but together they reflect a lack of empathy for the user that impacts a company culture. We should always be striving to make a user’s experience better and doubly so whenever it’s actually an easy fix. Other easily fixable examples I’ve seen are clearing entire forms when there’s an error with one field and not highlighting the field that’s giving the error.    I suspect the reason these aren’t fixed is a managerial problem. The application works and there’s no reason to go back when there are all sorts of new shiny things that can be built. No one wants to do a cost vs value analysis for these minor fixes so they stay the way they are. I suppose you need to either build things the right way immediately, fix it without letting anyone know, or resign to leaving it alone.    There’s a reason startups tend to have better products. They don’t go through analyses to determine whether to make minor changes, all it takes is for someone to decide that something needs to be fixed and the next deployment, probably within a few hours, will have it solved. Combined with the massive sense of ownership that comes with working at a startup, that’s a lot of improvements that would be done at a startup but not a larger company.",0,1,2012-07-28,1,startups,361,"The Startup Advantage - Details, Details, Details"
8,1,Why is everyone so down on Yahoo?,#product,"Reading recent tech coverage makes you think that each newly startup is more valuable than Yahoo. Yahoo is the 4th most visited site in the world with over 300 million users on Yahoo mail. This is a problem every startup should hope to have.      User acquisition is the most difficult task for a consumer startup. User attrition is an easier problem to solve than user acquisition. Yahoo doesn’t need to build a product that’s 10 times better than the competition, they just need to simplify and improve what they already have. Yahoo also has massive usage among the mass market with millions of people having Yahoo as their home page. These are not the same people that sign up for every startup featured on TechCrunch. Yahoo has challenges but worrying about user acquisition is not one of them. Yahoo will need to develop a vision and relentlessly pursue it. The culture will need to change and vested interests will need to be broken.      It’s easy to criticize Yahoo for ignoring Google and Facebook but impossible to say what Yahoo should be doing now. I look forward to seeing what happens to Yahoo with Marissa Mayer at the helm.",0,1,2012-08-05,1,yahoo,206,In Defense of Yahoo
6,0,Relationship between innovation and cannibalization.,#product,"{% include setup %}       I was rereading the HBR paper on  Strategies for Two Sided Markets  and came across a passage describing Apple’s mistake of trying to monetize both sides of their market, the consumers and the developers, rather than focusing on one like Microsoft did by giving away the SDK for free.    It got me thinking about Apple’s recovery. Many people credit the iPod with revitalizing Apple but I think there’s more than that. I suspect the bigger reason was the decline of desktop software and the ability to be productive on the web. Suddenly the network effects that existed by having software that only worked on Windows no longer existed. Software started migrating to the web and people were more willing to try new operating systems out. In 2006, I switched to Linux without too much trouble. It was also simple to find help online to deal with the various issues I ran into which made the transition easier. In some ways, Google helped Apple recover by speeding up the move to the web with a more accurate search and a good set of productivity apps.    In general, it’s damn difficult to overcome network effects. Google will not be replaced by a search engine. Facebook will not be replaced by a social network. These network effects will be broken by a behavioral change. Instagram rode this wave of behavioral change of the move to mobile and it was a savvy move for Facebook to make the acquisition. It makes you wonder what Instagram could have become had it stayed independent.    Innovation is cannibalization. By pushing the envelope of technology, pioneering companies cause behavioral changes that will give rise to companies that may end up replacing them. As Clay Christensen  notes , it’s rare for a mature company to put resources behind a disruptive technology that will cannibalize itself but it’s the only way to stay relevant. Only  13% of the companies  in 1955’s Fortune 500 made the list in 2011. It’s amazing to see how quickly things change and the pace is only getting quicker.",3,1,2012-08-12,2,"innovation, cannibalization",387,Eating Yourself - Innovation &amp; Cannibalization
6,0,I'm migrating my blog to Github,#blog,{% include setup %} I'm going to work on migrating my posts over from Wordpress and Tumblr on to here. Let's see how it goes.,0,1,2012-11-13,1,blogging,26,"Hello, Github!"
17,0,I migrated by blog to Github pages and wanted to share some notes of the process.,#blog,"{% include setup %} After a few hours of solid work I was able to get my new site up and running on Github pages. I got frustrated with having too many blogs and decided that I should finally get it together and consolidate everything. Within a few hours I was able to get it up and running on Github pages up and migrated my old Tumblr and Wordpress posts. Hopefully this encourages me to write more.  A few notes: 1. The  documentation  for Jekyll is great and makes it very easy to get started. 2. Jekyll comes with a few  migration  scripts that made it easy to move the old blog posts over. 3. There's a pretty strong community around it so it's easy to get started with themes. I ended up using  one  based on Twitter Bootstrap. 4. Github pages provide a custom domain option so you can host your entire site in Github. Other than the fact that it's free, you don't have to worry about your site dying due to heavy load. 5. An issue to be aware of is that the Jekyll parser is pretty strict and doesn't provide very helpful error messages. I had an issue that prevented some posts from migrating because they had a "":"" in the titles. To discover this, I had to migrate a few posts at a time until I was able to identify the issue.",3,1,2012-11-14,1,,245,Github Migration Notes
34,0,If you're doing a startup you need to pick an area that you're so passionate about that you can work through the trials and tribulations since it's like no other job you'll have.,"#product,#meta","{% include setup %}   Let’s start with a joke:          An American consultant was at a pier in a small coastal Mexican village when a small boat with just one fisherman docked. Inside the small boat were several large tuna. The American complimented the Mexican on the quality of his fish and asked how long it took to catch them.       The Mexican replied ""Only a little while."" The consultant then asked why didn't he stay out longer and catch more fish? The fisherman said he had enough to support his family's immediate needs. Then the American asked how he spent the rest of his time.       The Mexican fisherman said, ""I sleep late, fish a little, play with my children, take a siesta with my wife, Maria, and then stroll into the village each evening where I sip wine and play guitar with my amigos. I have a full and busy life, senor.""       The American consultant scoffed, ""I am a very successful business consultant and could help you. You should spend more time fishing and, with the proceeds, buy a bigger boat. With the proceeds from the bigger boat, you could buy several boats, and eventually you would have a fleet of fishing boats. Instead of selling your catch to a middleman you would sell directly to the processor, eventually opening your own cannery. You would control the product, processing and distribution. You would need to leave this small coastal fishing village and move to Mexico City, then Los Angeles and eventually New York City where you will run your expanding enterprise.""       The Mexican fisherman asked, ""But senor, how long will this all take?""       The consultant replied, ""Probably 15 to 20 years.""       ""But what then, senor?"" asked the fisherman.       The consultant laughed, and said, ""That's the best part! When the time is right, you would announce an IPO and sell your company stock to the public. You'll become very rich, you would make millions!""       ""Millions, senor?"" replied the Mexican. ""Then what?""       The American said, ""Then you would retire. Move to a small coastal fishing village where you would sleep late, fish a little, play with your kids, take siestas with your wife, stroll to the village in the evenings where you could sip wine and play your guitar with your amigos.""    Other than the chuckle, this joke got me thinking about how people view work. The joke suggests that you should only work to support your life outside of work. If you work more than that then the joke’s on you since you’re sacrificing your personal life. This is view that you can’t have if you’re starting a company. Entrepreneurs need to combine their personal and professional lives. If you’re running a startup and aren’t thinking about the market, your product, or your users when you’re in the shower or in bed you’re doing it wrong. More importantly, this should be natural and not forced. If you don’t enjoy thinking about your startup when times are good, how will you be able to do it when times are tough (which they will be)? We need our passion to get over the humps so if you’re not passionate about your startup when you’re starting out, you will abandon it when facing challenges. Your startup will end up consuming you so why not pick something that you care about?",0,2,2012-11-29,1,,621,Entrepreneurship is not a job
17,0,I had to upgrade an HD in a Macbook and I wanted to share my approach.,#meta,"{% include setup %} As a gift to myself I decided to upgrade the RAM and HD in my MacBook. The plan was to replace the old HD with the new one and then use the install disc to install Snow Leopard on the new HD before upgrading to Mountain Lion. Unfortunately, it turned out that I had a bad install disc and had to come up with another approach. The general idea was to upgrade to Mountain Lion first in order to create another boot disc.  1. Upgrade OS to Mountain Lion and make sure to have a copy of the installation file around 2. Put the new HD into an external enclosure 3. Create two partitions on the new HD, one should be 8 GB to hold the Mountain Lion install boot disc 4. Use  Lion Disk Maker  to create the boot disc on the 8 GB partition of the new HD ( instructions ) 5. Shut down the computer and replace your HD with the new HD 6. Boot your computer while holding down the option key 7. Select the Mountain Lion boot disc from the selection screen and install it on the other partition 8. Mountain Lion is now installed on the new HD 9. You can now remove the Mountain Lion boot partition and use that space for something else  An easier approach would have been to just use a USB flash drive or an SD card to create the boot disc. Unfortunately, I didn't have any that had a capacity of more than 8 GB so I had to resort to this hack.",2,1,2012-12-16,1,,286,The joy of upgrading an HD in a Macbook
26,0,There are sites that let you export your Instagram photos but they're slow. I wrote my own version that you can install to download your phoots,"#code,#product","{% include setup %}  I just hacked together a quick app to help download Instagram photos. At first, I tried using  Instaport  and  OpenPhoto  but both of them were backed up with others trying to do the same so I decided to create my own. It's basically a really simple python web app that allows you do a quick authentication with Instagram and then lets you downloads all your images to your hard drive.  It should be pretty easy to get started by following the readme on github:  https://github.com/dangoldin/instagram-download",3,2,2012-12-19,1,Download export Instagram photos,98,Self hosted Instagram export
7,0,My thoughts back on the year 2012,#meta,"{% include setup %} I'm finally in a position to do a ""Year in Review"" post that I’m comfortable writing. In March I left my full time job to pursue  Glossi , our startup, full time. In May, we were accepted into an  incubator  and had an amazingly productive four months. Unfortunately, none of us were passionate about the direction Glossi was headed and we’ve ended the year pursuing a new venture,  Makers Alley .  I learned a ton about myself but along with that came the realization of how much I don't know. Working as one of three cofounders on a startup forced me to develop a much broader set of skills. I understand the whole technology stack better and can actually make a passable website now (mostly thanks to  Bootstrap ) but I can also discuss trademark and copyright law with lawyers as well as do some tax planning with an accountant. I'm not too interested in pursuing law or accounting as a profession but it's great being able to follow along and chime in every once in a while with something useful.  More important than the skills, I've learned more about myself in the past year than I have in my entire professional life. I've discovered that money is not as important to me as I thought and that the control over my day is important to me, even if it does mean more work for less pay. I have a better understanding of my strengths and how they are best applied. I also know what my weaknesses are and am working on a few goals for 2013 which I’ll publish over the next few days. I'm still trying to figure out my exact passions but at least I'm starting to acknowledge that they need to be discovered.  Lastly, I want to acknowledge how lucky I am to even be in this position. It's wonderful having a  wife  and family who support me through all this, I know it can't be easy.",5,1,2013-01-01,1,2012 year,345,Year in Review 2012
10,0,List of my 5 goals for 2013 to improve myself,#meta,"{% include setup %} Now’s the time people are making resolutions for 2013 so I’m going to join the club. I’m publishing them publicly since that should help my motivation. I’m also calling them goals since a goal seems harder to abandon than a resolution. I’m hoping that having these goals be specific, however arbitrary, will also help me in achieving them.  Here goes:    	  	 Run 1000 miles in 2013   	I’ve gotten out of shape over the past couple of months and that’s a bad place to be in the late 20s. I think it’s important to get good habits now since that will help me maintain my health as I get older. Not to mention that being in better physical health will improve my acuity. 	  	  	 Write at least 2 blog posts a week   	The more I work with various people the more I realize the importance of communication. Writing doesn’t come easily to me and I spend the majority of time editing but I’m hoping that it’ll be easier by the end of 2013. And although writing is just one aspect of communication, improving that will lay a solid foundation for the others. 	  	  	 Meet up with 2 old acquaintances each month   	Going from a company with hundreds of employees to working with a cofounder reduces the number of people you have contact with. By restoring my old relationships I’ll be able to connect with old friends and strengthen my network. Something I’ve learned over the past year of meeting with various folk in the NYC startup community is to end every meeting with an offer to help and I’m going to adopt that attitude as well. 	  	  	 Develop 6 side projects   	This one’s here for a few reasons. One, I want to keep on improving and starting a project from scratch is a great way to work with new technologies and explore different approaches. There are many times that I want to go back to my existing code and rewrite it but why fix something that isn’t broken? Isn’t it better to put that energy into something new? Two, I want to give back to the community and putting these projects on GitHub will hopefully help someone. Three, this will just be a good outlet for when I need a break from the main gig. Four, I want to build my personal brand and having more more of my work publicly available will hopefully help. 	  	  	 Start a hands-on hobby   	This one stems from a personal belief that I just need to do something with my hands since I spend so much time in front of the computer. This may end up being drawing, painting or woodworking but the goal is to find something that allows me create something physical and not digital. I’ve already dug up some colored pencils and drawing paper and am in the process of signing up for a woodworking class at a hands-on coworking space in Brooklyn called  3rd Ward .",1,1,2013-01-02,1,goals resolutions self-improvement,510,2013 Goals
8,0,Tips to help you write better web scrapers,#code,"{% include setup %} I’ve done my fair share of scraping ever since I started coding and just wanted to share some tips I’ve picked up along the way. I think scraping is a great, practical way to get into coding that is also immediately useful. It also forces you to understand the HTML of a page which gives you a great foundation when you’re ready to create your own site.  Hope they’re useful!      Avoid it if possible   It is a bit odd that I’m starting off with this as the first tip but if there are alternatives definitely take a look at those; many sites come with an API and that may be a much better approach. Otherwise, every time there’s a change in the HTML structure you run a risk of breaking your scraper which will leave you scrambling to fix your code. It’s also a good idea to organize your code such that a change in the HTML for one of the scraped items does not break the others. For example, if you want to get the name and address of a restaurant from Yelp, have one method that will get the name and another that will get the address. This will most likely be less efficient so you’ll need to use your judgement to see whether the risk-speed tradeoff is worth it.      Use a library   Unless you’re doing a one off job, use a library. Every major language has one: Python has  BeautifulSoup , Perl has  HTML::TreeBuilder , Javascript has  htmlparser , and there’s no excuse to not use one. If you ever need to go back to make some changes (which you most likely will need to), you’ll be glad you did. You can also find libraries that let you simulate browser behavior by storing cookies and letting you submit forms. This gives you the ability to scrape sites that require a login. Some sites try to prevent scraping by obfuscating their HTML a bit in which case you’ll need to do either a string replacement or a basic regular expression to get it parsed by the library.      HTML/DOM inspectors are a must   Since scraping requires getting specific elements from a web page, we need to understand the HTML structure of that page. For me, doing this work within the browser works best since it gives you the ability to both see the HTML that’s responsible for a certain element and also gives you a console window which lets you test a scraping approach. The two browsers I’ve used successfully for this are  Google Chrome  and  Firefox  with the  Firebug  plugin.      User agent spoofing   Every time your browser visits a website, it submits a request that contains information about the browser. This is why some sites show a different page when you’re using a phone versus a computer. Every once in awhile you will need to trick the site into sending back the proper page by “spoofing” the user agent. A simple way to check if you need to do this is to view the source of a page in a browser and compare it with what you’re retrieving in your code. If they’re different, try changing the user agent and see if that fixes it.      Be clever   Looking at the source of a page may be a bit overwhelming and there may be easier ways of getting at that information so be clever! An example of two approaches that I stumbled across were to spoof a mobile browser and to call the AJAX url directly. Spoofing a mobile browser tends to give you simpler and more lightweight HTML which is easier to parse. Loading the content via AJAX lets you get at the content quicker and usually in a more structured format, like JSON or XML. These approaches won’t work on every site so you need to do some research and experiment a little to understand how each site is setup. After that you can figure out the best approach for your scraper.      Be specific   When scraping, you want to make your scraping code rigorous enough to not fail if the page structure ever changes. A good rule of thumb is to be specific when you write your scraper. Use a specific id rather than a class since the id is guaranteed to be unique. Similarly, avoid an ordinal approach where you reference the 2nd or 3rd div. Sometimes this is unavoidable but try to see if there’s another approach. Another useful tidbit is to use the more content-descriptive identifier in the page. For example, if you see a div with the address you want to scrape and that div has two classes, “location-address” and “blue-highlight”, use the “location-address” one since that’s defining what the content is, not how it’s displayed.      Save the HTML of the retrieved pages   It’s helpful to save each HTML page you’ve retrieved. It takes a few iterations to get your scraping code working and it’s quicker to just have the HTML on disk so you don’t have to download it every time the script runs. Another advantage is that if you discover a mistake in your code, you don’t have to redownload all the pages you’ve already processed. It only takes a few minutes of work and worth doing.      Monitor actively   Scraping is prone to breaking so make sure you monitor the job as it runs. It’s likely that your code will work well on one page but will fail on others. I tend to write my code to be a bit picky at first while I work out the kinks and once I’m confident in it I will build in some logic to deal with a missing value to make sure it continues to run. As I mentioned earlier, storing the HTML of the page will save you time if you need to update your scraper and need to rerun it.      Throttle your requests   If you don’t want your roommates pissed pissed at you, which will happen when Yelp blocks you for 6 months, throttle your requests. The simple way to do this is to have your code wait in between downloading pages and another approach is to use proxies to hide your true IP address. This will make it seem that the requests are coming from a variety of computers and keep your roommates happy.",6,1,2013-01-09,1,Scraping web HTML BeautifulSoup,1092,Web scraping like a pro
21,0,Has social media made us less likely to cause real change in the world since we're liking and sharing instead?,#meta,"{% include setup %} The recent revolt around Instagram’s TOS changes got me thinking about the revolt against SOPA/PIPA and the impact social media is having on cultural participation. We’re wired to want to improve things and when we come across what we feel is an injustice we want to change it. Unfortunately, social media has made us lazy. Sharing something on Twitter or Facebook gives us the nice, warm feeling that we’re actively contributing to a cause. Instead of going out and demonstrating in public, snail mailing our representatives, or providing financial support, we’re clicking a link and think we’re making a difference. PIPA/SOPA wasn’t stopped due to internet outrage but from people calling their representatives and doing more than just mentioning their opposition. Wikipedia and Reddit didn’t just put a message up saying they oppose PIPA/SOPA but blacked out their site. Would the result be the same if they just had a message stating they opposed it?  Social media is great at raising awareness, it’s just not very useful until someone down the line acts on it. Does the increase in awareness lead to actual change? This simplicity and reach also leads to a massive number of causes being championed. I can’t login to Facebook without seeing some cause being shared and promoted. Causes now need to market themselves as much as a consumer product. Are we really better off?  I can’t help but think of the pen and sword metaphor. The pen is mightier than the sword because the pen is able to get many swords. Does social media just give everyone pens or does it lead to more swords?  My way of dealing with this is to take a real action every time I share something on social media. If I share a Kickstarter project then I will donate to it. If I share opposition or support of a bill, I will call my representative. We’d be in much better shape if everyone did the same.",0,1,2013-01-11,1,social media Instagram SOPA PIPA,331,Is the pen mightier than the sword in a social world?
19,0,Fab and Groupon favor the consumer over the business and focus on short term vs long term growth.,#product,"{% include setup %}    Groupon  has fascinated me since they’ve launched. It popularized an entirely new business model, encouraged the launch of hundreds of competitors, and was able to IPO three years after being founded. This sounds great until you look at the performance after the IPO: the stock is down 80% and it’s consistently missing the quarterly goals.    The daily deals space isn’t as profitable as it used to be and they’re trying to become a tool platform for small businesses. To grow beyond daily deals, they’ve been on an acquisition spree. Over the past two years they’ve acquired a  scheduling startup , a  social shopping startup , a  POS system , and a  restaurant reservation system . I don’t think this will be enough for them to get seen as something bigger than daily deals.  Groupon grew by providing steep discounts to consumers but sacrificed businesses in the process. It will be a hard sell trying to get a business to use your tools when a few months ago you were telling them to discount their products more than 50%. I understand that they needed to do this to grow and I’m sure they even had a choice: the space was so competitive that if they didn’t do this someone else surely would have. It just puts them in a pretty tough spot.  Recent fast growing ecommerce businesses have also favored the consumer over the business. This leads to quick initial growth but causes problems in the long term.  Fab  is taking this approach as well by providing steep discounts on designer products. Consumers are loving it but what happens when there aren’t any businesses left who are willing to agree to such a discount? Sure, using Groupon and Fab can be viewed as a marketing expense but I suspect they and their investors want to be seen as more.  It’s difficult to balance the needs of the various sides of a marketplace. You do want to  subsidize one side  but it’s dangerous to favor the consumer side so much since it’s difficult to distance yourself from. Maybe it is the proper approach in the beginning but there needs to be a way to get out and I think that’s a tough problem.  Etsy  and  AirBnB  had slower growth but were able to align the incentives of the various sides of the market early on. It was easier for them since there’s overlap between the two sides (sellers are buyers and buyers are sellers) but I suspect this is still the right approach for long term success.",9,1,2013-01-15,1,Fab Groupon ecommerce,472,What do Fab and Groupon have in common?
8,0,Doing some data visualizations on my Twitter archive,"#dataviz,#datascience,#code","{% include setup %} I finally got access to my Twitter archive and decided to have some fun with it and also give me an excluse to play around with  matplotlib . The first step was just seeing what the data looked like and what information was available. Turns out that Twitter included a simple HTML page to let you browse your tweets but also provided CSV files for each month. The fields were pretty self explanatory but one ""gotcha"" was needing to convert the timestamp to my local time. I wanted to do a few data visualizations to see what my tweeting behavior was like and also see if anything insightful came out. As I started looking at the visualizations I noticed that I'm more active than I used to be and that I have a pretty stable relationship betweet my tweets, my RTs, and my replies. In the future, I'd like to explore how my usage of Twitter has evolved and also get to play around with the  NLTK library .  I've committed by ugly code to github if anyone wants to play around with it:  https://github.com/dangoldin/twitter-archive-analysis . I know the code is ugly. I'll clean it up one of these days.                              Apparently, I like to tweet evenings and nights.                                       You can see I like to take my Fridays and Saturdays easy. Since I also tend to tweet more frequently at night this indicates I'll go out Friday and Saturday nights.                                       I was pretty much quiet since I got on Twitter in 2008 but have been more consistent since 2012.                                       I must admit this one's here mostly because I wanted to do a heatmap but this does reinforce that I've been a more active on Twitter since 2012 and that I'm less active on Friday and Saturday.                                       I started off barely saying anything but it looks as if I'm consistently around ~90 characters per tweet.                                       I wanted to see whether my behavior around tweeting, retweeting, or replying has changed over time but this doesn't make it very clear due to the number of lines going on so I decided to normalize it - see next chart.                                       Now we're on to something. In the beginning I was basically posting short tweets about my life but more recently I have been more involved in the community aspects.",3,3,2013-01-19,1,twitter data visualization matplotlib nltk quantified self,570,Making sense of my Twitter archive
20,0,I think engineers are too impatient to participate in politics and give up too easily. Society even encourages this.,#meta,"{% include setup %} At Aaron Swartz’s memorial service in New York, Doc Searle said something that struck a chord: Aaron was one of the few tech people who would get involved in legal and political issues. It’s true - we hackers aren’t into it. We claim we’d be better off if there were more engineers in charge and yet we’re not making an effort to be those engineers. I’ve heard a variety of unconvincing reasons: it’s just not interesting; there’s too much bullshit; it’s more about selling than creating. I think the real reason is that we’re just too impatient.  Our roles and jobs have made us this way. Our work tends to have well structured problems that are solved through individual effort. Only when we have to rely on someone else do we become aware of how slow things move and how long things take. Even the agile methodology, for all its wonders, focuses on the short term and encourages small, easy achievable tasks. It’s no surprise that when we encounter something that takes longer than we’re used to that we dismiss it as not for us.  Impatience is also why I had difficulty as a product manager after coding for 5 years. I had a grand vision of what needed to be done but wasn’t able to execute it. I blamed it on the politics but it was really my impatience and immaturity. It was easier to work with developers to build the product features and just release them than it was to work with the actual users and get them on board. That would have required understanding their use cases, listening to everyone’s concerns, having a trial period, and all sorts of other things that would take too long.  Most of us do want to make the world better and do make an effort to contribute; we give up too soon. During the service, Roy Singham quoted Frederick Douglass:  “If there is no struggle, there is no progress.” Real progress takes time and we need to get comfortable with that if we want to see it happen.",0,1,2013-01-23,1,,352,Why aren't there more engineers in politics?
14,0,"Sharing the design and backend changes I've made to this blog since November, 2012",#blog,"{% include setup %} In November, I migrated my Tumblr and Wordpress blogs over to GitHub pages and have been making a few tweaks here and there. I started with the awesome  Jekyll-Bootstrap library  but wanted to share the changes I’ve made. It’s all hosted on GitHub so feel free to fork it.           Design changes              The version I started with didn’t have the Bootstrap responsiveness library so I added that in       Since I’m using it primarily as a blog, I updated the design to emphasize the blog aspect       Consolidated the pagination and social sharing widget to fit on one line       Incorporated some best practices from  Kaikkonen's blog typography guide                  Backend changes              Small improvements to SEO by giving ability to add keywords to each page       Added Open Graph meta tags to control what’s displayed when people share the page on Facebook       Made a few tweaks to the way the sitemap was being generated",2,1,2013-01-24,4,"design, backend, seo, blog",170,Blog updates since November
11,0,To fight patent trolls we need to go on the offensive,#meta,"{% include setup %}            A Patent Troll        Newegg recently defended  itself against a patent troll that sued them over a shopping cart patent. As a result, the patent was invalidated and Soverain Software will lose $2.5M from this and the $18M they won in 2011 from Victoria’s Secret and Avon. Unfortunately, they’ll still keep the tens of millions of dollars they “earned” in earlier years. Since virtually every ecommerce site has a shopping cart feature you’d think that this patent would have been invalidated sooner.   The reason it takes this long is that most companies settle when faced with a lawsuit and only a few fight back. Over time, companies that have a reputation for fighting back are sued less frequently and companies that do settle just pass the cost onto the consumer. It’s no surprise that these patents end up sticking around. Unfortunately, it’s a shitty situation for smaller businesses: they can’t afford a lawsuit and can’t afford to raise their prices.  What can we do to change these incentives around? Right now, a big advantage patent trolls have is that they make the first move and can choose who to sue and who to avoid. Why not bring the fight to them? A simple approach would be to find these these flawed patents and file for a  reexamination  with the USPTO.  Ask Patents  has already started collecting a database for prior art to challenge patent applications but this information can also be leveraged to challenge already granted patents. Another option would be to sue the patent troll directly, as Microsoft and Google  have done , which has an added benefit of a jurisdiction other than  East Texas . An extreme approach would be to create shell companies that intentionally violate these patents in order to challenge them. Imagine a hackathon whose sole purpose is to create sites and companies that violate these patents in order to troll the troll. Why not bring the fight to them?",6,1,2013-01-29,3,"patent trolls, law, newegg",391,The Patent troll troll
19,0,We need to start designing classes for the web from scratch rather than adapting classes from the classroom.,#meta,"{% include setup %} I’m making an effort to freshen up and improve my data skills so when I found out that two of my friends were going to take an  R class  on Coursera, I joined them. The class is pretty typical for an online programming class: each week there are a set of lectures to watch, a quiz to take, and a programming exercise to do. In addition to this, we also have a weekly Google Hangout to discuss the lectures, go over our programs, and share our R questions.  I realize we’re still at the dawn of online education but it feels as if the class has simply been moved from the classroom to the web, without any thought as to how the class can be structured to make the best use of the web. So far, the Google Hangout paired with the programming exercises is the most valuable. The programming exercises provide the structure and the Google Hangouts help us absorb the material better. We are able to share our solutions, analyze the pros and cons of the different approaches, and by explaining why we solved them a certain way we end up understanding the material better ourselves. Why can’t online classes be designed to take advantage of this? I understand that not everyone has someone to take a class with and yet having a partner provides a big benefit.  These online programming classes should take a lesson from  Project Euler . Project Euler is a series of puzzles that require both a mathematical and programming insight to solve. The brilliance is that they have forums for each problem that you can only access after solving the problem. But once you gain access, you can see other solutions, learn from them, and pick up tricks and approaches that you’ll need to solve future problems.  Pairing the structure provided by a class with this Project Euler community would create better online programming classes than we have now. My ideal online programming class would have the following structure:      No video lectures . The content can be better presented through text and visuals and helps people work on it at their own pace.    Each lesson would be focused on a particular problem . Start by describing a problem and then spend time going over different ways of thinking about it as well as the different tools available. This should be as interactive as possible where students can follow along by running the code on their own computer.    Limited time spent on defining terms . People are sitting in front of the computer and can do a search on Google or Stackoverflow to get more and better definitions than can be covered in a class.    Leveraging the volume of people taking the class . When you have thousands of people taking a single class you can do things that you just wouldn’t be able to do normally. For example, you can analyze people’s programming solutions to see which solutions are the most common, which run the quickest, or which are the shortest. Having that information available to students would be much better than just getting a numerical score.     This approach isn’t for everyone but that’s the point. Moving online will give us the ability to have classes custom tailored for each student but we need to start by thinking about building classes for the web from scratch rather than copying them from the classroom.",2,1,2013-02-01,4,"online education, online classes, coursera, udacity",582,Improving online programming classes
34,0,We use the OpenStates API to download bills from various states and compare them against each other in order to find similar language that will indicate bills written under a 3rd party influence.,"#datascience,#dataviz,#code","{% include setup %} This past weekend I participated in the  Bicoastal Datafest  hackathon that brought together journalists and hackers with the goal of analyzing money’s influence in politics. I came in with the idea of analyzing the evolution of a bill in order to see which politician made the various changes and relate that to campaign contributions. I quickly discovered that that wouldn't be very easy, especially in two days, but I did meet  Llewellyn , a journalist/hacker, who had a more practical idea of programmatically identifying bills across states that used the same language. The intuition behind this being that it would identify bills that were unlikely to have been written independently of one another and likely to have been influenced by a 3rd party.  We ended up with the following approach that we were able to code up during the weekend: 1. Use the OpenStates API to get the URL of the bills 2. Download the bills and convert each to raw text - from PDF and HTML 3. Extract 8 word phrases from each bill, excluding stopwords 4. See which phrases were duplicated across states 5. Examine the duplicate phrases to see which bills are most likely duplicates  Somewhat surprisingly, this approach led us to discover the following duplicate bills:   Firearms Freedom Acts   Shared the phrase:  manufactured without inclusion significant parts imported another state                                Indiana                                                        Michigan                                 Prohibit US government officials from enforcing firearm-related acts   Shared the phrase:  accessory ammunition owned manufactured commercially privately state remains                                Arizona                                                        Tennessee                                 Prevent pharmaceutical substitution of opioid drugs   Shared the phrase:  bear labeling claim respect reduction tampering abuse abuse                                New York                                                        New Jersey                                The code's up on  Github  so if you have any ideas or improvements - contribute and help out. In two days we were able to get something useful done and it's exciting to see what we can discover if we stick with it.",9,3,2013-02-05,5,"bicoastal datafest, hackathon, politics, laws, natural language processing",543,Identifying duplicate bills across states
18,0,Coursera can now offer classes for credit and I'm excited by the changes this will bring to education,#meta,"{% include setup %} I was excited to read that Coursera can now offer classes for college credit. I’m optimistic that this is a start of a trend that will change higher education. At first, this will be adopted by the motivated student - the one who took AP classes in high school and the price sensitive student - the one who took community college classes before transferring to a university. But over time, this will spread until it’s the dominant approach. It’s simply better. It may be more expensive to get a digital class together but it’s primarily an upfront cost that will be spent on getting the best lecturers, developing engaging lectures, and creating varied course content that’s optimized for different learning methods. This content can be accessed from anywhere and is always available.  These classes will serve as building blocks for entirely new curriculums and courses of study. Dependencies between classes will be created so if you’re having trouble with a part of one class you can quickly go to another class that covers that topic in more depth. Companies can get involved as well and help bring the costs down. For example, a company can subsidize a classes and in return be allowed to recruit students who score highest on various sections. Companies can even create their own funnels based on the performance over a set of classes. I don’t know if this is the right approach since it will lead to corporate influence in education but I’m excited by the opportunities. This is something prior generations never had and just a shadow of what future generations will have. I'm just glad it's so easy to keep on learning after college.",0,1,2013-02-08,3,"coursera, online education, udacity",284,More thoughts on online education
11,0,Analyzing and visualizing the text of Abraham Lincoln's speeches and writings,"#dataviz,#datascience","{% include setup %}  On Saturday, I finished  Team of Rivals  and while looking at my calendar noticed that it was also  Lincoln's  birthday this week. What better way to celebrate his birthday than to analyze his speeches and letters? I downloaded the  7 volume set  containing his speeches, letters, and essays from Project Gutenberg and spent a few hours on Sunday cleaning the text and writing a parsing script. On Monday, I started analyzing the text to see if I could make sense of it.  I was able to get 1,458 documents containing almost 16,500 sentences and a little over 547,000 words. I tried getting the date each letter was written or speech was given but was only able to get it for 60% of the documents. That was enough to get some insights.   Number of speeches/letters by year   I suspect a lot of his early writing and speeches and were lost since they just weren't preserved as well as his later speeches and letters        Trend of phrases   I wanted to examine the phrases that he most commonly used over time in order to see whether there were any noticeable changes and whether they meant something. Turns out there was some interesting stuff here that's highlighted in green.           Slavery  - There are references to slavery across the entire date range with the  Dred Scott decision  and the  Missouri Compromise  appearing as common phrases in the 1850s.        Civil War Generals  - You can trace the career of the generals during the Civil War based on their mentions.  General Hooker  was mentioned in 1862 and 1863;  General Meade  in 1863 and 1864; and  General Grant  in 1864 and 1865. This echoes history: General Hooker was replaced by General Meade in 1863 with General Grant being in command of the Union Army in October of 1863.        The Presidency  - When Lincoln was elected president in 1860, he started finishing his letters with the phrase ""Lincoln, President of."" During the presidency we also see mentions of his cabinet:  Stanton  and  Seward .      *The table below was generated by looking at the top 20 three word phrases used in each year range and then consolidated into a top 100 list across the entire dataset. The X indicates that the phrase was in the top 20 three word phrases for that year range. I highlighted the interesting rows in green.            phrase   1832-1845    1846-1853    1854-1859    1860     1861     1862     1863     1864     1865               the united states    X    X    X    X    X    X    X    X    X           of the united    X    X    X    X    X    X    X    X    X           i do not     X    X    X    X    X    X    X    X    X           the secretary of          X    X         X    X    X    X    X           secretary of war          X              X    X    X    X    X           in regard to     X    X    X         X    X    X    X    X           the people of    X    X    X    X    X    X    X    X    X           of the people    X    X    X         X    X    X    X    X           president of the     X    X    X    X    X    X    X    X    X           in favor of      X    X    X         X    X    X    X    X           my dear sir      X    X    X    X    X    X    X    X                as well as   X    X    X         X    X    X    X    X           so far as    X    X    X    X    X    X    X    X    X           dred scott decision                X                                         there is no      X    X    X    X    X    X    X    X    X           by the president          X    X    X    X    X    X    X    X           the supreme court    X    X    X         X         X                     united states and         X    X    X    X    X    X    X    X           of the union     X    X    X    X    X    X    X    X    X           that it is   X    X    X    X    X    X    X    X    X           it is a      X    X    X         X    X    X    X    X           that judge douglas             X                                         the dred scott             X                                         that there is    X    X    X    X    X    X    X    X    X           institution of slavery   X         X         X    X                          secretary of state        X    X    X    X    X    X    X    X           the missouri compromise                X         X                               to say that      X    X    X         X    X    X    X    X           of the state     X    X    X    X    X    X    X    X    X           the state of     X    X    X    X    X    X    X    X    X           of the government    X    X    X    X    X    X    X    X    X           major general mcclellan                          X    X    X                     of the country   X    X    X         X    X    X    X    X           secretary of the          X    X         X    X    X    X    X           of the army           X         X    X    X    X    X                it is not    X    X    X    X    X    X    X    X    X           of the potomac                       X    X    X    X    X           part of the      X    X    X         X    X    X    X    X           one of the   X    X    X    X    X    X    X    X                united states to               X    X    X    X    X    X    X           washington d c                       X    X    X    X    X           house of representatives     X    X    X    X    X    X    X    X    X           as to the    X    X    X    X    X    X    X    X    X           harper s ferry                       X    X    X    X    X           the public safety                        X    X    X    X                major general hooker                              X    X                     the gentleman from   X    X    X                                         lieutenant general grant                                        X    X           major general halleck                             X    X    X                major general meade                                    X    X                of the enemy          X              X    X    X    X    X           the union and         X    X         X    X    X    X    X           the day of   X    X    X    X    X    X    X    X    X           the president of          X    X    X    X    X    X    X    X           the rio grande        X                                              the senate and   X         X         X    X    X    X    X           to the senate              X         X    X    X    X    X           army of the           X              X    X    X    X    X           city point va                                       X    X           and house of                         X    X    X    X    X           executive mansion washington                         X    X    X    X    X           of the treasury           X              X    X    X    X    X           of the secretary                         X    X    X    X    X           of the bank      X         X                                         of the public    X    X              X    X    X    X                of the war        X    X         X    X    X    X    X           yours very truly               X    X    X    X    X    X                as may be    X    X    X         X    X    X    X    X           he did not   X    X    X              X                          lincoln president of                    X    X    X    X    X    X           m stanton secretary                               X    X    X    X           stanton secretary of                              X    X    X    X           the war department        X              X    X    X    X    X           i shall be   X    X    X    X    X    X    X    X    X           william h seward                    X    X    X    X    X    X           edwin m stanton                               X    X    X    X           for the purpose      X    X    X    X    X    X    X    X    X           general grant city                                      X    X           i have been      X    X    X         X    X    X    X    X           is to be     X    X    X         X    X    X    X                it will be   X    X    X         X    X    X    X    X           it would be      X    X    X         X    X    X    X    X           of all the   X    X    X    X    X    X    X    X    X           of the department         X              X    X    X    X                the post office      X    X              X    X    X    X                the public lands     X    X                   X    X    X                yours of the     X         X    X    X    X    X    X                at p m                            X    X    X    X           grant city point                                        X    X           h seward secretary                  X    X    X    X    X    X           i have no    X    X    X    X    X    X    X    X    X           in relation to   X    X    X         X    X    X    X    X           seward secretary of                     X    X    X    X    X    X           that i have      X    X    X    X    X    X    X    X    X           as follows to    X    X                   X    X         X           dear sir yours             X    X    X    X    X    X                sir yours of     X         X    X    X    X    X    X                dear sir i        X    X    X    X    X    X    X                ought to be      X    X    X    X    X    X    X    X    X           of the is    X    X    X    X    X    X    X    X                Phrase word clouds   I tried visualizing the table above as word clouds but in hindsight don't think it was the best way to display the data. It did give me an excuse to play around with  D3 library  though.       As usual, the code's up on  Github .",12,2,2013-02-12,7,"abraham lincoln, lincoln, speech, writing, essays, papers, natural language processing",1939,An analysis of Lincoln's words
20,0,No one is talking about the data Tesla is collecting and how valuable it will be in the future.,#meta,"{% include setup %} What’s lost in the  Tesla/NY Times discussion  is how much information Tesla is collecting. Tesla collected the location, the speed, and the battery charge throughout the journey and referenced it during the rebuttal. Is Tesla collecting this data for every car sold? Do the drivers know this data is being collected? If John Broder knew Tesla had this data from his drive  his review  would have turned out differently. We’re all in favor of truth and honesty in reporting but should it be this easy to share data? What prevents Elon Musk from digging into the driving data of a politician who proposes some legislation that will adversely impact Tesla and finds likely unethical behavior?  As  software eats the world , data will be collected from more and more areas of our lives. Target is already  figuring out  whether you’re pregnant and this is just from using your purchase history. Combine that with other data sources, increased computation power, and cheaper data storage and companies end up knowing us better than we know ourselves. We need to make sure that our privacy evolves alongside the data. Currently, the concept of data privacy is too abstract to make us care. We need to see the actual data and the derived results in order to see how valuable it is. Only then will we want to protect it.  Disclosure: I love what Tesla is doing and own Tesla stock. I also realize that this data is used to offer a better, cheaper product. At the same time, I believe we need to find the right approach to privacy when it comes to our data.",4,1,2013-02-21,3,"tesla, privacy, data",300,Tesla and privacy
23,0,I used Foursquare to plan my trip to New Orleans and had a great time. Technology is making travel a lot easier.,#product,"{% include setup %}                                    Last week, my wife and I took a vacation to New Orleans and it was the first time we used Foursquare to plan a trip. I asked friends for suggestions, looked at other Foursquare lists, and did some online research to create  my list  of 25 places that I wanted to visit while there. These places ranged from tourist magnets such as Bourbon Street and Cafe Du Monde to the more local places like Cafe Envie and Port of Call. Our typical approach in New Orleans was to go to a neighborhood we wanted to explore and then visit the places that were nearby on our Foursquare lists. Out of the 25 places I had on my list, I ended up visiting 16 which is a bit low but I made up for that by visiting a bunch of local places that I wouldn’t have discovered otherwise.  Before I had a smartphone, I remember drawing a map every time I went somewhere new so I wouldn’t get lost. I’m no longer doing that but still need to come up with a rough plan of when to visit the various places. The next step would be something that takes my list and applies a route finding algorithm to come up with an agenda that gets me to visit all the places while taking into account distance, open hours, and the venue type. Combine this with  Google Glass  and you get  a pretty awesome way of exploring a new city. I’m excited.",2,1,2013-02-22,4,"foursquare, google glass, travel, augmented reality",288,Vacationing with Foursquare
15,0,Dealing with the madness of having multiple accounts for each of my startups and projects,"#meta,#product","{% include setup %}            Over the past year, I cofounded two startups and launched a bunch of side projects. Since they all had potential, I wanted to make each as standalone as possible and ended up with dedicated accounts for each. This meant that I had a flood of accounts for each, ranging from the various Google products to Sendgrid and AWS to Freshbooks and Quickbooks. Unsurprisingly, this turned out to be an unmanageable pain in the ass.  It gets worse. We ran into a trademark issue and had to change our company name from Glossi to  Pressi  and transfer our branded assets. This meant handing over our domain and since we’re heavy users of Google’s products, losing access to our email, our documents, and our calendars. To migrate, we had the great fortune of having to forward the important emails and share the important documents to our new account.  This led to me an epiphany that we’re using today. Only have unique email. Everything else can be managed through individual accounts until it’s necessary to create company accounts. And even then, only create accounts that are absolutely necessary, which will typically be the financially dependent ones (Freshbooks, Stripe, etc). This allows us to not worry about having a flood of Google tabs open and we get to avoid the adventure of figuring out whether a doc we’re looking for has been shared on a personal or company account. When something does need to be shared with someone outside the company, we share it with our corporate clone and manage it from there.  By no means is this a perfect solution but it works for me and I only wish I stumbled unto it sooner. How do you make it work?",1,2,2013-02-25,4,"startups, multiple accounts, time management, productivity",311,Account management madness
16,0,I recently finished Bruce Schneier's Liars and Outliers and wanted to write a quick review.,#books,"{% include setup %}            I’ve been a fan of Bruce Schneier ever since I read his  post about security theater  in the post 9/11 world. As soon as I discovered that he wrote a book,  Liars and Outliers , I added it to my to-read list and just finished reading it over the weekend. It’s one of those books that is obvious as you read it but spawns a ton of thoughts. He develops a framework that he uses to analyze security and trust in individuals, organizations, and differently-sized societies.  Trust is the foundation that’s allowing the world to become faster paced and interconnected. We’re interacting with people all across the globe, our organizations and businesses are larger than ever, and we’re more dependent on technology than ever. Modern life depends on these complex trust systems and Schneier does a great job explaining the various interactions and the impact technology is having. As others have said, the 21st century will be about data and the rise of social networks, wearable computers, and the quantified self movement are an indicator of the type of data that will be collected. We need to make sure proper systems are in place to prevent abuse and Liars and Outliers provides a great framework to think about these issues and prepare us for the data century.",2,1,2013-02-26,4,"bruce schneier, security, trust, liars and outliers",256,Book Review: Liars and Outliers
18,0,The old version of the iOS App Store would immediately close when downloading a new app. Why?,#design,"{% include setup %} In old versions of the iOS App Store, every time you downloaded a new app it would close the App Store and navigate to the screen with the now-downloading app. Recent versions of the App Store keep it open and force you explicitly exit. I’m surprised that the App Store didn’t launch with the new behavior - it must have been a conscious decision since the development effort for both seems similar.  The only time the original approach is faster is when users intend to download a single app. I suspect this is actually the most common scenario and someone at Apple decided to design the App Store to optimize for it. Unfortunately, they didn’t consider the frustration of having to download multiple apps - even if one only does this a fraction of the time. Training yourself to hit the home button after downloading a new app is a lot easier than training yourself to scroll to the App Store icon, clicking on it, and resuming the app search. Even if 90% of the time a user is only downloading a single app, the other 10% matters is significant given a large enough cost. One can forgive Apple for launching the iPhone with this behavior - no one knew how people would use the App Store. But why did it take years to release the update?",0,1,2013-03-01,4,"apple, ios, app store, user experience",230,"App Store, what took so long?"
16,0,The post office should introduce price discrimination to make up for the $5B loss in 2012,#pricing,"{% include setup %} I recently came across Jeff Jordan’s  post  about revamping the post office so it’s no longer losing more than $5 billion a year. Jeff suggests the obvious solution of raising prices but I think a more clever approach would be to start price discriminating. Everyone who needs to mail a letter has to pay 46 cents for a stamp but why not come up with tiered pricing. People who need to send something urgently can pay more than a dollar while others who only care that the letter arrives can pay 20 cents. The postal service would need to ensure their systems are able to track how full or empty each shipment is but this would allow them to ship the cheaper, less urgent mail with the more urgent mail to maximize the shipping space. Another way to price discriminate would be to give a discount for mail that’s picked up at the post office within a few days rather than being delivered to the home.  I took a quick look at the USPS  financials for 2012  and if the average price of a first class delivery increases from 42 cents to 52 cents, the post office would be profitable given the same volume. I realize that’s a 24% jump in price but if it’s done via a price discriminatory approach, such as introducing multiple price points based on delivery guarantees, it won’t feel as drastic.                                 Service Line               Revenue               Volume               Unit Price               New Price               New Revenue                                               First Class               $28,867               68,696               $0.42               $0.52               $35,721                                   Standard               $16,428               79,496               $0.21               $0.21               $16,428                                   Shipping + Packages               $11,596               3,502               $3.31               $3.31               $11,596                                   International               $2,816               926               $3.04               $3.04               $2,816                                   Periodicals               $1,731               6,741               $0.26               $0.26               $1,731                                   Other               $3,785               498               $7.60               $7.60               $3,785                                   Total               $65,223               159,859               $0.41               $0.45               $72,078                     Airlines have been price discriminating since Sabre launched in the 60’s, coupons have been around for 100 years now, and retailers have been offering discounts on out of season items for even longer. Hardware and software improvements are streamlining operations all over the place and are allowing companies to price more efficiently than ever. I’d love to see the government do the same.",2,1,2013-03-05,4,"post office, usps, postal service, price discrimination",416,Discriminatory Pricing in the Post Office
21,0,We need to stop looking for short cuts and do the necessary work. Focus on the means not the ends.,#meta,"{% include setup %} Entrepreneurs are familiar with the elevator pitch. The idea is to give a pitch in 30 seconds (the duration of an elevator ride) that is compelling enough to an investor that it leads to a follow up meeting where you can go through your pitch deck. An entrepreneur coming up with an elevator pitch is similar to a politician trying to come up with sound bites that are easily digestible, look good on the news, and stick in people’s minds.  Why are we so intent on diluting our message? So much substance is lost when we simplify and condense. We mock politicians when they speak in sound bites and yet we do the same thing when we pitch investors. We both want to draw attention to ourselves and stay top of mind but why take shortcuts? Investors will come to you if you build a great product, get customers, and generate revenue. Voters will support you if you empathize with them and support their community. We need to stop looking for the easy way out and just do the work, success will follow.",0,1,2013-03-09,5,"elevator pitch, politics, startups, business, success",188,Just do the work
20,0,My experience building a pseudo static site where the static site is hosted on S3 and is generated dynamically.,"#code,#aws","{% include setup %} Reading  Katie Zhu’s post  on NPR’s news app architecture got me curious about a setup where most of the content is static and can be hosted on S3 and EC2 is primarily used to generate the static content which is then uploaded to S3. The benefits were obvious:      Cost:  S3 is cheaper than EC2.    Reliable:  S3 doesn’t go down near as frequently as EC2.    Scalable:  Since it’s primarily static you don’t have to worry about additional capacity or dealing with caching, databases, and all the other fun things.    Simpler:  There are no weird server issues here. As long as you generate the right content and your rendering is good, you don’t need to worry about a web server acting up.     I’ve been meaning to write a script that would scrape Hacker News in order to show me the top content I missed while sleeping. I had some time this weekend and decided to give it a go using this “pseudo-static” approach. The result is called Yet Another Hacker News Reader ( YAHNR ) and you can take a look at the code on  GitHub . Turns out it was pretty simple to write and the most difficult part was thinking differently about the problem. Whereas I’d keep the content in a database I ended up storing them in static JSON files and instead of having the logic to generate the HTML page live on a web server I have it using Mustache templates.  I’ve become a fan of this approach and think every developer should try it out. It offers a new perspective and most apps will have some components that’ll be able to leverage this sort of setup. Right now, if you run a static blog and want comments, you can use  Disqus . You can use  Firebase  to build entire web apps that do all the work on the client side. As more and more services become available via Javascript, this approach becomes more and more practical.",5,2,2013-03-12,4,"S3, EC2, static site, web server",359,Mmmm... pseudo static sites
14,0,We just released an update to Makers Alley that lets you buy pieces.,#product,"{% include setup %}            A brief one today.  This past week, Sandy and I have been super busy getting a new version of  Makers Alley  out that allows you to customize and buy furniture. We’re launching with two makers that have items for sale but we’re busy adding more.  Withers &amp; Grain  specialize in using reclaimed wood from the 5 boroughs and do their own wood and metal work.  Mark Grattan  is a furniture designer who has designed a furniture collection for Makers Alley in a geometry-inspired style. Take a look at their pages, watch their videos, and customize their pieces. If you have any feedback let me know.",3,1,2013-03-16,3,"furniture, makers alley, design",165,Makers Alley v2
10,0,I used some old jeans to make an art piece,#meta,"{% include setup %} At the beginning of 2013, I set a  goal  to do something with my hands to contrast with the constant life in front of the screen. I finally finished my first “art” project this past weekend and documented the result. I had a stash of old, torn jeans that were just taking up space and instead of throwing them out I decided to have some fun. Here’s the process and end result.                              1. Find a silhouette of the NYC skyline                                      2. Replicate the silhouette by cutting pieces out of old jeans and arranging them on a piece of cardboard wrapped in another shade of jeans.                                      3. Use some fabric glue to attach the pieces to the canvas                                      4. The result - now all it needs is a frame",1,1,2013-03-20,3,"jeans, jean art, dyi",220,"One art, please"
16,0,The weev verdict got me thinking about email addresses and how they are no longer private,#meta,"{% include setup %} Last week, Andrew “weev” Aurenheimer, was sentenced to 41 months for going through publicly accessible AT&T URLs which exposed the email address of 114,000 iPad owners. I don’t want to talk get into the absurdity of the sentence or how AT&T should be the one held accountable for this “ security .”  I’m more interested in the fact that people still consider an email address to be private information (although I do realize that the leak also revealed iPad ownership information). This may have been the case years ago when we arrived on the internet but right now, our email addresses are everywhere. We give it to every new website we sign up for and we display it proudly on our websites. I’m sure my email address appears on dozens of spam lists for sale on the internet. Google already gives 3000 results when I search for my email address.  The definition of what is and is not private changes as a society evolves. Technology has been increasing the pace and society has yet to catch up. Most of the people in the tech world are pretty aware of the trends but the majority of people are surprised by how much information they’re sharing whenever they touch a digital device. And it’s only going to get worse. If we’re this concerned about our email addresses, how will we feel when people use Google Glass to look up our Facebook or LinkedIn profiles just by looking at us?",1,1,2013-03-24,5,"weev, privacy, email addresses, technology, society",255,Email addresses are private?
22,0,I'm a big fan of the new trend that puts a ton of content on one page. Especially on news sites.,"#product,#design","{% include setup %} A recent trend in website design I’ve been seeing is the long single page. My first distinct memory of seeing it is from  Karma  but I’ve started noticing it everywhere. It runs the gamut from non-profit  causes  to video game  PR firms . In fact, we’re even using this approach for the Makers Alley  homepage .  Surprisingly, it’s starting to make inroads on news sites as well. Whereas before news sites would have an article spread across 20 pages (looking at you Business Insider) in order to increase page views and show more ads, some news outlets are actually improving the user experience. Both  NPR  and the  Washington Post  have posted pieces that leverage this approach and it creates a significantly more engaging read. It’s a pleasure reading long form content this way, richer media adds to the experience and minimizing mouse clicks avoids the disruption of a page load.  I hope it stays.",6,2,2013-03-26,5,"website design, karma, single webpage, news, single page sites",184,"Welcoming the long, single webpage"
15,0,Did the Mailbox app need to use a server or was it about marketing?,#product,"{% include setup %}           After a couple of weeks on the waiting list I finally got access to the Mailbox App. It’s a huge improvement over the standard mail app and my mobile email consumption habits have improved significantly. I’m still not at “inbox zero” but am making my way there.  I don’t know much about iOS development but one thing I’ve been wondering about is whether they could have written it to not use a remote server. Regarding scaling Mailbox, they  wrote :    A critical part of Mailbox scaling is its brand new infrastructure. Mailbox relies on servers in the cloud to do things like send push notifications, download email as fast as possible, and handle “snoozed” messages.    From reading developer docs, it does seem you need a server to do push notifications but I wonder if there’s a way to schedule notifications on the client side. Conceptually, there’s nothing the server needs to do that can’t be done client side via simple polling. This way, the server load becomes non existent and scaling issues are avoided. Even a hybrid approach could have worked: use the server approach when possible but fall back to polling if the servers are overwhelmed. I just can’t help but think that there must have been a way to have the same functionality client-side. The cynic in me wants to say that it was done to build up hype but it’s equally likely that I just don’t know iOS development. It seems odd that crippling an app would help with marketing. I hope that’s not what it takes to sell for $100M.",1,1,2013-03-29,2,"Mailbox, marketing",290,Mailbox: Is the server required?
23,0,"The new Gmail compose is a step backwards in usability. We need to use data to help understand users, not replace them.",#design,"{% include setup %}            A few days ago, Google made the new compose default on Gmail. It went from a separate page to a popup that’s accessible from anywhere in Gmail. And for the vast majority of the time, it’s better: it’s quicker to get to and makes it easy to reference other emails while writing a new one. Unfortunately, for attaching an image (not embedding it inline) or doing some heavy formatting, it’s a huge step backwards and makes me want the old compose back.  I’m sure the data backed up the decision. Only a tiny fraction of all messages needed this additional functionality so why worry about it? The problem with this approach is that even taking into account the infrequency, the cost of the workaround is large enough to cause a usability problem for the power users. It’s akin to the  old version  of the iOS App Store that would close itself every time you downloaded a new app. Sure that was great when you only wanted to download a single app but it made every other scenario significantly worse.  In the rush to be data driven, we shouldn’t forget the actual users and what they’re trying to do. A data driven approach should be used to improve our understanding, not replace it. Otherwise, we run the risk of “nice to have” features replacing the “must have” ones.",1,1,2013-04-02,4,"gmail, ux, analytics, metrics",254,The new Gmail compose
16,0,Coke and Pepsi make healthier soda over Passover. Why don't they do it year round?,"#product,#meta","{% include setup %}           During Passover, Coke and Pepsi sell sugar based versions of their sodas in order to stay kosher for Passover. These high fructose corn syrup (HFCS) free sodas are extremely popular and people stock up while they can. I don’t know whether this is due to the better taste, the nostalgia, or the limited supply but these sugar based versions are definitely more popular. I wonder what would happen if either Coke or Pepsi decided to go “all in” on sugar and launch a marketing campaign against HFCS based food and drinks. I’d love to look at the margins of sugar vs HFCS based sodas and see what the market share increase would need to be in order to offset the switch to sugar. My gut tells me that pursuing this strategy would be a win but the companies are too entrenched in their current process that it’s just not going to happen. Smaller soda manufacturers, such as Boylan, GuS, and Moxie, are growing by differentiating themselves from the big guys and are emphasizing the healthier ingredients. I’m hopeful that this will pressure Coke and Pepsi to make their soda healthier. Unfortunately, what’s more likely to happen is that they will just acquire the niche manufacturers position them to appeal to the more concious consumer, similar to what’s happening to  craft breweries .",1,2,2013-04-05,7,"passover, coke, pepsi, soda, breweries, local, craft",247,"Coke, Pepsi and Passover"
17,0,I got a Raspberry Pi and was able to turn it into a media center using Raspbmc,#code,"{% include setup %}            I’ve been interested in the Raspberry Pi ever since I first saw it mentioned in the tech news and finally got to play with it over the past few days when my brother (thanks  Simon !) lent me an extra one he had. I’ve been in need of a better media center setup ever since my DisplayPort cable stopped working so I decided to try out Raspbmc, a Raspberry Pi based media center.  I scavenged an SD card from my camera and a microUSB AC adapter from my old Droid phone which I somehow still had lying around. With those two, I was able to install Raspbmc but couldn’t get any farther without a wifi adapter. It took the wifi adapter a few days to get delivered but it worked right out of the box and I had a functional media center. Surprisingly, I didn’t need a keyboard at all and was able to run through the entire setup using SSH and a downloadable iPhone app that acts as a remote. The most time-consuming part was setting up a Samba shared folder under Mountain Lion and adding it to Raspbmc using the onscreen UI.            It works well. It solves my “must-have” problem of using my TV to play videos that are on my computer and also has a bunch of “nice-to-haves”. The two big ones are AirPlay support which allows streaming of audio and video from iOS devices and the ability to use my iPhone as a remote. Only thing left is getting an enclosure so it’s not just lying on the floor.  Here’s the setup:          Raspberry Pi  - $25 or $35 model        Wifi Adapter  - $10 on Newegg       microUSB AC Adapter - I found one but should be around $5       SD Card - I had one but can find one for around $6 on  Amazon        HDMI Cable - $2 and up on  Monoprice",5,1,2013-04-10,4,"raspberry pi, raspbmc, media center, airplay",371,Raspbmc
23,0,I never realized this until I read The Idea Factory but cell phones don't have dialtones and landlines do. Why is that?,"#meta,#history","{% include setup %} While reading  The Idea Factory , I came across an interesting passage that explained why cell phones don’t have dialtones:   Meanwhile, Phil Porter, who had worked with [Richard] Frenkiel on the original system, came up with a permanent answer to an interesting question. Should a cellular phone have a dial tone? Porter made a radical suggestion that it shouldn’t. A caller should dial a number and then push “send.” That way, the mobile caller would be less rushed; also, the call would be connected for a shorter time, thus putting less strain on the network. That this idea—dial, then send—would later prove crucial to texting technology was not even considered.    It’s amazing that although this suggestion was made in 1971, we’re leveraging it more than 40 years later with text messaging. How many other technologies and businesses are built on top of SMS that wouldn’t have existed without this decision? I’m sure an SMS-like technology would have come along regardless of this decision but it still makes me wonder how significantly past technological decisions influence us in the present.  An additional meta thought: this is an example of one of those things that gladly lives in the subconscious that has no reason to bubble up to consciousness. I’m sure if someone were to ask me point blank to compare dialtones between landlines and cell phones I’d immediately get it but without a push I never would have thought of it. I wonder how many other connections there are stuck in our heads waiting for a spark to bring them into our consciousness.",1,2,2013-04-12,4,"cell phones, dialtone, land lines, technology",277,Why don't cell phones have a dialtone?
19,0,I wrote a simple plugin to automatically insert a Follow this discussion on HN link using client side Javascript,#meta,"{% include setup %} This past weekend I wrote a  small jQuery plugin  that automatically inserts a “Follow this discussion on Hacker News” link on a recently submitted web page. The motivation was to automate the current workflow that consists of first submitting a post to Hacker News, getting the URL of the comment thread, and then updating the original post to link to the thread. I also wanted to see if it could be done entirely in Javascript so that the code could be included on static HTML pages and not require a backend server.  After some research, I settled on the following approach:  1. Use  Firebase  to store a mapping of URL to the Hacker News thread id. I chose Firebase since it provides a way to read/write using Javascript. 2. Use the  HN Droid API  to retrieve recently submitted HN posts for a given user via JSON. 3. If any of the recently submitted posts match the provided url, store that thread id in Firebase and execute the user defined callback function.  It works as expected but has a few limitations:  1. It relies on Firebase and doesn’t use authentication so someone can modify the database to point to another comment thread. 2. It relies on a 3rd party Hacker News API so if that ever goes down it won’t be able to pull recently posted links to Hacker News. 3. The HN API call only pulls the most recent submissions so the plugin will not be able to get the comment thread for older posts. 4. Since Firebase prevents certain characters from being used in a key, I do some string replacement to clean the string which would allow someone to cause a string collision. 5. The HN API isn't real time and uses a cached version so it may take a bit of time for the link to get retrieved. 6. The code hasn’t been thoroughly tested so may have some weird errors. It’s also my first “real” jQuery plugin so it may not follow best practices.  In general, I’m a proponent of offloading as much work as possible to the client side and believe this will become the norm as the technology improves. We’re already using Disqus to handle comments and Firebase as a database and I expect more services to become available via client side Javascript. This will keep pages simple, reduce server costs, and outsource non-core components to specialized vendors.",3,1,2013-04-15,5,"coding, hacking, jQuery, javascript, hacker news",423,Automatically add a “Follow on HN” link
15,0,I got featured on the HN home page and wanted to share the results.,#dataviz,"{% include setup %} A week ago, I wrote a blog post and submitted to Hacker News. Within a few hours it made it to the front page and I wanted to share the aftermath.         The post generated ~29,000 visits to the blog post over the next few days with the biggest traffic spike occurring on Saturday.                       The post ended up being featured in the NY Times  Bits blog  which accounted for ~2,900 of the total visits; the  Gizmodo network  which accounted for ~1,000; the Guardian, which accounted for ~100; and CNET which accounted for ~60.       The way it spread is pretty interesting: I submitted to HN on Friday afternoon, it was picked up by the NY Times Bits Blog that evening and Gizmodo US on Saturday. After that, it expanded to the rest of the Gizmodo network, including the  UK  on Sunday and  France  on Tuesday.  CNET  and the  Guardian  both picked it up on Monday.       Gizmodo added an Amazon affiliate link to the book I quoted, The Idea Factory, but did me the favor of linking to my startup, Makers Alley. I suppose that makes us even.       Only 80 people ended up visiting the  Makers Alley  site, which is 1/3rd of one percent of the total visitors. These visits were pretty evenly split between the link in the Gizmodo article and the link from my blog.       I have no idea why it took off and don’t consider it one of my better posts. I basically quoted a passage from a book and added a bit of my own commentary. I suspect the topic was appealing due to nostalgia and a bit of geek lore.       It’s surprisingly hard to get on to the Hacker News home page these days but it does drive a significant amount of traffic. I joined HN five years ago and it was orders of magnitude easier to end up making it to the main page.       If you write, do it for yourself and not for the recognition. And if you don’t write, start writing. Nathan Marz has a  great post  that everyone who's interested in blogging should read.       It’s great having my blog hosted on Github pages. It’s free and I don’t have to worry about server load.",8,1,2013-04-19,3,"hacker news, writing, blogging",478,What does getting on the HN front page get you?
16,0,Software can replace hardware if we let it. The smartphone can be that central point.,"#product,#meta","{% include setup %} Last week, three isolated events gave me a glimpse of how powerful mobile can be. Tech pundits have been saying that for a while now but experiencing it firsthand is definitely more convincing.         I went for a run with only my phone to keep me company. After my run was done, I wanted to grab a cup of iced coffee and realized that Starbucks gave me a free drink on my birthday. Downloading the app on my phone allowed me to get a drink without having cash or a wallet.       While checking out at a grocery store, a friend showed me CardStar which allowed him to store all his loyalty cards on his phone. Since then, I’ve imported all my loyalty cards that have just been sitting in a drawer into my phone.       After getting a Raspberry Pi and installing Raspbmc, I was able to use my phone as a remote control just by downloading an app.     These behaviors are different and yet they’re all converging on the phone. What they have in common is that  software is replacing hardware . Hardware doesn’t need to become smarter, it just needs to be able to sync with our phones which can do the heavy lifting. The functionality then becomes limited by software which can be updated more cheaply and quickly than the hardware. It also solves the problem of hardware companies trying to develop software that results in a terrible user experience. Do cars really need the ability to  read a Twitter  feed? And if they do, why not just do it via a simple Bluetooth connection and an audio streaming app on a phone?",2,2,2013-04-24,4,"hardware, software, smartphones, technology",295,Smart software; dumb hardware
24,0,I recently remembered the way older games used to deal with age verification and piracy protection and wanted to share them for posterity.,#meta,"{% include setup %} In the early 90s, being a kid new to the US and new to computers I developed an addiction to computer games. I’d play everything that I got my hands on and remember sharing floppy disks with school friends. Unfortunately, I was plagued by two issues that had pretty clever approaches: age verification and piracy protection.            The first manifested itself in  Leisure Suit Larry . I was as giddy as only a kid can be when I got my hands on it. Unfortunately, that went away when I was required to take an “age quiz” as soon as the game loaded. The age quiz consisted of a series of multiple questions that only an adult would be able to answer. These ranged from factual ones such as “Who recorded ‘Let it be’?” to comical ones such as “Do girls really have cooties?” I do have memories of playing it so I must have figured out some way around the verification. I must have either guesses correctly some of the time or took notes of the answers that allowed me to play.  Another game I enjoyed was a basketball game that I suspect was  Lakers vs Celtics and the NBA Playoffs . Unfortunately, I got the disk from a friend and it had a nifty way of dealing with piracy. When starting the game, it would ask to provide information that could only be found in the game manual, for example asking for the 7th word on the 15th page.  Sadly, both of these approaches disappeared as task switching became standard in the newer operating systems and internet access became common.",2,1,2013-04-29,5,"gaming, privacy, drm, leisure suit larry,",300,Some gaming nostalgia
25,0,"If you send out the personal welcome emails from the CEO to every user that signs up, make sure you're responding to the replies.",#product,"{% include setup %} Recently, I’ve been receiving many startups sending out “personal emails” from the CEO or cofounder around 30 minutes after signing up. The idea is to engage the new user by showing them that there’s a real person behind the service that cares and to offer any help that they may need. There’s a great  article  on Segment.io about this tactic as well as a few other emails that can be sent to improve retention. This technique is called “drip marketing” and there are a bunch of companies offering it as a service - the ones I can immediately think of are  Vero  and  Intercom ; and Mixpanel is moving into this space as well with their  Notifications  product. There are also a variety of open source packages available, I’m familiar with  django-drip  for Django and Dan Shipper’s  Faucet  for RoR.  The twist is that you actually need to respond to the people who reply to the email. There have been numerous times where I’d reply to this email without ever receiving a response. At least I understand that the email was most likely automated; I suspect most users wouldn’t be so understanding. I’m not sure why I need to point this out but if you do decide to send out these personal emails, make sure you’re actually going to respond to each reply. Otherwise you’re better off not sending that email in the first place.",6,1,2013-05-02,2,"Drip marketing, personal email",268,Follow through on that personal welcome email
20,0,Thinking about Netflix and bundling is giving some ideas about the future of video and what we'll start seeing.,#product,"{% include setup %} Something that’s been stuck in my head is the relationship between Netflix and bundling. On one hand, we’ve been wishing that cable came unbundled so we’d be able to just pay for the shows we want to watch. On the other hand, we have Netflix which is striving to let us stream every TV show and movie whenever and wherever we want. Why don’t we care that Netflix is actually a bundled product?  I'm sure the major reason is that it’s just not worth worrying about since Netflix is only $7.99 a month; especially when cable TV bills can easily go past $100. Maybe we like the new shows that are exclusive to Netflix (House of Cards, Hemlock Grove, and Arrested Development) and are happy to pay for them; the rest of the content available on Netflix is just an added benefit. Maybe we just don’t view Netflix as being a bundled service at all: the reason I have Netflix is to be able to watch anything I want when I want.  I wonder about the reasons because it helps me think about the future:     Does Netflix want to be the central repository of all video content that can be accessed at any time? What happens when the existing content producers keep raising licensing fees to extract as much as they can?   Does Netflix want to focus on producing its own content? Is it just a TV channel with a unique distribution channel and monetization approach? Does this mean that we’ll start seeing competing TV show/movie producers creating their own Netflix like service? How easy will it be for consumers to find this content if it’s heavily fragmented?   Will the future consist of niche shows and movies? Kickstarter has been used to raise money for the  Veronica Mars  movie as well as Zach Braff’s  “Wish I was here. ” Will we just have thousands of shows that are just supported by small groups of passionate fans?     I suspect we’ll see a combined approach. Mass market won’t be going away since we all want to stand around the water cooler and chat about the latest episodes but we will start having more and more shows and movies that are catered to our interests and passions. This specialization has been happening throughout the 20th century to our physical products and it’s going to extend to the emotional ones. I don’t know whether it’ll be Netflix, Kickstarter, or some unknown company that’ll make it happen but I do believe it’s inevitable.  Disclosure: I own Netflix stock.",2,1,2013-05-05,5,"netflix, video, kickstarter, media, entertainment",451,"Netflix, bundling, and the future of video"
16,0,Some best practices I've picked up after working on Django products over the past 18 months,"#python,#code","{% include setup %} I’ve discovered that every new project lets me correct mistakes from my earlier attempts by allowing me to start from scratch. This is especially true with a web framework such as Django that has a ton of little nooks and crannies that take a while to explore and understand. It’s usually not worth it to go back and fix something that’s not broken on a functional product but starting a new project lets me do it right from the beginning. Now that I’ve developed and launched (with  Sandy  and  Marc ) two serious Django-based products as well as bunch of smaller ones, I wanted to document some personal best practices I’ve picked up. Obviously, I'm still learning and I may be completely wrong with them so let me know if you disagree. If you’re interested in a deeper look at some of the topics let me know and I can write up another post going into detail about a particular topic.         Use  virtualenv : Virtualenv lets you create a virtual environment for each project you’re working on with its own version of Python and its own libraries. I’ve also created alias commands for my major projects that make moving to and activating the virtualenv of that project a single command. Note that using a virtualenv does make a few things more difficult (such as installing  MySQL-python , setting up nginx, configuring  fabric , getting supervisor running) but they’re all surmountable via Stackoverflow and Google.        Use  South : A simpler way of handling database migrations in Django. It’s natural to be updating your database models as the app grows and South makes the migration a little bit easier. It’s not perfect and every once in a while I’ll need to revert some of the migrations and craft them by hand but it’s still better than the alternative.        Use  Fabric : Fabric gives you the ability to set up your own set of commands that can interact with a remote server. This lets you do git pulls, deployments, and run any other command on a server without needing to manually SSH. This becomes especially useful when you have your app served by multiple machines with each one having a different role.        Use  Supervisor : Supervisor monitors the running processes and can restart any that go down.        Nginx/Gunicorn vs Apache: I’ve used both and don’t have strong feelings about either one. I think there’s more information online about getting Apache running but I’ve found Nginx/Gunicorn a bit easier to configure and debug. The other benefits I’ve gotten from Nginx/Gunciron is that it’s less memory intensive out of the box than Apache and I was able to get it to play nicely with Supervisor. In full disclosure, I haven’t really tried to do the same with Apache and it may very well be possible.        Use S3 for static files: Hosting your static files as well as user-uploaded files on S3 is a nice win. You don’t have to worry about serving static content and you can also move the static elements away from your web server. Another benefit I’ve found is that once you move to multiple web servers, it’s nice having all static content on a 3rd party on S3 since that allows all web servers to remain stateless and insync. Otherwise you have to worry about a user uploading a file to one web server and then having to copy it over to the other one to make it accessible.        MySQL/PostgreSQL vs RDS: Unless you plan on monetizing immediately, I suggest using MySQL/PostgreSQL. RDS ends up getting pretty expensive and configuring it isn’t as straightforward as modifying a local installation of MySQL or PostgreSQL. If you end up running into scaling issues you can make the move to RDS relatively easily (especially with MySQL) by dumping and reimporting your database and updating your production settings file.        On Django packages: Install new packages using pip instead of just downloading them into your project folder unless you know you’ll be modifying them. Even then, the well written packages let you customize their behavior by writing your own views, templates, and middleware that can exist outside the installed package. This will keep your project much simpler and better organized, and will force you focus on your app rather than trying to hack someone else’s.     After writing this, I realize I need do another post about the Django packages I’ve found to be useful. I'll put that together in a future post.  Edit: Here's the  follow-up post  where I cover useful packages.",9,2,2013-05-07,5,"Django, hacking, coding, web development, startups",817,Eighteen months of Django
22,0,Here are the Django and Python packages I've found to be useful after working with Django over the past 18 months.,"#python,#code","{% include setup %} On Tuesday, I shared some  best practices  I picked up while using Django. This is a follow up post to share the packages that I found useful as well as various hiccups I encountered when using them.               django-registration  and  django-social-auth : Combined, these packages let you handle the basic user registration and activation. Most likely, you will end up having to customize them a bit to do what you want. For example, allowing a user to register using an email address instead of a username or requiring an email address for a user who signs up using Twitter. A small issue that annoyed me is that the signals generated by these two packages occur at different points: django-registration generates signals that includes the request while django-social-auth generates signals that contain the response from the OAuth provider. Depending on your use-case, it may be worth it to use the  simple backend  for django-registration, it automatically activates and logs-in the newly registered users, making your app a bit easy to get into.        django-storages  and  boto : If you plan on using S3 to host static content, definitely take a look at these. They provide backends to make it easy to save and access your static content to S3 without having to deal with the AWS API. I ran into some issues using this along with Cloudfront and django-compressor but I was able to fix them by looking at  Stackoverflow .        django-compressor : This is a neat library that will compress and minify your JS and CSS, check if anything’s been updated, generate an upload the result to static files location, and update the HTML to point to the new location. This makes sure that users never end up with older, cached versions of your static files. One thing to note is that you need to make sure that your Javascript are properly formatted and all end in a semi-colon; otherwise you run the risk of the compression failing. I know that there are other Django compressors  out there  but I’ve been happy with django-compressor.        sorl-thumbnail  and  PIL : If you allow users to upload images this is a must have. It provides a standard way of resizing the images and caching the result. The library comes built in with support for cropping and a variety of other processing options so you don’t have to worry about it. One thing to note is that if a user is loading a page where none of the images have been generated yet, it will delay the page load until all of the images are generated. As long as you know the required sizes of all images, you can run a task on the  backend to generate  each of the images. You may have trouble installing PIL in a virtualenv but doing some Googling it should be easy to figure out.        django-extensions : Just a neat library that comes with additional management commands to make developing Django easier.        django-debug-toolbar : This intercepts every Django request and provides some debug information to help you optimize your code. The most useful piece to me is being able to see the SQL queries that are being executed and helps me figure out what needs tweaking/caching.        django-crispy-forms : If you’re using Twitter Bootstrap, this is a library that lets you generate Bootstrap forms in Django.        django-celery  and  celery : This is a way to run tasks in the background. With Pressi, we initially started with some management commands behind some cron jobs but we ended up switching to Celery when we wanted to distribute it across multiple machines and have built in support for threading and error handling. One thing to note is that we used RabbitMQ as the backend but it takes a bit of time to setup and I’m still struggling to understand the ways to manage it. A lot of people have been using Redis as the backend successfully and I think I’ll give that a go in future projects.        mongoengine  and  pymongo : If you’re using Mongo, take a look at mongoengine, which serves as an ORM for Mongo, and is built on top of PyMongo, a Mongo API. Mongoengine makes it very easy to change your models from a relational database to an documented-based one by keeping the field types and model definitions similar. Be aware that document-based databases are significantly different from relational ones and that although cosmetically your models look similar, the interaction with the backend is very different. You shouldn’t switch to MongoDB just because you can - make sure you’re switching for the right reasons. For Pressi, we use a hybrid approach where we use MongoDB to store a user’s social media content with everything else stored in MySQL. Something to be cautious of is that both of these libraries have been evolving pretty quickly and we ran into an issue where we weren't able to consistently connect to a MongoDB instance until we stumbled unto the right versions of the libraries (in our case, 0.6.20 for mongoengine and 2.4.1 for pymongo).        django-haystack : When you’re ready to graduate from implementing a search using QuerySet filters to an indexing backend, take a look at Haystack. It provides a pretty simple search interface that integrates pretty well with Django and supports a few different backends. We ended up settling on the  Xapian  backend because it was supposedly simpler but ran into some trouble installing it inside a virtualenv until I found  this post . Note that although Haystack supports multiple backends, not all features are supported by every backend so make sure the backend you choose supports everything you need. I believe Solr has the most functionality out of the box but we wanted to keep it simple for Makers Alley.        django-postman : We just implemented this for Makers Alley but it’s a very simple way of doing user to user messaging. It comes with the standard messaging features (inbox, reply, archive, delete) but one thing I wish it had was a way to include attachments.        Fabric : I mentioned this in the previous post but wanted to reiterate it since it makes building and deploying your code easy. It also forces you to think about your environment and you end up with a better structured project as a result.        South : Another package I mentioned earlier that makes it significantly easier to deal with database migrations in Django. The only time we've run into issues using South is when two of us were making changes to the same model in parallel branches. Even then it's easy to replace the two flawed migrations with a functional one.        Unidecode : This isn’t a Django specific library but we found it useful when cleaning up unicode data. If you ever get random unicode exceptions in your code, Unidecode should be able to help.        BeautifulSoup  and  PyQuery : If you need to do some HTML scraping in Python, take a look at BeautifulSoup. It turns HTML code into an object that’s easy to navigate and search. After getting more and more familiar with jQuery, I found a python alternative in PyQuery but am still getting comfortable with it. If you come from the jQuery world I’d try using PyQuery first; otherwise I’d try BeautifulSoup.        requests : Just a nice and simple replacement of urllib and urllib2 that makes it much simpler to make HTTP requests. Your code becomes cleaner, more readable, and more expressive.     I tried to highlight the libraries that have made developing in Django easier but I’m sure there are tons more. I’d love to hear about them so do share.",29,2,2013-05-10,6,"Django, python, hacking, coding, web development, startups",1450,Eighteen Months of Django: Part 2
22,0,Google offering free Adwords credits to businesses might have some pretty large effects on the auction marketplace and drive bids up.,#meta,"{% include setup %} Since we’re using Google Apps for Business for our startup, we’ve been getting a bunch of emails trying to get us to sign up for Adwords. The latest promotion is offering a credit of $300 if we spend $100. It’s a pretty common marketing tactic and tons of companies have similar promotions. What’s special about Google is that they’re running an auction for every single click and by giving some businesses free money, they’re driving the prices up for the entire market.  Without being at Google, it’s impossible to know what effect this has but I tried to do some estimates. According to the  NY Times , Google had 4M businesses on Google Apps at the end of 2011 and by June of 2012 it was up to 5M. So over the first 6 months of 2012, Google Apps gained 1M businesses. If each of those businesses was offered $300 a credit and took Google up on it, there would be an extra $300M in the Adwords market provided by Google, in addition to that business’s Adwords contribution. If we compare this against Google’s advertising revenue during the first  two quarters  of 2012, $20,750M, we get 1.4%. That may not seem that big but think of every bid on Google Adwords increasing by 1%. Of course, not every business took Google up on the offer so the true number is going to be lower but I’d bet even then it’s still enough to affect the market.  I was trying to think of an analogy but had trouble. I initially thought this is similar to a casino giving some of the poker players free money but then realized that the majority of that money will still end up in the hands of players. I then thought it would be akin to ebay offering free credit to some buyers, but once again, most of the value would still captured by the marketplace participants; either by sellers getting higher prices or by buyers being able to buy more or better items. I finally settled on the idea of an amusement park offering free passes to random people. Those people end up making the park more crowded for the visitors who paid yet still buy concessions and the amusement park profits.  I suppose this is what happens when the dominant company in a market full of near-zero marginal cost products runs a promotion. I don’t know whether Google has a responsibility to their existing businesses to keep the auction fair but I know if I were advertising on Google I wouldn’t be too happy with Google giving free credits to my competitors.",2,1,2013-05-15,5,"Google, Adwords, auction, bidding, PPC",461,Google's “free” Adwords credits
22,0,We built a small app that let us add attachments to django-postman by leveraging the the jquery-file-upload library.,"#python,#code","{% include setup %} After doing a round of customer development for  Makers Alley , we discovered that customers really wanted to communicate with makers about their pieces. In true MVP fashion, we got the first iteration out in a day by using  django-postman  to handle the user to user communication. Within a few days, we quickly discovered that text messages weren't enough and we needed to support file attachments, otherwise makers can’t easily show their designs and customers can’t share what they like. Unfortunately, django-postman does not support attachments and we didn’t want to have to incorporate another messaging library. Another constraint was that we were already using the awesome  jQuery File Upload  library (in truth, a modified  Django version by Sigurd Gartmann ) to allow makers to upload images when managing their storefronts.  We wanted to leverage our existing file upload system but also incorporate it with the django-postman messaging library without having to modify any of the code in django-postman. We weren’t able to find  anything on StackOverflow  that dealt with this issue so we were left with writing our own. Here’s the approach we ended up taking that might come in handy for anyone else running into the same problem. The code needs some cleaning and I need to add some error checking but I’m sharing it with the excuse of “perfect is the enemy of good.”  We built a new app, postman_attachments, that would serve as the intermediary between the file upload piece and django-postman.                  models.py: Attachment that would map the django-postman Message model to an uploaded file         {% highlight python %} class Attachment(models.Model):     message = models.ForeignKey(postman_models.Message)     attachment = models.ForeignKey(fileupload_models.GenericFile)      def __unicode__(self):         return str(self.message) + self.attachment.__unicode__(){% endhighlight %}                     api.py: Versions of pm_write and pm_broadcast that would do the same work as the original but would also map the attachments between         {% highlight python %} def pma_write(sender, recipient, subject, file_ids=[], body='', skip_notification=False,         auto_archive=False, auto_delete=False, auto_moderators=None):          ### Same code as in pm_write          for file_id in file_ids:             f = GenericFile.objects.get(id=file_id)             a = Attachment(message=message,attachment=f)             a.save(){% endhighlight %}                     forms.py: In our case, we needed to tweak the FullReplyForm and created our own version that included a new “file_ids” field to hold the ids of the uploaded files. The full solution would need to make versions of the other forms included in django-postman.         {% highlight python %} allow_copies = not getattr(settings, 'POSTMAN_DISALLOW_COPIES_ON_REPLY', False) class FullReplyImageForm(BaseReplyForm):     """"""The complete reply form.""""""     if allow_copies:         recipients = CommaSeparatedUserField(label=(_(""Additional recipients""), _(""Additional recipient"")), required=False)      file_ids = forms.CharField(required=False,widget=forms.HiddenInput())      class Meta(BaseReplyForm.Meta):         fields = (['recipients'] if allow_copies else []) + ['subject', 'body', 'file_ids']      @transaction.commit_on_success     def save(self, recipient=None, parent=None, auto_moderators=[]):         ### Bunch of code from original save method in BaseWriteForm from django-postman         file_ids = [x for x in self.cleaned_data.get('file_ids').split(',') if x]         ### Bunch of code from original save method in BaseWriteForm from django-postman         for file_id in file_ids:             f = GenericFile.objects.get(id=file_id)             a = Attachment(message=self.instance,attachment=f)             a.save(){% endhighlight %}          In addition, we needed to override the default django-postman templates to display the attachments for a message as well as include the necessary javascript to deal with the jQuery File Upload piece.         view.html         {% highlight html %}{% raw %}          {{ message.body|linebreaksbr }}        {% if message.attachment_set.all %}                      Attachments                          {% for a in message.attachment_set.all %}                 {{ a.attachment.file.url }}               {% endfor %}                             {% endif %}{% endraw %}{% endhighlight %}             view.js         {% highlight javascript %} $(document).ready(function(){   var upload_ids = [];   $('#fileupload-attachments').bind('fileuploaddone', function (e, data) {     console.log('Done uploading product images');     $(data.result).each(function(){       upload_ids.push(this.id);     });     console.log( upload_ids.join(',') );     $('#id_file_ids').val( upload_ids.join(',') );      if ($('#fileupload-attachments td.preview').length == upload_ids.length) {       console.log('Enabling input');       $('#reply-form button[type=""submit""]').removeAttr('disabled');     };   });    $('#fileupload-attachments').bind('fileuploadstart', function (e, data) {     console.log('Disabling input');     $('#reply-form button[type=""submit""]').attr('disabled','disabled');   });    $('#fileupload-attachments').bind('fileuploadpreviewdone', function (e, data) {     if ($('#fileupload-attachments td.preview').length == product_image_ids.length) {       $('#fileupload-attachments tbody.files tr').remove();     };   }); });{% endhighlight %}          The last minor thing we needed to do was update our urls.py file to override the standard django-postman urls to have them use our custom form.         urls.py         {% highlight python %} url(r'^messages/reply/(?P [\d]+)/$', 'postman.views.reply',         {'form_class': FullReplyImageForm},         name='postman_reply'), url(r'^messages/view/t/(?P [\d]+)/$', 'postman.views.view_conversation',         {'form_class': FullReplyImageForm},         name='postman_view_conversation'), url(r'^messages/', include('postman.urls')),{% endhighlight %}          I’d love to release this publicly but don’t have much experience creating standalone Django apps. If you have experience in open sourcing Django apps let me know - I’d love to get this out there as a standalone app or somehow incorporated into django-postman.",6,2,2013-05-17,5,"Django, python, django-postman, attachments, jquery file upload",848,Adding attachments to django-postman
28,0,Viewing a long article as a single page should preserve my spot on the page. It's really not a difficult tech problem and a big usability win.,#design,"{% include setup %}            When reading a long form piece, I favor the single-page view. Unfortunately, I usually don’t find out that it’s longer than a page until I’ve finished the first page. At that point, I switch to the single page view which causes the entire page to reload and I have to skim the page to find the spot where I stopped reading.  Why haven’t any of the major content sites dealt with this yet? It’s not a technical problem. All they’d need to do is have an anchor tag on the single page view to indicate the spot that the reader should be shown if they click the “View as Single Page” link after reading the first page. Do they just want to force their readers to look at the ads again? I’m sure this small change would save people hundreds of hours each day.  If you know of any sites that handle this scenario well, let me know; I’d love to check them out and give them a shout out.",0,1,2013-05-22,4,"usability, design, UX, long form content",196,"Save my reading spot, damn it"
22,0,I share some analysis I did on investing in tech stocks now vs decades ago and share my current investment approach.,#finance,"{% include setup %} I initially set out to write a post to complain about how difficult it is for an average investor to “hit it big” these days by investing in a tech company at its IPO but ended up changing my thesis after digging into the data. It’s still possible to get the same returns as it was in the 1980s but it’s not possible by a long-term investment in a single company.  To start, I came up with a sample of large tech companies and looked at their performance since their IPO as well as their annualized return.                    Company  IPO Year  Total Return  Annualized Return                         Apple  1980  12,250%  16%             Microsoft  1986  33,800%  24%             Cisco  1990  60,800%  32%             Yahoo  1996  1,808%  19%             Amazon  1997  14,894%  37%             Ebay  1998  2,817%  25%             Netflix  2002  2,564%  34%             Google  2004  708%  24%             LinkedIn  2011  86%  -7%             Facebook  2012  36%  -64%             Tesla  2010  407%  60%            From this limited sample, it looks as if it’s still possible, but more difficult, to get the annualized returns of the 1980s and 1990s. It’s impossible to know whether the total returns will be comparable but I suspect that it’s going to be extremely difficult, if not impossible. To get a Microsoft-like return, Tesla would need to end up with a market cap of $760 trillion, excluding inflation.  In a way, this is obvious. As more and more money pours into the VC industry, companies can afford to stay private longer and just keep on raising more funding. This gives company management and investors more control, keeps the company leaner, and limits public information. Unfortunately, this leads to most of the growth occurring before the IPO with retail investors not able to capture any of the value.  While I’m optimistic that the  JOBS Act  will help, we unaccredited investors still need a way to invest right now. After leaving my finance job, I tried to replicate the traditional investment approach by doing research, analyzing statements, and reading coverage. But over the past few years I’ve been too busy and have just been investing in companies that I use and like. Unsurprisingly, this new approach has led to me invest in tech companies. Surprisingly, I’m doing better than ever before. Over the past 2 years, I’ve bought stock in three companies: Tesla, Netflix, and Yahoo with the lowest gaining 66%. I realize these returns can’t keep on going so the question becomes when to sell and invest in something else. For this, I’ve been looking at market caps compared to other companies in the same industry to estimate their potential. In my case, Tesla has a market cap of $11B while Audi’s is $26.6B and Toyota’s is $191B, and definitely has room to grow. The standard disclosure when giving financial advice is “past performance is not an indication of future results” and it’s definitely true in this case. I’m just glad I found an approach that suits me.",1,1,2013-05-24,5,"stocks, stock market, investing, IPO, venture capital",470,Investing in tech stocks
25,0,Search engines have a conflict between sources that have authority and sources that have original content. I show an example of it going wrong.,#meta,"{% include setup %} A while ago I read Bruce Schneier’s Liars and Outliers and came across a neat passage:   There was this kid who came from a poor family. He had no good options in life so he signed up for the military. After a few years he was deployed to a conflict infested, god-forsaken desert outpost. It was the worst tour of duty he could have been assigned. It was going to be hot and dangerous. Everyday he had to live with a hostile populace who hated his presence and the very sight of his uniform. Plus, the place was swarming with insurgents and terrorists.   Anyhow, one morning the soldier goes to work and finds that he's been assigned that day to a detail that is supposed to oversee the execution of three convicted insurgents. The soldier shakes his head. He didn't sign up for this. His life just totally sucks. ""They don't pay me enough,"" he thinks, ""for the shit I have to do.""   He doesn't know he's going to be executing the Son of God that day. He's just going to work, punching the time clock, keeping his head down. He's just trying to stay alive, get through the day, and send some money back home to Rome.   Bruce mentions that he found this on the internet and  cited it appropriately  in the footnotes. But when I tried Googling for the phrase ""There was this kid who came from a poor family"" the  top links  were  people citing  Liars and Outliers, including my own  highlight on Readmill . I even came across a  page  that linked to the  original source  that Bruce cited before I found a link to the original source.  I realize this conflict between authority and originality is a challenge for search engines but it seems that they rank authority ahead of originality. This leads to the unfortunate consequence that if any major site cites your personal blog they will appear earlier in the search results. I had this occur with my  post  on the history of why cell phones don’t have dialtones; searching for “cellphones dialtone” shows the Gizmodo link ahead of my blog’s. The nice thing is that Google seems to be getting better - right now searching for “There was this kid who came from a poor family” shows the original source in the third position; a few months ago it was at the bottom of the first page. Let’s hope this trend continues.",7,1,2013-05-29,3,"seo, search engines, google",462,On SEO: Authority vs Originality
20,0,I wrote a simple web app that allows you to get cicycling directions from one Citibike station to another.,#code,"{% include setup %}               Photo by  @rafat      On Wednesday, I took my first bike ride using New York City's new  Citibike  program. So far it's been great but one issue I ran into is being able to plan a trip. Google offers cycling directions from place to place but doesn't take into account the Citibike stations. On the other hand, the Citibke app shows the rental stations but doesn't make it easy to find directions from one station to another unless you're already at one of them.  I decided to actually do something about it and wrote a  little web app  that lets you pick two Citibike stations and retrieves the cycling directions between them using the Google API. It's definitely not perfect and the user experience needs to be improved but it does what it was designed to do.  What I find amazing is how simple it was to write the app - it took me less than 90 minutes to go from having an idea to having something that's usable. The list of stations are available in  JSON from the Citibike site  and Google makes it very easy to use their services to show a map and get directions.The best part is that this app is completely static since it's just using client side Javascript and Google's APIs. I've written about this  before  but I'm convinced that more and more services will become available through APIs which will lead to more and more apps and sites being built this way.",5,1,2013-06-01,3,"citibikenyc, bike sharing, google maps api",286,Citibike Station to Station Directions
28,0,I updated the Citibike directions app to be more general and provide directions from any NYC address to another rather than just Citibike station to Citibike station.,#code,"{% include setup %} To coincide with the launch of Citibike, I wrote a  simple web app  that provided cycling directions from one Citibike station to another. The biggest piece of feedback I received was that people care about getting from place to place rather than from one Citibike station to another. Based on this feedback, I  updated the app  to provide directions from any New York City address to another by breaking every trip down into three steps: the first is to walk to the nearest Citibike station, the second is to bike from one station to another, and the last is to walk to the destination. A limitation I ran into is that Google’s  Direction Service  doesn’t support different transit methods for multiple waypoints. This, combined with my desire to get it out there, is why the design’s not as good as it should be. I’ll see if I can improve it over the next few weeks. People have also been telling me this needs to be on mobile so I’m going to use this as an excuse to jump into mobile development. I’m excited.",3,1,2013-06-04,4,"citibikenyc, citibike, bike sharing, google maps api",205,Citibike Directions: Second Attempt
10,0,I attempt to solve the Priceonomics Puzzle using Prolog.,"#prolog,#code","{% include setup %} The  Priceonomics blog  is one of my favorites so when I saw that they had a  programming puzzle  up I decided to have some fun with it. And what’s more fun than hacking around with a quirky, esoteric programming language? I remember having fond memories of playing around with Prolog in middle school so decided to dig it up again in an attempt to solve this puzzle.  Prolog is pretty different than the mainstream programming languages, it belongs to the logic programming language category and relies on defining a variety of relations and then querying these relationships to get results. A simplified way to think about it is you define a set of equations and tell Prolog to ""solve for X"".  This leads to some interesting behavior. Many functions end up being bidrectional with the Prolog version of a ""concat"" function being a good example. The first argument is a list, the second is the separator, and the last is the resulting string. Passing in all 3 will return true if the concatenation statement is true. Passing in the list and the separator will tell us what the concatenated string is. Passing in the separator and a concatenated string is equivalent to a ""split"" function. The only piece it's not able to figure out is the separator given the list and the concatenated string. Unfortunately, I'm not familiar enough with Prolog to explain why.  {% highlight prolog %} ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', 'Prolog is sweet'). true.  ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', 'Prolog is not sweet'). false.  ?- atomic_list_concat(['Prolog', 'is', 'sweet'], ' ', X). X = 'Prolog is sweet'.  ?- atomic_list_concat(L, ' ', 'Prolog is sweet'). L = ['Prolog', is, sweet].  ?- atomic_list_concat(['Prolog', is, sweet], X, 'Prolog is sweet'). ERROR: atomic_list_concat/3: Arguments are not sufficiently instantiated{% endhighlight %}  For the first pass, I decided to ignore the web side and just focus on defining the exchange rate relationships and have Prolog tell me which exchanges would work. The way it works is that we define a profit to be defined in terms of two intermediate currencies. We can then ask Prolog to give us the currency chain that will result in a profit.  {% highlight prolog %} exchange(usd,eur,0.7779). exchange(usd,jpy,102.459). exchange(usd,btc,0.0083). exchange(eur,usd,1.2851). exchange(eur,jpy,131.711). exchange(eur,btc,0.01125). exchange(jpy,usd,0.0098). exchange(jpy,eur,0.0075). exchange(jpy,btc,0.0000811). exchange(btc,usd,115.65). exchange(btc,eur,88.8499). exchange(btc,jpy,12325.44).  % Calculate profit for a usd->x->y->usd currency chain profit(First, Second, Profit) :-     exchange(usd,First,P1),     exchange(First,Second,P2),     exchange(Second,usd,P3),     Profit is P1 * P2 * P3.  arb :-     profit(First, Second, Profit),     Profit > 1.0,     write('usd '),     write(First), write(' '),     write(Second), write(' usd '),     write(Profit), nl, fail.  :- arb.  % Results: usd eur jpy usd 1.0040882716200001 usd eur btc usd 1.0120965187500002 usd btc jpy usd 1.0025512896{% endhighlight %}  The next step was to get it to retrieve and parse the JSON from the Priceonomics server. After doing a ton of searches and reading a ton of documentation I was able to get it to work. As a next step I'll try to see if I can get it to return currency chains of arbitrary length.  {% highlight prolog %} :- use_module(library('http/json')). :- use_module(library('http/json_convert')). :- use_module(library('http/http_json')). :- use_module(library('http/http_client')). :- use_module(library('http/http_open')).  parse(I) :-     test(CP=S) = test(I),     atomic_list_concat(L,'_', CP),     [A, B] = L,     atom_number(S,R),     assert(exchange(A,B,R)).  % Calculate profit for a usd->x->y->usd currency chain profit(First, Second, Profit) :-     exchange('USD',First,P1),     exchange(First,Second,P2),     exchange(Second,'USD',P3),     Profit is P1 * P2 * P3.  arb :-     http_get('http://fx.priceonomics.com/v1/rates/', JsonIn, []),     json_to_prolog(JsonIn,PrologIn),     PrologIn = json(L),     maplist(parse, L),     profit(First, Second, Profit),     Profit > 1.0,     not(First = Second),     not(First = 'USD'),     not(Second = 'USD'),     write('USD '),     write(First), write(' '),     write(Second), write(' USD '),     write(Profit), nl, fail.  :- arb.  % Results (Might change each run): USD JPY EUR USD 1.0071833283714342 USD EUR JPY USD 1.007164983424893{% endhighlight %}  I'm sure a Prolog pro would have been able to do this much quicker and better but I had a surprisingly fun time doing it. I got a bit frustrated trying to translate the JSON into Prolog relationships but actually getting it to work made it worth it. Trying a whole new programming category is a great way to get more creative and forces us to think about problems differently. Prolog may not be the most practical language but exposing us to new concepts and approaches makes it valuable.",2,2,2013-06-07,3,"prolog, priceonomics puzzle, priceonomics",731,Fun with Prolog: Priceonomics Puzzle
21,0,"I fear that the trend to be constantly entertained is dangerous and going to cause problems, especially for newer generations.",#meta,"{% include setup %} The real time news cycle bothers me. Every time theres some news there are countless reactions on Twitter and quick, shoddy write ups on various ""news"" sites. Unfortunately, by the time someone does the research and writes a thoughtful response, we've moved on to the next piece of news. We're reaching the point where writing something stupid quickly is becoming more valuable than writing something thoughtful but late.  Twitter’s strength is its weakness. The 140 character limit makes it very easy for anyone to share an opinion but that also leads to everyone sharing an opinion. Of course, its ability to break and spread news is invaluable. I just wish that the more thoughtful, well-researched pieces could get past the noise. This week, I would have preferred to see a few insightful pieces about WWDC rather than the same exact WWDC coverage from dozens of sites.  This wouldn't be a problem if it were isolated to Twitter but it's becoming the norm. The  Lincoln-Douglas debates  lasted hours and candidates had an hour for a rebuttal. These days, we’re lucky to get a rebuttal longer than a few minutes. The average shot length for movies  decreased  from over 6 minutes in the 1930s to close to 4 minutes now. Even investors are getting in on the trend and want entrepreneurs to have 30 second elevator pitches instead of real conversations.  Clearly I’m simplifying and there are countless other reasons for these changes. At the same time, this trend towards constant stimulation and gratification is dangerous. It reminds me of a  study  that showed that kids who had more patience and self control ended up with higher SAT scores more than a decade later. If we get addicted to constant entertainment, how are we going to tackle on the challenging problems that require focus?  As a kid I used to lie in bed and read a book for hours but now find myself taking a break every 15 minutes to check up on my digital life. This bothers the hell out of me. I can’t imagine the effect it’s having on kids who’ve never even had a chance to be left alone with a good book.",3,1,2013-06-12,3,"constantly entertained, twitter, creativity",384,Constantly entertained
17,0,"Technology is causing us to move away from central regulatory agencies to crowdsourced, self-regulated communities.",#meta,"{% include setup %} The past few years have seen the rise of the share economy with companies such as AirBnB, Sidecar, Lyft, and TaskRabbit seeing massive growth. Unfortunately, they’re getting significant opposition from government and the entrenched special interest groups. Most of the pushback is under the guise of consumer safety and that regulations exist to protect the consumer.  Regulation is necessary when there’s an information asymmetry between a service provider and a consumer. In such cases, regulations help bridge that information gap and make the consumer more comfortable making the transaction. But the internet has been chipping away at this gap by building communities where people can share reviews and experiences. Yelp, Angie’s List, and Google are the largest of these traditional review sites but reviews are starting to appear everywhere that money is changing hands. Ecommerce sites offer reviews and ratings of the products they’re selling. The share economy companies self-regulate by offering communities with well thought out rating systems. Without well functioning communities they wouldn’t survive.  Consumer safety regulations are making way for ratings and reviews. We’re replacing centralized regulatory agencies with crowdsourced, self-regulating communities. Some regulation will always be necessary, especially in places with large information asymmetries, but these places are constantly shrinking. Of course the entrenched companies are fighting these trends but they should be focused on innovating themselves rather than battling the inevitable.",0,1,2013-06-16,6,"regulation, share economy, airbnb, sidecar, lyft, taskrabbit",233,Regulation and the share economy
21,0,Startups need to push boundaries to grow and I share an experience we had pushing a boundary at Makers Alley.,#meta,"{% include setup %} Startups need to use everything in their arsenal to grow. A big part of it is playing in the grey area between moral and immoral. Do you create fake users and comments to portray an active community? Do you reply to posts on Craigslist trying to get visitors to your site? It’s also much easier to play in this area when you’re a startup - you’re most likely too small to be noticed and even if you are the press won’t spend much time on it. Google already gets a ton of flak every time someone complains about losing business due to a search engine update, imagine what would happen if a Google employee was caught spamming Craigslist.  It’s important for all companies, and especially startups, to test these moral boundaries but there’s no clear answer of what the boundaries actually are, just shades of gray which will vary from company to company and from team to team. I believe that until you get some resistance you need to keep on pushing otherwise you never know that you’re doing enough.  At  Makers Alley , our lesson came when we wanted to increase the amount of makers signing up. We decided to create pages for all makers in an area and then email each of them a link to “claim” their page. In order to make the pages look appealing we took images and descriptions from their individual sites. The results were mixed: as expected most emails didn’t even get a response but the ones that did had a wide range of reactions. Some of the makers gladly claimed their page and loved that their content was automatically pulled. Yet others were pissed that we used their copyright images on our own site without their permission. We weren’t comfortable knowingly upsetting some users and quickly removed the images from the unclaimed pages. We continued running a few other tests to try to maximize the “sign up rate per email” and ended up settling on a simple email that asks the makers to sign up and getting rid of the claim functionality. But without making the misstep in the beginning we wouldn’t have been able to settle on this approach. I do worry that less scrupulous companies have a higher chance of success but I can only take actions I’m comfortable with.",1,1,2013-06-20,3,"startups, morals, boundaries",396,Pushing moral boundaries
27,0,Sales approaches can be seen as varying by inbound vs outbound and proximity to customer. They all work but we're trending towards a self serve model.,#sales,"{% include setup %} Something I’ve been thinking about is the variety of sales approaches. On one extreme, you have pharmaceutical companies sending sales reps to visit doctors offices to try to get them to prescribe their drugs. On the other you have companies such as MixPanel and Dropbox which rely on a self serve approach. And in between you have companies such as NewRelic which offer a self-serve trial and try to upsell you with emails from a sales rep.  Depending on a product’s complexity and its cost structure your sales approach may be limited but it’s always worth seeing the other approaches available and if any of them may fit. It’s likely that an approach that didn’t work a year ago may work right now. A simple way to check is to look at newly launched competitors in your space and see how they’re acquiring customers.  After trying to come up with an exhaustive list of approaches I figured out it's easier to just rank them across two dimensions:          Proximity : This is both physical proximity as well as familiarity with your customer. It’s much easier to sell when you’re in the same room as them and know their story than when you’re sending out a generic email.        Inbound vs outbound : A customer already having an interest in your product is much better than trying to interest him from scratch.     Here’s my attempt at coming up with matrix showing where different companies would lie based on their sales approach.      It’s possible to be profitable by being in any spot; a higher acquisition cost will just lead to a higher price. That’s why an Oracle installation can  cost millions  of dollars a year and why the enterprise Dropbox product is around  $125/user/year . I believe that current trends favor businesses in the inbound/self-serve quadrant. This is due to people becoming more comfortable with technology as software gets better and easier to use and the ability for companies to offer free-trials with near-zero marginal cost. A corollary is that there’s an opportunity to compete with businesses outside the quadrant by creating simpler, cheaper versions of their product. The first version will suck compared to the existing products but as long as it’s cheaper and still solves a problem you should be able to get some customers and revenue (a la  Lean Startup ). Over time, you can continue to grow and keep on building our product until you’re competing with the existing companies (a la  Innovator’s Dilemma ).",4,1,2013-06-21,7,"sales, innovator's dilemma, lean startups, mvp, saas, startups, business",456,Where are you on the sales matrix?
16,0,I used to think business frameworks full of bs but now find them very useful.,"#meta,#product","{% include setup %} Tom Tunguz wrote a  great post  yesterday sharing the frameworks he uses to evaluate and analyze startups. For this post, I’m not interested in the content (which is great for anyone building a company) but I am interested in the concept of business frameworks and their application. When I was younger and came across a “business” framework I would dismiss it as obvious and move on. Now, I’m aware of how valuable a good framework can be. A good framework imposes structure that leads to a clearer though process with better results. At the same time, it needs to be simple to apply but be expressive enough to describe the complexity of a business. Being human, we also don’t want to think about our own fallibility and weaknesses which makes it difficult to critique our businesses. We also want to solve problems on our own rather than share our uncertainties with others. A framework serves as an impartial third party where you go through and fill in the blanks until you discover you aren’t in as good of a shape as you thought. Now you can work on growing your company instead of avoiding self-criticism.  I recall struggling to fill out the Lean Canvas for Pressi after reading one of the lean startup books. It took me a few hours with multiple breaks and online searches but I ended up with a much better understanding of our business. It became easier to see where the risks were and gave us tons of ideas around acquiring new users and generating revenue. None of these ideas were groundbreaking and I’m sure we would have gotten to them eventually but it was valuable getting to them earlier since we were able to take them into account when building the product.  I’m not sure why investors don’t require pitching startups to share these instead of a pitch deck, they seem much more useful.",1,2,2013-06-25,4,"business frameworks, startups, lean canvas, business model canvas",328,Business frameworks are actually useful
30,0,"While flying out of Lagaurdia Airport, I decided to observe the way people interacted at the Biergarten, a restaurant that required customers to order and pay using an iPad.","#product,#meta","{% include setup %}            Last week, I had a morning flight out of Laguardia Airport and being into all things tech decided to grab a coffee at a place called Biergarten since they had iPads at every seat. Turned out that the only way to order and pay was by using the provided iPad with the attached credit card reader. I had 30 minutes to kill before my flight and decided to spend it observing the interactions others had with this ordering system.  During the 30 minutes, I saw 6 people approach the bartender and every single one tried to order directly from the bartender without paying any attention to the iPads. Surprisingly, none of them gave up after being told they had to order using the iPad although two couldn’t figure out how to use the iPad and needed help. The major points of frustration were finding the app and then realizing that you needed to “check out” before submitting the order.  I suspect no one actually benefits from this sort of setup. The supposed benefits to Biergarten are that they’re able to hire fewer people and collect payments upfront but I’m not sure it’s worth it given the high usability cost to the consumer. The staff is now kept busy explaining how to operate the iPads and customers are significantly slower at ordering than a trained waitstaff would be. In addition, if an iPad is the only way to order then they can’t have more customers than there are iPads - no one can get anything to go or have a drink standing up unless there are iPads available. The only way this is a good idea is if the iPads are able to attract more customers.  Essentially, the company is trying to externalize the cost of serving customers to the customers without taking into account their experience and frustration. You want to make it as easy as possible for people to give you their money and forcing technology down your customers’ throats isn’t always the answer. As optimistic as I am about technology making things easier, it’s going to be difficult for brick and mortar places to move to a self serve model. It’s simply easier to give someone cash or a credit card and have them do the work than doing it yourself.",0,2,2013-07-01,4,"laguardia, brick and mortar, ipad, self-serve",422,Externalizing externalities in brick and mortar
39,0,It's possible for a design to be too good that leads to a boost in vanity metrics. We saw this happening with Pressi which caused us to do a bunch of adhoc work to deal with the growth.,#design,"{% include setup %} This is a bit of a first-world problem but it’s possible for a design to be too good. A great design may lead to an increase in your vanity metrics but that won’t necessarily translate into a successful business. In fact, it’s likely that these low-value users will increase your costs.  When we redesigned the landing page for Pressi (formerly Glossi) we saw the signup rate from our landing page shoot up to to close to 34% from below 5%. Unfortunately, our retention rates were abysmal and we were stuck supporting thousands of Pressi pages that were not seeing any engagement. This led to a massive increase in our AWS costs that we had to scramble to contain. The solution was to be smarter about the frequency of our data pulling as well as minimizing the amount of data we were storing for our users. In hindsight, we should have solved our retention problem before trying to grow our users but we were too obsessed with our user growth numbers to do the right thing.  I realize that user growth is a problem that many startups would love to have and that it’s foolish to choose a crappy design over a great one. At the same time, if you’re not tracking the metrics that align with what you’re trying to accomplish, a surge in growth will be more damaging than beneficial. The corollary to this is that if your design sucks and yet you’re still getting signups then you must be onto something.  For those interested, here’s the signup flow that had the ~34% signup rate. The awesome design work was done by  Marc .                                   The Pressi landing page                                           Pressi signup step two                                           Pressi signup step three",1,1,2013-07-03,4,"design, startups, growth, pressi",365,A design that's too good?
18,0,Meetings are both valuable and a time sink. I share my thoughts and approach to taking meetings.,#meta,"{% include setup %} I struggle with this one. Some days I feel as if I should take every meeting since it’s impossible to know where it can lead. One meeting can completely change a business, generate some consulting work, or lead to new friendships. At the same time, taking every meeting would eat up a chunk of time and most meetings end up fading from memory.  I’m still figuring out my approach but do believe that having fewer, more meaningful relationships is more valuable than having many fleeting ones. Unfortunately, it’s not clear what will end up being meaningful before the meeting. Currently, I try to take every first meeting or at least have a phone call but have been scheduling them all on a single day, early in the morning, or late in the evening to avoid disruption. I’m also trying to make every meeting valuable by taking follow up notes in order to reach out later if I come across anything relevant or if I need to send an introduction. Probably the most important thing I’ve learned is that it’s easier to rejuvenate an older relationship than to create a brand new one so I’ve been making an effort to catch up with at least two former acquaintances each month.  There’s no single approach to meetings that will work for everyone but relationships are important regardless of what you do and it’s essential to maintain and grow them. I’d love to hear how others deal with meeting overload.",0,1,2013-07-07,3,"business, meetings, entrepreneurship",252,Meetings: to take or not to take?
15,0,I'm excited by the Javascript visualization libraries that encourage interactive graphics and story telling.,"#dataviz,#javascript","{% include setup %}      Something I’ve always enjoyed is messing around with data. For me, the first part has always been to plot the data to get a quick understanding of the dataset. Is there any obvious distribution visible? What are the data ranges? Are there any clusters that fit a known pattern? Does the data look clean or are there a ton of outliers? Does the data even make sense? Only then would I start the analysis and modeling piece.  At first, I’d just dump the data into Excel to generate various charts but moved on to using Perl and Python to generate charts when I learned the value of reusable code. While at  Yodle , I picked up R which gave me more power than what I knew to do with and introduced me to a whole new set of visualizations and models. Recently, I’ve been having a blast using  D3  and  Vega . The biggest appeal is that they’re in Javascript so they can run in all modern browsers and make it very easy to support interactive behavior. The best analyses always tell a story and allowing users to interact with the data is a great way for them to craft their own story. I’m hopeful that such tools will improve data accessibility and get people excited about gleaning their own insights.",3,2,2013-07-09,5,"d3js, d3, vega, data visualization, javascript",252,D3 and Vega
27,0,Building Makers Alley we ran into a wide set of features we needed to implement that make marketplaces pretty hard from a technology standpoint as well.,#product,"{% include setup %} There are countless posts discussing the business and marketing challenges when building a marketplace but I wanted to discuss the issues on the tech side. While we ran into technical challenges building  Pressi  they were mostly issues with scaling and dealing with the various social network APIs. With  Makers Alley , we didn't run into scaling or API issues but had to deal with a ton of functionality in order to be seen as a credible marketplace. Individually, the features are simple for an intermediate developer to build but there are a lot of them with varying degrees of nuance and logic that need to be worked out.  Note that some of these issues are only applicable to ""maker"" marketplaces where the merchants make the pieces to order. In those cases, I refer to them as makers rather than merchants.  In no particular order:    	  Payments :  Stripe  and  Balanced  have made this significantly simpler but one thing to watch out for is that you will need to have a merchant signup process to collect the required regulatory information if you want to automate disbursements.  	  Shipping and Tracking : Makers take a different amount of time to make each piece. Buyers should know this information before placing an order and makers need to be able to change it depending on their schedule and order load. Merchants need the ability to mark an order as shipped and possibly provide a tracking number to the buyer. You should notify buyers when their order has shipped.  	  Logistics : While you're not holding inventory, you're charging customers and expect the merchants to fulfill their end of the agreement. How do you deal with a merchant sending an order late or not being able to fulfill an order? Do you want to have merchants approve every order they receive or do you assume that they'll be able to fulfill it? How about a single order containing items from many merchants?  	  Returns : No matter how good the products are there will always be someone who's unhappy with an item and you need to have a return/refund policy. For small items it's simple to figure out the logistics but how do you deal with someone wanting to return a dining table to a merchant a few states away? Where should the item be sent and who's responsible for paying the shipping and handling cost? What happens to the returned item?  	  Messaging : We discovered this a bit late but customers really want a way to talk to the merchant. Some buyers will want to customize an order and may want to both send exchange photos with the merchant to make sure they're getting what they want.  	  Shopping Carts : There are a bunch of existing solutions out there but we weren't able to find one that fit the needs of a two sided marketplace that supported customizable product options. there are a lot of things we take for granted when using a full fledged site like Amazon - making changes to your shopping cart, buying from multiple merchants, applying rebates and discounts, and getting recommendations. A possible edge case is merchants running out of inventory while someone is going through the checkout process.  	  Orders : Both merchants and buyers need to see a history of their orders. The implication is that once an order is placed it needs to be immutable and timestamped so that changes to the items are only reflected going forward. In addition, orders can get messy since a single order may be spread out across multiple merchants and items. What happens if one merchant can fulfill their half of the order while the one can't? Do you issue a refund for part of the order? What if the customer only wanted the items as a package deal?  	  Taxes : At some point you need to start dealing with taxes with each state having their own regulations and rates. We haven't implemented the details here yet but I suspect interstate commerce can get complicated quickly.  	  Reviews : Before a purchase, buyers want to see reviews of an item. After a purchase, buyers may want to rate and review the items. Should merchants have the ability to respond or challenge a review?  	  Search : This is a big one. Buyers need to be able to quickly find what they're looking for or they'll give up and go somewhere else. What criteria can users search for? Are you going to deal with typos and misspellings? Should you support faceting? How should you tier your prices? Do you need to support geospatial search? This is probably the biggest piece that requires understanding your audience and tailoring the search experience to them.  	  Images : It's rare that someone will buy a physical item without at least seeing a picture of it first. Makers need a simple way to upload multiple images and change the order in which they are displayed. The code should also be smart about generating thumbnails that can be used on different pages - search/listing, product view, shopping cart, etc.  	  Changing Inventory : Makers will need to be able to modify and remove what they're selling. At the same time, you need to have a record of the history so that buyers and makers can look at prior sales.     Most of these issues can (and should) be handled manually at the beginning either through the backend or through email but this approach won't scale. The goal is to be able to support the various use cases even if they have to be done manually. This will make you look credible to your customers and also give you a sense of which cases are the costliest and need to be automated. Technology shouldn't be the primary focus for a marketplace business and you will most likely fail due to a lack of users on one of the sides. At the same time, the technology behind a marketplace isn't simple since you're basically smashing together an ecommerce site with a social network. If you have any additional thoughts or questions definitely let me know and I'll try to help.",4,1,2013-07-13,3,"marketplaces, ecommerce, startups",1040,Marketplaces are hard
22,0,Over the weekend I updated my old Yahoo fantasy football stats scraper to use Scrapy and wanted to share some thoughts.,"#python,#code,#datascience","{% include setup %} Last week, someone reminded me of an old project I had on GitHub that scraped fantasy football stats from Yahoo. Unfortunately, it was antiquated and failed to retrieve the data for the current season. I’ve also been interested in trying out the  Scrapy  framework and decided this would be a good opportunity to give it a shot. I tried finding a sample project that dealt with authentication as a starting point but wasn’t able to find one so hopefully my attempt can serve as an example to others.  The full project is  available on GitHub  but I wanted to highlight a few of the components:    	  parse method : This submits a form POST to the Yahoo login page which authenticates the session. The key point here is to specify a callback function which will continue the existing session. {% highlight python %} def parse(self, response):     return [FormRequest.from_response(response,                 formdata={'login': self.settings['YAHOO_USERNAME'],                 		  'passwd': self.settings['YAHOO_PASSWORD']},                 callback=self.after_login)]{% endhighlight python %} 	  	  parse_stats method : In previous projects, I struggled with separating the crawling from the parsing since the page would have information that would relevant to both - for example I would want to extract information from a page as well as find the next page to scrape. Scrapy offers a nice solution by letting you return different types from the same method. Returing a Request will lead to another page being crawled but one can also returned the scraped structured data via an Item. In the case of the scraper, I return the fantasy football stats on each page via Items but also return a Request when I want to navigate to the next page of stats. {% highlight python %} def parse_stats(self, response):     hxs = HtmlXPathSelector(response)      # Parse the next url     next_page = hxs.select('//ul[@class=""pagingnavlist""]/li[contains(@class,""last"")]/a/@href')     next_page_url = 'http://football.fantasysports.yahoo.com' + next_page.extract()[0]     count = int(RE_CNT.findall(next_page_url)[0]) # Don't go past a certain threshold of players     current_week = int(RE_WEEK.findall(next_page_url)[0])      self.log('Next url is at count {} with week {}'.format(count, current_week))      if current_week   self.settings['MAX_STATS_PER_WEEK']:             yield Request(self.base_url.format(self.settings['YAHOO_LEAGUEID'], current_week + 1), callback=self.parse_stats)         else:             yield Request(next_page_url, callback=self.parse_stats){% endhighlight python %}} 	  	  XPath expressions : In the past, I'd use either BeautifulSoup or PyQuery to traverse the DOM but found XPath expressions to be simpler. There’s less code to write and the expressions are easier to understand and have a higher information density. {% highlight python %} stat_rows = hxs.select('//table[@id=""statTable0""]/tbody/tr') xpath_map = {     'name': 'td[contains(@class,""player"")]/div[contains(@class,""ysf-player-name"")]/a/text()',     'position': 'td[contains(@class,""player"")]/div[contains(@class,""ysf-player-detail"")]/ul/li[contains(@class,""ysf-player-team-pos"")]/span/text()',     'opp': 'td[contains(@class,""opp"")]/text()',     'passing_yds': 'td[@class=""stat""][1]/text()',     'passing_tds': 'td[@class=""stat""][2]/text()',     'passing_int': 'td[@class=""stat""][3]/text()',     'rushing_yds': 'td[@class=""stat""][4]/text()',     'rushing_tds': 'td[@class=""stat""][5]/text()',     'receiving_recs': 'td[@class=""stat""][6]/text()',     'receiving_yds': 'td[@class=""stat""][7]/text()',     'receiving_tds': 'td[@class=""stat""][8]/text()',     'return_tds': 'td[@class=""stat""][9]/text()',     'misc_twopt': 'td[@class=""stat""][10]/text()',     'fumbles': 'td[@class=""stat""][11]/text()',     'points': 'td[contains(@class,""pts"")]/text()', }{% endhighlight python %} 	     This also got me thinking about the evolution of my approach to scraping. In 2006, I was into Perl and scraped using the LWP::Simple, WWW::Mechanize and the HTML::TreeBuilder libraries. After I moved on to Python I switched to using urllib and BeautifulSoup. Most recently, I’ve started using the wonderful requests library along with PyQuery. Conceptually, these approaches are the same: first retrieve a web page and then extract the data you want by traversing the DOM. Scrapy does the same thing internally but by removing a ton of the boilerplate, it lets you focus on the key problems in scraping - figuring out what page to scrape next and figuring out how to extract the content. The rest is handled by Scrapy itself - including file storage, retries, throttling, and probably a ton more that I haven’t gotten a chance to explore yet.  This also gives me some time to work on the actual draft algorithm. My goal is to create a strategy that’s using a value based approach combined with my schedule. The idea is that I shouldn’t pick the players that will have the highest point total over the season but the ones that will have more points during my tough matchups. Of course, it’s almost all luck but I’m still looking forward to attempting this approach.",2,3,2013-07-17,4,"scrapy, scraping, yahoo fantasy football stats, fantasy football",769,Scraping Yahoo fantasy football stats with Scrapy
23,0,"As data becomes more important in software, it will pose a challenge for startups that do not have access to this data.","#data,#meta","{% include setup %}  I’m convinced that the future of software lies in data. Data has always been important but now we actually have cheap ways of analyzing it with constant improvements in data extraction and machine learning algorithms. We’re also tethered to our digital devices which are collecting tons of data that’s waiting to be analyzed.    I worry that it’s going to get increasingly more difficult to build a software startup in the future as large companies develop data monopolies. Imagine trying to write language translation software without having access to Google’s data? Or trying to do audio transcription by relying on publicly available data? It’s going to be impossible to compete by relying on publicly available data source while large companies build out their internal data monopolies - especially by using their existing products to  subsidize the cost  of collecting this data. Data also begets more data. By giving us great experiences, we’re willing to provide more and more information that is then used to launch new products which have us surrendering more and more data.    No matter how good an algorithm is it still needs data to be useful and I hope we’re not shooting ourselves in the foot by volunteering our data so easily. I’d love to see companies that collect user-contributed information be required to have it shared with their users so that they can have it used by other services. It’s not going to solve everything but it’s a step in the right direction.    Successful startups have always had to overcome challenges so the data monopoly problem will just be more of the same and should hopefully lead to some new approaches. An example that comes to mind is how  Duolingo  is able to generate revenue by selling document translations that are transformed into language lessons that are then done freely by the community. I’m excited to see new business models that are able to innovate past this data gap.",2,2,2013-07-21,4,"data monopoly, startups, business, entrepreneurship",343,Beware the data monopoly
14,0,"A recent experience with locksmiths got me thinking about regulation, entrenchment and security.",#meta,"{% include setup %} A month ago I needed to duplicate a set of keys. In the past, I’d just go to the cheapest looking hardware store and they’d easily replicate my keys for around $2 each. This time, I tried the same approach but was told that they weren’t authorized to handle the keys I had and directed me to another locksmith. That locksmith told me that they wouldn’t be able to duplicate it without approval from my management company and also charged $18 for a duplicate. Amazingly enough, they were only able to duplicate one of the keys and I had to go to yet another locksmith (and get another approval) to get the last key duplicated.  I’m not sure if they’re trying to increase the security or whether they’re just trying to create an artificial monopoly for the locksmiths but it’s definitely a pain in the ass for the consumer. This is a standard approach for many industries: add needless complexity under the guise of security and then use that to justify higher prices. This is also why additional regulation isn’t always the answer - it leads to complicated, entrenched systems that can’t be easily innovated upon.",0,1,2013-07-22,5,"locksmiths, security, business, startups, regulation",200,Security through monopoly
28,0,The inbox is becoming a critical piece of our workflows and we need to have better standards to add functionality rather than relying on 3rd party apps.,#product,"{% include setup %} There are only a few tabs I consistently keep open all day on my computer - Gmail, Google Calendar, Hacker News, and New Relic. Out of these, Gmail is the most important with my entire day running through it. The value of having a presence in the inbox hasn't been lost on companies and there are a ton of third party apps that make Gmail more useful -  Rapportive ,  YesWare ,  ToutApp , and  Boomerang . Even Google itself has been providing ""Lab features"" to augment the default inbox behavior.  One thing that bothers me is that this additional functionality is only provided via browser extensions. The only time I recall seeing an interactive behavior is when a form is embedded in an email and even then it's at the mercy of the  email client's implementation . I'd love to see a new inbox standard adopted that allowed email messages to be richer and more interactive rather than having to rely on a separate browser extension. Imagine being able to send out surveys that can be completed without leaving an email, emails that are able to show whether a product is available in real time (I know this can be done via server side rendered images but it's a hack), or being able to change the content text based on the time the email is viewed. These are trivial examples but this would open up potential for uses that we can’t even imagine. Of course, we’d have to be even more wary of smarter spam and inbox trickery but the potential value is worth that cost.",5,1,2013-07-27,6,"gmail, inbox, rapportive, yesware, toutapp, boomerang",289,The power inbox
24,0,"Most guides on deploying Django only cover the individual packages. I wanted to share the way I deploy Django using Nginx, Virtualenv, and Supervisor","#python,#code","{% include setup %} After yet another attempt to deploy a  Django  application I decided to document the steps required to get everything up and running. The tutorials I’ve seen tend to focus on individual pieces rather than on the way all these packages work together which always led to me a lot of dead ends and StackOverflow so this will hopefully address some of those issues.  In particular, I want to focus on the configuration rather than the installation of the various packages since that’s covered in the package documentation.  I don't know if this is the best way to deploy Django but it's the approach I've been able to come up with by stumbling around and getting help from the docs, Google, and StackOverflow. If there are better ways out there please let me know.    	  		  Gunicorn - /home/ubuntu/project/scripts/start.sh  		 The nice thing here is that we define the port to serve our application on so we can serve multiple projects on a single server with each one using a different port. Note that the settings approach used here is from  Two Scoops of Django . 		  {% highlight bash %} #!/bin/bash set -e DJANGODIR=/home/ubuntu/project DJANGO_SETTINGS_MODULE=project.settings.prod  LOGFILE=/var/log/gunicorn/guni-project.log LOGDIR=$(dirname $LOGFILE) NUM_WORKERS=3 # user/group to run as USER=ubuntu GROUP=ubuntu cd /home/ubuntu/project source /home/ubuntu/project/venv/bin/activate  export DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE export PYTHONPATH=$DJANGODIR:$PYTHONPATH  test -d $LOGDIR || mkdir -p $LOGDIR exec /home/ubuntu/project/venv/bin/gunicorn_django -w $NUM_WORKERS \   --user=$USER --group=$GROUP --log-level=debug \   --log-file=$LOGFILE -b 0.0.0.0:8001 2>>$LOGFILE{% endhighlight %}      	  Nginx  - /etc/nginx/sites-enabled/project  	 The key parts here are that we're redirecting all www.project.com requests to project.com, serving the static files using Nginx rather than rely on Gunicorn, and passing other requests to the Gunicorn server running on the port defined in the Gunicorn start script above. 	  {% highlight nginx %} server {     # Redirect all www.project.com requests to project.com     listen 80;     server_name www.project.com;     return 301 http://project.com$request_uri; }  server {     listen   80;     server_name project.com;     # no security problem here, since / is alway passed to upstream     root /home/ubuntu/project/;     # serve directly - analogous for static/staticfiles     location /media/ {         # if asset versioning is used         if ($query_string) {             expires max;         }     }     location /admin/media/ {         # this changes depending on your python version         root /home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib;     }     location /static/admin {         autoindex on;         root   /home/ubuntu/project/venv/lib/python2.7/site-packages/django/contrib/admin/;     }     location /static/ {         autoindex on;         alias   /home/ubuntu/project/assets/;     }     location / {     # This section is to redirect all http traffic to https if desired     # if ($http_x_forwarded_proto != 'https') {     #   rewrite ^ https://$host$request_uri? permanent;     # }          client_max_body_size 5M;         client_body_buffer_size 128k;         proxy_pass_header Server;         proxy_set_header Host $http_host;         proxy_redirect off;         proxy_set_header X-Real-IP $remote_addr;         proxy_set_header X-Scheme $scheme;         proxy_connect_timeout 300;         proxy_read_timeout 300;         proxy_pass http://127.0.0.1:8001/;     }     # what to serve if upstream is not available or crashes     error_page 500 502 503 504 /media/50x.html;{% endhighlight %}      	  Supervisord  - /etc/supervisord/gunicorn-project.conf  	 Here we just specify the location of the Gunicorn start script so Supervisor can manage it.  {% highlight ini %} [program:gunicorn-project] directory = /home/ubuntu/project user = ubuntu command = /home/ubuntu/project/scripts/start.sh stdout_logfile = /var/log/gunicorn/project-std.log stderr_logfile = /var/log/gunicorn/project-err.log{% endhighlight %}",2,2,2013-07-30,4,"django, nginx, virtualenv, supervisor",545,"Run Django under Nginx, Virtualenv and Supervisor"
19,0,I wrote up an extremely short and simplified history of manufacturing and some quick predictions about the future.,"#meta,#history","{% include setup %} Working on  Makers Alley , I've spent a fair amount thinking about the evolution of manufacturing and wanted to share an extremely condensed history.  For most of human history, people either made what they needed on their own or traded with a local craftsman. Over time, this led to a specialization in skills and also the rise of the apprenticeship model. Since trade was mostly local, it was difficult to build a large business and most businesses were family run with parents passing down skills to their children.  This practice remained consistent until a couple of hundred years ago when water and steam power starting taking hold. For the first time, work could be done independently of human labor and started the trend of specialized machines replacing specialized people. The increase in machine efficiency and the reduced skill of workers led to drops in the cost of labor and cheaper products.  The next major shift occurred when electricity became prevalent. This allowed factories to be built anywhere power was available and the locations were now chosen based on the price of labor and the cost of shopping. Thus, many factories ended up being built near cities with harbors and railroads.  At this point, globalization was still in its infancy since the transportation costs were extremely high due to lack of automation and standardization. Only when containerized shipping took off in the second half of the 20th century did shipping costs plunge and allowed companies to move their factories to locations with even lower labor costs. On a side note, read Marc Levinson's  The Box  to understand the impact of the shipping container.  This is the current situation with the majority of manufacturing being done abroad using materials that are sourced from across the world and then shipped and sold worldwide as final products. It’s impossible to predict what will happen over the coming decades but the combination of rising labor costs, demand for customizable products, and 3D printing suggest that manufacturing is going to start moving back towards local, agile methods. At first, it will probably be a hybrid approach with the bulk of the components still being mass made but then customized in our homes from 3D printed parts. Over time, as the quality and cost of 3D printing improves, more and more of the components will be customized, printed, and assembled at home. We’ll see the creation of a new profession - a combination of industrial designer, modeler, and tastemaker who’ll need to help us navigate this new manufacturing world. I’m excited.",2,2,2013-08-02,6,"manufacturing, history, 3d printing, factories, production, customization",438,A brief history of manufacturing
25,0,I'm launching the beta of Better 404 - a tool used to help improve 404 pages by offering suggestions to visitors and metrics to owners.,#product,"{% include setup %} I don’t understand why websites try to compete on having the cleverest 404 page. The fact that someone ended up on a 404 page is a sign that something is broken but instead of trying to fix the problem they try to distract their visitors by making them laugh. It’s equivalent to getting to a restaurant and seeing an amazing menu only to discover that it’s closed.  We can’t always control which URLs our visitors will type in or click on but we can control what they see when they get there. Instead of trying to distract them with humor why not offer suggestions for what they may have wanted to see? The majority of 404 visits are the result of typos which could be fixed with a simple spell check and the remainder are due to moved pages which can be solved by notifying the linker or providing a redirect.  I’ve had some free time over the past month and put together the basics of a simple tool to help sites improve their 404 pages. Appropriately, it’s called  Better 404  and I’m currently running a beta period to get feedback and work out any kinks. If you manage a site and are interested in trying this out, let me know and I’ll help you get started.",1,1,2013-08-07,3,"404 pages, better 404, improved 404",225,Introducing Better 404
30,0,I've recently noticed more mainstream sites adopting flat design. I wonder if this is due to the upcoming update of iOS and whether other sites will start adopting it.,#design,"{% include setup %} I don’t know whether it’s due to the upcoming version of iOS or Windows 8 but it feels as if flat design is getting more and more common. In the past couple of weeks, I’ve noticed two “mainstream” sites,  Thesaurus.com  and  Optimum , adopt a flat design which I suspect is the first design change they’ve made in years. Many companies are updating their iOS apps in time for the fall release and I understand the motivation to want to fit the style but it’s interesting to see websites doing the same. I wonder whether we’ll see more sites adopting this flat design in the next couple of months.",2,1,2013-08-11,2,"design, flat ui",163,Rise of flat design
32,0,The cynic in me says that Netflix waited this long to bring profiles back in order to get more people to stop sharing accounts in order to make their history private.,"#product,#pricing","{% include setup %}               Netflix  recently reintroduced  profiles  so now each household member can get their own recommendations, recently watched items, and instant queue rather than being forced to share the same polluted profile. This is an awesome win for Netflix customers but it’s been bugging me that they didn’t do this sooner; it’s such an obvious feature that it should have been built as soon as Netflix realized that multiple family members would be sharing their account.    The cynic in me says they waited 5 years to bring profiles back in order to force family members to sign up for additional Netflix accounts if they wanted to keep their account history private and the only reason they enabled this functionality now is that they’ve either already captured the bulk of these additional accounts or that customers have been asking for this for so long that ignoring the requests makes Netflix look callous. Even now, the profile functionality remains open with all users of a Netflix account being able to access any of the profiles, forcing household members who value their privacy to sign up for additional accounts.  I may be completely wrong and Netflix is correct that making profiles more complicated will cause significantly more usability problems for the majority of users while only benefiting a tiny percentage of customers that care about private profiles. Simplicity is critical for the success of a consumer product but I’m skeptical when user experience and simplicity are used as excuses to avoid a simple feature change. In this case it could be as simple as adopting a model similar to  parental controls  which are set via the web interface for each profile.",3,2,2013-08-14,2,"netflix, business",316,"Netflix profiles, why now?"
16,0,I've been volunteering teaching AP Computer Science and wanted to share some thoughts and experiences.,#meta,"{% include setup %} This year, I started volunteering at a program called  TEALS . The long term goal is to improve computer science education in the United States by having tech professionals volunteer their time to teach computer science classes in schools that want to offer computer science classes but don’t have the necessary teachers. Over time, the goal is to have the in-service teachers in each class learn the material so that they will be able to teach it in the future. Currently, the program exists in 65 high schools across 12 states and offers both Intro to Computer Science and AP Computer Science but I’m looking forward to seeing it expand nationwide and into middle and elementary schools.  Throughout the year, I plan on documenting my experiences remote teaching at a high school in Kentucky as well as sharing the lessons I’ve learned. So far, it’s been a little more than a week and I already developed a much bigger appreciation for teachers and the effort required. The most time consuming piece so far has been preparing daily lessons that balance the requirements of the AP test and everyone’s skills and interests while still being engaging enough when delivered via a video chat. I also discovered the difference between lecturing and teaching: my initial approach was to just go through the prepared slides but am now spending a lot more time thinking about tricky concepts and the exercises that will both get the point across and keep students engaged. The best approach so far has been starting a class with a brief explanation of the concepts, going through some tricky exercises, and then diving into code to put it all together.  If you have computer science experience this is a awesome program to volunteer for so please feel free to reach out to me if you have any questions.",1,1,2013-08-18,4,"TEALS, AP Computer Science, teaching, high school",316,On Teaching AP Computer Science
33,0,One of the more annoying things to do is split an AWS account into multiple ones as your needs change. This is a quick description of what I had to go through.,#aws,"{% include setup %} When we launched  Pressi , I had it set up under my personal AWS account. Recently, we needed to move it into a separate AWS account and I wanted to share the steps to help others running into the same issue. Unsurprisingly, most of the effort went into planning and figuring out the migration steps and order in which they should be done. We weren’t able to eliminate downtime entirely but we reduced it as much as we could.  The services migrated included Route 53, an EC2 instance, ELB, S3, and Cloudfront. At the high level, we copy every service we can (EC2, ELB, S3, Route 53) to the destination account before redirecting client traffic to the new account. After that, we migrate the remaining services (Cloudfront) and make updates to existing ones (Route 53, EC2) to point to the destination account.  Migrating EC2 and ELB:    	 Create the destination AWS account  	 Create an AMI of the instance on the original account  	 Share this newly created AMI with the destination AWS account  	 Launch the AMI in the destination account  	 Set up the load balancer in the destination account to mirror the original     Migrating S3/Cloudfront:    	 Create an S3 bucket in the destination account and copy the files over from the original bucket to the destination bucket. We used  Bucket Explorer  for this piece but needed to change the file permissions in the destination bucket manually to mirror those in the original account. One thing to watch out for is that S3 bucket names need to be unique so your code will need to be updated to reference the new name.  	 Update the Cloudfront record in Route 53 to point to the destination account. Note that after the migration runs you can also update the Cloudfront record in the original account to point to the Cloudfront CNAME of the destination account.  	 Cloudfront requires unique CNAME records so we give it a temporary name until you kick off the migration. As soon as you do, you will need to remove the CNAME record from the original account and add it to the destination Cloudfront account.     Migrating Route 53:    	 Copy the records from the original account to the destination account.  	 Make sure to update the Start of Authority (SOA) and Name Server (NS) records in the original account to have the same values as the ones in the destination account to speed up the DNS propagation.     Migrating the code:    	 This will entirely depend on the application but the goal is to update your code to reference the services on the destination account.  	 Due to the non-immediate nature of DNS propagation, you will most likely need to run two code bases - one on the original account pointing to some of the original services and one on the new account pointing to the destination services. Depending on the statelessness of your code, this may lead to a variety of sync issues and will require some intricate code to handle properly. In our case, we had MySQL running on the EC2 instance so while the app was running simultaneously under two AWS accounts the database would get out of sync with some users hitting the original setup and others hitting the destination. Luckily for us only a few tables were affected and we had to run a few manual SQL queries to deal with the issue but it could have been a lot worse.     The last step is to update your domain registrar NS records to point to the destination account and wait for the migration to occur. Note that the migration will happen gradually so you should look at the server logs on both accounts to make sure there’s no traffic hitting the server in the original account.  The lesson here is that migration becomes a whole lot easier if you keep your architecture as stateless and modular as possible. This way the services are loosely coupled and can be migrated one at a time rather than having to do everything at once. Your app also becomes significantly easier to scale since additional EC2 instances can be provisioned without having to worry about them getting out of sync. The non-instantaneous nature of DNS complicates the migration but a stateless architecture helps address most of the issues. Our migration didn’t go 100% smoothly but having mostly stateless services definitely helped us avoid major problems.",2,1,2013-08-24,7,"AWS, EC2, ELB, Cloudfront, S3, splitting AWS, migrating AWS",760,Splitting an AWS account
21,0,With a little bit of JavaScript knowledge you can cleanly extract information from a web page while avoiding formatting issues.,"#python,#javascript,#code","{% include setup %} How many times have you tried copying something from a webpage into Excel and discovering that the formatting got completely messed up and forced you to clean the data up manually? With just a bit of knowledge about HTML and CSS you can use JavaScript to get the information you want without having to struggle with the formatting issues.  In my case, I participated in a fantasy football draft and wanted to share the list of players I drafted with a friend. Unfortunately, copying and pasting didn’t work so I decided to jump into JavaScript. Hope these steps give a sense of how to approach a simple scraping problem. The idea is to use the browser’s inspect element feature to find the pattern that the element we’re interested in have in common. Then, we use JavaScript to find the elements matching that pattern and extract the information we want.                   1. The page we want to parse - please ignore the quality of my fantasy team.                                    	 2. Use the Chrome ""Inspect Element"" feature to figure out the HTML/CSS of the element we're interested in. In this case, the element containing player name has the class value “name playernote”.                                    	 3. Run a JavaScript command to get all the HTML elements that have those classes.       	{% highlight javascript %}document.getElementsByClassName('name playernote'){% endhighlight %}                                           	 4. Store those HTML elements in a variable so we can quickly iterate through the list. 		{% highlight javascript %}players = document.getElementsByClassName('name playernote'){% endhighlight %}                                           	 5. Use JavaScript to go through the previous list and extract the player name. Then we can just copy and paste the list of names without having to deal with the formatting issues.       	{% highlight javascript %}for (var i = 0; i                         In addition to extracting information, JavaScript can be used to interact with a web page. This comes in handy when you want to automate a certain action on a site that would take too long to do manually. For example, I was able to code up some quick JavaScript that would go through a list of my Facebook friends and invite them to like my startup’s new page. Hope this little JavaScript hack comes in handy and let me know if you have any questions.",0,3,2013-08-26,5,"html, css, javascript, crawling, scraping",529,Extract info from a web page using JavaScript
30,0,Something I've been wondering about is the tradeoff between simplicity and power in product design. Is it possible to achieve both? What are some examples where it's been achieved?,#design,"{% include setup %} Although I come from a backend background, I’ve been spending more and more time on the UX side of things and have been picking up quite a bit - a combination of using Twitter Bootstrap on my projects, subscribing to the Hack Design lectures, and following a ton of designers on Twitter.  Something that’s been bothering me is this obsession with trying to make every product as intuitive and approachable as possible. That’s the right approach when focusing on mass market consumer products but if you’re building internal tools or targeting power users a simple, approachable product might be antithetical to what you actually need.  The tradeoff is between a product that people can immediately start using versus a product that takes time to learn but becomes significantly more powerful when mastered. The developer equivalent would be using a basic text editor vs vi or emacs. The text editor is easy to start using but you hit a productivity ceiling quickly; vi or emacs, on the other hand, take a while to learn but you become significantly more productive than if you were using a text editor.  The challenge is knowing your audience and building the product that will solve their problems. Sometimes it will need to be simple and other times it will need to be complex. This applies at multiple levels - the product may for the most part be simple but certain features will need to be complex in order to be useful.  Many websites and apps have adopted the approach of where it’s extremely easy to get started but provide advanced features for the users that desire and discover them. Excel provides shortcuts for the power user that make it possible to do anything without touching the mouse. Gmail, in addition to shortcuts, provides a “labs” feature that lets users enable more advanced features.  I’m interested in what happens as companies grow and try to increase their market. Some may have started with a complex product that solved a niche problem that they want to simplify in order to appeal to a bigger audience. Others may have started with a simple product that they now want to position to power users. In both cases the challenge is being able to support both use cases without negatively impacting either one. Maybe the right approach is to launch the new product under a different name but I’m curious to see creative solutions that aren’t about adding shortcuts or a settings page.",0,1,2013-08-28,4,"design, ux, startups, product management",416,Simplicity vs power in product design
22,0,We should learn from others but realize that almost all advice is black and white whereas startups are shades of gray.,"#product,#meta","{% include setup %} When I was making the leap into the startup world I read every post I came across that talked about people’s experiences and guides in running a startup. The goal was to learn as much as I could form others and apply these hard-fought lessons my own startup. Now that I’ve been working on a startup for almost two years I realize how much startups differ from one another and how black and white these guides tend to be. You can read two posts that will promote contradictory approaches. Should you focus on revenue or growth? Should you raise money or bootstrap? Should you go with a freemium model or paid only? Should you go solo or get a cofounder? Should you focus on consumers or the enterprise?  None of these questions have a universally right answer. What worked for one startup will not necessarily work for another one. There are just too many differences; the product, market, teams are all different. Time plays a huge factor as well. In a field as quickly moving, and novelty loving, as technology what worked 6 months ago may not have a chance right now. Startups are tough. If it were as simple as just following a how-to guide the success rate of startups would be an order of magnitude higher than what it actually is.  The best we can do is be aware of the available options and try to understand why certain strategies worked for others. We shouldn’t ignore what we read but we also shouldn’t emulate an approach just because others succeeded with it. We need to be the experts of our markets and imitating others only undermines that knowledge.",0,2,2013-09-02,2,"startups, entrepreneurship",287,Startups aren't black and white
25,0,If you have a site that uses infinite scroll make sure you don't have anything clickable in the footer. It's a terrible design pattern.,#design,{% include setup %}          	    		        		         	               I’m not sure why this needs to be said but if your site offers infinite scroll make sure you don’t have anything clickable in the footer. I’d expect the occasional site to succumb to this but I was surprised to see it happening on LinkedIn. All I wanted to do was read the developer docs but unfortunately the link is located in the footer which provides a nice challenge of clicking the link before new content is loaded. I wasn’t quite able to get it and ended up just searching Google for the LinkedIn documentation link. If your site’s content is only accessible via a Google search you have a problem.,1,1,2013-09-03,5,"design, anti-pattern, footer, infinite scroll, linkedin",150,Design anti pattern: footer under infinite scroll
29,0,"In order to get more sales and increase market size, companies need to focus on educating their customers. They'll be able to decrease acquisition costs and increase retention.",#sales,"{% include setup %} A trend I’ve been seeing lately is companies boosting their sales by focusing on customer education. The successful companies don’t just focus on the results their product will deliver but also spend time explaining why those results are important and how the product works and how it can be used.  This approach seems obvious to me. Borrowing some terminology from  Crossing the Chasm , the early adopters will use your product as long as it solves an existing problem but education will help the remaining, slower adopting customer segments discover that they even have a problem and look to you for a solution. In my opinion, the major benefits of customer education are to reduce acquisition costs and improve retention. Acquisition costs will drop as you start relying more on inbound interest rather than on outbound sales. Retention will increase since customers that sign up willingly will stick around longer than customers who needed to be coaxed into it by a sales rep. These “self serve” customers will also be more likely to blame themselves when encountering problems rather than whoever got them to sign up. In addition, by developing original and useful content you’ll help your SEO score which will drive more potential customers to look at your products. Your trustworthiness will also improve since you’ll be offering free and useful knowledge.  In many ways, freemium and free trial products are pursuing this strategy by allowing their products to be used with the hope that these users will turn into customers after they discover that they’re able to solve real problems using the product. A sales rep can then work with the customer to understand his needs and pick the right product offering to solve his problem.  I believe we’ll be seeing much more of these self serve, education products in the coming years and it’s important to get involved in this shift now. In my mind, the two companies that have been doing a great job with it are  New Relic  which is providing a great way to try their monitoring service and Mixpanel, which has a  dedicated portal  focusing on analytics education.",3,1,2013-09-06,4,"product, marketing, sales, startups",369,Want more sales? Start teaching
16,0,Just a quick review of my visit to the Museum of Math in New York.,#meta,"{% include setup %}            Although I’ve been meaning to visit the  Museum of Math  ever since it opened in December, I only got the chance to do it this Labor Day. I wanted to share my thoughts and encourage everyone who can to visit.  I love the mission. Math should not be taught in a vacuum and having various activities that each showcase different mathematical properties is a great approach to get kids (and adults) engaged while learning some math. Some of the activities that stood out to me were bikes with differently sized square wheels that can only go around a certain diameter track; a ""helix"" shape that explains multiplication by lighting up a fiber between the numbers and highlighting the resulting value; and a fractal tree generator that would use your body to create the trunk and branches. I enjoyed these since they had an interactive physical component that provided immediate feedback.  There were also a bunch of activities that were primarily software based. Two examples are a ""kaleidoscope"" drawing tool and an app that explores 3D shapes and functions by letting you tweak the parameters. These weren't very engaging and most were abandoned quickly. In addition, some of the tools either had broken sensors or were buggy which made them less fun than they should have been.  A museum should not be able to replaced with iPad apps and MoMath places too much emphasis on the software. They should move away from these apps and focus on the physical exploratory activities that cannot be recreated at home. I found that there was very little continuity between the exhibits and wish they did a better job curating so that lessons from one could be applied to another. This would limit the breadth but would make the experience more valuable. Similar to an art museum, they could have a base collection that never changes as well as exhibits that rotate every couple of months.  The museum has been open for less than a year and I’m optimistic it will only improve. I only wish there were more math museums opening up.",1,1,2013-09-11,4,"momath, museum of math, museum, math",372,MoMath visit
27,0,It's so much easier to offer advice to others than to take it yourself. This is just a post of how I'm trying to overcome it.,#meta,"{% include setup %} Since becoming active in the startup scene, I've been meeting a ton of founders and am annoyed by how much easier it is to offer suggestions than to apply them to myself. My most common suggestion, in true lean startup fashion, is to advocate a quicker or cheaper way to validate the market before building a product and yet it’s extremely difficult to take my own advice. I’ve been working on  Better404  on and off for two months now and know I should get it in front of potential customers and yet I keep on making minor tweaks and updates to the product.  I'm not sure if this is due to how much easier it is to say something versus executing it or just a fear of failure but it’s something I’m becoming more and more aware of. At least now I’m aware of this bias and and can work on correcting it. What’s been working well so far is coming up with a framework that can be used to evaluate a startup in a similar space and then using it to evaluate my own. The challenge is coming up with the framework without biasing it knowing that you will be using it to evaluate your own project. Nonetheless, this additional indirect step does help.  I’ve been looking at other SaaS businesses and listing what they could be doing better and what they’re doing well as well as what I’d do if I were in their situation. Having a concrete target for this sort of analysis is a great way to come up with a large number of specific suggestions and although many of these aren’t relevant a few can be applied to what I’m working on.  To succeed, it’s critical to be able to look at your startup from an unbiased, external perspective. This is even more true as a solo founder since there’s no one else bringing their own ideas and experiences. I’d love to know how others are overcoming this.",1,1,2013-09-16,3,"suggestions, advice, startups",341,Offering suggestions
19,0,I wanted to share an example from Pressi where we applied a quantitative approach to solve a problem.,"#meta,#datascience","{% include setup %} The tech world is conflicted about how much math a developer needs. Engineers working on quantitative systems or data science clearly require advanced math and there are also countless engineering roles where math is unnecessary. My experience is that even if you don’t use math, having a mathematical mindset makes you significantly more productive. You’re able to quickly estimate the complexity of various tasks and hone your intuition. You’re also able to quickly recognize patterns when refactoring, especially when working in a functional language. A basic understanding of probability and statistics is a great way to analyze the performance of your code as well as help you model and understand your application behavior. I wanted to share a quick story of how a mathematical approach came in handy when working on Pressi.  First, a little bit of background.  Pressi  is a social media mashup page that takes the content a user posted across a variety of social media networks and creates a “Flipboard” style web page to showcase it. At launch, we had a simple cron job that would run every hour and pull new data for each of our users. Over time, we migrated to a task system that let us run these retrieval tasks in parallel and split across multiple machines. Using this approach, we were able to scale well and handle the increased volume but our hosting costs saw a big jump so we went looking for a solution.  Luckily for us, we tracked the history of each social network data pull (containing user, network, datetime, and # of items pulled) and doing a quick query told us that close to 92% of our requests resulted in no data being retrieved. The intuition behind this is that most people will not be posting on every social network every hour. By eliminating these calls we’d be able to drastically cut our hosting costs.  This analysis got us thinking about the ideal case which is for us to pull a moment immediately after it’s posted. One way to achieve it was to leverage the push updates that some of the social networks supported but we wanted to find a more general way that could tell us when we should pull the data for a particular user/network pair.  To figure this out, we looked at another distribution: the average number of moments shared by a user on a network per day. This let us look at the number of users who were extremely active on social media down to the users that pretty much only had accounts. We then dumped this data into Excel in order to come up with ranges that we’d use to segment our user/network pairs in order to see how often we should attempt to pull their data. For example, a user that on average posted 20 updates a day on Facebook would have their Facebook data pulled every 4 hours but a user who posted on Instagram less than once a day would have their data pulled once a day. This also gave us a way to estimate how many fewer calls we'd need to make compared to what we were currently doing and therefore approximate the cost savings. The result of this update was that we dropped the number of useless requests from ~92% to just over 40%. This was by no means perfect but gave us improvement we needed. An additional update we modeled out but but didn't get a chance to implement was to look at day of week and hourly patterns in order to identify when users were actually posting rather than treat every day and hour the same way. The data clearly showed that users had well defined schedules which would have led to another nice improvement.  The key lesson here was that we started by leveraging the data we collected to identify the major cause of our cost increase and then identified the metric we wanted to optimize. In our case it was to reduce the volume of empty requests we were making while making sure that we did not significantly increase the average number of moments that were retrieved for non empty calls. Otherwise, we could make 1 request a week for each user/network which would pretty much drop the number of useless requests to zero but blow up the number of average moments retrieved per call. We could have chosen a variety of other metrics but went with this one since it was intuitive, easy to model, and easy to test. The other neat property is that it’s self correcting so if a user changes their behavior on a particular network we’d shift them into another bucket.  None of the math we used was very complicated and although we tried playing around with a few statistical distributions to model out the user posting behavior we ended up quickly abandoning those when we saw the impact we’d get from a simple approach. I’d bet that almost every code base has something that can be improved with a little bit of mathematical analysis.",1,2,2013-09-17,4,"math, programming, engineering, quantitative",852,Programming and math
22,0,Excel has a bad reputation but it's great at doing quick analysis and should be a part of everyone's tool set.,#datascience,"{% include setup %}            Excel has developed a reputation of being bloated, slow, error prone and used primarily by ""business people"" who don't have real quantitative skills. Just like anything else, Excel is a tool that can be misused but is significantly more useful than people give it credit for.  The most important benefit Excel provides is making data approachable and fun. By making it approachable Excel opens up data analysis to a ton of new people that come into it with their own experience and knowledge. Sure they may not have data scientist skills but they're still able to run some neat analyses and derive useful insights.  The fun makes it very easy to experiment and try a lot of different ideas by making the cost of failure so low by providing quick feedback and visuals. The value of writing a formula and then dragging it down, quickly seeing the calculations is massive. This gives the quick feedback that encourages people to keep on driving their analysis. And although Excel's visualizations are simple, they provide a fast way to visualize the data and hopefully lead to more analysis. Similar to the way we use Python for a quick project instead of Java, it's much easier to run a quick analysis in Excel than in a ""real"" language such as R.  My typical approach to quantitative problem is to write a query to retrieve the data I want and then immediately dump it into Excel for a quick analysis. This lets me apply some pretty basic formulae and visualizations to to see if there's anything worth pursuing in more depth. Only then will I move to R or Python to do a deeper analysis. Even then, I most likely rewrite the code to make it ready for production. This approach forces me to focus on the data and dimensions I want to analyze. Excel only serves as a way to quickly explore the data before deciding whether there’s anything worth pursuing.  The only tool I can think of that comes close is  Tableau  but my experience has been that it has a somewhat steeper learning curve and doesn’t support the flexibility to quickly add and adjust various calculations. Replacing Excel is tough. I use Google Docs for working with documents and yet for my data I use Excel rather than Google Spreadsheets.",1,1,2013-09-20,4,"excel, data science, quantitative engineering, tableau",415,In defense of Excel
26,0,Sharing a pricing strategy I've been using for smaller consulting projects where I will steeply discount the rate if I go over my project estimate.,"#product,#pricing","{% include setup %} I've been doing some consulting work over the few months and wanted to share a pricing model that’s been working well for smaller projects. I’ll sit down with the client to understand the scope of the project and work with them to break it down into smaller, more manageable components. Based on this break down, I’ll estimate the time required for each piece and come up with an estimated total time. I charge my usual hourly rate for the work that falls within the estimated time but will charge a steeply discounted rate for every hour that goes over.  This aligns incentives since it gives me an incentive to complete the work within the estimated time and helps the client feel more comfortable that the project won’t run past its estimate for the sake of me working more hours. The other major benefit is that it forces us to scope out the project together so we’re both on the same page and avoid any future surprises.  I don’t think this approach would work well for larger projects since those are significantly more difficult to estimate but any feature can be broken down this way. For larger projects, I think trying to bill weekly as  advocated by Patrick  is the right approach if you’re able to agree with the client on it. In my case, I have multiple projects going on simultaneously so it's been difficult getting a weekly rate worked out.",1,2,2013-09-28,3,"consulting, pricing, contracting",250,Pricing small consulting projects
25,0,I've been noticing various ecommerce sites offering discounts via popup just to enter an email. This is a compilation of the ones I've seen.,#product,{% include setup %}   	  		  A couple of months ago I started noticing popups on various ecommerce sites offering a first purchase discounts in exchange for entering an email address. Every time I noticed this happening I took a screenshot to track the offer and compile a list of the retailers using this approach. I’m still collecting examples and would more of them but so far the going rate seems to be anywhere from 10 to 25% off the first order. The pitch is pretty compelling and I think most people would gladly give up their email for the possibility of a discount. I’d also love to know what impact the magnitude of the discount has on the sign up rates; I suspect it’s minimal but definitely better than gaining entry to a sweepstakes or a lottery. 		  	  	      Company   Offer      Ann Taylor  Lottery    Blue Nile  Lottery    Bonobos  20%    Gap  25%    Wayfair  10%    West Elm  10%    Williams-Sonoma  10%     	             	                  Ann Taylor offers the chance to win a vacation                  	                  Blue Nile offers the chance to win a diamond                  	                  Bonobos offers 20% off your first purchase                  	                  Gap offers 25% off your first purchase                  	                  Wayfair offers 10% off your first purchase                  	                  West Elm offers 10% off your first purchase                  	                  Williams-Sonoma offers 10% off your first purchase,0,1,2013-09-30,5,"ecommerce, marketing, discount, rebates, coupons",344,Going rate for an email address
20,0,The iPhone 5C is more expensive than people expected to avoid cannibalizing iPhone 5S and serves as a 'Decoy effect',"#product,#pricing","{% include setup %}  Many people  expected  the iPhone 5C to be priced low in order to compete with the cheaper Android phones in countries without carrier subsidies. The news that the 5C’s starting price was $549 left many in the tech community  surprised and concerned  with many believing that the price needed to be lower than $400 in order to compete worldwide.    I'm definitely speculating but I believe the reason for such a high price for the 5C was to avoid cannibalizing the sales of the 5S while also framing the comparison to be iPhone vs iPhone instead of iPhone vs Android. When people go smartphone shopping they see that the 5S is “only” $100 more than the 5C and pay the difference for the more premium product. If the 5C were significantly cheaper people would be comparing it to a similarly priced Android phone which may encourage them to go with the Android or they'd compare it against the 5S which would get many to purchase the much cheaper 5C instead.     Localytics  put together a  great chart  showing the share difference between the 5S and the 5C which supports this view. The 5S is outselling the 5C in every country by a large margin. And although Apple could get higher sales volume with a cheaper 5C, I believe that the cannibalization of the 5S and the reduction in margins would have made Apple less profitable overall. At the same time., charging a lower price for the 5C to get more market share at the cost of profit may have been the right long-term decision.     	  		   	      Pricing products that are sold worldwide is a tough problem since every country has its own economic environment with unique shopping behaviors. This challenge is magnified for products that can easily be bought in one country and resold in another. The opportunities also vary significantly: the US is a mature smartphone market where carrier subsidies exist and make the buyer less price sensitive while in China many people are getting smartphones for the first time and lack carrier subsidies that will help reduce the initial sale price. We’ll just have to wait to see whether this was the right pricing decision.              Note: This reminded me a pricing table the Economist had a while back that looked like an error. There were three options: 1) a web-only subscription for $59, 2) a print subscription for $125, and 3) a print and web subscription for $125. In this case, the last two options were the same price but option 3 offered more value so it was clearly better than option 2. Yet by having option 2, the Economist got people to compare option 3 against option 2 rather than option 3 against option 1. This phenomenon is called the “ Decoy effect ” and worth understanding, especially if you’re a marketer.",6,2,2013-10-01,6,"apple, iphone, 5S, 5C, pricing, marketing",572,Why's the iPhone 5C so expensive?
34,0,While blogging I had to deal with copying an Excel table into HTML and generating a BCG style growth-share matrix. Here are some tools I came up with that make it easier.,"#javascript,#code","{% include setup %} Over the course of this year, I’ve been writing two posts a week and been running into various formatting/design issues, two of which I finally dealt with earlier this week. One was embedding an Excel table into a blog post and the other was creating a BCG style “growth-share” matrix.  To convert a table from Excel to HTML I would write Excel formulae that would wrap each cell in a &lt;td&gt; tag and then wrap each row in a &lt;tr&gt;tag. I’d then copy and paste the result into the text editor to add the header row and finish up the styling. To generate a growth-share matrix, I’d just use Google Drawing or Keynote to draw the axes and labels before taking a screenshot and cropping it into a square.  The solution to these was a bit of JavaScript with some help from  StackOverflow . These tools are hosted on  GitHub  and accessible via  http://dangoldin.com/js-tools/  and are under the MIT License. As I run into more of these I'll keep on adding various tools to this list. If you have any suggestions or want to add your own let me know.",3,2,2013-10-05,4,"JavaScript, growth-share matrix, bcg matrix, html table",213,Some JavaScript Tools
21,0,I used to have a ton of cynicism when I was younger but am now feeling a lot more idealistic.,#meta,"{% include setup %} As I've gotten older and most likely more mature, I've become far less cynical. I used to be dismissive of people trying to improve things and believed that they were just wasting their time and nothing would change. Yet as a I've gotten older I've come to appreciate this effort even if it doesn't lead to noticeable progress.  The fact that someone is working for their beliefs should be applauded. The waste is dismissing others’ work while sitting in front of a computer or a TV. We all want to see progress and yet we exert effort belittling others that are actually committed to making things better. If we applied this effort into our own passions we’d be all be much better off.  Teddy Roosevelt said this better than I ever could:   It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.",0,1,2013-10-12,1,,277,Decline of cynicism
16,0,Just a review of where I am with my goal of running 1000 miles during 2013,#running,"{% include setup %}      I'm not entirely sure why, but I started off 2013 with the goal of running 1000 miles which breaks down into a little more than 19 miles a week. Remarkably, I stuck with it and am somehow at 822 miles for the year and need to average 16/week for the remainder of 2013 to hit the goal. Yet it took me a surprisingly long time to work up to a weekly distance of 19 miles and even longer to consistently run more than 19 miles a week. My first run was less than 1.5 miles and I only started consistently eat into my deficit in June. It took me until the end of August to actually reach the 19 mile cumulative weekly average I need to maintain until the end of the year. It’s been an awesome adventure and I’ve even managed to run three half marathons and improved my time from 1:58:11 on an easy course to 1:56:11 on a challenging one. Here’s a table I put together showcasing my running progress over the course of the year. I’ll update it at the end of the year after hopefully achieving the 1000 mile goal.      Week #  Start of Week  Cum Dist Needed  Dist Ran  Cum Dist Ran  Cum Run Avg      1  1/1/2013  19.23  5.43  5.43  5.43    2  1/8/2013  38.46  11.94  17.37  8.69    3  1/15/2013  57.69  13.65  31.02  10.34    4  1/22/2013  76.92  11.88  42.90  10.73    5  1/29/2013  96.15  12.64  55.54  11.11    6  2/5/2013  115.38  19.16  74.70  12.45    7  2/12/2013  134.62  15.64  90.34  12.91    8  2/19/2013  153.85  0.00  90.34  11.29    9  2/26/2013  173.08  25.24  115.58  12.84    10  3/5/2013  192.31  24.27  139.85  13.99    11  3/12/2013  211.54  3.31  143.16  13.01    12  3/19/2013  230.77  7.09  150.25  12.52    13  3/26/2013  250.00  12.60  162.85  12.53    14  4/2/2013  269.23  7.89  170.74  12.20    15  4/9/2013  288.46  18.54  189.28  12.62    16  4/16/2013  307.69  12.80  202.08  12.63    17  4/23/2013  326.92  11.62  213.70  12.57    18  4/30/2013  346.15  21.60  235.30  13.07    19  5/7/2013  365.38  26.67  261.97  13.79    20  5/14/2013  384.62  25.10  287.07  14.35    21  5/21/2013  403.85  12.03  299.10  14.24    22  5/28/2013  423.08  13.73  312.83  14.22    23  6/4/2013  442.31  26.14  338.97  14.74    24  6/11/2013  461.54  23.71  362.68  15.11    25  6/18/2013  480.77  32.66  395.34  15.81    26  6/25/2013  500.00  19.94  415.28  15.97    27  7/2/2013  519.23  23.51  438.79  16.25    28  7/9/2013  538.46  40.54  479.33  17.12    29  7/16/2013  557.69  35.68  515.01  17.76    30  7/23/2013  576.92  29.05  544.06  18.14    31  7/30/2013  596.15  37.82  581.88  18.77    32  8/6/2013  615.38  11.37  593.25  18.54    33  8/13/2013  634.62  33.23  626.48  18.98    34  8/20/2013  653.85  16.83  643.31  18.92    35  8/27/2013  673.08  34.41  677.72  19.36    36  9/3/2013  692.31  27.70  705.42  19.60    37  9/10/2013  711.54  34.76  740.18  20.00    38  9/17/2013  730.77  9.19  749.37  19.72    39  9/24/2013  750.00  32.75  782.12  20.05    40  10/1/2013  769.23  23.66  805.78  20.14    41  10/8/2013  788.46  16.40  822.18  20.05    42  10/15/2013  807.69  0.00  822.18  19.58    43  10/22/2013  826.92  0.00  822.18  19.12    44  10/29/2013  846.15  0.00  822.18  18.69    45  11/5/2013  865.38  0.00  822.18  18.27    46  11/12/2013  884.62  0.00  822.18  17.87    47  11/19/2013  903.85  0.00  822.18  17.49    48  11/26/2013  923.08  0.00  822.18  17.13    49  12/3/2013  942.31  0.00  822.18  16.78    50  12/10/2013  961.54  0.00  822.18  16.44    51  12/17/2013  980.77  0.00  822.18  16.12    52  12/24/2013  1,000.00  0.00  822.18  15.81",0,1,2013-10-13,1,running,435,My running progress
22,0,Instead of talking about a lack of time we should realize it's about our priorities and actually think through our choices.,#meta,"{% include setup %} I'm frustrated by the expression ""I don't have time"". As my friends and I have gotten older, I’ve been hearing it more and more frequently. I’ve even caught myself using when trying to come up with an excuse when coordinating evening or weekend plans.  The reason I dislike the phrase is that it’s equivalent to saying ""it's not a priority"" and yet we phrase it such that we convince ourselves it’s something outside our control rather than due to the choices we make. I could go out until 3 AM if I make that a priority over running 6 miles in the morning before heading to work just like I could go catch a movie  instead of working on a side project. If we expressed our choices in terms of priorities rather than time we’d be more likely to deal with them.  Life is full of constraints and it’s impossible to do everything we want. This will only get worse as we get older and deal with more “grown-up” things. Better to develop the right mindset now rather than realize it later.",0,1,2013-10-14,4,"life, time, time management, philosophy",188,But I don't have time
38,0,There's some weird stuff going on in SEO land. Someone is ripping off my mom's site (along with a ton more) and monetizing them via AdSense. Just trying to do some investigation to see what's going on.,#product,"{% include setup %} My mom owns a small local business in suburban NJ,  The Do Re Mi School , that’s akin to an after-school program where music, dance, art, language and math is taught. Being surrounded by a family of engineers, we’ve been helping her on the tech side and my brother created the web site she’s been using it for the past couple of years. It’s based on Drupal and allows her to make changes without having to dive into the tech details. This approach has been working well she’s recently started using YouTube, Facebook, and Twitter to help with her marketing and social efforts.  Earlier this week, she sent an email saying there was a new website, bestnewjerseyartsschool.com, that was completely ripping off her site. They claim to be Do Re Mi and have copied various parts of the content, including paragraphs of text and various images. They’ve even created a YouTube video, www.youtube.com/watch?v=lvE6tBl8xU4 that's embedded on the site’s home page.  Looking at the whois and dns info doesn’t reveal much since they’re using a privacy protection service. All I know is that the domain was registered in April using WildWestDomains, is protected by DomainsByProxy, and that the name servers are under  hbuse.com  which claims to be ""Hosting by unbelievably sweet elepehants"" and yet doesn’t contain any real content and is not in Google's index. The interesting thing is that looking at the whois info for hbuse.com indicates it was registered by someone with a PO Box in Nevada with an email address that also seems to be anonymyzed. I found a site,  yougetsignal , that allowed me to search for other sites that were all hosted on “hbuse.com” and came up with a list of ~270 sites that all look to be site rip offs. They all seem to follow a pattern of having the location, the service, and possibly an adjective (best being the most common). Below's a screenshot of some of the sites that I found sharing the host.      I don’t know what the motivation behind these. They are all running adsense so I suspect part of it just a way to generate easy ad revenue but the more cynical part of me thinks it’s a way to blackmail the existing sites into buying these fake domains to avoid SEO penalties.  It’s terrible that honest business owners have to deal with these things. They don’t have the background to know what to do and many are not even aware that their brands are being manipulated, damaged, and monetized. There’s also an SEO risk that these rip-off sites will start dominating the search results and hurting business even more. And since these sites are using AdSense, Google is able to generate more revenue.  I’d love to know what I should do next but my current thinking is that I should send a DMCA takedown notice for my mom’s site and report this entire list of rip-off sites to Google and hope that they stop AdSense from running on the sites and remove them from search results.  I worry that as it becomes easier and easier to generate written content using software we’ll see more and more of these scenarios where it’s going to become increasingly difficult to find the source of the original content and real site owners are hurt.  Update: So the way my mom discovered this other site is because she got a notice from Getty images that she was using one of their images without licensing it. When she asked which image they sent her a link to that other site that has her contact info on it. I guess that answers the question as to why that other site still uses the actual contact information.",3,1,2013-10-20,4,"seo, adsense, blackhat, blackmail",667,What the SEO?
26,0,"I rant about a few more ui/ux issues I've run into. Login forms resetting emails, sign in vs register placement, new vs existing contact management.",#design,"{% include setup %} I don’t know why, but I’ve become more aware of the UI/UX of various sites and apps that I encounter. Whereas before I might have gotten frustrated about some behavior, I’m now starting to get annoyed whenever I encounter something that’s obviously crummy. Here’s a few of the more recent design anti-patterns I’ve been noticing.       Submitting a login form with the wrong password removes the entered email address. Especially on mobile, where it both takes longer to type and typos are more common, it’s crappy having to type both my email and password again if I made a simple typo in my password or just don’t know which of my passwords I used. A quick hack I saw that makes this a bit easier is to add a keyboard shortcut to your phone to replace “@@” (or any other character set) with your email address.                                Confusing placement of sign in and register. I forget which app I saw this in but as you can screen from the screenshot I’m on the sign in screen and yet the button under the form is to register, which causes the app to load the registration screen. The sign in button is up top which is a confusing flow since the user goes down the page first before having to go back to the top. The fact that the app uses a flat UI makes this worse since there’s not a lot of differentiation between the sign in and register actions.                                   Create new versus add to existing contact. This is probably the most “first-world” one here but without knowing who is currently in your address book it’s impossible to know whether you want to create a new contact or update an existing and contact. My current approach is to choose add to existing, realize that I actually don’t have that contact in my address book, and then go back a few screens and choose create new. A common database operation is “insert or update” - insert if it doesn’t already exist and update if it does. I’d love to have something like that to manage my address book.",0,1,2013-10-23,4,"design, ios, ui, ux",410,Some more design ranting
18,0,I did a few months of consulting work and wanted to share some of the lessons learned.,#meta,"{% include setup %} To supplement my income while working on a startup, I took on a few consulting projects and wanted to share some lessons learned. It seems that everyone’s consulting experience is different so consider mine experience as just another data point.       I was able to get more work from my existing network than anything else I tried. As soon as I told people I was looking to take on some consulting projects I was able to get interest and referrals. If I didn’t have that I’m not sure how I would have gotten my first few projects.     It took longer than I expected to agree on a project’s scope and get the contract signed. My approach was to do a call or meeting to understand the goals of the project and then break it down into components with an estimated time and cost for each piece. I liked this approach since we were able to discuss the priorities of various pieces and talk about the risks associated with each.     It took longer to get paid than I expected. I was confident that I’d get paid but it took a few emails and meetings to get the payments made. The part that helped was getting an initial deposit before starting the work.     The biggest benefit was the flexibility to choose when and how to work on the projects.  Unfortunately, this flexibility is better in the abstract. I didn’t find the flexibility that valuable since almost everyone I know is working at a full time job which causes me to also follow a pretty standard schedule.     Most of the knowledge I gained was on the business/marketing side rather than on the tech side. I wasn’t doing challenging work and for the most part didn’t get a chance to work closely with others. The projects I did were also pretty independent so I had to resort to Google and Stack Overflow to help me deal with various questions that came up.     The projects I had were not critical to the company and were mostly “nice to haves.” This had the effect of me not feeling very aligned with the company vision which made the projects less interesting than they should have been. I’m not sure if this was due to the way I positioned myself for consulting work or due to the companies not wanting to outsource their critical projects.     A shared GitHub account worked amazingly well. The client was able to track the progress and provide feedback at various stages. This required me to commit well documented, working code but it definitely made communication easier. I also had a staging environment set up for my projects which let the clients see the code in action.     I wrote a  post  last month on pricing smaller consulting projects and wanted to highlight that again. I would come up a time estimate for a project that would be billed at my usual rate. Any work that spilled over would be billed at a discounted rate. This gave clients confidence that my estimate was reasonable and gave them a sense of the total project cost.     This was my first time doing serious consulting work and it’s a mixed bag. I enjoyed the flexibility but didn’t find it being a huge deal. I was also taking on projects that paid the bills but weren’t the most exciting. My biggest gripe was that I felt I wasn’t learning as much as I would have had I been working as part of a team. This gave the illusion that I was falling behind on my skills and not improving as much as others were.If I were to do it again, I’d want to specialize in a particular field and only do projects that fit in with my passions and interests. I’d also want to get it to the state where I’d be working alongside others rather than being entirely independent.",1,1,2013-10-27,3,"consulting, contracting, engineering",673,Lessons from consulting
24,0,Follow up to my previous post around a website stealing content from my mom's site and claiming it as their own. A happy outcome!,#product,"{% include setup %} Last week I  posted  about a site my mom discovered that had copied the content from her site and has been positioning as their business. I had no clue what the motivation behind it was other than thinking it was just a sketchy way to either blackmail the real business owner or use black hat SEO tactics to increase traffic and monetize using AdSense.  After sharing my problem and getting a ton of advice, I sent a DMCA request to the host, Colocation America, and received a surprisingly quick reply. Within a few days I was on the phone with a representative from the site claiming that my mom had signed up for a service that was offering free mobile websites and that’s why my mom’s business information and content had been appearing on the other site. My mom doesn’t recall signing up for any site and I believe her - she’s been sending me nearly every offer she receives asking whether it’s legitimate and worth doing and I don’t recall ever seeing this one.  I suspect they use this as a cover for if they get caught and until then they make some money from their ads. In any case this particular issue was resolved pretty quickly and I wanted to thank everyone who helped. In particular,  Rob Adler  did a ton of research on the infringing site and  Matt Cutts  helped send this to the relevant teams at Google. The web’s not always getting worse!",3,1,2013-10-30,2,"seo, website",260,What the SEO? Followup
17,0,I share a bit about my philosophy on running and how it's developed since I started.,#running,"{% include setup %} In honor of today’s NYC marathon, I finally finished up this post that’s been sitting in my drafts folder the past few weeks.  I’ve never been into running until the beginning of this year when I decided to run 1000 miles. This led to me to three half marathons and is actually making me consider doing a full one. It’s amazing where a habit and a bit of effort can take you. Initially, I ran just to hit my goal and only signed up for races in order to keep myself motivated and on track. Now, it’s become significantly more than that. There are so many things outside my control yet running is solely about my effort and willpower. If I fail it’s my defeat and if I succeed it’s my victory. I can easily skip a run on a cold, rainy morning and yet I know I’m just deceiving myself and I’ll have to make it up later. Running is one of the simplest things to do and that’s a huge appeal. The human bodies have evolved to run and kids start running as soon as they learn to walk. As our world becomes increasingly complex it’s nice being able to escape with a quick run. Whether it’s running or something else, it’s important to have an outlet that rewards us based on our efforts. The further away this activity is from our day jobs the better.",0,1,2013-11-03,1,running,243,Why I run
14,0,Is the MongoHQ hack causing more companies to bring their security in house?,"#meta,#product","{% include setup %} Over the past few days my inbox has been filled with security alert emails caused by the  MongoHQ  database hack. I’m impressed by the number of customers MongoHQ was able to sign up - they spanned the gamut from sites that I don’t even recall signing up for to startups that have been getting significant buzz.  If a database as a service company is able to get hacked it doesn’t leave me optimistic about the way other companies are securing our data. As much as these “as a service” products make our lives easier they bring an increased risk to our business and more importantly our customers. Sure their security will be better than someone who’s setting up a MongoDB instance for the first time but that has to be balanced against the fact that a hosting site offers a much higher reward for a hacking attempt. Access to the infrastructure provides a lot more information than hacking an individual site.  I used to believe that doing security internally was dumb but now I’m not so sure. No one will care about hacking a small site and if it turns out that the site is becoming successful you can dedicate the resources to properly secure it. At the same time, with so many people sharing passwords across multiple accounts it only takes one careless site to undermine the efforts of all the others.  Some of the security alerts I’ve received have mentioned that they plan on managing their database internally rather than relying on a third party; I wonder if this is the start of a trend.",1,2,2013-11-07,4,"MongoHQ, security, hack, infrastructure",275,Security in the wake of MongoHQ
27,0,Excel on a Mac has a completely different set of shortcuts that it makes me wonder if it was intentionablly hobbled to keep people on Windows.,"#datascience,#product","{% include setup %} The longer I’ve been involved in tech the fewer Windows laptops I’ve been seeing. It seems that to even be considered a startup you need to be giving your employees MacBooks. My conversion came years ago when I made the move from Linux in order to be able to run Excel since neither OpenOffice nor Google Spreadsheet were cutting it. Unfortunately, even after years of effort, I still can’t get to the same level of productivity as I had when using Windows during my consulting days. It’s entirely due to the shortcuts. Some of the shortcuts just changed while others simply disappeared.  The difference is most likely due to a different keyboard layout on a Mac but the cynic in me can’t help but think that it’s also a way to keep the Excel power users on Windows. No one in the finance or consulting industry will switch to Macs until the actual workflow of using applications is the same between Mac and Windows. The more power user focused an app is the more difficult it is to convince its users to switch from one OS to another. People already get annoyed when a minor change is introduced by a new version; I can’t imagine the reaction a new workflow would produce.  This is why the web apps are so intriguing, they’re able to maintain their look, feel, and functionality no matter where they’re accessed giving users the ability to choose the hardware that fits them. I’m already replacing Word with Google Docs but it’s going to take quite a bit of effort to get spreadsheet apps to the same level. In the mean time, I’m trying to replace Excel with R. I’m not there yet but improving every day.",0,2,2013-11-09,5,"excel, office, mac, os x, windows",295,Is Excel on a Mac intentionally hobbled?
14,0,Quick way to connect to an RDS instance using R through an SSH tunnel,"#R,#sql,#code,#aws","{% include setup %} In my quest to replace Excel with R I’ve been spending the past week trying to do everything in R. It hasn’t been that easy with many things taking longer due to me having to reference the R docs but one thing that’s been great so far is being able to quickly run a query on Amazon’s RDS and pull data into a data frame for quick analysis. Being able to wrap this into a reusable function makes things even better. The one thing that makes it tricky was not being able to connect to RDS directly but having to tunnel through an EC2 instance. Below are the steps to replicate the setup.  In your shell, run the following command to set up the SSH tunnel: {% highlight bash %} ssh -L  : :  @  {% endhighlight %}  Now in R: {% highlight r %} install.packages(‘RMySQL’) # Install the R MySQL library library(RMySQL) # Load the library m<-dbDriver(""MySQL"") # Load the driver con<-dbConnect(m,user='username',password='pass',host='127.0.0.1',dbname='db',port=3307) # Connect to the local instance res<-dbSendQuery(con, 'select * from table') # Execute the query data <- fetch(res, n = -1) # Load the retrieved data into a data frame dbClearResult(dbListResults(con)[[1]]) # Use this to free the connection {% endhighlight %}",0,4,2013-11-15,4,"R, RDS, MySQL, RMySQL",218,RDS and R
26,0,I came across an old post that I was going to write after reading Twitter's API changes last August. Amazing what becomes obvious in hindsight.,#product,"{% include setup %}      I was going through my drafts and stumbled onto one that was going to criticize Twitter’s API changes that they  announced last August . In light of last week’s IPO I thought I’d finally finish it up.  The blog post described the changes Twitter planned on making with the intent of taking control of the developer ecosystem. The changes included being a lot more strict with their API by limiting the number of users a client could authenticate, reducing the volume of API calls, and requiring all Twitter content to be displayed the same way. The post also included a matrix indicating that Twitter did not want anyone developing on the consumer/engagement side but the rest being open.  At the time, many developers (me included) felt betrayed. We made Twitter successful and now we were limited in what we could do. In hindsight, these moves have been obvious. Since then, Twitter’s been rolling out a ton of changes that wouldn’t have been possible without controlling the entire experience. It also gave an indication of their monetization model. At the time, some thought that Twitter would try to monetize by selling access to the data feed or by offering premium features that large brands could use to control their pages. Turns out it was just advertising. By controlling the way every tweet is seen they’ve been able to roll out sponsored tweets and now, images.  I still think they took the easy way out by choosing an obvious business model and yet I can’t really fault them given last week’s result. It does make me wonder whether advertising is the business model of choice for an IPO.",1,1,2013-11-16,3,"Twitter, business model, IPO",302,A Twitter flashback
18,0,I joined TripleLift's engineering team as a data wrangler and am excited about the space and opportunity.,#meta,"{% include setup %} Just a quick update on my professional life. I recently joined  TripleLift ’s engineering team. I met the founders while at  ERA  and liked the problem they were solving. It was also time for me to move on from my other projects so when I found out they were growing it was a pretty easy decision. Being a startup, it’s hard to pinpoint exactly what everyone’s responsibilities are since everyone becomes a generalist but I’ve been focused on the data side. This entails developing our various data pipelines, leveraging the data we have to improve performance and unlock new opportunities, and doing some light data science to help model and understand the native advertising space.  In 2012, US marketers spent close to $15B on internet display ads and this number’s only growing. The old banner ad model is a huge chunk of it but it’s being supplanted by better, smarter technologies that provide significantly higher engagement rates. Social networks have embraced native advertising that blends into their products rather than being delegated to a sidebar or a banner that’s immediately ignored. Native advertising is a much better approach and I believe it’s going to take up the majority of display budgets within the next couple of years. TripleLift is doing some incredibly awesome things by allowing publishers to design sites around their content and then easily integrate ads that enhance the user experience rather than disrupt it while advertisers are able to get their ads in front of users that actually enjoy seeing them. It’s an evolution of display advertising that’s better for everyone involved that I can’t see it failing. Clearly I’m excited.  PS. TripleLift is hiring so let me know if you want to talk about the various roles and what we’re doing.",2,1,2013-11-23,4,"triplelift, advertising, native advertising, display advertising",305,I'm joining TripleLift
27,0,I'm currently using Google Analytics for my blog but am going to also start using MixPanel and a bunch of meta data to track additional info.,#blog,"{% include setup %} Every time I launch a new website one of the first things I do is add  Google Analytics  to start gathering data. This blog was no different but I’ve recently been wondering whether Google Analytics is the right way to measure a blog. It’s great for tracking the total number of visitors, where they’re coming from, and how long they’re staying but I wish there was something that was optimized for blogs rather than something that was designed as a general solution.  My ideal blog analytics tool would help me understand both how readers are finding my content as well as how they’re engaging with it. It would also automatically segment my blog’s readers so I’d be able to quickly tell the different types of readers I have and what each of the groups is interested in seeing. Many of my posts end up being shared but I only discover that when I look at the referrals in Google Analytics. I’d also love to get a notification whenever one of my posts is shared on another site or social networks so I can participate in the discussion as it’s happening rather than days later.  I’m not sure about the available tools but I have some experience with  MixPanel  and am going to see if I can jerry rig it to do what I want. It’s great for doing simple funnel and segmentation analysis but it should also be flexible enough to do a ton more stuff. One thing I’ve been thinking about is generating additional meta data for each of my posts (topics, number of words, number of images) and then feeding that into MixPanel to see what impact they have. I’ll share both the source code and results once I have it up and running.",2,1,2013-11-26,4,"blog, analytics, google analytics, mixpanel",308,Blog analytics (blogolytics?)
25,0,A few years ago I wrote a genetic program that generates a Connect 4 bot. This provides a quick overview of how it works.,"#code,#python","{% include setup %} Over Thanksgiving break I was going through some old GitHub repos and found an interesting one I wanted to share. It’s a  Connect 4 bot  that’s evolved through a genetic program. The goal of the strategy is to choose a column to move to that will give the highest probability of a win given a board position. To figure out the move column, the genetic program simulates play of strategy against strategy and gives the most successful ones a greater chance of reproducing into the next generation. The idea is that over time the resulting strategy will be the most fit.  The way a typical genetic program works is represented is through a tree structure with the leaf nodes (terminals) containing the various features of the input and the non-leaf nodes containing functions to evaluate the values in the leaf nodes. This way, the program can evaluate any input and we can create new functions by taking subbranches from one tree and combining them with another.  I used the  PyEvolve framework  which took care of all the simulation code so the bulk of my work was spent in figuring out which features and functions to use as well as a way of tracking the intermediate strategies so I could store the resulting strategy for later use. The features I ended up using where the number of own and opponent’s pieces adjacent to the move, the number of own and opponent’s 3 piece segments that would be created with the move, and the height of the column. I experimented with a few functions but ended up keeping a simple set of four - add, subtract, multiply, and an “is greater than” function.  In the end, the best I could get was a genetic program that was able to beat a random move strategy a little over 70% of the time. Unfortunately, this “optimal” strategy failed to win against a real strategy, such as  minimax . I suspect the strategy would have done a bit better had I trained it against a smarter set of strategies but I doubt it would have ever been able to compete with the minimax approach. I’m mostly amazed that by starting with a few features and some simple functions it’s possible to evolve a strategy that’s actually better than random. I doubt I can use this approach in a professional project but it’s still great being exposed to it.",3,2,2013-11-30,3,"genetic programming, ai, connect 4",418,Genetic programming Connect 4
37,0,The majority of the sites I visit use a slew of JavaScript libraries. I decided to do a quick analysis and see if there are any patterns of the libraries used based on the site type.,"#code,#javascript","{% include setup %} I recently installed Ghostery and am amazed by the number of JavaScript libraries being loaded on the sites I visit. Almost every site I visit has at least one analytics library, a few advertising libraries, and some social network sharing libraries.  To be a bit more quantitative, I pulled the libraries used by 20 of top sites to see if anything stood out. The biggest surprise was how differently the various types of sites used these libraries. Every single publisher used DoubleClick and yet only a quarter of them used Google Analytics while 80% of the social networks I looked at used Google Analytics and only 40% used DoubleClick. The other interesting piece was how many more libraries an average publisher uses compared to a social network or ecommerce site. Five of the 13 publishers I looked at included at least 20 JavaScript libraries while the most libraries included by a social network was 4, which was Pinterest. The bulk of these additional libraries tend to be advertising specific so it’s not that surprising that publishers have more of them but the difference in volume was shocking. I’ve included the data at the bottom of this post in case someone wants to take a stab at it but something on my todo list is to automate the process of gathering this info rather than relying on Ghostery and copy and paste. Once I get get it done I’ll follow up with another post analyzing the larger set of data.  Even if these libraries get cached in the browser it’s still quite a lot of JavaScript that’s executed every time a site is loaded. It’s insane that Forbes is loading 39 libraries every time a page is seen. I suspect most people use AdBlock not because of ads but because of the degraded performance of a site having to load these libraries and the associated images. I’m aware that publishers are in trouble but I don’t think adding more and more libraries to eke out additional revenue is a sustainable model.     # of Libraries by Site      Site  Site Type  # of Libraries      Forbes  Publisher  39    BBC  Publisher  33    The Guardian  Publisher  25    Washington Post  Publisher  24    DailyKos  Publisher  23    NY Times  Publisher  19    USA Today  Publisher  18    Huff Po  Publisher  17    ABC  Publisher  15    Fox News  Publisher  15    Amazon  E Commerce  12    CNN  Publisher  10    ESPN  Publisher  9    Ebay  E Commerce  8    Yahoo  Publisher  6    Pinterest  Social Network  4    Facebook  Social Network  3    Tumblr  Social Network  3    Reddit  Social Network  2    Twitter  Social Network  1           # of Sites by Library      Library  # of Sites      DoubleClick  17    ScoreCard Research Beacon  14    Google Adsense  10    ChartBeat  8    NetRatings SiteCensus  8    Facebook Connect  8    Quantcast  8    Google Analytics  7    Datalogix  7    Right Media  7    Omniture (Adobe Analytics)  6    AppNexus  6    Moat  6    Audience Science  6    Evidon Notice  5    MediaMath  5    TrackingSoft  5    Adobe Test &amp; Target  4    Visual Revenue  4    Aggregate Knowledge  4    Acxiom  4    Google AdWords Conversion  3    AdRoll  3    Criteo  3    DoubleClick Spotlight  3    Facebook Social Plugins  3    Twitter Button  3    Outbrain  3    Quigo AdSonar  3    Atlas  3    Rubicon  3    Advertising.com  3    Krux Digital  3    BrightRoll  3    eXelate  3    BuzzFeed  2    Optimizely  2    Typekit by Adobe  2    AdXpose  2    Casale Media  2    Media Optimizer (Adobe)  2    VoiceFive  2    AdMeld  2    Amazon Associates  2    Facebook Exchange (FBX)  2    OpenX  2    PubMatic  2    TRUSTe Notice  2    Facebook Social Graph  2    MediaMind  2    [x+1]  2    BlueKai  2    Brilig  2    Media Innovation Group  2    Neustar AdAdvisor  2    SpotXchange  2    24/7 Media Ad Network  2    Dynamic Logic  1    Gravity Insights  1    Crazy Egg  1    DoubleClick Floodlight  1    FreeWheel  1    Gigya Socialize  1    MixPanel  1    Specific Media  1    Twitter Badge  1    ValueClick Mediaplex  1    Janrain  1    Parse.ly  1    Yahoo Analytics  1    Burst Media  1    PulsePoint  1    eBay Stats  1    Genome  1    ADTECH  1    Google +1  1    DoubleClick DART  1    Adzerk  1    Effective Measure  1    Mindset Media  1    Rocket Fuel  1    Brightcove  1    New York Times  1    WebTrends  1    ForeSee  1    Google AJAX Search API  1    Integral Ad Science  1    Media6Degrees  1    Adap.tv  1    AddThis  1    AMP Platform  1    DoubleClick Bid Manager  1    i-Behavior  1    Intent Media  1    Lotame  1    Martini Media  1    Media.net  1    Tacoda  1    Tapad  1    TidalTV  1    TradeDesk  1    Turn  1    Undertone  1    SimpleReach  1    Tealium  1    Bizo  1    New Relic  1    Trove  1           Avg # of Libraries by Site Type      Site Type  # of Sites  Avg # Libraries      Publisher  13  19.46    Social Network  5  2.60    E Commerce  2  10.00           Raw Data      Site  Library  Site Type      ESPN  Adobe Test &amp; Target  Publisher    ESPN  ChartBeat  Publisher    ESPN  DoubleClick  Publisher    ESPN  Dynamic Logic  Publisher    ESPN  Evidon Notice  Publisher    ESPN  Google Adsense  Publisher    ESPN  Gravity Insights  Publisher    ESPN  NetRatings SiteCensus  Publisher    ESPN  ScoreCard Research Beacon  Publisher    ABC  BuzzFeed  Publisher    ABC  ChartBeat  Publisher    ABC  Crazy Egg  Publisher    ABC  DoubleClick  Publisher    ABC  DoubleClick Floodlight  Publisher    ABC  Facebook Connect  Publisher    ABC  FreeWheel  Publisher    ABC  Gigya Socialize  Publisher    ABC  Google AdWords Conversion  Publisher    ABC  NetRatings SiteCensus  Publisher    ABC  Omniture (Adobe Analytics)  Publisher    ABC  Optimizely  Publisher    ABC  Quantcast  Publisher    ABC  ScoreCard Research Beacon  Publisher    ABC  Typekit by Adobe  Publisher    DailyKos  AdRoll  Publisher    DailyKos  AdXpose  Publisher    DailyKos  AppNexus  Publisher    DailyKos  Casale Media  Publisher    DailyKos  ChartBeat  Publisher    DailyKos  Criteo  Publisher    DailyKos  DoubleClick  Publisher    DailyKos  DoubleClick Spotlight  Publisher    DailyKos  Evidon Notice  Publisher    DailyKos  Facebook Connect  Publisher    DailyKos  Facebook Social Plugins  Publisher    DailyKos  Google Adsense  Publisher    DailyKos  Google AdWords Conversion  Publisher    DailyKos  Google Analytics  Publisher    DailyKos  MediaMath  Publisher    DailyKos  MixPanel  Publisher    DailyKos  Quantcast  Publisher    DailyKos  ScoreCard Research Beacon  Publisher    DailyKos  Specific Media  Publisher    DailyKos  TrackingSoft  Publisher    DailyKos  Twitter Badge  Publisher    DailyKos  Twitter Button  Publisher    DailyKos  ValueClick Mediaplex  Publisher    Fox News  Adobe Test &amp; Target  Publisher    Fox News  AdRoll  Publisher    Fox News  DoubleClick  Publisher    Fox News  Evidon Notice  Publisher    Fox News  Google Adsense  Publisher    Fox News  Janrain  Publisher    Fox News  Media Optimizer (Adobe)  Publisher    Fox News  Moat  Publisher    Fox News  NetRatings SiteCensus  Publisher    Fox News  Outbrain  Publisher    Fox News  Parse.ly  Publisher    Fox News  Quigo AdSonar  Publisher    Fox News  ScoreCard Research Beacon  Publisher    Fox News  TrackingSoft  Publisher    Fox News  Visual Revenue  Publisher    Facebook  Aggregate Knowledge  Social Network    Facebook  Atlas  Social Network    Facebook  DoubleClick  Social Network    Twitter  Google Analytics  Social Network    Yahoo  Datalogix  Publisher    Yahoo  DoubleClick  Publisher    Yahoo  Right Media  Publisher    Yahoo  ScoreCard Research Beacon  Publisher    Yahoo  VoiceFive  Publisher    Yahoo  Yahoo Analytics  Publisher    Amazon  AdMeld  E Commerce    Amazon  Amazon Associates  E Commerce    Amazon  AppNexus  E Commerce    Amazon  Burst Media  E Commerce    Amazon  DoubleClick  E Commerce    Amazon  Facebook Exchange (FBX)  E Commerce    Amazon  Google Adsense  E Commerce    Amazon  OpenX  E Commerce    Amazon  PubMatic  E Commerce    Amazon  PulsePoint  E Commerce    Amazon  Right Media  E Commerce    Amazon  Rubicon  E Commerce    Ebay  Aggregate Knowledge  E Commerce    Ebay  Datalogix  E Commerce    Ebay  DoubleClick  E Commerce    Ebay  eBay Stats  E Commerce    Ebay  Genome  E Commerce    Ebay  MediaMath  E Commerce    Ebay  Right Media  E Commerce    Ebay  TRUSTe Notice  E Commerce    Tumblr  Google Analytics  Social Network    Tumblr  Quantcast  Social Network    Tumblr  ScoreCard Research Beacon  Social Network    Pinterest  DoubleClick  Social Network    Pinterest  Facebook Connect  Social Network    Pinterest  Facebook Social Graph  Social Network    Pinterest  Google Analytics  Social Network    Huff Po  Adobe Test &amp; Target  Publisher    Huff Po  ADTECH  Publisher    Huff Po  Advertising.com  Publisher    Huff Po  DoubleClick  Publisher    Huff Po  Facebook Connect  Publisher    Huff Po  Facebook Social Plugins  Publisher    Huff Po  Google +1  Publisher    Huff Po  Google Analytics  Publisher    Huff Po  MediaMind  Publisher    Huff Po  Moat  Publisher    Huff Po  NetRatings SiteCensus  Publisher    Huff Po  Omniture (Adobe Analytics)  Publisher    Huff Po  Quantcast  Publisher    Huff Po  Quigo AdSonar  Publisher    Huff Po  ScoreCard Research Beacon  Publisher    Huff Po  TrackingSoft  Publisher    Huff Po  Twitter Button  Publisher    CNN  ChartBeat  Publisher    CNN  DoubleClick  Publisher    CNN  DoubleClick DART  Publisher    CNN  Facebook Connect  Publisher    CNN  Facebook Social Plugins  Publisher    CNN  Krux Digital  Publisher    CNN  NetRatings SiteCensus  Publisher    CNN  ScoreCard Research Beacon  Publisher    CNN  Twitter Button  Publisher    CNN  Visual Revenue  Publisher    Reddit  Adzerk  Social Network    Reddit  Google Analytics  Social Network    BBC  [x+1]  Publisher    BBC  Acxiom  Publisher    BBC  AdMeld  Publisher    BBC  Advertising.com  Publisher    BBC  AdXpose  Publisher    BBC  Aggregate Knowledge  Publisher    BBC  AppNexus  Publisher    BBC  Atlas  Publisher    BBC  Audience Science  Publisher    BBC  BlueKai  Publisher    BBC  BrightRoll  Publisher    BBC  Brilig  Publisher    BBC  Casale Media  Publisher    BBC  Datalogix  Publisher    BBC  DoubleClick  Publisher    BBC  DoubleClick Spotlight  Publisher    BBC  Effective Measure  Publisher    BBC  Facebook Exchange (FBX)  Publisher    BBC  Google Adsense  Publisher    BBC  Media Innovation Group  Publisher    BBC  MediaMath  Publisher    BBC  Mindset Media  Publisher    BBC  NetRatings SiteCensus  Publisher    BBC  Neustar AdAdvisor  Publisher    BBC  Omniture (Adobe Analytics)  Publisher    BBC  OpenX  Publisher    BBC  PubMatic  Publisher    BBC  Right Media  Publisher    BBC  Rocket Fuel  Publisher    BBC  Rubicon  Publisher    BBC  ScoreCard Research Beacon  Publisher    BBC  SpotXchange  Publisher    BBC  TrackingSoft  Publisher    NY Times  Acxiom  Publisher    NY Times  AppNexus  Publisher    NY Times  Atlas  Publisher    NY Times  Audience Science  Publisher    NY Times  Brightcove  Publisher    NY Times  ChartBeat  Publisher    NY Times  Datalogix  Publisher    NY Times  DoubleClick  Publisher    NY Times  eXelate  Publisher    NY Times  Facebook Connect  Publisher    NY Times  Google Adsense  Publisher    NY Times  Krux Digital  Publisher    NY Times  Moat  Publisher    NY Times  NetRatings SiteCensus  Publisher    NY Times  New York Times  Publisher    NY Times  ScoreCard Research Beacon  Publisher    NY Times  Typekit by Adobe  Publisher    NY Times  VoiceFive  Publisher    NY Times  WebTrends  Publisher    The Guardian  24/7 Media Ad Network  Publisher    The Guardian  AppNexus  Publisher    The Guardian  Audience Science  Publisher    The Guardian  BrightRoll  Publisher    The Guardian  ChartBeat  Publisher    The Guardian  Criteo  Publisher    The Guardian  DoubleClick  Publisher    The Guardian  Evidon Notice  Publisher    The Guardian  Facebook Connect  Publisher    The Guardian  Facebook Social Graph  Publisher    The Guardian  ForeSee  Publisher    The Guardian  Google Adsense  Publisher    The Guardian  Google AdWords Conversion  Publisher    The Guardian  Google AJAX Search API  Publisher    The Guardian  Integral Ad Science  Publisher    The Guardian  Media6Degrees  Publisher    The Guardian  MediaMath  Publisher    The Guardian  NetRatings SiteCensus  Publisher    The Guardian  Omniture (Adobe Analytics)  Publisher    The Guardian  Optimizely  Publisher    The Guardian  Outbrain  Publisher    The Guardian  Quantcast  Publisher    The Guardian  Right Media  Publisher    The Guardian  Rubicon  Publisher    The Guardian  ScoreCard Research Beacon  Publisher    Forbes  24/7 Media Ad Network  Publisher    Forbes  [x+1]  Publisher    Forbes  Acxiom  Publisher    Forbes  Adap.tv  Publisher    Forbes  AddThis  Publisher    Forbes  Advertising.com  Publisher    Forbes  Aggregate Knowledge  Publisher    Forbes  AMP Platform  Publisher    Forbes  AppNexus  Publisher    Forbes  Audience Science  Publisher    Forbes  BlueKai  Publisher    Forbes  BrightRoll  Publisher    Forbes  Brilig  Publisher    Forbes  Datalogix  Publisher    Forbes  DoubleClick  Publisher    Forbes  DoubleClick Bid Manager  Publisher    Forbes  DoubleClick Spotlight  Publisher    Forbes  eXelate  Publisher    Forbes  Google Adsense  Publisher    Forbes  Google Analytics  Publisher    Forbes  i-Behavior  Publisher    Forbes  Intent Media  Publisher    Forbes  Lotame  Publisher    Forbes  Martini Media  Publisher    Forbes  Media Innovation Group  Publisher    Forbes  Media.net  Publisher    Forbes  Moat  Publisher    Forbes  Omniture (Adobe Analytics)  Publisher    Forbes  Quantcast  Publisher    Forbes  Right Media  Publisher    Forbes  ScoreCard Research Beacon  Publisher    Forbes  Tacoda  Publisher    Forbes  Tapad  Publisher    Forbes  TidalTV  Publisher    Forbes  TradeDesk  Publisher    Forbes  TRUSTe Notice  Publisher    Forbes  Turn  Publisher    Forbes  Undertone  Publisher    Forbes  Visual Revenue  Publisher    USA Today  AdRoll  Publisher    USA Today  Audience Science  Publisher    USA Today  BuzzFeed  Publisher    USA Today  ChartBeat  Publisher    USA Today  Datalogix  Publisher    USA Today  DoubleClick  Publisher    USA Today  Evidon Notice  Publisher    USA Today  Facebook Connect  Publisher    USA Today  Google Adsense  Publisher    USA Today  Media Optimizer (Adobe)  Publisher    USA Today  Moat  Publisher    USA Today  Quantcast  Publisher    USA Today  Right Media  Publisher    USA Today  ScoreCard Research Beacon  Publisher    USA Today  SimpleReach  Publisher    USA Today  Tealium  Publisher    USA Today  TrackingSoft  Publisher    USA Today  Visual Revenue  Publisher    Washington Post  Acxiom  Publisher    Washington Post  Adobe Test &amp; Target  Publisher    Washington Post  Amazon Associates  Publisher    Washington Post  Audience Science  Publisher    Washington Post  Bizo  Publisher    Washington Post  ChartBeat  Publisher    Washington Post  Criteo  Publisher    Washington Post  Datalogix  Publisher    Washington Post  DoubleClick  Publisher    Washington Post  eXelate  Publisher    Washington Post  Google Adsense  Publisher    Washington Post  Krux Digital  Publisher    Washington Post  MediaMath  Publisher    Washington Post  MediaMind  Publisher    Washington Post  Moat  Publisher    Washington Post  Neustar AdAdvisor  Publisher    Washington Post  New Relic  Publisher    Washington Post  Omniture (Adobe Analytics)  Publisher    Washington Post  Outbrain  Publisher    Washington Post  Quantcast  Publisher    Washington Post  Quigo AdSonar  Publisher    Washington Post  ScoreCard Research Beacon  Publisher    Washington Post  SpotXchange  Publisher    Washington Post  Trove  Publisher          You can also grab the entire set of data  here .",1,2,2013-12-01,6,"javascript, analytics, advertising, google analytics, doubleclick, publishers",855,Drowning in JavaScript
40,0,Marketplace startups are very popular these days and do provide value but I'm just not sure why there are so many home cleaning ones out there. That doesn't seem to be the best market for this type of model.,#product,"{% include setup %} The recent rise of marketplace startups is great and benefits all except the incumbent. They provide much needed liquidity and transparency to markets that helpfully reduce costs to the consumer and increase volume to the provider.  Yet I’m surprised by the number of home cleaning service startups out there. I’m aware of  HomeJoy ,  Exec ,  GetMaid ,  MyClean , and  HandyBook , but am sure there are countless other copycats. The Uber approach works because it’s for an immediate service with a one time transaction where the value provided is somewhat of a commodity. This is not the case with home cleaning services. The range of quality among cleaners varies significantly more than the quality among drivers and I’d be willing to have a good cleaner come in at a slightly inconvenient time rather than a poor cleaner at the perfect time. And once I find a cleaner I like I’d want to book them directly rather than go through the company again. This way I can get a lower rate while also giving a cleaner more than they’d otherwise make from using the service. This would violate the company’s terms but I don’t see how they can be enforced.  I’m sure there’s still a large market for such cleaning services, I’m just not as  optimistic as others  seem to be. I see it being great for one off events - cleaning after a party, getting your apartment ready a visit from the parents, or preparing your home for sale. I just don’t see how this is a huge market that warrants all these startups. I understand it’s just the beachhead but it doesn’t seem to be a very strong one. Have the better ones already been done? Uber is dominating car service. Food delivery is full of one-time transactions that need to be met quickly but it’s already a mature market. Laundry is another one and there are two startups I’m aware of working on it - Washio and Prim. HandyBook also offers handyman services although they seem to be more focused on home cleaning. I really don’t know why home cleaning startups are so popular.",6,1,2013-12-09,1,,386,Why are there so many cleaning startups?
33,0,If you're a frequenty user of MySQL you should become familiar with the information_schema.columns table. It's very handy for getting information about all the columns and tables in the MySQL database.,"#code,#sql","{% include setup %} Something that’s been really helpful to me in understanding a MySQL database is the built in  information_schema.columns  table. It provides information on every column in the database and is queryable just like any other table. This makes it easy to quickly find all tables that have a particular column name or all columns that are the same data type. There have been countless times where I knew the data existed somewhere but couldn’t recall which table it was in. Querying the information_schema.columns table for the foreign key helped me quickly figure it out. Below are some sample queries that retrieve data from the information_schema.columns table:  {% highlight sql %}select table_schema, table_name from information_schema.columns where column_name like '%user_id%’;  select * from information_schema.columns where column_name like '%time%’;  select * from information_schema.columns where data_type = 'datetime'; {% endhighlight %}  Since it’s just like any other table, except for being read-only, you can write jobs that access the data. Something I had to recently do was write a quick script to generate fake data. All a user has to do is specify the table name to populate and the script would look up the columns and their types from information_schema.columns and dynamically generate the INSERT statements. For example, if a column was of type varchar it would generate a random text string less than or equal to the length constraint and if it were an int it would generate a random number. It wasn’t perfect and only handled foreign keys that were specified by the user but it was great given the effort. A later version could use the information provided by two other information_schema tables, table_constraints and key_column_usage, to get rid of this manual step. If you’re a frequent MySQL user, familiarizing yourself with the tables in information_schema will make you significantly more efficient.",1,2,2013-12-15,3,"mysql, inforamtion_schema, information_schema.columns",321,Using the information_schema.columns table
28,0,Bank of America has a really crappy way of notifying them of travel plans. No idea why they decided to implement it in such an odd way.,#design,"{% include setup %} Before leaving for a trip to India, I wanted to make sure that I’d be allowed to access the ATM so I decided to contact my bank. Surprisingly, Bank of America was modern enough to allow me to do this online. Unsurprisingly, the UX was lacking.  Instead of just asking which country I was traveling to using a simple autocomplete or dropdown they have a three step process. First, I get to choose whether I’m traveling domestic or international. If internationally, I get presented with four options that are just the first letter of each country name. After choosing a country range bucket, I can finally pick the actual country.              I understand when an inferior UX decision is made because it’s cheaper to implement but in this case it must have actually been more difficult. Instead of having a single dropdown or autocomplete they have three different input elements. Even if the first selection is necessary, there’s no need to split 206 countries into 4 separate lists.  I’m not sure what I was expecting but it’s still frustrating seeing such decisions being made. I’d love to know the reasons.",0,1,2013-12-20,4,"bank of america, ux, design, ui",239,"Why Bank of America, why?"
36,0,Just a thought exercise on a new way to build software applications. To support complicated worfklows and systems it's easier to build support for CSV file uploads rather than trying to find the optimal design.,#product,"{% include setup %} Something that I've been thinking about ever since I worked as a product manager focused on internal tools is being able to run a product entirely through CSV file uploads. Instead of building a UX designed to handle bulk operations and complicated workflows you build support for file uploads and handle the business logic entirely on the backend. The motivation is that it’s extremely difficult to build a UI that’s going to be as powerful and flexible as a simple CSV file, especially when outside tools, such as Excel, can help generate these files.  This approach also has the nice property of decoupling the input from the core business logic. Over time, tools and interfaces can be built that are optimized for specific use cases without having to modify any of the backend. Effort can be spent on improving workflows that are already being done rather than building support for workflows that may or may not be common. Permissions and controls can also be added that make the application accessibility to a wider range of users.  Many companies spend tons of time building advanced tools that will never be as powerful as Excel paired with a simple file upload. Workflows vary significantly across users and most products impose a single approach. Why not build more general tools that take advantage of the unique work styles?",0,1,2013-12-21,5,"product management, csv, products, design, software",230,CSV powered products
48,0,During my trip to India I bought an unlocked phone and spent an absurdly long time trying to get a SIM card. This is a guide on what you need to do in order to get a SIM card in India and the challenges I ran into.,#meta,"{% include setup %} I’ve heard about the wonders of an unlocked phone and decided to try it out during my recent trip to India. The idea was to get a cheap unlocked Android phone that I’d be able to use on this and future trips. I was able to get a relatively cheap Samsung phone but it took me a surprisingly long time to get a working SIM card. This post is a description of the steps I took as well as some advice for others trying to do the same.  First off, to get a SIM card as a foreigner in India, you need to have a copy of your passport and visa, a passport sized photo, and a local to act as a reference. After giving this information to vendor they will do the necessary paperwork, call the reference, and if everything goes well they will activate your SIM card within 24 hours after which you will need to call them to verify and start using the service.  My first attempt was in New Delhi where I went to an Airtel shop based the advice of my uncle. Unfortunately, I didn’t know I needed to have a passport sized photo but was referred to a nearby computer shop that was able to print them out at the cost of 10 rupees (~17 cents) a piece. I was able to buy a regular sized SIM for 300 rupees (~$5) but was told it would take around 24 hours to activate and would only be cut after that. Unfortunately, I had to leave Delhi for a wedding so didn’t get a chance to get it cut to a micro SIM until I had already arrived in Mumbai. By that point, I was in a different city and no longer able to activate a Delhi SIM card although it took me multiple days to figure that out.  After going back and forth to the Airtel shop in Mumbai a few times, and discovering a new hoop I had to jump through every time, I was about to give up until I shared my problem with someone at my hotel. He took me to a nearby stand which was able to take care of everything for me within a few hours. This went smoothly since I had a few of the passport photos left and he was willing to act as my local reference. Total cost was 600 rupees (~$10) and included 250 rupees of credit.  Now that I had a functional phone, it worked great. It took me a little bit of time to understand the prepaid model but once I did I actually preferred it more than the postpaid one I have in the US. You can go to the dozens of mobile vendors around cities which will glady load some money unto your account. You can then activate various services either by using these vendors, doing it online, or via text messaging. At any time you can text various numbers and codes in order to get the balance you have left on your plans as well as add new ones. With my 250 rupee balance (~$4) I was able to buy 150 MB of 3G for 44 rupees (~75 cents) and load the rest into a national dialing plan.  Having a phone that works wherever you go is immensely convenient. Traversing and exploring Mumbai became significantly easier and more fun when we were able to get the phone working. We were able to explore the city without having to worry about getting lost and were able to discover and research local gems. It wasn’t as serendipitous as just walking around but we hopefully struck the right balance.  The challenge was in getting the SIM card working and I’m sure the process will vary in every country. My advice is to do research on how to get a prepaid SIM card before travelling and come prepared with everything you need so you can get the process started soon after arriving. If you know you’ll only be spending time in a city for a single day it may not make sense to get the SIM card there since it may not be possible to activate it in another city - I’m not sure if this is due to my experience or just the way things are done in India but it’s something to be aware of. You can also try contacting the hotel you’re staying at since they should have had experience helping their guests get SIM cards.  T-Mobile recently launched a  global plan  and I’m sure more and more carriers will follow suit. Until then we’re stuck with SIM cards and the unique challenges of obtaining one in different countries.",1,1,2013-12-23,4,"SIM card, travel, unlocked phone, 3G",798,Getting a SIM card in India
32,0,There are very few global tech products - those that work wherever you are in the world. A big reason is due to infrastructure differences and I don't see this changing soon.,#product,"{% include setup %} The trip to India got me thinking about “global products” that work the same wherever they are. It’s surprisingly difficult to find tech products that fit this description. Cell phones will almost always work internationally but roaming charges make it impractical. Having an unlocked phone helps but you still need to get a SIM card which is a  hassle  in many countries.  Even something as standard as a laptop isn’t as easy to use as it should be. Wifi connectivity varies depending where you are with most cities being great fickle elsewhere. Dealing with voltage conversion and plug adapters is something that always comes up. I’ve learned to travel with an adapter kit that includes enough combinations to be able to charge my laptop wherever I go.  The one product that actually worked as expected was the GPS running watch my wife got me for our anniversary. No matter where I was it was able to lock on to a satellite within a few minutes and accurately track my run. The only issue was charging which I was able to do via a USB cable connected to my laptop. Even that shows a weakness since if I didn’t have a laptop I wouldn’t have been able to charge the watch.  The funny thing is, each of these products was designed to work globally - it’s just that the infrastructure differences prevent that from being a reality. Whether it’s having a different set of of plugs or a particular way of getting a SIM card it’s not the product that’s the problem. As powerful as our products are they’re still operating within an infrastructure. And since products evolve faster than infrastructure we’ll continue to see this inconsistent product behavior around the world. Maybe by the time we colonize space we’ll have a consistent global infrastructure.",1,1,2013-12-25,3,"global products, infrastructure, technology",315,Global products
20,0,Smartphones have the potential of increasing global literacy in addition to the more obvious changes we're going to see.,#society,"{% include setup %}                              Source:  Business Insider                In 2012 global smartphone ownership surpassed PC ownership and smartphones are still seeing massive growth. The obvious consequence is that many people who’ve never owned a computer are starting to own smartphones and that’s having a huge impact on the world. Almost everything will be affected - not just technology but also business, politics, and general culture. As these smartphones get more powerful and pervasive we’ll see applications that we can’t even imagine right now.  What’s not being discussed is the impact this will have on the world’s literacy rate. In 2010, the  global literacy rate  was estimated to be 84% but increasing smartphone ownership should drive it higher. Having something in your pocket that is both a business and entertainment device will encourage people to learn all its features. Sure one can just familiarize oneself with the various icons and key combinations to achieve certain results but I suspect being exposed to a smartphone’s potential will also serve as motivation to learn more.  Of course, this is just hopeful speculation on my part but I think we tend to view technological change through a tech filter. There’s a whole other world that’s difficult for us to imagine so we tend to not think about it too much. I had a professor,  Prof Levent Orman , discuss the impact that the car had on the world. The direct effect was the replacement of horses but the long term effects were the rise of highways and suburbs and a change in American culture. Smartphones are one of the technologies that will have such an impact, it’s just impossible to know what it will be.",3,1,2013-12-26,3,"smartphones, literacy, education",328,Smartphones and literacy
23,0,The difference in taxi fares between New York City and Mumbai is huge and highlights the difference in the cost of labor.,#datascience,"{% include setup %} Something else that struck me during my trip to India was the difference in taxi fare between  New York City  and  Mumbai . I expected them to be different but the magnitude of the difference was shocking. In NYC, the base fare is $2.50 and increases 50 cents for each additional 1/5th of a mile or 60 seconds of not moving. In Mumbai, the rate starts at 19 rupees (~32 cents) and includes the first 1.5 km. After that it’s 12.35 rupees (21 cents) for each additional km and 30 rupees (50 cents) for an hour of not moving.      Distance (mi)  Wait Time (min)  Total NYC Fare ($)  Total Mumbai Fare ($)  Est NYC Gas Cost ($)  Est Mumbai Gas Cost ($)  Est NYC Driver Profit  Est Mumbai Driver Profit      1  1  5.50  0.39  0.18  0.24  97%  38%    1  2  6.00  0.44  0.18  0.24  97%  45%    1  5  7.50  0.59  0.18  0.24  98%  59%    2  1  8.00  0.72  0.35  0.48  96%  33%    2  2  8.50  0.77  0.35  0.48  96%  38%    2  5  10.00  0.92  0.35  0.48  97%  48%    5  1  15.50  1.71  0.88  1.20  94%  30%    5  5  17.50  1.91  0.88  1.20  95%  37%    5  10  20.00  2.16  0.88  1.20  96%  45%    5  20  25.00  2.66  0.88  1.20  97%  55%    10  5  30.00  3.57  1.75  2.40  94%  33%    10  10  32.50  3.82  1.75  2.40  95%  37%    10  20  37.50  4.32  1.75  2.40  95%  44%    10  30  42.50  4.82  1.75  2.40  96%  50%    20  10  57.50  7.14  3.50  4.80  94%  33%    20  20  62.50  7.64  3.50  4.80  94%  37%    20  30  67.50  8.14  3.50  4.80  95%  41%    50  0  127.50  16.58  8.75  12.00  93%  28%    100  0  252.50  33.15  17.50  24.00  93%  28%    1000  0  2502.50  331.40  175.00  240.00  93%  28%      The differences are crazy. A short ride will cost $5 in NYC but only 40 cents in Mumbai. Even if we look at the limit where we’re always moving and there’s no stopping, a NYC fare will cost 7.55 times   1   that of one in Mumbai. Given these differences, I was surprised to discover that gas is 40% more expensive   2   in Mumbai. If we assume an average car gets 20 miles a gallon, it works out that in NYC the profit to the driver is over 90% of the total fare whereas in Mumbai it’s closer to 30%. The fare pricing echoes this: standing still for an hour costs 50 cents in Mumbai but $30 in NYC. This is simplified since there are many other factors at play, ie the  NYC medallion system , but it’s still a massive difference in labor costs.  This reminds me of something I read about the pricing of soda in grocery stores. In the US, a 12 pack of Coke is only slightly more expensive than a 20 oz bottle whereas in countries with lower labor costs they’re much closer to the actual unit costs. The reason is the same - the cost of labor in US contributes the most to the cost of an item whereas in countries with lower labor costs it’s the item cost that’s the bulk of the final item price.   1  7.55 = 2.5/(1.61 &times; 12.35/60)  2  $3.50/gallon in NYC vs 78 Rupees/gallon ($4.80) in Mumbai",5,1,2013-12-29,4,"taxi pricing, taxi fares, NYC, Mumbai",485,Taxi pricing in NYC vs Mumbai
16,0,A standard end of the year post to talk about my blog stats over 2013.,#blog,"{% include setup %} Now that I actually have over 100 posts for the year I can actually follow the trend and highlight the most popular ones as well as share some data from my Google Analytics account. This is the first year I’ve seriously committed to blogging and didn’t think I’d enjoy it as much as I did. I will continue to write at least twice a week in 2014 so it will be interesting to see how next year’s data compares against the data from 2013. Thanks for reading and definitely let me know if you have any topics you want me to write about.                                                A general overview of 2013 traffic. I'm honestly surprised by the number of visitors I've had but it's mostly come from a few posts that ended up getting signifcant traffic from Hacker News and Twitter. Note that the bounce rate dropped near the beginning of the year since I added a  Google Analytics event  to consider 15 seconds of being on the site as a ""read"" event.                                                         The obligatory top posts. One thing I discovered is that I am terribly poor at predicting which posts will be the most successful. We'll see if I get better at this in 2014. Here are the links in clickable form:                                                   Why don't cell phones have a dialtone?                                                                   Drowning in JavaScript                                                                   Eighteeen months of Django                                                                   What the SEO?                                                                   Fun with Prolog: Priceonomics Puzzle                                                                   In defense of Excel                                                                   Getting a SIM card in India                                                                   Eighteeen months of Django: Part 2                                                                   The power inbox                                                                   Run Django under Nginx, Virtualenv and Supervisor                                                                   Coke, Pepsi and Passover                                                                   Scraping Yahoo fantasy football stats with Scrapy                                                                                                    Definitely surprised by how significant mobile and tablet traffic was. I imagine these will only increase in the coming years.",16,1,2013-12-30,5,"blog, 2013, year in review, blog stats, blog analytics",510,2013 blog stats
14,0,Some visualizations done using R on top of my RunKeeper data from 2013.,"#dataviz,#code,#R","{% include setup %} What better way to celebrate running 1000 miles in 2013 than dumping the data into R and generating some visualizations? It’s also a step in my quest to replace Excel with R. I’ve included the code below with some comments as well as added it to  my GitHub . If you have any ideas on what else I should do with it definitely let me know and I’ll give it a go.  PS. It's great when web services allow users to export their data and wish more would start doing the same.    	                               	Cumulative distance. You can see a few flat areas in February and November when I took a break due to some minor injuries.                                                     	Distance run by month. Unexpected drop in November due to a break but pretty solid otherwise.                                                     	Distance run by week. Not much new information here that's not covered in the monthly graph.                                                     	Cumulative time. Very similar shape to that of cumulative distance.                                                     	Cumulative time vs distance. Superimpose one on top of the other to compare the shapes. Started off slowly but started getting faster in October.                                                     	Speed by run. I got significantly faster in October but slowed down again in December.                                                     	Speed by month by distance quantile. The idea here was to look at my improvement in speed but controlling for distance. Echoes the previous chart showing my speed improvement in Oct for the longer distances.                                                     	Speed distribution by distance quantile. Another view that looks at the distribution of my speeds for all runs in a given distance quantile. Not much here but I was expecting to see that I'd have a faster pace for shorter runs.                                                     	Speed vs Distance scatter plot. Another way to look at the relationship between speed and distance but not many new insights here. Slight correlation between speed and distance. This is pretty much because as I got faster I started doing longer runs. It'll be interesting to see how this changes in 2014.                                                     	Speed vs Distance scatter plot clustered. An attempt at clustering the runs by speed and distance. In this case they were basically clustered by distance since the speed didn't vary significantly.                       {% highlight r %} library(ggplot2) library(grid) library(gridExtra) library(reshape) library(scales) library(lattice) library(ggthemes)  data = read.csv(""cardioActivities.csv"", check.names=FALSE)  summary(data)  data <- data[order(data$Date),] # Sort ascending by date data$ymd <- as.Date(data$Date) data$month <- as.Date(cut(data$ymd, breaks = ""month"")) data$week <- as.Date(cut(data$ymd, breaks = ""week"")) + 1 data$distance <- data$'Distance (mi)' data$distance_total <- cumsum(data$distance) data$speed <- data$'Average Speed (mph)' data$time_hours <- data$distance/data$'Average Speed (mph)' data$time_hours_total <- cumsum(data$time_hours) data$time_mins <- data$time_hours * 60 data$time_mins_total <- cumsum(data$time_mins) data$distance_total_norm <- data$distance_total/sum(data$distance) data$time_hours_total_norm <- data$time_hours_total/sum(data$time_hours) data$qs <- cut(data$distance, breaks = quantile(data$distance), include.lowest=TRUE) # Quantile data by distance run  # Generate a new data frame by qs and month to make plotting easier data.qs_monthly <- ddply(data, .(qs, month), function(x) data.frame(distance=sum(x$distance), time_mins=sum(x$time_mins))) data.qs_monthly$speed <- data.qs_monthly$distance/(data.qs_monthly$time_mins/60)  # Summarize the data by qs to make plotting easier data.summary <- ddply(data,~qs,summarise,mean_speed=mean(speed),sd_speed=sd(speed),mean_distance=mean(distance),sd_distance=sd(distance))  # Cluster each of the runs by speed and data m <- as.matrix(cbind(data$speed, data$distance),ncol=2) cl <- kmeans(m,3) data$cluster <- factor(cl$cluster) centers <- as.data.frame(cl$centers)  # Normalize cumulative distance and time data.normalized <- melt(data, id.vars=""ymd"", measure.vars=c(""distance_total_norm"",""time_hours_total_norm"")) data.normalized$variable <- revalue(data.normalized$variable, c(""distance_total_norm""=""Distance"", ""time_hours_total_norm""=""Time""))  png('rk-speed-vs-distance.png', width=800, height=800) ggplot(data=data, aes(x=speed, y=distance)) +   geom_point() +   theme_economist() +   scale_color_economist() +   geom_abline() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Distance') +   ggtitle(""Speed vs Distance"") dev.off()  png('rk-speed-vs-distance-clusters.png', width=800, height=800) ggplot(data=data, aes(x=speed, y=distance, color=cluster)) +   theme_economist() +   scale_color_economist() +   geom_point(legend=FALSE) +   geom_point(data=centers, aes(x=V1,y=V2, color='Center'), size=52, alpha=.3, legend=FALSE) +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Distance') +   ggtitle(""Speed vs Distance - Clustered"") dev.off()  png('rk-speed-month-qs.png',width = 800, height = 600) ggplot(data = data.qs_monthly,   aes(month, speed)) +   geom_line() +   facet_grid(qs ~ .) +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle(""Speed by Month"") dev.off()  png('rk-distance-month.png',width = 800, height = 600) ggplot(data = data,   aes(month, distance)) +   stat_summary(fun.y = sum,     geom = ""bar"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle(""Distance by Month"") dev.off()  png('rk-distance-week.png',width = 800, height = 600) ggplot(data = data,   aes(week, distance)) +   stat_summary(fun.y = sum,     geom = ""bar"") +   scale_x_date(     labels = date_format(""%Y-%m-%d""),     breaks = ""4 week"") +   xlab('Week') +   ylab('Distance') +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Week') +   ylab('Distance') +   ggtitle(""Distance by Week"") dev.off()  png('rk-speed-distribution-qs.png',width = 800, height = 600) ggplot(data, aes(speed, fill=qs)) +   geom_density(alpha = 0.5) +   geom_vline(aes(xintercept=mean_speed), data=data.summary) +   facet_grid(qs ~ .) +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Speed') +   ylab('Density') +   ggtitle(""Speed Distribution by Distance"") dev.off()  png('rk-distance-cumulative.png',width = 800, height = 600) ggplot(data=data, aes(ymd, distance_total)) +   stat_summary(fun.y = sum, geom = ""line"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Distance') +   ggtitle(""Cumulative distance"") dev.off()  png('rk-speed-daily.png',width = 800, height = 600) ggplot(data=data, aes(ymd, speed)) +   stat_summary(fun.y = sum, geom = ""line"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Speed') +   ggtitle(""Speed by Run"") dev.off()  png('rk-time-cumulative.png',width = 800, height = 600) ggplot(data=data, aes(ymd, time_hours_total)) +   stat_summary(fun.y = sum, geom = ""line"") +   scale_x_date(     labels = date_format(""%Y-%m""),     breaks = ""1 month"") +   theme_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   xlab('Month') +   ylab('Time (hours)') +   ggtitle(""Cumulative Time"") dev.off()  png('rk-time-vs-distance-cumulative.png', width=800, height=800) ggplot(data.normalized,   aes(x=ymd, y=value, colour = variable, group=variable)) +   geom_line() +   theme_economist() +   scale_color_economist() +   theme(axis.text.x = element_text(angle = 80, hjust = 1), plot.title=element_text(hjust=0.5)) +   theme(legend.title=element_blank()) +   xlab('YMD') +   ylab('') +   ggtitle(""Time vs Distance (normalized)"") dev.off() {% endhighlight %}",1,3,2014-01-04,5,"runkeeper, stats, R, running, visualization",1342,Visualizing RunKeeper data in R
21,0,Surge pricing is a huge deal with the on demand economy and I wanted to share some of my thoughts.,"#product,#pricing","{% include setup %} Every time there’s a big event or terrible weather, there’s a slew of complaints about Uber’s surge pricing. By now, you’d think that Uber customers would expect this to happen and yet they’re surprised when a $10 cab ride turns into a $100 Uber ride. I suspect Uber’s already done as much as it can on the messaging side; psychologically it’s just tough for someone to take a $10 ride one day and then a day later pay an order of magnitude more.  Every Uber transaction involves three parties - the customer, the driver, and Uber. In every case it’s up to Uber to set the prices in order to get the supply (drivers) equal to the demand (customers). In most cases, these are in alignment since people are willing to pay more for an Uber than a cab for the convenience. the problem occurs when the demand side gets too large and Uber needs to drastically increase prices in order to encourage more supply. Uber should have enough data by now to be able to determine the prices that will lead to supply being equal to demand for every demand level but that doesn’t solve the perception problem.  One  suggestion  I liked was having Uber drop their margin on these high demand days in order to maintain goodwill. Uber  supposedly takes ~20%  of each fare and that could remain the case for low fares but Uber can drop that on surge days in order to reduce the customer cost. This way a $10 drip gives Uber $2 but a $100 trip no longer needs to bring Uber $20 and can be set closer to $80. The issue is that people won’t care that a $100 trip now costs $80. Instead they’ll hear that a $10 trip now costs $80. The only way to make consumers happy would be for Uber to have a deep negative margin for the surge days.  Another option is to move to an auction model. Each customer would specify where they are, where they want to go, and what they want to pay and it would be up to a driver to either accept or ignore that offer. This way Uber can achieve  perfect price discrimination  with both drivers and customers getting what they want.  Uber must have considered both of these approaches. The latter one would require a significantly different product with more complicated logistics and a more difficult pitch but it would keep the various parties aligned to their reserve price. The former approach, on the other hand, would be much easier and cheaper to achieve. I believe Uber would be still be profitable if they took a negative margin on the rare surge day and they could offset it with a small increase in the margin on a normal day. The only thing I can think of is that they’re taking the long term view and are hoping to change the customer perception of what’s fair. I don’t know if they’ll succeed but if they do I expect many more services start adopting these “purer price” models.",3,2,2014-01-05,4,"surge pricing, product, uber, marketing",549,Surge pricing ideas
21,0,This is a comparison of taxi prices across various cities. I've included a few visualizations showing the magnitude of the differences,"#dataviz,#code,#R","{% include setup %} I initially set out to add some visualizations to an earlier post comparing taxi fares between NYC and Mumbai based on some reader suggestions. After a few visualizations, I wasn’t discovering anything new and decided add taxi fare data from other cities to make it more interesting. I ended up simulating rides in different cities on  worldtaximeter.com  and combining that with the data from  taxiautofare.com  and  www.numbeo.com  in order to break down each city’s fare into a base fare, the included distance, the rate per local distance unit, and the rate per minute. Since each city’s fare came in local units I also had to convert to miles (sorry world) and US dollars (sorry again). Using R we generate the fares for the various combinations of distances and stoppage times and start diving into the data. As usual, the data and code are up on  GitHub  with contributions, corrections, and suggestions welcome. I’d also love to get the real rates for the cities so either do a pull request or let me know what they are in the comments and I’ll update the post.      City  Base  Inc Dist  Per Dist  Per Min  Dist Cvr  Fare Cvr  $ Base  $ per Mile  $ per Min  Ratio  $ per Hr      New York  2.50  0.00  2.50  0.50  1.00  1.00  2.50  2.50  0.50  5.00  30.00    Mumbai  19.00  1.50  12.35  0.50  1.61  0.02  0.32  0.33  0.01  39.77  0.50    London  2.20  2.00  1.70  0.52  1.61  1.64  3.61  4.49  0.85  5.31  50.71    Amsterdam  2.66  0.00  1.95  0.32  1.61  1.36  3.62  4.27  0.44  9.81  26.11    Tokyo  712.00  0.00  188.00  56.00  1.61  0.01  6.84  2.91  0.54  5.41  32.26    Aberdeen  2.40  0.90  1.10  0.37  1.61  1.64  3.94  2.90  0.61  4.79  36.41    Austin  2.54  0.20  1.30  0.67  1.61  1.00  2.54  2.09  0.67  3.12  40.20    Baltimore  1.80  0.15  1.36  0.44  1.61  1.00  1.80  2.19  0.44  4.98  26.40    Barcelona  2.05  0.00  0.98  0.38  1.61  1.36  2.79  2.15  0.52  4.15  31.01    Berlin  3.00  0.00  1.58  0.41  1.61  1.36  4.08  3.46  0.56  6.20  33.46    Boston  2.60  0.23  1.73  0.54  1.61  1.00  2.60  2.79  0.54  5.16  32.40    Chicago  2.25  0.18  1.11  0.37  1.61  1.00  2.25  1.79  0.37  4.83  22.20    Dublin  4.09  1.00  1.03  0.37  1.61  1.36  5.56  2.26  0.50  4.48  30.19    Edinburgh  3.00  0.52  1.20  0.36  1.61  1.64  4.92  3.17  0.59  5.41  35.13    Ibiza  3.25  0.00  0.98  0.35  1.61  1.36  4.42  2.15  0.48  4.51  28.56    Las Vegas  3.30  0.00  1.49  0.53  1.61  1.00  3.30  2.40  0.53  4.53  31.80    Los Angeles  2.85  0.18  1.67  0.50  1.61  1.00  2.85  2.69  0.50  5.38  30.00    Madrid  2.04  0.00  0.98  0.32  1.61  1.00  2.04  1.58  0.32  4.93  19.20    Malaga  1.42  0.00  0.84  0.34  1.61  1.36  1.93  1.84  0.46  3.98  27.74    Mallorca  3.00  0.00  0.80  0.29  1.61  1.36  4.08  1.75  0.39  4.44  23.66    Manchester  2.35  0.43  1.00  0.28  1.61  1.64  3.85  2.64  0.46  5.75  27.55    Melbourne  3.20  0.00  1.61  1.04  1.61  0.89  2.85  2.31  0.93  2.49  55.54    Montreal  3.45  0.00  1.70  0.63  1.61  0.93  3.21  2.55  0.59  4.34  35.15    New Delhi  40.00  0.00  15.00  1.67  1.61  0.02  0.67  0.40  0.03  14.46  1.67    Paris  2.20  0.00  1.14  0.75  1.61  1.36  2.99  2.50  1.02  2.45  61.20    Rome  2.80  0.00  1.52  0.44  1.61  1.36  3.81  3.33  0.60  5.56  35.90    San Diego  2.50  0.00  1.67  0.46  1.61  1.00  2.50  2.69  0.46  5.85  27.60    San Francisco  3.10  0.00  1.39  0.47  1.61  1.00  3.10  2.24  0.47  4.76  28.20    Seattle  2.50  0.16  1.55  0.52  1.61  1.00  2.50  2.50  0.52  4.80  31.20    Sydney  3.40  0.00  2.06  0.91  1.61  0.89  3.03  2.95  0.81  3.64  48.59    Toronto  4.25  0.14  1.74  0.53  1.61  0.93  3.95  2.61  0.49  5.29  29.57    Vancouver  3.20  1.00  1.85  0.50  1.61  0.93  2.98  2.77  0.47  5.96  27.90    Washington DC  3.00  0.00  0.93  0.26  1.61  1.00  3.00  1.50  0.26  5.76  15.60    Zurich  6.00  0.00  3.80  1.15  1.61  1.10  6.60  6.73  1.27  5.32  75.90    Beijing  13.00  3.00  2.30  0.30  1.61  0.17  2.21  0.63  0.05  12.34  3.06    Shanghai  14.00  0.00  2.40  0.50  1.61  0.17  2.38  0.66  0.09  7.73  5.10    Moscow  245.00  0.00  26.53  14.00  1.61  0.03  7.35  1.28  0.42  3.05  25.20    Bangkok  35.00  0.00  6.00  1.67  1.61  0.03  1.05  0.29  0.05  5.78  3.01    Buenos Aires  1.81  0.00  1.00  0.18  1.61  1.00  1.81  1.61  0.18  9.20  10.50    Cairo  2.50  0.00  1.25  0.28  1.61  0.14  0.35  0.28  0.04  7.19  2.35    Dhaka  250.00  0.00  35.00  4.17  1.61  0.01  3.25  0.73  0.05  13.51  3.25    Istanbul  2.80  0.00  1.73  0.33  1.61  0.46  1.29  1.28  0.15  8.44  9.11    Jakarta  6000.00  0.00  3550.00  500.00  1.61  0.00  0.49  0.47  0.04  11.43  2.46    Lagos  3.32  0.00  3.06  0.16  1.61  1.00  3.32  4.93  0.16  31.58  9.36    Manila  50.00  0.00  13.60  1.75  1.61  0.02  1.10  0.48  0.04  12.51  2.31    Rio de Janeiro  4.70  0.00  1.70  0.37  1.61  0.42  1.97  1.15  0.16  7.38  9.35    Seoul  2800.00  0.00  1050.00  206.00  1.61  0.00  2.63  1.59  0.19  8.21  11.62      Using this information we can run a few interesting analyses:                                                          USD per minute vs USD per mile.  The most obvious check is to see the most and least expensive cities by the two dimensions we have - distance and time. The results are expected - Asian and African cities tend to be the least expensive and European cities being the most expensive. Within Asia there's pretty significant variance with South and Southeast Asia being the cheapest but Seoul and Tokyo being more expensive. A city that stood out was Lagos - it has the one of the lowest per minute fares but one of the largest per mile fares. I don't know why this is the case but I suspect it has something to do with t sure why this is the case other than the roads being in poor condition and the price needing to take that into account.                                                                          Keep time fixed at 10 minutes but vary distance.  The idea here is to look at how quickly the prices increase by distance for different cities. This echoes the previous chart of Europe and Lagos having the highest per mile fares.                                                                          Keep distance fixed at 4 miles but vary time.  Similar to the previous plot but look at the way price will increase as a function of time. Not much new data here.                                                                          What does $10 get you?  Another way to look at expenses is to see the maximum time and distance $10 will get you in different cities. This is similar to looking at the inverse of the per minute and per mile prices.                                                                          What does $10 get you (zoomed)?  This zooms in the bottom left corner of the previous plot. Turns out that having $10 in an expensive city doesn't go very far.                                                                          Ratio of $ per mile vs $ per minute.  The goal was to see how many times a mile was more expensive than a minute for the different cities. The reason we see such high ratios is that the price of gas has a lower variance from city to city than the cost of labor - this leads to cities with low labor casts having significantly higher ratios.                                                                          Heatmap of fares as a function of time and distance.  I wanted this to be a bit more insightful in order to be able to compare all cities against each other across both dimensions but the extreme differences make it difficult to visualize. This highlights once more how expensive Zurich is compared to the other cities. The heatmaps below cluster the cities by the sum of price per mile and price per minute in order to visualize them along similar price scales.                                                                          Heatmap of the fares as a function of time and distance by city (1st quartile).                                                                           Heatmap of the fares as a function of time and distance by city (2ng quartile).                                                                           Heatmap of the fares as a function of time and distance by city (3rd quartile).                                                                           Heatmap of the fares as a function of time and distance by city (4th quartile).",15,3,2014-01-09,3,"taxi prices, visualization, analysis",1707,Taxi prices around the world
30,0,"I read a ton but can't find an ebook reader that I like. The problem is that the creation, distribution, and consumption of content are coupled into closed ecosystems.",#product,"{% include setup %} I'm an avid reader and have embraced the move to digital. An internet connection gives me access to thousands of books with a device that’s thinner than a single book. What I grapple with are the reading apps - I can’t find one that does everything I want.  On my iPad, I have iBooks, Readmill, Oyster, Kindle, and ShuBook with each having a separate use case. iBooks and the Kindle app are for books that I purchased from iTunes and Amazon, respectively. Oyster is a great ebook subscription service but I’m limited to the books available in their library. I discovered ShuBook when I wanted to host my own ebook server but have switched to Readmill due to the much nicer reading experience, a web interface to manage my library, and cross-device syncing.  Ideally, I’d be able to use Readmill for everything. I don’t mind paying for books but I do mind paying to be locked into a particular ecosystem. The creation of content should be decoupled from the distribution of content which should be decoupled from the consumption of content. Yet these days they’re tightly coupled. The only real way to overcome these restrictions is to become a digital pirate. It sucks that customers are forced to break laws in order to get the best experience.",0,1,2014-01-15,3,"ebooks, reading, DRM",222,Ebook readers
22,0,We still view our products as being software but in the future we'll start seeing our products and technology as our equals,#product,"{% include setup %} In 2013, I gave myself a goal of running 1000 miles. I used RunKeeper to record my runs and used its goal feature to track my progress and quickly see how much I had left. Two days before the new year, I was able to hit my goal and got a little notification from RunKeeper congratulating me on achieving my goal. This small notification got me thinking about how emotion is built into our products. RunKeeper doesn’t care whether it was a 1 mile or 1000 mile goal - the reaction I get would be the same. Yet if I shared these two achievements with my friends, the reactions I get would be completely different. Sure, an algorithm could be designed to treat accomplishments of various difficulties differently and can even be adapted to take into account that to some people, running one mile is equivalent to others running 1000 miles.  I doubt a smarter algorithm would actually make a difference. We might appreciate the intelligence of the algorithm but we’re not going to believe that this digital praise was authentic or that our software actually cares. We already have Google Now promising to give us the information we need when we need it and Amazon is trying to ship products to our doors before we even place the order. Yet as smart as these are, they’re not emotional. Even Siri is just an algorithm. As technology gets smarter, I wonder whether future generations will feel this way. We’ll continue to see these improvements as simply smarter software and better data but I doubt future generations will feel the same way. We’ve seen how dumb our technology has been and won’t be able to think of it as anything more than software. Future generations will be born and grow up in a world surrounded by smarter and better versions of what we have and won’t be saddled with this bias. Many believe the singularity will happen in our lifetimes but I think this will have the larger effect - that we’ll start viewing technology as our equal.",0,1,2014-01-19,3,"emotional products, technology, society",350,Emotional products
28,0,Something I've started doing since joining TripleLift is doing the bulk of my developing on a remote EC2 instance and think it offers a ton of benefits.,"#code,#aws","{% include setup %} One of the first things I was given when joining  TripleLift  was a Macbook Air and an Amazon EC2 instance to do my development work on. Before that, every company I worked at would give me a pretty powerful computer so that I’d be able to do my development work locally. At first, coding on a remote instance took some getting used to but now I'm a fan of this approach.    	 It allows me to work from any computer and paired with the highly portable Macbook Air I can work from virtually anywhere. On the flip side, it relies on a connection to the internet so if the internet ever cuts out it’s difficult to do work unless you also have it checked out locally.   	 It’s a great way to simulate a production environment. Especially on OS X where many packages require significant Stack Overflowing to figure out, being able to install libraries that will be used in production is a great way to work out the kinks and be confident that your code will run as expected.   	 Along these lines, the entire team will end up with a very similar environment which makes it very easy to give and receive help without having to get used to an entirely new environment.   	 If your application relies on EC2 it’s a great way to become familiar with the AWS ecosystem as well as reduce latency between Amazon’s various services. This is useful when you have a significant volume of data going to and from S3 and want to make it as quick as possible.   	 There are a ton of tools to make this easy. I’ve recently discovered the  SFTP plugin  for  Sublime Text  which lets you edit your files locally that are automatically synced to the remote instance. That paired with emacs or vim on the instance are all you need.     The biggest drawback is that you end up relying on the internet in order to get the most out of this set up. It’s possible to have your code synced locally for editing but getting set up to run locally defeats the purpose of having a remote instance since you have to install and configure the various packages. Given that there’s internet almost anywhere I go I think this trade off is worth it.",3,2,2014-01-23,5,"developing, coding, EC2, AWS, Amazon",407,Developing on a remote instance
12,0,PostgreSQL is surprisingly good at solving a variety of coding tests.,"#sql,#code","{% include setup %} Most developers are familiar with the FizzBuzz code test which is a quick way to filter out developers who can’t code. At Yodle, we had our own, slightly more challenging problem. The challenge was read in a text file and then print out the frequency each word appears in descending order. It’s more complicated than FizzBuzz but it assesses a variety of skills. The solution needs to do the following:       Read the file     Split the text into a list of words delimited by non-letter characters     Convert each word to lower case     Compute the frequency each word appears     Sort the results in descending order by frequency     Print this sorted list     I thought it would be fun to see if I could do it in PostgreSQL and was surprised by how quick and easy it was. The most challenging part was figuring out how to read the file - after that it was just using a few of the built in functions to clean and organize the text.  {% highlight sql %} DROP TABLE IF EXISTS temp_t; CREATE TABLE temp_t (c text);  COPY temp_t(c) FROM '/tmp/data.txt';  select lower(data.w) as word, count(1) as num_words from (   select regexp_split_to_table((select string_agg(c,' ') from temp_t), E'[^\\w]+') as w ) data where data.w <> '' group by word order by num_words desc, word; {% endhighlight %}  It also turns out to be very simple to do FizzBuzz in PostgreSQL. The nice part of the PostgreSQL solution is that it can easily scale to adding a 3rd combination. for example print Dozz if the number is divisible by 7. In the PostgreSQL solution, it would just require adding a row whereas in the standard solutions it would require a bit of work and would increase the chance of a bug.  {% highlight sql %} DROP TABLE IF EXISTS fizzbuzz; CREATE TABLE fizzbuzz (   num int,   text varchar(4),   priority smallint );  insert into fizzbuzz (num, text, priority) values (3, 'Fizz', 1), (5, 'Buzz', 2);  select coalesce(string_agg(fizzbuzz.text, '' order by fizzbuzz.priority asc), nums.num::text) as text from (   select generate_series(1,100) as num ) nums left join fizzbuzz on nums.num % fizzbuzz.num = 0 group by nums.num order by nums.num asc; {% endhighlight %}  Clearly PostgreSQL isn’t the right tool for every task but it’s surprising how powerful it can be given the right problem. It’s also a great way to think differently about a problem - even if you end up choosing a more standard solution.",0,2,2014-01-25,3,"coding tests, postgresql, fizzbuzz",421,Solving coding tests in PostgreSQL
46,0,"As great as the various blog platforms, such as Medium and Svbtle, are, I manage my own blog. Managing my own blog gives me the control I want and prevents me from having to rely on the provided marketing that I have no control over.","#blog,#meta,#product","{% include setup %} Given the recent news of Medium  raising $25M  and Svbtle  opening up  to the public I thought it would be an appropriate time to explain why I’m not using either of them. They’re both simple, clean products that allow writers to concentrate on their writing rather than configuring the dozens of options available in other blogging platforms. They’ve also done a great job with the typography that makes the content enjoyable to read. Compared to the other content websites out there, they’re incredible fast - they have a minimal structure and don’t load a ton of external content - especially when compared to the major publishers out there now such as the news sites and the social networks.  Yet I’m not writing on either of them, nor on Tumblr, Wordpress, or Quora even though I tried each one. For me, writing is about personal expression and being able to control the entire experience - both from the content generation up to the consumption - is important to me. I realize my design will never be as elegant as theirs but at least I can change it whenever I want. A year ago I wanted to include the D3 library for a  post  - this would have been impossible with Svbtle or Medium and I would have had to use static images. Recently I wanted to share  some charts  that I generated but on the first pass I realized they were too large for the content - with a few small tweaks to my theme I was able to incorporate them into my blog. I’ve also been thinking about using Mixpanel to track various events - something I’d never be able to do without full control.  The custom design is part of it but the other value lies in decoupling. I want to be able to decouple the creation of content from the presentation of content from the spreading of content. As an engineer, I like the fact that I’m not tied down to any platform - I know I can get additional pageviews by leveraging the built-in marketing networks these platforms provide but having marketing integrated into a creation tool feels dirty. I’d rather rely on Twitter and Hacker News to share my content. Sure it’s more difficult but it’s a more lasting way to get followers and readers for your content rather than the platform itself.  By being independent, I never have to think about how these platforms evolve and what the impact will be. They’ll have to monetize at some point and I don’t want to worry about that outcome. We’re already seeing massive changes in the way content is produced and consumed and being able to experiment with various approaches and technologies is important - especially for someone in technology. Relying on a third party that’s trying to do too much betrays that.  PS - I just realized I never mentioned how I host my blog. It’s currently hosted on Github pages using the  Jekyll-Bootstrap  plugin. At the moment, it gives me the control I want, deals with usage spikes, and is free. If anything ever changes, I can quickly pull everything down and host it on my own.",5,3,2014-02-02,6,"medium, svbtle, blogging, github pages, content, creation",562,Why I manage my own blog
33,0,I found Nathan Yau's R script that plots GPS data and made a few changes to it to add a map overlay and the ability to focus in on a particular area.,"#dataviz,#code,#R","{% include setup %} Earlier today I read Nathan Yau’s  post  that had a quick  R script  to plot GPX file data onto a map. I was able to quickly load up my RunKeeper data from 2013 and came up with a pretty cool visualization of each of my outdoor runs. Since my runs occurred across multiple cities and continents the visualization turned out to be very sparse without a great sense of where the runs were. I made a two quick changes to the script to make it more useful for my data: a map overlay to see where in the world I ran and an ability to view a zoomed in area of the map. I’ve included the updated script and the resulting plots below.  {% highlight r %} # From Nathan's script library(plotKML) library(ggplot2) library(maps)  # GPX files downloaded from Runkeeper files   lat_south & routes$long > long_west & routes$long                                                        A few specks here and there - clearly visible runs in the NY/NJ area as well as some in Virginia, New Orleans, and San Francisco. Can also see a few runs in India.                                                                         Zoom in on my runs in the Hoboken/NYC area. I don't have the lat/long coordinates here but if I had them it would be pretty easy to generate a map overlay.",4,3,2014-02-05,6,"running, visualization, analysis, gps, gpx, runkeeper",513,Visualizing GPS data in R
22,0,Travel is a great way to improve creativity and companies should be encouraging it. Instead many are trying to restricti t.,#meta,"{% include setup %} Now that I’ve started blogging I realize how important traveling is to creativity. After my trip to India I had a ton of different blog ideas. Some came from comparing the two cultures - for example cab rides and mobile phone business while others just came from realizations, such as the lack of truly global technology products. Many dismiss travel as a luxury but it’s a great way to bring a new perspective and let thoughts settle. In my case, it felt as if these connections formed subconsciously based on what I’ve been thinking about and doing actively for a year. It’s not surprising that our conscious experiences drive these subconscious connections but it’s interesting how stark this realization was. Prior to blogging, I never would have had an idea and immediately think of writing about it but it’s become a consistent thought. Travel encourages this serendipitous thought and companies should be encouraging it. Instead, many black ball employees who take a vacation and make employees feel guilty for taking some time off.",0,1,2014-02-09,3,"travel, vacation, creativity",177,Travel more
33,0,"virtualenv is great but it requires a bit of work to get it running for various services. Below are the ways I've gotten it working with Nginx, Gunicorn, Supervisor, Celery, and Fabric.","#python,#code","{% include setup %} One of my favorite things about Python is being able to use  virtualenv  to create isolated environments. It’s extremely simple to use and allows you to have different versions of Python libraries used by different projects.  The thing that's tricky is getting virtualenv set up on a production environment under different services since each one requires a slightly different configuration. I’ve gone through my projects and collected the various ways I’ve gotten it running for different services. I’m sure I could have done it differently but the following worked for me and will hopefully come in handy to others. If you have any questions or I'm not being clear enough let me know and I'll updat the post with more information.       Nginx and Gunicorn under Supervisor.      Nginx  - The configuration isn't anything different than normal except that you may need to specify some specific paths that are within your virtualenv  {% highlight nginx %}   Static files needs to point to virtualenv directory location /static/admin {   autoindex on;   root   /home/ubuntu/app/venv/lib/python2.7/site-packages/django/contrib/admin/; } {% endhighlight nginx %}      Gunicorn  - I have a shell script here that's used to set the various paths and options that configure Gunicorn  {% highlight bash %} #!/bin/bash set -e DJANGODIR=/home/ubuntu/app DJANGO_SETTINGS_MODULE=app.settings.prod  LOGFILE=/var/log/gunicorn/guni-app.log LOGDIR=$(dirname $LOGFILE) NUM_WORKERS=2 # user/group to run as USER=ubuntu GROUP=ubuntu cd /home/ubuntu/app source /home/ubuntu/app/venv/bin/activate  export DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE export PYTHONPATH=$DJANGODIR:$PYTHONPATH  test -d $LOGDIR || mkdir -p $LOGDIR exec /home/ubuntu/app/venv/bin/gunicorn_django -w $NUM_WORKERS \   --user=$USER --group=$GROUP --log-level=debug \   --log-file=$LOGFILE -b 0.0.0.0:8000 2>>$LOGFILE {% endhighlight bash %}      Supevisor  - Here we just point our configuration file to the shell script for Gunicorn  {% highlight ini %} [program:gunicorn-myapp] directory = /home/ubuntu/myapp user = ubuntu command = /home/ubuntu/myapp/scripts/start.sh stdout_logfile = /var/log/gunicorn/myapp-std.log stderr_logfile = /var/log/gunicorn/myapp-err.log {% endhighlight ini %}        Celery  under Supervisor.   In this case we just configure Supervisor to start virtualenv path for celery. A cool feature is being able to specify the environment variables - in my case to pass in the Django settings module.   {% highlight ini %} [program:celery] ; Set full path to celery program if using virtualenv command=/home/ubuntu/myapp/venv/bin/celery worker -A myapp --loglevel=INFO  directory=/home/ubuntu/myapp user=nobody numprocs=1 stdout_logfile=/var/log/celery/worker.log stderr_logfile=/var/log/celery/worker.log autostart=true autorestart=true startsecs=10  environment =   DJANGO_SETTINGS_MODULE=myapp.settings.prod {% endhighlight ini %}      Fabric .   The idea here is to make sure all our remote install commands are run after activiating the virtualenv.   {% highlight python %} from __future__ import with_statement from fabric.api import * from contextlib import contextmanager as _contextmanager  env.activate = 'source /home/ubuntu/myapp/venv/bin/activate' env.directory = '/home/ubuntu/myapp'  @_contextmanager def virtualenv():     with cd(env.directory):         with prefix(env.activate):             yield  @hosts(env.roledefs['db']) def rebuild_index():     with virtualenv():         run(""python manage.py rebuild_index"") {% endhighlight python %}",6,2,2014-02-10,7,"virtualenv, python, nginx, gunicorn, supervisor, celery, fabric",480,Using virtualenv in production
30,0,I recently caught myself using Foursquare as a contact book when I couldn't recall someone's name or company but did remember that they like using Foursquare to check in.,#product,"{% include setup %} A few days ago I discovered a new use case for Foursquare when I was meeting up with a friend. We were catching up and during the course of the conversation I realized I needed to introduce him to someone I had met earlier. Unfortunately, I completely blanked on his name and company. All I recalled was that he frequently checked into his company on Foursquare. Sure enough, when I opened up Foursquare I saw that he had checked in there that morning.  Clearly Foursquare isn’t a contact book app but it provided enough adjacent information and triggered enough thoughts to come in handy. It’s not surprising that we use apps and products in unintended ways but it’s always great being able to catch ourselves in the act.",0,1,2014-02-13,3,"foursquare, product design, product management",133,Foursquare as a contact book
36,0,Sometimes it's tough deciding whether you should use a heuristic or algorithmic approach. I tend to favor heuristic ones for quick and dirty projects but will opt for an algorithmic one for more complicated work.,#datascience,"{% include setup %} Something that’s come up frequently in my quantitative work is balancing heuristic and algorithmic approaches. It’s surprisingly difficult to get the first attempt at an algorithmic approach working properly - it’s not an academic exercise and real world issues will always appear. Over time I’ve found myself writing heuristic checks and tweaks to deal with the various edge cases the algorithmic approach encounters. For example, setting the min and max bounds on the results of a function or adjusting the slope of a curve if it ends up being set in the wrong direction.  It makes me wonder why I didn’t just start with a heuristic approach and worked on an algorithmic approach later after I’ve collected enough data and had a better understanding of the environment. The challenge is that a heuristic approach is only a temporary solution. It will be be difficult to maintain and improving it will require additional hacks and tweaks. A heuristic approach is great at setting a quick baseline but long term improvement will only come from a more rigorous approach.  An example would be writing an algorithm that bids on Google Adwords. A heuristic approach would take yesterday’s bids on a set of keywords, look at their performance, and adjust them or down based on a few simple rules. A simple heuristic might be to allocate budgets to different keywords based on their conversion rates. Unfortunately, this wouldn’t handle the case of different keywords having different costs and volume. Incorporating these would require additional rules and introduce more complexity.  An algorithmic approach would be to model the relationship between cost, impressions, and click through rates for each keyword and then optimize for total conversions. Each model would be designed to predict a dependent variable based on a set of independent variables and would require a statistical approach to make sure the results were statistically significant and safe to use. Each model would require its own research and set of tests but would lead to a more scalable system. Since the models would be independent of one another, you’d be able to improve them individually. It may turn out that our cost calculation model is great but the one that estimates impressions needs more improvement. Now you can focus on the model that needs the most work rather than trying to globally optimize the whole system.  As with anything, these are tools and their usage depends on the situation. It’s difficult to come up with a rule of when to use one over the other but I tend to favor heuristics when I need to do something quick and know it’s not going to require significant changes. If it’s a complicated problem that will require ongoing work, I’ll opt for the more rigorous, algorithmic approach. It will take more work initially but will be better in the long term.",0,1,2014-02-15,4,"data science, algorithms, heuristic, algorithms",479,Heuristic vs algorithmic approaches
16,0,There's a neat iterative algorithm to generate a Sierpinski triangle that I implemented in D3/JavaScript.,"#dataviz,#javascript,#code","{% include setup %}  There's a little known algorithm for constructing a  Sierpinski triangle  that is surprisingly easy to implement.  1. Start the three vertices that form a triangle 2. Pick a random point inside the triangle 3. Pick a random vertex 4. Go halfway from a the random point to the vertex and mark that point 5. Go to step 3 using the result of 4 as the starting point  I'm trying to get better at D3 and thought it would be a good exercise to code it up. The resulting image is below (generated using 10,000 points) and the JavaScript is in the following  file . Next up is to write a new script that allows a user to specify the number of vertices and the adjustment factor - the  Sierpinski carpet  can be generated with 4 vertices and a distance adjustment factor of a third rather than a half.       {% include D3 %}  {% include custom_js %}",3,3,2014-02-19,4,"sierpinski triangle, D3, visualization, JavaScript",175,Sierpinski triangle in D3
14,0,A quick JavaScript that allows you to specify your own Sierpinksi generation strategy.,"#dataviz,#javascript,#code","{% include setup %}  As a follow up to my previous  post , I modified my Sierpinski generation  code  to allow specifying the number of sides and the distance ratio for each iteration of the loop. The Sierpinski triangle can be generated with 3 sides and a distance ratio of 0.5. Increasing the number of sides and decreasing the ratio leads to some interesting patterns - it looks as if for a given N, we get N shapes each consisting of N shapes. I suspect this is a fractal pattern - similar to the triangle - but it's difficult to confirm given a fixed screen resolution. I'd love to know what's going on here and whether there's a relationship between the number of sides and the distance ratio.                                       N = 4, ratio = 0.4                                                      N = 10, ratio = 0.2                                 # of sides                                    Distance ratio                                            Generate!                    {% include D3 %}  {% include custom_js %}",2,3,2014-02-21,3,"Sierpinksi, gasket, triangle",290,More Sierpinski fun
14,0,I just finished tagging my posts. It's not perfect but it's a start.,"#blog,#meta",{% include setup %} Turns out that tagging and categorizing blog posts is more difficult than I thought. I start with one set of tags but as I go through my posts I realize that my initial set of tags no longer make sense and I need to restart. The challenge is finding the set of tags that are specific enough to categorize a single post yet general enough that they can be applied to other ones. I haven’t found the perfect set of tags yet but did manage to go through and tag each of  my posts . Over time I hope to improve the tag taxonomy and update the existing posts. I’d love to hear suggestions on how to effectively organize my posts and examples of other blogs that are doing this well.,1,2,2014-02-26,2,"blog, tags",140,Blog posts are now tagged
26,0,Offering discounts is dangerous since consumers may start associating your brand with discounts. It can also lead to an entire industry being driven by discounts.,#pricing,"{% include setup %} Nearly every week I receive an $8 off $25 coupon from delivery.com. I’m sure the intent is to generate awareness and develop a habit but it’s having the opposite effect on me: I’m being trained to only order when I have a coupon. Couponing is tough - too little and it will have no impact but too much and you run the risk of training your customers to only react to deals which will result in you needing to have higher prices to maintain your margin.  I recall reading that the retail clothing industry is essentially sales and coupons. Since people are trained to only buy on sales, retailers will set an initial high price and use discounts to drop it to something that will appeal to consumers. No wonder clothing retailers have sales practically every week and definitely every holiday. Ron Johnson tried changing this when he became the CEO of JCPenney but wasn’t able to do it before his ouster. I suspect even if he had more time he wouldn’t have been able to do it without the support of other retailers. Even then, each would have a strong incentive to deviate, a la  Prisoner’s Dilemma , so they might just end up exactly where they started.  This begs the question of why other industries haven’t embraced the discounting model. My guess is that it would take a significant amount of effort to change consumer perception that a single company wouldn’t be able to apply and it’s just too complicated to orchestrate - especially when the payoffs are uncertain since competitors can quickly move to this model as well.",1,1,2014-03-02,4,"discounts, marketing, sales, retail",277,Trained for discounts
28,0,I received an email from imo.im earlier this week indicating that they will stop supporting 3rd party chat clients. Amazing what the 19B WhatsApp acquisition did.,#product,"{% include setup %}  Earlier this week I received the following email from imo.im:      It’s amazing what $19 billion can do. For years imo.im has supported third party chat clients but within a couple of weeks of the WhatsApp acquisition they’ve abandoned that support to focus on their own network and become the next WhatsApp. For a while now they’ve been building features to support this move - videos in August, stickers in January - and I wonder what would have happened if they focused on their platform earlier. Now they’re just playing catch up to WhatsApp, Kik, Line, and countless others. Timing is critical and I suspect it’s too late for imo.im to be entering the first party messaging fray.",0,1,2014-03-06,3,"imo.im, whatsapp, messaging",135,Follow the WhatsApp money
36,0,Following up to my Drowning in JavaScript post I take a look at the top 100 Alexa sites and see how many external files they're loading and the impact that has on page load time.,"#datascience,#dataviz","{% include setup %} Since writing the  Drowning in JavaScript  post I’ve been meaning to take a stab at automating that analysis and seeing if I could generate some other insights. This weekend I finally got around to writing a quick PhantomJS script to load the top 100 Alexa sites and capture each of the linked resources as well as their type. The resulting data set contains the time it took the entire page to load as well as the content type for each of the linked files. After loading these two datasets into R and doing a few simple transformations we can get some interesting results.                                                          Average load time.  To get a general sense of the data this plots the average time it took to load each URL. The interesting piece here is that multiple foreign sites take a while to load (163.com, qq.com, gmw.cn, ..) - I suspect a big reason is that there's quite a bit of latency since I'm based in the US. Another observation is that many news sites tended to load slowly (huffingpost.com, cnn.com, cnet.com). The Google sites loaded extremely quickly (all less than 1 sec) as did Wikipedia.                                                                          Load time boxplot.  This provides a bit more information on the load times by showing the min/max values as well as the median and the percentiles. Not a significant amount of new insight here.                                                                          Number of requests.  Huge variance here as well - rakuten.co.jp loaded almost 800 external files while the Google sites are all less than 10.                                                                          Number of request vs time to load.  This leads to the question of whether sites that are loading more files take a longer time to load. By plotting a scatter plot between the two it's pretty clear that all things being equal more files increase page load time.                                                                          Number of requests vs time to load linear fit.  A simple regression of load time as a function of the number of file requets confirms this. On average, each additional file leads to an additional 20 milliseconds of load.                                                                          File type frequency.  We can also take a look at the most common type of file requested. As expected, images are the majority of requests followed by JavaScript.                                                                          File types by url.  Not a lot of insight here but the colors sure are pretty. One thing that does standout is that if a site has a significant amount of file requests they tend to be of multiple types.                                                                          File type correlation.  We can plot a simple correlation of file type found on a page to see whether there are any file types that tend to be included together. Not much going on here.                                                                          Multiple linear regression.  And just for fun we can run a regression to see whether a particular file type leads to significantly worse load than others. I was hoping to show that having a lot of JavaScript hurts performance but that doesn't seem to be the case. I suspect it's due to the innate time differences it takes to load some sites (in particular sites outside the US) vs others.                      As usual, the code’s up on  GitHub .",11,2,2014-03-09,4,"site speed, javascript, phantomjs, data analysis",842,Examining the requests made by the top 100 sites
28,0,Website loading times are drastically different between NYC and Beijing - as expected the sites in China load faster in Beijing but slower in NY and vice versa.,"#datascience,#dataviz","{% include setup %} Over the weekend I wrote a quick script to crawl the top 100 Alexa sites and  compare them  against one another in terms of load times and resources being loaded. I shared my code on GitHub and earlier today I got a great pull request from  rahimnathwani  who ran the script in Beijing, using home ADSL, and wanted to share his dataset.  I suspected that that many sites were loading slowly for me due to my geographical distance from them and with this dataset we’re able to compare the load times between NYC and Beijing for these sites. Unsurprisingly, most sites in Asia do load faster in Beijing but the average load time is much longer, 3.4 seconds in NYC vs 11 seconds in Beijing. A surprise was how slowly rakuten.co.jp loaded in Beijing - over 50 seconds on average and I suspect this is due to the huge number of images being loaded. I suspect internet speeds also played a part in the differences here so this isn’t a perfect comparison.  Below are some visualizations highlighting the differences in a few different ways. I’d love to get my hands on more data so if fee free to submit a  pull request  with your data and I’ll rerun the analysis. I’ve also included the R code that generate the plots below for those curious to see how they were done.                                                          Parallel plot.  The idea here is to see whether the lines are mostly horizontal or if they're steep. Horizontal lines would indicate that sites are universally slow (and fast) while steep lines indicate that some sites load much faster in one city compared to the other.                                                                          Load time differences.  Here we sort the sites by the difference in average load time, NYC minus Beijing. Most of the sites loaded faster in NYC but I suspect the biggest reason was due to internet speed differences. The sites that loaded faster in Beijing are for the most part in China.                                                                          Scatter plot.  A different perspective than the parallel plot but trying to answer the same question. We do notice a few outliers here which we can investigate by adding text labels.                                                                          Labeled scatter plot.  This provides a nice look at the outliers but makes it impossible to look at the sites that loaded quickly in both NYC and Beijing.                      {% highlight r %}times <- read.csv(""out-times-beijing.csv"", sep=""\t"", col.names=c(""url"", ""time"")) times$url <- as.character(times$url) final <- ddply(times,~url,summarise,mean_time_beijing=mean(time),sd_time_beijing=sd(time))  times2 <- read.csv(""out-times.csv"", sep=""\t"", col.names=c(""url"", ""time"")) times2$url <- as.character(times2$url) final2 <- ddply(times2,~url,summarise,mean_time_nyc=mean(time),sd_time_nyc=sd(time))  combined <- merge(final,final2,by=""url"") combined$time_diff <- combined$mean_time_nyc - combined$mean_time_beijing combined.m <- melt(combined, id.vars=c('url'), measure.vars=c('mean_time_beijing', 'mean_time_nyc'))  png('crawl-stats-comparison-parallel.png', width=600, height=600) ggplot(combined.m) +   geom_line(aes(x = variable, y = value, group = url)) +   theme_tufte() +   ylab(""Load Time (ms)"") + xlab("""") dev.off()  png('crawl-stats-comparison-time-diff-bar.png', width=600, height=600) ggplot(combined, aes(x=reorder(url, -time_diff), y=time_diff)) +   geom_bar() +   theme_tufte() +   coord_flip() +   xlab(""Load Time Diff (ms)"") +   ylab(""URL"") dev.off()  png('crawl-stats-comparison-scatter.png', width=600, height=600) ggplot(combined, aes(x=mean_time_beijing, y=mean_time_nyc)) +   geom_point() +   theme_tufte() +   xlab(""Beijing Load Time (ms)"") +   ylab(""NYC Load Time (ms)"") dev.off()  png('crawl-stats-comparison-scatter-text.png', width=600, height=600) ggplot(combined, aes(x=mean_time_beijing, y=mean_time_nyc)) +   geom_text(aes(label=url), size=3) +   theme_tufte() +   xlab(""Beijing Load Time (ms)"") +   ylab(""NYC Load Time (ms)"") dev.off() {% endhighlight %}",7,2,2014-03-11,5,"site speed, web performance, nyc, beijing, data analysis",724,Website load times: NYC vs Beijing
16,0,Crawling the Oyster books web API and having some fun with the resulting data set.,"#datascience,#dataviz","{% include setup %} I’m an avid reader and signed up for  Oyster  as soon as I discovered them. Since then, every time I wanted to read a new book my first step has been to check Oyster. If the book wasn’t available I’d get it the old fashioned way and read it via Readmill, another great app.  One feature I wish Oyster had was the ability to see the overlap between their available collection and what I had in my “to read” list. The only way to do this now is to go through my list one book at a time and then search for it using the Oyster iOS app since the search functionality isn’t available via the web. Being lazy, I really didn’t want to do this and started searching for a quicker way. By browsing their website and looking at the network requests in Chrome I noticed two interesting API calls being made - one to get the book “sets” and another to get the books with a set.  These API endpoints turned out to be publicly accessible and it only took a short Python  script  to retrieve the books and dump them into a CSV file. This got me a little less than 3,000 books - turns out that the publicly accessible data is only a fraction of the entire collection and my endeavour wasn’t as fruitful as hoped.  I did manage to get a set of over 4,000 books and decided to have fun with it.                                                          Num of authors by num of books written.  Very few others appear more than once in the data set. This may be due to the limited data set or Oyster's job in editing the publicly accessible collections, maybe both.                                                                          Distribution of ratings.  Ratings are clustered around 4 with very few ratings under 3. This is most likely a biased set since the Oyster editors would have chosen the highest rated books to be featured in their sets.                                                                          Num of books by author.  Kurt Vonnegut has over 20 books available on Oyster with Shakespeare in the number 2 spot.                                                                          Ratings by author box plot.  Just a quick box plot to see the rating distribution by author.                                                                          Rating vs # of books.  Doesn't look as if the # of books an author has written on Oyster has any relationship with their rating. I thought maybe authors with higher average ratings would appear more frequently.                                                                          Rating over time by author.  This was a reach but I wanted to see whether an author was most likely to have better ratings earlier or later in his or her career. In this case it looks as if the publish date isn't the original authorship date so not a very useful analysis.                                                                          Publisher ratings.  Similar to authors, we can take a look to see whether some publishers have significantl higher ratings than others. This is a bit more useful since there's a lot more data per publisher than there is per author. I couldn't make much sense of the results here.                                                                          Avg number of pages by decade.  I wanted to see whether books were getting longer or shorter so did a quick plot of the average number of published pages by decade since the year was too fine. The publish dates aren't entirely accurate so I wouldn't read too much into this.                                                                          Avg rating by decade.  Similar to the previous plot but looking at the average rating rather than the number of pages. Seems to be pretty steady to me although this may be due to the dataset being a curated list of top books.                                                                          Rating vs date.  Another way to look at the previous plot but plotting each book rather than the average by decade. Not much going on here although this may be due to the biased dataset and flawed publish dates.                                                                          Rating vs number of pages.  This is an interesting one - are longer books more popular? Most of the books are clustered around a couple of hundred pages but longer books do tend to have a higher average rating. I'm not sure why this would be the case but would guess that only someone who's already interested in a long book would read it or stick with it enough to leave a review.                      As usual, the code's up on  GitHub .",14,2,2014-03-16,3,"oyster, dataviz, api",1128,Fun with the Oyster books API
25,0,I moved away from Gmail to Fastmail and couldn't be happier. Gmail just got to be a big pain in the ass to use.,#product,"{% include setup %}        Over the course of the past year I’ve become more and more pissed off at Gmail. I loved using Gmail when it launched - it made writing and reading email a pleasure. It was simple, clean, and responsive. Now it’s the opposite. All actions feel slow. The initial page load takes a substantial amount of time and then I get to wait for the various page elements to load - including a chat list that I’m almost never signed into and integration with a slew of other Google products. Loading emails or new tabs is noticeably slow and the search is sluggish for a company whose main product is a search engine.  This past weekend I decided to see what was out there and discovered  Fastmail . I’ve been using over the past few days and it’s been great. Emails are quick to load and send and the navigation feels snappy and responsive. I’m not sure how well it will work as my inbox grows but so far I’m impressed. It feels like Gmail when it launched almost 10 years ago. It feels odd to describe it in terms of Gmail since I’m bashing it but I can’t think of a better way.  I did a quick anecdotal test by looking at the networks tab in Google chrome as each loaded. Gmail loaded in 6 seconds after making nearly 150 requests and retrieving 338 kb while Fastmail loaded in a little over 300 milliseconds after making 18 requests and retrieving 128 kb. Repeating this a few times showed similar results. Others seem to be having the same issue since Google’s first auto-suggestion is “why is gmail so slow” when typing in “why is gmail”. The switch is also much simpler than I expected - I just have Gmail forwarding everything to my Fastmail account and it’s completely transparent to the outside world. In the future I plan on migrating everything to use my new email address but for now this is a good intermediate step. I haven’t heard much from others moving away from Gmail so I’d love to hear your experiences if you made the switch.",1,1,2014-03-18,2,"fastmail, gmail",409,Goodbye Gmail
18,0,There's a pretty cool visualization that shows your browsing history through the sites' favicons. Interest patterns emerge.,#dataviz,"{% include setup %} I came across a neat Chrome extension called [Iconic History](http://shan-huang.com/browserdatavis/) that generates a history of your browsing history through favicons. The value of a good visualization is that it’s able to quickly provide a new perspective to something that seemed mundane and forgotten. I’ve looked at my browser history numerous times and but never thought much of it until I looked at the pattern of icons. It’s obvious that my usage occurs in bursts - I will go through multiple emails when going through my inbox or refining a search. My usage has also changed since I stopped using Gmail for my personal email and started using Fastmail. There’s the occasional new site but for the most part I’m a creature of habits - email, search, facebook, and Hacker News constitute the bulk of my internet activity. I’m honestly surprised by how much activity is taken up by a few sites. I suspect most people are similar - a few sites make up the majority of the page views. It would be great to see what this looks like for others and see if any general patterns emerge - I’m sure almost everyone people will have some mix of email, search, and Facebook but I’m curious to see what the outliers are.",2,1,2014-03-25,3,"data visualization, browser history, icons",234,Visualizing my browsing history
8,0,A few interesting and amusing technology anecdotes.,#meta,"{% include setup %} I’ve always been interested in hacker lore and have recently started compiling a list of tech-related stories and anecdotes that I found amusing. My ideal story includes an odd, somewhat ridiculous, situation that required a bit of technical ingenuity to solve while highlighting an arcane corner case and providing some glee.  So far, I’ve only been able to recall and find two such anecdotes but will add more as I discover them. Depending on how many I gather I may put together a permament list page. If you have any to contribute let me know and I’ll add them to this post.  The two so far:        Print this file, your printer will jam       The case of the 500-mile email",2,1,2014-03-28,2,"hacker lore, stories",137,Hacker lore
41,0,People will either argue that AWS is more expensive than the alternative or worth it due the ability to scale. The real value is in the options available and being able to use them to build the system you want.,#aws,"{% include setup %}      Every time Amazon announces a price drop there are always people pointing out that it’s still more expensive than other cloud computing services such as Linode or Digital Ocean. The Amazon fans then respond by saying sure AWS is more expensive but the value is the ability to scale quickly when needed.  For me, the biggest value behind AWS is the ecosystem and the included optionality. When building large scale web services it’s tough to know every issue you will run into and more often than not your needs and implementation will change. AWS provides a ton of available tools that make growing and scaling easier beyond the hardware itself. You may start with using EC2 for your server and S3 for hosting your static assets but over time you may start using Cloudfront as a CDN and Redshift for your analytics and EMR to process your various logs. That’s the biggest value in AWS - not being able to launch new machines quickly but having a set of infrastructure options that can be specialized to fit your needs.  It used to be that the physical hardware was orders of magnitude more expensive than engineers but this hasn’t been true for decades now - it’s perfectly reasonable to look for ways to reduce yours costs especially if it can be done quickly but obsessing over hardware costs, especially while you’re still growing, is a red herring. Building large systems is tough and the fewer things you have to worry about the better - using AWS reduces the chance that you will run into a scenario where you’re just not able to do something without changing your host and rewriting your architecture.",0,1,2014-03-30,3,"AWS, cloud computing, infrastucture",296,AWS is about infrastructure optionality
23,0,It's possible to use PostgreSQL to define recursive relationships. In this case we define a Fibonacci number generator using a PostgreSQL query.,#code,"{% include setup %} Earlier today I was researching whether it was possible to generate Fibonacci numbers using a SQL query. A Google search turned up a  short PostgreSQL  query that uses a recursive approach. Since this is recursion, the query starts by defining a base case and then goes on to define a generation step with a stopping limit.  {% highlight sql %}with recursive f as (     select 0 as a, 1 as b     union all     select b as a, a+b from f where a < 100000 ) select a from f {% endhighlight %}  It’s interesting to see the edge features of a language and I find that query languages tend to have the most striking ones. My experience with the various SQLs has been that at the basic level they’re very similar but diverge significantly at the edges.",1,1,2014-04-04,2,"postgresql, fibonacci",151,PostgreSQL Fibonacci
27,0,Readmill announced that they were shutting down and it got me thinking about third party apps and what it would look like to host them locally.,#product,"{% include setup %} It never bothered me when apps were acquired and shut down but the  Readmill news  hit me hard. It was one of the truly “free” ebook readers and never got in my way. It fit my behavior perfectly - I would download my books from wherever, drag them into the Readmill web app, and have them permanently accessible on my iPad after a quick sync.  My first reaction was wishing that it would be open sourced but that got me thinking about third party services. Numerous people have been saying how dangerous it is to rely on third party services but until Readmill it never really mattered to me. Sure, in the abstract it’s better to have everything hosted on your own but in reality it’s impossible to get to the same level of quality and experience for everything we use. We’re constantly balancing tradeoffs and we’re biased to favor the short term factors, such as ease of use and simplicity of set up, rather than long term ones, such as privacy, control, and data ownership.  I did some research on self hosting and came across  Sandstorm  - it’s pitched as a “personal cloud platform” and seems to be the solution to this reliance on third party apps. The idea behind it is that you have your own server and can download and install various cloud apps that will then have access to whatever data you give them. I’m eager to try this out. In the case of a Readmill replacement - I’d love to be able to host something on my own server to act as the backend and then download an iPad app that can connect to it. Both the iPad app and server can still be updated as new versions are rolled out but there’s no risk of the apps being shut down.  The business model would resemble Wordpress. The technology itself would be open source but if someone doesn’t want to run their own server they can pay to have their apps hosted somewhere else. There’s also room for a marketplace of premium or specialty apps that can be sold similar to the way themes and plugins are sold for Wordpress. People are already buying apps on the various app stores - it’s not a big leap to imagine people purchasing apps for their personal servers.",2,1,2014-04-07,3,"cloud, hosting, personal cloud",397,Self hosted services
35,0,"I got lucky with my computer experience. I grew up with computers, saw the internet as a teen, and got a smartphone after college. I wonder whether future generations will have the same experience.",#meta,"{% include setup %} I’ve been thinking about my history with computers and the impact they’ve had on me. I grew up just as computers were becoming mainstream, the spread of the internet coincided with my teens, saw the rise of “Web 2.0” during college, and got my first smart phone a few years after college. It’s fascinating to think about how much has happened to the world since the rise of computers and the varying experiences everyone’s had.  Nearly everyone has experienced the internet but at completely different points. Some experienced it when it was just text and had to use Archie, Gopher, and telnet to discover and consume content. Others joined through the AOL floppy discs and had to get multiple land lines in order to connect over dial up. Others avoided it for as long as possible but got dragged in when joining an office. And others are only getting seeing it now due to the spread of smartphones. Age is a huge part of the experience too. First using the internet as a child is different than using it as an adult. We have our own experiences that affect our interaction and dictate the experience we’ll have.  Each experience comes with its pros and cons but I’m happy where my experience fell on the spectrum. I got to deal with the joys of DOS, floppy disks and 16 colors and was able to experience the early days of the internet with a 14.4k modem and Lycos. I do wish I could see what it was like to code in the 70s and 80s when the engineering world was much smaller and one had to deal with a ton of constraints that we currently take for granted.  I wonder whether future generations will have similar experiences to what I had or whether technological advances will either be too predictable and make change appear gradual or not significant enough to warrant attention. I believe I got lucky since so many of the advances were consumer oriented and pro-hacker. Hopefully that the future brings more of the same.",0,1,2014-04-12,3,"computers, history, technology",353,My computer experience
17,0,I have a bunch of memories of growing up with computers and wanted to share them.,#meta,"{% include setup %} Inspired by yesterday’s post I decided to compile a list the memories I’ve had growing up with computers - hopefully they spark others. I tried to keep these in chronological order but I made some mistakes.  - The increase in the number of colors on a monitor and the various *GAs (CGA, EGA, VGA, XVGA) - Booting of one floppy disk and then running programs off another - Having both a 3.5 inch and a 5.25 inch floppy disk drives - Playing Prince of Persia for the first time - Using Norton Commander rather than the DOS command line - Shareware and the tons of paper catalogues selling games by mail - Upgrading to a 486 DX2 - Installing Windows 3.1 from a ton of floppy disks - The wonderful blue screen of death - Getting a second phone line in order to use dial up - Getting AOL instant messenger and my first screenname - Upgrading to a 56k modem - Using NetZero as an ISP - Hosting my first website at Geocities (wish I knew where it was and could dig it up) - Lycos and AltaVista - Finally getting a cable modem - Learning Pascal and C++ in high school - Warcraft 2, Starcraft, Diablo, Total Annihilation, Shattered Galaxy and LAN parties - Using Google for the first time - Signing up for the Gmail beta - Getting my first smartphone, a Motorola Droid I  Since then, I’ve had a ton of experiences but they feel incremental. I guess being steeped in tech for so long gives that perception.",0,1,2014-04-13,3,"computers, technology, history",248,Some computer memories
36,0,We tend to split apps into frontend and backend. It may make more sense to think about our apps in terms of functionality which may lead to a tight coupling between the frontend and backend.,#code,"{% include setup %} Lately, I’ve been thinking about tightly coupled systems and how prevalent JavaScript has become on the web.  Tightly coupled systems scare me. They will undoubtedly break and bring down big chunks of your infrastructure. The solution is to think about your system in terms of various independent services that are responsible for only doing a few things well that won’t bring down the rest of the system if they fail. This approach makes it easier to maintain your code as it grows and also reduces the risk of massive failure. The challenge is figuring out how to break your project down into these services and being sure to revisit that decision as you grow.  JavaScript is pervasive in the modern web. I’ve been using Ghostery for the past couple of months and am constantly amazed by how many external JavaScript libraries are loaded on popular sites. It’s not surprising to see dozens of libraries being included and evaluated. They range from advertising, to tracking, to adding functionality, and it’s incredibly rare to see just one.  On the surface, these two thoughts are different but their intersection is interesting. Similar to the way these sites include additional functionality by loading external scripts, we can compartmentalize various functionality into standalone components and make them available to our applications via simple APIs. In the case of a web app, we’d expose functionality through client side JavaScript libraries that would be coupled with a backend that does the heavy lifting. Rather than slicing horizontally, which is what typical apps do by having a separate UI and a s separate API, we can learn from these external libraries and slice vertically by function.  We integrate tools such as Google Analytics, Stripe, Disqus and MixPanel into our apps without a second thought and we should strive to write our code the same way. This allows us to choose the right tool for the job. If it’s a simple, low volume API that will be used internally, go ahead and do it quickly in a scripting language such as Python, Ruby, or PHP. If, on the other hand, the service will get a ton of requests, you can implement it in Node. In the extreme case of a site that’s using a ton of content, it may make sense to have the content hosted on S3 and just being retrieved by JavaScript called from the client - then the backend can be solely dedicated to providing the dynamic functionality.  This is a pretty extreme approach with it’s own set of challenges. It will definitely require more thought up front on how you want your application to work and will require a different approach than we’re used to but I feel this is the right approach if you’re building for scale. Every application should be broken down into components to see which would benefit from different approaches. If it turns out that two components have drastically different requirements, it might make sense to build them as completely standalone services and only communicate amongst each other via APIs.  This is nothing new, people have been preaching  service oriented architectures  for decades but I think we’ve forgotten it when thinking in terms of “web.” It feels more intuitive to split services in terms of frontend and backend but the right approach is to think in terms of actual functionality. It may turn out to be that tightly coupling the frontend and the backend is the right decision.",1,1,2014-04-16,5,"coding, architecture, development, engineering, coding",587,Vertical integration and web development
13,0,Just released a new JavaScript tool that geocodes a list of addresses.,,"{% include setup %} Over the weekend I dug up an  old repository  I started to contain a running collection of  JavaScript tools  to make my life easier. Ever since I created it it had two tools - one to convert CSV/TSV text into a bootstrap table and the other to generate a “BCG style” matrix. Earlier today I coded up another script - a quick way to geocode a list of addresses. All you have to do is enter a list of address you want geocoded, one per line, and the script will use the Google Maps API to geocode each one with the resulting latitude/longitude being written to an HTML table. If you have any other suggestions for a quick tool let me know.",2,0,2014-04-21,5,"javascript, tools, csv to bootstrap table, bcg matrix, bulk geocoding",133,Bulk geocoding tool
19,0,Craigslist has become the web's fertilizer. Everyone is using it to grow their business - whether legitimately or not.,"#meta,#product","{% include setup %} Craigslist has become the fertilizer of the web. This realization came to me last week when I needed to get a replacement phone and decided to search for one on Craigslist.Filtering past the obvious scams I thought I find a legitimate offer and reached out. Within a few minutes I received the following response from ""Kyle"":   Hi a guy bought this from me, but I can tell you where I got it from. I got 3 of these from http://enetcweb.com/dibzees and I resold them for some extra money. The trick is to watch for bidding to slow down and then put in a bid. That's what I do and I win most of the time.    And this was from a listing that seemed legitimate! The vast majority of listings were clearly fraudulent that promised either amazing deals or was the same posting duplicated a dozen times with slightly different wording. Even beyond the fraud and scams there are probably tons of startups trying to take advantage of the network that Craigslist offers. Some are listing their products and services to validate their market and others are reaching out to owners of various listings trying to sell them on something.  At Makers Alley we posted a variety of products to Craigslist to understand the market. How many people would click through to our site? Would people more interested in buying custom furniture from individual makers or from a brand? Would anyone go through the entire checkout process?  We only received responses to ads that were positioned as independent makers and each of these responses was from another startup trying to get us to sign up for their platform. This feels like a perverse version of  ""The Gift of the Magi""  - startups exchanging services with other startups - without any real consumers benefiting. I'd love to know what percentage of Craigslist is startups interacting with startups without either knowing the identity of the other. I suspect that for some verticals the number is shockingly high. This is also a massive example of how powerful the network effect is - the tiny sliver of value available in Craigslist is still enough to keep it useful.",1,2,2014-04-25,2,"craigslist, business",373,Craigslist: the web's fertilizer
37,0,Toyota is starting to replace the robots in their factories because only by dealing with the details of a process can it be improved. I think this is a great approach and applies well to coding.,#product,"{% include setup %} Last week I read an interesting article about  humans replacing robots  in Toyota's factory. The thesis being that only humans are creative enough (right now at least) to develop new skills and processes to deal with production inefficiencies. This rings true - in order to improve a manufacturing system you need to understand the entire process, from the raw ingredients up to the way consumers end up using the product. It's more difficult to do these days as products become more complicated with an increasing number of specialized components and I'm glad to see companies taking a longer term view and focusing on the value of human creativity rather than short term cost cutting.  Writing code is similar, it's easy to take off the shelf libraries and use them in an application. This is a perfectly reasonable approach when starting out and you need to get something built quickly; in fact this is the prefered approach so you don’t fall into the trap of premature optimization. But as you scale the big improvements will only come when you understand both the high level goals of what you’re writing and the low level details of how they are achieved. Then you can focus on removing and rewriting extraneous code and improve the components that are the bottlenecks. Without this understanding it’s very easy to waste time optimizing the wrong components rather than figuring out where the big wins will come from.",1,1,2014-04-28,4,"toyota, coding, product, manufacturing",259,System knowledge and human creativity
30,0,I recently moved my personal code over from Linode to Digital Ocean. I was hesitant at first and worried about the migration but it went as smoothly as possible.,#devops,"{% include setup %} Ever since I saw that Digital Ocean charged $5/mo, I’ve been meaning to migrate my sites and projects over from Linode but have been wary of dealing with the various issues that would ensue. I finally bit the bullet earlier this week and it went surprisingly smoothly.  My biggest concern was forgetting to copy some files that specified some esoteric settings I came up with when I first set up the projects. Luckily I didn’t run into this issue and most of the effort was spent in trying out my sites and looking at the log files to see which libraries were missing.  Here’s a quick overview of the process - the Digital Ocean  migration guide  was a big help.  - Follow the guide and install/configure rsync on both boxes - Use rsync to migrate the relevant files and folders. In my case it was everything in /var/www, the sites-enabled apache folder, and the MySQL dump - Install apache, MySQL and the other likely required packages. An overkill approach would have been to list every package on my old box and install it on the new one. - Load the MySQL dump into the new instance - Restart apache and go through the configuration settings. All the issues were due to disabled apache modules (headers, deflate, expires) and enabling them resolved them. - Go through each of the configured sites and make sure they worked. No downtime wasn’t a requirement for me so I ended up changing the DNS settings one by one confirming that each site ran properly. The PHP sites worked immediately but the python sites needed some packages installed via pip. - Copy the cronjobs from the old box to the new one  None of my projects were complicated and the migration went as smoothly as possible. Most of the time was spent waiting for the files to copy or packages to install and I never ran into an issue that didn’t have an immediately obvious fix. The error log was a big help - it gave me a quick way to identify problems and the missing packages. If you’re on the fence about migrating to Digital Ocean and the only thing holding you back is worrying about the migration I suggest just going for it. Worst case is you spend a few hours playing around with a new box.",1,1,2014-05-02,4,"linode, digital ocean, migration, devops",403,Migrating from Linode to Digital Ocean
23,0,Two common problems in Excel is filling in missing values and doing a cross join. Here are two ways to do it.,#code,"{% include setup %} During my consulting years I’ve done a ton of Excel and noticed people getting frustrated by two seemingly simple operations. The first is getting a worksheet with gaps in a column and needing to fill it with values from the cells above and the second is doing a cross join between two sets of values.  The solution to the gap filling can be done by explaining the solution in such a way that it can be implemented via an Excel formula. The best I could come up with is “If a gap is a value, take the value of the closest non empty cell above it, otherwise keep its value.” We can create a formula in another column that takes this approach and after coming up with the new cell values and pasting them over the originals. In the image below, the formula in cell D2 is  =A1  and the formula in D3 is  =IF(A3="""",D2,A3)  with D4 down being relative copies of D3.      The cross join problem is similar - we have two sets of values and need to enumerate each combination. The key point is realizing that we know what the values should be in a particular row and deriving the formula to get those values. My approach uses integer division to get the value in the first column and modulo to get the value in the second column although any function that’s deterministic should work. In the image below, the formula in cells D2 through D25 is  =INDEX($A$2:$A$5,(ROW()-2)/$H$2+1)  and the formula in cells E2 through E25 is  =INDEX($B$2:$B$7,MOD(ROW()-2,$H$2)+1) .      The file with the two approaches can be grabbed  here .",1,1,2014-05-03,3,"excel, cross join, gap fill",315,Gap fills and cross joins in Excel
36,0,An idea to reduce costs and improve uptime is to use S3 for all static content and have a quick client side JS call to determine what to use when. Dynamic generation isn't always necessary.,#devops,{% include setup %} An approach to scaling sites that I haven’t seen used much is using S3 as much as possible and falling back to it in case the dynamic elements are either not needed or unavailable. Many sites will host their static assets on S3 but there’s a lot more that can be pushed that way.  Reddit gives logged out users  cached content  rather than dynamically generating a page. That way logged in users get the full experience but logged out users may see a slightly out of date site. Content rich sites would benefit significantly from this approach - it would reduce cost and ensure uptime. If it turns out that the site does go down you can flip a switch and serve the cached/static content to everyone while the site is brought back up.  Current frameworks allow you to cache various elements of a page so they don’t need to be regenerated every time but they’re still dependent on the web server. If that goes down the page won’t be generated. An interesting idea might be to use client side JS to make a quick request to a server to see if it’s up and if not fall back to an HTML file on S3. I don’t know any sites that take this approach and would love to see some examples.,1,1,2014-05-10,3,"s3, scalability, javascript",238,Site down? Fall back to S3
28,0,I was curious to see my most commonly used shell commands so wrote a quick awk/shell script to figure that out. Let me know what you find.,#code,"{% include setup %} I spend a large chunk of time working in the terminal and was curious to see what my most commonly used shell commands were. This also gave me an opportunity to practice writing one liners and learn a bit of awk.  {% highlight bash %}history | cut -d' ' -f4 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r{% endhighlight %}  The script is simple - look through my command history, extract the first word, and count the number of times that word appears. I was surprised to see git at the top but it makes sense - I tend to run it as a sequence (git status, git commit, git push) so it leads to an inflated count. The rest make sense - they’re a mix of the standard navigation commands as well as command related to my current projects. Next step is to set up a cron job to track this usage over time and see how it changes.      Command  Frequency      git  347    ls  103    python  89    fab  70    cd  49    ssh  28    cat  28    ping  23    emacs  22    stash  15    rm  15    rake  15    pip  14    cdblog  14    pwd  12    jekyll  12    connectec2  11    sudo  9    workon  7    wc  7    phantomjs  6    history  5    head  5    c_do  5    brew  5    sh  4    mv  4    make  4    grep  4    sass  3    redis-cli  3    open  3    mkvirtualenv  3    find  3    celery  3    source  2    sed  2    redis-server  2    mkdir  2    echo  2    dig  2    cp  2",0,1,2014-05-12,4,"shell, bash, scripting, command line",169,Most commonly used shell commands
27,0,People are excited to see the impact Oculus Rift will have on gaming but I'm more excited to see the impact it will have on productivity tools,#meta,"{% include setup %} The Oculus acquisition got me thinking about the impact it would have on software development. We currently have a slew of editors and IDEs that are making us more productive and I wonder whether there's a place for VR. I don't think it's going to be as extreme as  Minority Report  (at least at first) but I do expect some things to get much easier.  Typing is currently much quicker than any other form of data entry and I don't imagine VR making this any better. While writing this I took a break and tried looking at the letters making up this sentence on my keyboard and it was slower - not to mention the mistakes that will likely occur during transcription. The only thing that would make data entry faster would be a direct neural connection which isn't going to be happening any time soon.  Navigation and context switching might become easier. I currently spend a fair amount of time tabbing through windows until I find the right one - a visual approach might make this process much better as long as it's implemented well. I'm also significantly more productive with an additional monitor - if VR is able to increase my working area I suspect I'd be more productive.  Debugging should get better. Being able to quickly examine various states during the course of debugging is extremely useful and I haven't seen a tool that makes this simple. An interesting, scifi-like solution would be to somehow provide a three-dimensional view of code execution and be able to view your code from an additional dimension. Being able to quickly go back and forth through time would make tracing code significantly easier.  Everyone's expecting VR to have a huge impact on gaming but I'm more interested in seeing the unforeseen use cases emerge. These will have an impact not only on entertainment and consumption but also on creativity and productivity.",1,1,2014-05-15,3,"oculus right, productivity, software engineering",328,Coding in a VR future
31,0,If you run a server that's open to the world. Take some time to make it secure. It doesn't take long and makes it significantly less likely you'll get hacked.,"#devops,#code","{% include setup %} I recently migrated to Digital Ocean and spent some time beefing up its security. One of the things I looked into was the various SSH attempts being made and to see if there was a pattern. Luckily, I’m running Ubuntu and every SSH attempt is logged by default to /var/log/auth.log and all it required was a quick one liner to see the failed attempts by username.  {% highlight bash %}grep ""Invalid user "" /var/log/auth.log | cut -d' ' -f8 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r | head -n 100{% endhighlight %}      Username  Count      test  141    postgres  116    oracle  88    web  75    test2  74    admin  59    jboss  49    ubuntu  45    webmaster  43    user  42    tech  40    debian  40    testuser  39    server  38    penguin  38    shoutcast  36    rdp  36    www  35    radio  35    ftp  33    test3  30    student  29    guest  29    toor  21    public  19    testing  15    tester  15    students  15    var  13    gov  9    adm  9    x  8    nagios  8    zabbix  7    z  7    y  7    w  7    vyatta  7    u  7    t  7    shell  7    s  7    r  7    q  7    p  7    o  7    n  7    michael  7    m  7    l  7    k  7    j  7    i  7    h  7    g  7    f  7    e  7    dup  7    d  7    ch  7    c  7    b  7    a  7    sales  6    office  6    home  6    data  6    bash  6    apache  6    administrator  6    v  5    test1  5    teamspeak  5    ssh  5    plesk  5    master  5    linux  5    ircd  5    http  5    walid  4    vnc  4    ust  4    ts  4    temp  4    telnet  4    smmsp  4    smart  4    samba  4    org  4    operator  4    net  4    named  4    mike  4    library  4    info  4    hacker  4    git  4    ftpuser  4    dan  4    cc  4      The usernames were all over the place - from generic ones (such as test, admin, ubuntu, guest) to the names used by various services (postgres, oracle, nagios) to letters of the alphabet. There was also a slew of common English first names. In total, there were ~1500 unique usernames that attempted to access my box.  The auth.log file also contains the IP address of each attempt and we can easily summarize by that.  {% highlight bash %}grep ""Invalid user "" /var/log/auth.log | cut -d' ' -f10 | awk '{a[$0]++}END{for(i in a)print i,a[i]}' | sort -k 2 -n -r | head -n 100{% endhighlight %}      IP  Count      162.13.41.12  874    176.31.244.7  733    216.127.160.146  572    195.50.80.169  382    66.219.106.164  359    199.33.127.35  220    112.167.161.194  98    128.199.226.160  66    198.50.120.178  60    189.85.66.234  37    14.18.145.82  29    166.78.243.86  23    222.190.114.98  22    130.126.141.74  18    178.208.77.133  17    61.160.213.171  8    49.213.20.249  8    23.253.51.76  7    178.254.8.177  7    193.107.128.10  5    121.167.232.196  2    107.182.134.51  2    82.221.106.233  1    74.3.121.10  1    72.225.239.90  1    111.74.134.216  1      In this case, the total number of IP addresses is significantly smaller with only 26 unique IP addresses trying to connect. I took a look at a few and some of them look to be legitimate sites that may have been compromised.  If you have a box open to the world, you should make sure it’s secure. A small program that makes this easy is fail2ban - it scans log files and bans IPs that have had too many failed attempts. Two other quick fixes are to disable password authentication entirely and rely solely on public key authentication which is significantly harder to crack and change the default SSH port from 22 to something else. These should be enough to eliminate the bulk of attempts and keep your box secure.",0,2,2014-05-16,4,"security, ssh, ubuntu, server",420,Examining ssh login requests
31,0,MixPanel sends an email to login to your account after a few failed login attempts. I wonder if the approach of being able to login solely through an inbox would work,"#product,#design",{% include setup %} MixPanel has a clever way of handling failed login attempts. Instead of locking the user out of the account or forcing a password reset they send an email with two links - one to log in to the account directly and another to reset the password. I don’t recall ever seeing this approach before and wish more sites started doing it. This approach also obviates the need to even have a password - a site can just send a “login link” for an entered email address and the user can login via their inbox. This is similar to the way we login via the various social networks but instead of being sent to a social network for confirmation we are sent to our inbox. The only friction is having to go to your inbox to click on the link but since most people keep their inboxes open all day I don’t see this as a huge problem. The other advantage is security - most people use the same password across multiple sites so if one is compromised the others become vulnerable. Under this approach each site will have its own security controls and it becomes impossible for one site’s shoddy security to affect another’s. This is probably too drastic of a change for most users but I’d love to see sites start embracing this model.,0,2,2014-05-20,4,"security, mixpanel, logging in, design patterns",245,Logging in through your inbox
26,0,Fab is moving away from flash sales into becoming a private label. This reminded me of my experience building Makers Alley and what we learned.,#product,"{% include setup %} Fab recently  laid off  a third of their staff as they transition from designer flash sales into customized goods and their own private label. The business is tough and reminded me of our experiences building  Makers Alley . We initially set out to build a place where people can buy customized, personal furniture from local designers. The idea was was consumers would benefit from being able to get items that are custom made and can be customized to fit individual styles while supporting a local business and makers would have a new avenue to sell their products and build their brand.  Unfortunately, we faced huge obstacles on the consumer side. We thought that with such a feel good story and compelling vision we’d have no trouble attracting people to buy furniture but it was extremely tough. We launched during the flash sales era where everyone was on a hunt for deals and discounts. We considered showing discounted prices by inflating the original price but that just didn’t feel right and we didn’t want to diminish the work of our makers and designers.  I recall talking to one of our woodworkers who told us about being approached by Fab which wanted to include some of his pieces but they wanted him to sell his pieces at too steep of a discount in addition to Fab taking a cut that it made no economic sense to do it. This validated our belief that we did not want to go down the discount route but that still didn’t help us attract consumers - especially when they were being inundated with expiring deals and flash sales. Fab wanted to promote good design from new designers but their flash sales model prohibited them from working with the designers and makers who needed it the most.  Fab’s new direction feels bittersweet. It sucks for any entrepreneur to realize that a business model doesn’t work and it absolutely sucks to have to lay off amazing people who actually believed in your vision but at the same time I do think that their new model is more sustainable and better for the long haul, not to mention that it’s now more similar to what we were trying to do Makers Alley.  Building a custom label should be easier than starting from scratch since Fab’s already associated with good design but their challenge will be changing the mindset of their customers from expecting great deals to be willing to pay more for curated, unique designs. This is going to be tough and there’s a lot more competition in this space. I suspect their biggest competitor is Etsy and that’s why they’re focusing on their private label in order to move away from that model and become more like the Warby Parker for design.",2,1,2014-05-24,4,"Fab, design, furniture, ecommmerce",484,On Fab's latest move
18,0,I wrote a quick script to make it easy to generate fake data for SQL-like databases.,"#code,#devops","{% include setup %} A problem I occasionally run into is needing to generate a bunch of fake data and insert it into a database table. My usual approach has been to generate this data in Excel and then use a series of string concatenations to generate the necessary insert statements which I’d then execute in the SQL client. After doing this one too many times I decided it was time for a better, more automated approach and  hacked one together  in JavaScript. It’s currently a part of my js-tools  GitHub repo  and suggestions are welcome. One thing I definitely need to add is the ability to specify the range of possible values for each field rather than using a hardcoded distribution.",3,2,2014-05-29,4,"sql, fake data, generate sql data, javascript",160,Generate fake SQL data using JavaScript
21,0,Data science doesn't need to be complicated. Anyone who can write a script can contribute and we should encourage that.,"#datascience,#meta","{% include setup %} Data science has earned the reputation of being complicated and inaccessible to those without an advanced degree but it doesn't have to be this way. The goal of data science is simply to unlock insights and value from data. There's no need to make it more complicated than that. Of course, there are times where the data requires some domain knowledge or is just too big for someone without the necessary experience to work with but I believe that most places have enough low hanging fruit that anyone who can write a quick script can contribute and do data science.  This can be as simple as looking at a site's log files to figure out the most popular pages and how long they take to load in order to identify slow pages that can be sped up. Another quick task can be writing some queries to provide summary statistics across varying dimensions and visualizing them to see if any patterns emerge. A more advanced project can be going through a codebase and implementing a system to help track metrics in a way that makes future analysis easier. None of these require advanced quantitative knowledge and there's no reason that anyone should feel unqualified to dabble in data analysis. In my experience the most value has come from someone noticing something interesting and asking the right questions that led to a more thorough analysis. The more people that approach data with a curious mindset the more valuable a company's data becomes.  There's always the risk of discovering something  spurious  so it's important to validate discoveries but I'd rather have signals and noise than silence - especially if this encourages more people to become interested in data. At first, this can pose a problem for the people who need to deal with the noise but over time people will become more aware of what's valuable and can help identify areas of further analysis. This is the way to build a data driven culture - not by hiring a few data scientists.",1,2,2014-06-02,2,"data science, analysis",346,Approachable data science
34,0,"I ran into a weird bug today where a database record was updated but would sometimes revert back to older values. As with most Heisenbugs, this turned out to be a timing/concurrency issue.",#code,"{% include setup %} I ran into an odd bug today where a database entry was reverting itself after a seemingly simple update. For  Better404 , a customer can change the design of their 404 page but it turns out that every once in a while a change would go through but within a minute would revert back to the previous value. At the same time, update queries run directly via the MySQL client ran fine and were not being reverted - just the ones made through the site. To see what was going on, I enabled full query logging in MySQL (SET GLOBAL general_log = 'ON') and sure enough I saw a lagging query that would update the record to the prior values. Stepping through the code I was able to figure out the cause.  As with most  Heisenbugs , it turned out to be a timing/concurrency issue. As part of a user updating their settings, we would kick off a job to crawl and index the modified domain. After running, the job would update the record with the newly crawled timestamp. Unfortunately, the other fields were updated as well and since this indexing job was kicked off before the record was updated, stale data was written to the database. After figuring out the cause, the fix is easy. One option is to kick off the indexing job after the database is updated with the new values so that the indexing job will use the new values. The second option is to modify the indexing job to only update the relevant fields. Both options were trivial to implement so I played it safe and did both. The first required changing the order of some lines and the second was just specifying an optional parameter to the Django model’s save method. Below’s a visual representation of what was happening and the two fixes.      Unfortunately, concurrency and timing bugs tend to be the most difficult ones to figure out but whenever there’s non deterministic behavior they should be at the top of the suspect list. It’s important to know the tools we’re using and their default behavior - it’s possible that the approach they take differs from how we think they work and becomes a major source of bugs and frustrated debugging efforts.",2,1,2014-06-07,3,"bugs, hacking, coding",403,Debugging a reverting database update
28,0,I recently replaced Evernote with Sublime Text and it's been great. I get the full power of the Linux shell and avoid getting locked in to Evernote.,#meta,"{% include setup %} Although there’s been a ton of services launching trying to help people do everything under the sun I’ve been finding myself going back to simple tools. One of these has been replacing Evernote with text files that are synced via Dropbox after getting annoyed with Evernote one too many times. It’s great, I edit files in  Sublime Text  and came up with my own naming format to make search easier. If that fails, I just use grep and find and almost always find what I’m looking for. Since the files are plain text, every Linux command is a tool. It’s trivial to do bulk search/replace using sed or compose one liners to do various filters and counts. It’s easy to sort files by time or size and being able to do a regex search comes in handy when you only have a vague idea of what words you used when writing a note.  I wish more services would approach their products the same way - only focus on one thing, do it well, and allow other services to integrate with it in a simple way. This belief has been the foundation of the  Unix philosophy  but sadly most businesses haven’t embraced it due to a desire to encourage lock in and increase switching costs. Hopefully this changes in the future.",2,1,2014-06-08,3,"productivity, evernote, sublime text",232,Replacing Evernote
21,0,How can eBay still not allow people to choose the timezone for a start time? It's currently stuck in PDT.,#design,"{% include setup %}      I recently needed to sell something on eBay and encountered an issue I thought they would have taken care of by now. Apparently you can pick the start time for an auction but it has to be in PDT - there’s no way to choose another time zone. The change is trivial and one would think that a $60B company would be able to support multiple time zones in their core product. Someone brought this up in  the forums  in 2012 and it turns out that time zone support is only present in the forum to allow users to see posts with a local time.  Whenever I see seemingly obvious UX anti-patterns it makes me think there must have been an ulterior motive. In this case, I suspect not having a time zone may lead to a smoother distribution of auction end times which keeps product demand high and can distributes bids more evenly throughout the day. Another reason may be the desire to reduce risk - time zones are difficult to get right and it’s possible that eBay doesn’t want to expose themselves to the liability. The most likely reason may be that they are just not investing heavily in the use experience. They’re already the market leaders and it may make more sense to focus on marketing and selling rather than on making it easy to list a product. If someone’s already started to list a product it’s unlikely that the lack of a timezone will cause him to change his mind - it didn’t in my case.",1,1,2014-06-12,3,"design, ebay, user experience",289,An eBay design rant: timezone support
18,0,I recently had my Pinterest account hacked which led to a bunch of spam tweets on Twitter.,#meta,"{% include setup %} Earlier today a I got a message on Twitter letting know that my Twitter account was hacked. Sure enough when I looked at my tweet history I saw a slew of weight loss tweets linking to a Pinterest pin. Turns out that my Pinterest account was compromised and since it was connected to Twitter every time someone pinned a weight loss link it got shared on Twitter.  The fix was simple - block the Pinterest app from within Twitter, disconnect Twitter from within Pinterest, and reset my Pinterest password. Unfortunately, none of these can be done via the apps nor the mobile sites. Instead, both provide a minimal settings page with no clear way of accessing the complete settings. Since I wasn't near a computer, I had to use the Twitter app to delete the spam tweets that were being posted a few times each hour.  This is a frustrating design pattern. Sites and apps should default to a mobile optimized experience but if any functionality is missing they should allow users to fall back to the web view. Arguably, that option should always be available since a user may have a specific flow in mind and shouldn't be forced to learn a new approach - especially if something's urgent. In my case, this wasn't that huge of a deal but I can see how it could have been.  The other lesson is that in this world of interconnected apps you're only as strong as the weakest app. Twitter followers don't care why they're seeing spam and having TFA enabled on Twitter won't help you if another account is compromised.",0,1,2014-06-15,3,"pinterest, twitter, security",273,On having my Pinterest account hacked
36,0,I attended a talk by Prof Michale Stonebraker where he shared his thoughts about the future of databases. In particular I like that many of these new databases are standardizing around a SQL-like syntax.,"#sql,#data","{% include setup %} A couple of weeks ago I attended a talk by  Professor Michael Stonebraker . For those unfamiliar with him, he’s a database researcher responsible for PostgreSQL, Vertica, VoltDB and a dozen others. During his talk he shared his thoughts about the future of databases and what we can expect to see in the coming years. His main point is that databases are becoming more and more specialized and it will be very common for companies to run multiple types of databases that are optimized for different uses cases.  This is already happening at the larger tech companies and it’s spreading downwards. Data is becoming increasingly important and having the tools available to leverage it is a critical advantage. It’s impossible to find a single database that can be used to run a transactional site, support complex yet quick analytics queries, scale to terabytes of data, and still maintain synchronization between its various nodes. Each of these use cases requires a database that’s optimized for that need and an application that knows how to leverage that database.  The neat thing is that many of these newer databases have embraced a SQL-like query syntax so it’s surprisingly easy to get started. The challenge is that this similarity is only skin deep and the implementations are drastically differently both in terms of how the data is stored as well as how the queries are executed. So although it’s simple to write a query that will execute on both Redshift and PostgreSQL it’s likely that this query isn’t as efficient as it can be on one or both of the databases.  This is the right approach for the specialized-database future. By providing a standard interface it makes us more comfortable with introducing a new database into our stack while providing very different functionality under the surface. It’s likely that the first implementations won’t be ideal but as teams become more comfortable with these new systems the implementations will evolve. I hope this pattern of standardizing around a simple interface becomes more popular. Then the backend can be designed for a variety of use cases without forcing the users to completely change the way they think.",1,2,2014-07-05,8,"databases, sql, data, mysql, postgresql, vertical, voltdb, redshift",371,The future of databases
22,0,I used the Tweepy library to write a quick script to retrieve a Twitter user's followers and followees multiple levels deep.,#code,"{% include setup %} After reading Gilad Lotan’s  post  where Gilad bought 4,000 Twitter followers in order to analyze them, a  friend  of mine was inspired to analyze his followers to see if he could get any insight and come up with a neat visualization. The first step was downloading a dataset containing his followers and followees as well as the followers and followees for each of those accounts - the idea being that by going two levels deep you see how similar the various accounts are to each other based on who and what they follow and whether there are any patterns.  I offered to write a short script to help him pull the data and it turned out to be easier than I thought due to the excellent  Tweepy library . The biggest challenge was figuring out how to use Tweepy to deal with Twitter’s absurdly strict API limit (15 requests per 15 minutes) since the documentation was a bit sparse but after discovering the Cursor object it became surprisingly easy to iterate through the results and wait for 15 minutes for API errors.  The code currently works in a user id rather than username world since I wanted to avoid making additional calls but that can be implemented in the end to just pull the usernames for every user id in the dataset. The  code’s  up on Github so feel free to try it out and let me know if you run into any issues.",4,1,2014-07-07,3,"twitter, api, tweepy",269,Retrieving a Twitter user's followers and followees
25,0,I recently got an Android phone and have been enjoying the Google Now experience. I'm hopeful that Google opens it up to third parties.,#product,"{% include setup %} As much as it pains me to admit it I’m really enjoying Google Now. I’m aware of how much information I'm sharing with Google to make it helpful but at the moment I find the tradeoff worth it.  It came in especially useful as I've been traveling over the past couple of weeks:  - Show the official exchange rate when traveling. This may not be perfect, especially in the case of ""blue markets,"" but it's nice having a rough idea of how much a US dollar is worth. - Flight information. Since my flight details get sent to my Gmail account, I can quickly tell whether my flight's delayed and what terminal and gate it's scheduled to depart from and arrive to. This is useful to have when I need to make a transfer since I can quickly see where my next flight departs from. - Flight boarding passes. In addition to the flight information Google Now also shows the boarding passes for my checked in flights. I didn't have to do anything to board a plane other than activate my home screen and place it against a scanner. - Hotel information. Similar to flight information, I get a card telling me where my hotel is and how to get there. - Google calendar integration. This is an obvious one but I run my life through Google calendar. This gives me constant notifications of what I have to do when and as long as I enter an address for my events I also get an estimate for when I should leave.  A concern is that to actually make it useful I have to integrate more and more of my world with Google and I expect this to get worse as more Google Now cards are developed. The optimist in me hopes that Google Now will be opened up to third party developers in a future version of Android but the cynic suspects it's not going to happen.",0,1,2014-07-09,2,"google now, android",327,On Google Now
26,0,Amazon is offering a $6 CPM to app developers who launch an app on the Fire phone. An aggressive move to build out their ecosystem.,#product,"{% include setup %} I’m a bit late to the Amazon Fire Phone party but wanted to chime in with a perspective I haven’t seen written about. Amazon is offering a  $6 CPM to mobile app developers  that launch an Amazon app during Auguster and September. Given that typical CPMs are  less than a dollar  with premium publishers like Facebook and Twitter getting  close to $6 , this is a very aggressive move by Amazon to build out their ecosystem.  A common refrain mentioned is that ecosystems drive smartphone adoption. This is why it's extremely difficult to compete against iOS and Android which combined have  72% share  of the smartphone market. Amazon is trying to jumpstart this by offering a potentially huge sum to developers if they're able to get the users. Amazon will have a difficult time getting a large market share but I suspect they will find a niche in a particular customer segment and a few app developers will be rewarded.  It's also interesting to contrast this with Microsoft's approach of paying developers a flat sum to create a Windows phone version of their app. On one hand, this approach allows Microsoft to be selective since they can pay to get the apps they want. On the other hand they run the risk of paying for an app without any users. The interests between Microsoft and it's app developers are not as aligned as those between Amazon and it's app developers.  One thing that may have influenced this decision is that Amazon's OS is Android based so it's significantly less work to port an app to work on Amazon's phone compared to a Windows Phone. The pitch to developers is akin to saying make a few changes to your app and potentially earn a bunch of money while Microsoft's would be write a new app in a new language and we'll pay you for your efforts.  Almost all Amazon phone coverage has been bearish and I wonder whether this move will have an impact. Apps to drive smartphone adoption but with so many good options already available replicating an ecosystem won't be enough.",4,1,2014-07-10,2,"amazon fire phone, smartphones",415,Amazon courting app developers for the Fire phone
29,0,Rather than wanting to try out the new hot thing we should become masters of a few set of tools. It is a lot more productive and efficient.,#meta,"{% include setup %}  Actually, as the artist gets more into his thing, and as he gets more successful, his number of tools tends to go down. He knows what works for him. Expending mental energy on stuff wastes time.  &nbsp;&nbsp;- Hugh MacLeod,  Ignore Everybody     This quote refers to art but it can just as easily apply to code. As developers, we’re constantly exposed to new tools and technologies and are curious to try them out. Everything new looks shiny and we imagine it will solve all the problems we’re facing. Yet almost always new tools bring their own set of problems and take time to learn. Instead of constantly chasing something new we should try to master what we’re already using - the value of that will most likely outweigh playing with a new toy. It’s better to rely on a small set of tools that we understand well rather than have a superficial knowledge of dozens of tools and technologies.  Of course it’s important to try out new tools since many of them are useful but it’s dangerous to rely on new tools exclusively and use them for a new project just because they’re the next big thing. To get some exposure to new tools, I will use them in toy projects or during hackathons so I can get a sense of how they work, what the strengths and weaknesses are, and how much I enjoy using them. Only then will I consider using them in a real project.",1,1,2014-07-13,3,"tools, productivity, engineering",256,Good artists use fewer tools
27,0,I'm working on a project using Netty - a low level framework that forces you to create your own HTTP requests from packets. A wonderful learning experience.,#meta,"{% include setup %} I’m currently working on an application using  Netty , a low level network framework, and it’s given me a wonderful education of the HTTP protocol. Prior to this project, every web application I’ve worked on has leveraged a framework that removed the low level details. They built the HTTP requests from multiple packets, took care of various encoding issues, dealt with keep-alive connections, came with built-in support for sessions and cookies, and in general made it extremely easy to get a web server up and running.  Writing a Netty application is completely different than using a framework such as Django, Ruby on Rails, or Node. You get a much better understanding of how TCP and HTTP work and doesn’t actually take that much time once you get the hang of it. Building an HTTP request from individuals packets was completely novel and finally getting it to work made me feel the same way as when I built my first site. If you’re interested in or currently working in web development and you haven’t worked with a low level framework, take a weekend off and give it a try - it’ll give you a newfound appreciation of how the modern web works.",1,1,2014-07-14,4,"netty, http, low level, web framework",209,Getting low level with HTTP
26,0,A quick guide on how to set up HTTPS on an EC2 instance that's running Nginx without having to upload the SSL certificate to an ELB,"#code,#devops","{% include setup %} I recently needed to set up HTTPS for my side project,  better404.com . Amazon makes it easy to  set up  by uploading it directly to an ELB but in my case it’s hosted on a single AWS instance so I didn’t want to pay for an ELB that would be more expensive than my one instance. I’ve heard horror stories and expected the worst but it turned out surprisingly easy. Hopefully these steps can help someone else out.       Get an HTTPS certificate. I bought three certificates from Namecheap a couple of months ago when they were running a  promotion.     Go through the certificate generation process. I found  this guide  that explains how to do it detail and worked well for me. The only things I had to change where the  Nginx certificate configuration folder path (/opt/local/nginx/conf/certs => /etc/nginx/certs/) and replacing the filenames to be more specific (domain.* to better404.*). Note that this process is not immediate and you will need to send the contents of your CSR file to the SSL provider and they will respond back with the SSL certificate to use.     Enable SSL in Nginx. The previous guide provides some information here and I’m including the relevant parts from my configuration. I chose to redirect all traffic to HTTPS rather than supporting both simultaneously. {% highlight nginx %}#Nginx config server {     listen *:80;     server_name www.better404.com;     rewrite        ^ https://$server_name$request_uri? permanent; }  server {     listen *:80;     server_name better404.com;     rewrite        ^ https://$server_name$request_uri? permanent; }  server {     listen *:443 ssl;     server_name better404.com;      ssl on;     ssl_certificate certs/better404.pem;     ssl_certificate_key certs/better404.key;{% endhighlight nginx %}     Allow Nginx to bind to the IP address. One thing that’s not mentioned in the guide and required a bit of digging around is that you need to allow Nginx to bind to the non local IP address - otherwise it can only access the private IP address set by AWS. There’s a  quick guide  on how to do this I found on StackOverflow.     If you have any questions feel free to leave a comment and I’ll try to help out.",4,2,2014-07-15,4,"https, ssl, ec2, nginx",375,Set up HTTPS on EC2 running Nginx without ELB
33,0,A simple way to automate AWS deployments is to use the AWS CLI to retrieve information about every instance and then use the instancedata endpoint to retrieve information about the current instance.,#devops,"{% include setup %} A little known feature in AWS is an endpoint that allows you to retrieve various information about about the requesting instance. If you log in to one of your EC2 instance and make a simple request to http://169.254.169.254/latest/meta-data/instance-id you will get back the id of that instance. Similarly, you can get all sorts of  other instance information , including the public hostname, the instance region, and instance type.  This can be useful when you want to automate a simple deployment where you have a few instance with a variety of roles. A lightweight approach would be to use the  AWS CLI  to retrieve a list of all running instances along with their tag names, make a request to the meta-data/instance-id endpoint to get the id of the current instance, and then look up that id in the instance list in order to figure out what the role of this instance should be. Then execute the appropriate set of scripts to configure the instance properly.  More advanced solutions would involve using  Chef ,  Puppet , or  Opswork  but they come with a steep learning curve and overkill for a simple application. If it turns out your application is growing you can always upgrade to a more robust deployment solution.",5,1,2014-07-16,4,"aws, deployment, automation, devops",240,A simple way to automate AWS deployments
22,0,The rise of food trucks shows capitalism at work. Failed construction food trucks turned into a whole new type of cuisine.,#meta,"{% include setup %}                             Source:           EcoVeganGal.com                       Food trucks have taken over every city I've been to. A decade ago the best you could find was a taco truck but now food trucks run the gamut from the simple taco up to experimental vegan. Priceonomics has a  great piece  on the rise of the food truck as well as a fascinating look at the economics of the food truck industry. If you haven't read it yet definitely check it out.  The summary is that the decline of the housing market in 2008 led to a drop in construction which caused many food trucks to sold at a discount. These food trucks were then bought by aspiring chefs who wanted a low risk way to start a restaurant. The cheap trucks gave chefs a way to experiment with varying cuisines and locations without incurring the massive costs of starting a restaurant.  This is a perfect example of how capitalism should work. Industries that are no longer profitable make way for new ones that leverage existing infrastructure. In the future these food trucks will transform into miniature carriers that will carry drones that will deliver food throughout cities. When one door closes, another door opens.",2,1,2014-07-23,3,"food trucks, capitalism, business",250,"When one door closes, another door opens"
31,0,Due to a dumb error I locked myself out of an account and the only way back in was by entering the last 4 digits of an old credit card.,#meta,"{% include setup %} I thought I've seen every design anti-pattern out there but had the luck to run into a new one a couple of days ago. I was buying domains on  Namecheap  and ended up going through checkout without verifying the payment details. Turns out that I had an old credit card on file which led to a declined payment. I was redirected to a page that told me to update my payment methods but instead of doing that I ended up hitting back and refreshed the page which triggered another failed charge attempt. One more and I'm locked out of my account.  Ironically, other than speaking to a rep the only way to unlock my account was by entering the last 4 digits of the credit card which I no longer have. It only took a few minutes to clear that up with the rep and it was basically my  fault but it's still interesting to see security questions based on ephemeral information. Old accounts are likely to have outdated credit cards, phone numbers, and addresses. In those cases it's too easy to get locked out and be stuck with having to speak to a service rep - and I suspect most companies won't be as responsive as Namecheap.",1,1,2014-07-27,1,security questions,215,Ephemeral security questions
15,0,Bundling makes sense for low quality products. High qualit products should be sold separately.,#pricing,"{% include setup %} In the quest to reduce the amount of stuff I own I've been going through various cabinets and boxes and trying to list everything on eBay. The most common items are old cables with no corresponding devices (or any ideas what these devices even are) and old DVDs.  Looking at the historic prices for these items doesn't make me happy - a Lenovo laptop charger is less than $10 while a Raging Bull DVD is a couple of bucks. But this entire process got me thinking about bundling. Bundling makes sense when selling cheap products. It's not worth the time to list these individually and it's likely that there are only a few people interested in each item. Bundling them makes it more likely that various items will appeal to a variety of buyers and increase competition. The Lenovo adapter may appeal to one person while a Game Boy charger may appeal to another. By having them in the same lot they are competing against each other and are willing to bid higher to get what they want.  High quality items are competitive on their own. The sum of the individual sales will be greater than the value of the bundle. The intuition is that bundling premium items when buyers only want a single item will decrease buyers' willingness to pay more for the extra items.  HBO's the standard example - it has enough consumer demand that it can stay independent and charge a premium. Other channels need to bundle and subsidize each other. I suspect many would be more successful breaking out, such as ESPN, but they're either stuck in contracts or fear change.",0,1,2014-07-29,4,"bundling, pricing, auctions, HBO",278,Low quality? Start bundling!
31,0,"At Pressi, we decided to hire someone from Odesk to help us come up with a list of prospects. We ended up finding someone on Odesk using a 'screening' approach.",#meta,"{% include setup %} While working on  Pressi , we found a niche selling ""social media mashup pages"" to colleges and small universities. Once we discovered it we needed a quick way to find these colleges and identify the contact details of their marketing or social media directors. Searching for this information was not the most efficient use of time for our small team so we went looking for other options.  Two options that stood out were  Mechanical Turk  and  Odesk  but they were designed for quick and simple tasks. Using them for complex tasks would result in poor quality results. One advantage that Odesk had was that it allowed us to work with the same person for many tasks - something we couldn't figure out how to do using Mechanical Turk. This allowed us to come up with a set of potential candidates based on their project interest and skillset. We gave each of them the same set of problems to do and compared the results. Using this approach we discovered someone who was the right balance of cost and quality and we ended up working with her over the next few months to compile this list.  There's a lot of talent on platforms such as Odesk and Mechanical Turk but it's not easy to find. A good approach is to develop a set of tasks that you're looking for someone to do and use that as a proxy for an interview. Giving this set of tasks to a few dozen people will lead to a few that stand out and can hired on for longer term work.",3,1,2014-07-31,3,"odesk, remote working, on demand economy",279,Hiring people on Odesk
29,0,Lyft offered a great promotion in NYC - 50 free rides. It's possible to chain a bunch of short rides in order to get a long ride for free.,#meta,"{% include setup %} In honor of their NYC launch,  Lyft  came up with an awesome promotion - 50 free rides, up to $25 each, over the next couple of weeks. This had the desired effect - a bunch of my friends are giving it a shot but since everyone else is doing the same it's difficult to find an available car. And when you do get a car you end up paying the peak demand rate rate.  One way to get around this is to break down a long trip into a series of shorter trips. The driver will have to agree to this but they have an incentive to do so since they will not need to find another passenger and will earn a base fare every trip. A minor inefficiency is that the driver will need to stop to mark the trip as completed and confirm the next trip. It's also easy to go over $25, especially in prime time, so you need to stop more frequently than you think you need to. If only Lyft or Uber showed the real time cost of a trip you'd be able to stop at the optimal times.  We used this approach yesterday when going from midtown Manhattan to Brighton Beach and were able to do it with 6 Lyft trips. The first two ended up going over the $25 rate so we became aggressive stoppers after that. The driver mentioned that a bunch of his passengers used the same approach to get free rides to Long Island and Connecticut. I'm just waiting for Lyft to plug this loophole - a simple way would be to not allow the same consecutive driver/passenger pair.",1,1,2014-08-03,3,"lyft, uber, car sharing",285,Getting the most out of Lyft's 50 free rides
38,0,Code deployment is something all tech companies need to do and there's been a lot of progress over the past decade in managing it. I was thinking of how my process has evolved and what I've learned.,#devops,"{% include setup %} I’ve been working on various tech related projects for over a decade now and have gone through a variety of approaches to deploying code. I’m far from an expert but though it would be helpful to jot down what I’ve seen and where I'm hoping to get.  - FTP upload, no version control: I developed my first few sites locally and then just copied them over to the host server via FTP. This worked well for simple projects where I was the only contributor.  - Version control, single branch: Once I discovered version control I immediately found it helpful. Version control made it easy to work with others but our deployment was still manual. When we were ready to deploy we would log in to our server, run the necessary commands to update the database schema, and then do pull/update to get a new version of our code base.  - Version control, single branch, more automated deployment: Logging in every time to do a deployment was a pain so we started using  Fabric  to automate deployments. Fabric allowed us to execute scripts on multiple machines without having to manually log in to each one. Since each box had a set of roles we were able to set up Fabric to deploy by role (ie deploy this change to the DB server, deploy this change to all webservers).  - Version control, multiple branches, more automated deployment: Another improvement was following git best practices and setting up a production branch with everyone working on development branches that would then be merged into master. When the deployment was ready to go out it would be merged into production. The value here was that when we ran into a bug on production, we were able to fix it without having to merge in a bunch of new features.  - Version control, multiple branches, automated testing, automated deployment: This is the ideal state. Each of our repositories is tested enough that code changes are automatically tested, merged, and deployed to production. The process should also be smart enough to handle db migrations and would be to revert changes if any problems arise. In addition, each box may have a different set of required systems libraries and packages and an automated deployment should be able to automatically configure a server with the necessary packages. I know  Chef  and  Puppet  are used for this but I’m only exploring them now.  Something to add is that there’s a huge incentive to make your stack as stateless as possible - for example having multiple web servers behind a load balancer that don’t need to share any state with other webservers directly. This makes it simple to spin up new servers when there’s more demand and improves scalability. Unfortunately, it’s not always possible and complicated deployments end up having coupling - especially when high performance is required. In that case adopting a declarative approach when configuring your instances helps bring some sort of statelessness - for example using AWS tags to declare an instance to be of a particular type and using the region information to dictate what other instances it needs to connect to. Otherwise you’re stuck trying to define a complicated topology via config files. I’d love to know how massive companies manage their deployments - I know Facebook has a  custom process  that will deploy new code to a set of boxes and then use BitTorrent to share it to others but I’d love to be able to compare that with those of others, for example Google and Amazon.",4,1,2014-08-09,4,"aws, code deployment, release management, devops",611,Evolution of code deployment
21,0,Thinking about book prices and how they compare to other common items makes you realize how cheap books actually are.,#pricing,"{% include setup %} While the pricing battle between Amazon and Hachette rages on, I’ve been thinking about the relationship between price and value. A typical ebook on Amazon costs $9.99 while a movie in a theater, especially one in New York, can cost more than $10. And yet the book takes longer to experience - a movie is over within 2 hours while a book can be enjoyed for hours. Or how about a beer or coffee, they’re two to three times cheaper than an ebook but are consumed an order of magnitude faster book and only provide immediate gratification.  Amazon released data indicating that dropping the price of an ebook from $14.99 to $9.99 (33%) leads to a 74% increase in number of books sold. I understand that a lower price increases demand but it’s still ridiculous that we’re that concerned about a $5 price difference for a book when we’re spending that much on a coffee.  We’re so used to spending that much on a coffee that we’ll pay it without question and we’re so convinced that ebooks should be less than $10 that we refuse to pay more. It’s amazing what habit and expectations can do. I understand this bias and yet I still have a hard time believing that by skipping two cups of coffee I can buy a book. It definitely makes me appreciate the effort required to change people’s perceptions of what’s a fair price.",0,1,2014-08-11,4,"books, amazon, hatchette, pricing",245,Books are insanely cheap
18,0,I'm putting together a MySQL class and would love to get some feedback on the proposed structure.,"#data,#meta","{% include setup %} I’m clearly biased but I believe technology is critically important and we should be spending more effort teaching it than we are now. To that end, I’ve been volunteering with  TEALS , a national program that allows professionals to teach Computer Science classes in a local high school. Something else I’ve been working on is developing a MySQL class to give as part of the  Coalition 4 Queens  program. As part of the process I wanted to share what I’m thinking of doing and would love to get some feedback to hopefully improve it. The general idea is that it will consist of 3 or 4 sessions with each session lasting a couple of hours. The class will be opt-in and the students should have some technology background.  Session I  - Overview of MySQL and relational databases. What are they? How are they used? What are the alternatives? - Provide a quick overview of the normal forms and what they mean. What impact does it have when they’re violated and go over what well designed databases have. - Introduce the dataset we will be working with. This will mostly likely be a dataset I’ll pull from some of my side projects that will hopefully be relevant. Currently, I’m thinking of using a database containing some fantasy football data that I’ve scraped. - Make sure everyone has MySQL installed or can get it installed.  Session II  - Revist the dataset we’re working with and explain the relationships between the various tables and columns. - Go over the basic syntax of a query: SELECT, FROM, and WHERE. - Go over the basic INSERT statement.  Session III  - Review the basic syntax of a query and introduce the JOIN operations. Use joins to answer some simple questions from our dataset. - Introduce the GROUP BY functionality and the ways it can be used to summarize data. Use this in conjunction with joins to explore our dataset. - Develop some complicated and slow queries and introduce the idea of INDICES so everyone is aware of why they are useful.  Session IV  - Go over table creation and have the students come up with some interesting aggregate tables. - Provide a quick overview of how to diagnose a query for performance and how to test a query to make sure it was written correctly. - Discuss the various system tables (information_schema schema) and the various system commands that can be used to get a better understanding of MySQL",2,2,2014-08-17,3,"mysql, databases, tech classes",412,A MySQL class proposal
21,0,I got a free trial of LinkedIn business plus and had a fun time figuring out how to cancel it.,#product,"{% include setup %} It’s just too easy to rant against LinkedIn but I can’t help it. They recently offered me a free month of business plus so I took them up on it. Little did I know (although I should have expected it) that canceling would be a maze that I still may not have escaped.      The cancel screen hides the downgrade to free option and automatically chooses a paid “recommended account” with a bright clickable “Downgrade Account” button. And then, when you actually do manage to downgrade, it’s not clear from the account settings page that you downgraded since it still displays as the premium account option. Maybe when my free month is up it will downgrade or maybe I’ll get charged - how am I supposed to know? I do see a note that says canceled and I suspect I’m in the clear but there’s no way to actually confirm other than contacting support.      It’s one thing to optimize your funnels to get more conversions and revenue is one thing but tricking your users to subscribe is entirely different. If your product relies on this sort of “optimization” you really should think of another business model.",0,1,2014-08-18,4,"linkedin, optimization, user experience, ux",226,Trick your users into staying
30,0,Everyone extols the value of having a cofounder but it's critical that the founders are aware of each other's situations. Otherwise this will cause problems when things get tough.,#meta,"{% include setup %} Countless people have written about cofounder conflicts in a startup but I rarely see anyone talk about how important a similar situation is - financial and personal. There are no problems when things are going well and it's only when things start going poorly, which they inevitably will, that these issues surface.  A founder that doesn't have a lot of savings will have a different relationship to fundraising than the founder who has enough savings to keep going. The former will push to fundraise early while the latter will want to wait and search for the best opportunity.  A founder that's single will have a different lifestyle and priorities than someone who is married or in a serious relationship. The founders will have a different approach to work and may have a hard time agreeing on a culture that fits them as well as the rest of the team.  A founder that wants to start a company is different than a founder who wants to strike it rich is different from a founder who believes in changing the world. Each of them are valid perspectives but put them in a room together and it will be impossible to make even the simplest decisions.  At  Pressi , we had our fair share of founder conflicts and in hindsight a big part of it was how different our situations were - it's definitely beneficial to have a team come from diverse background and provide multiple perspectives but it's also critical that everyone understands where everyone else is coming from. And it's important to do this when things are going well, otherwise they will become bigger issues when things are going poorly and every emotion and event is magnified.",1,1,2014-08-22,2,"startups, cofounders",291,Cofounders and their situations
14,0,I just updated my Yahoo fantasy football stats scraper for the 2014 season.,"#data,#code,#python","{% include setup %} This might be too late for some but I dug up my Yahoo fantasy football stats scraper from last year and  updated it to work  for the 2014 season. The old version used the great  Scrapy  framework but unfortunately Yahoo changed something on their end that made the login spoofing too difficult to do via a backend script. The new approach uses  Selenium  to open up a Chrome web browser, login to Yahoo, and then iterate through each page of stats and downloads the data into a CSV file.  Note that the code was designed around my league’s settings and that the column order in Yahoo will depend on the scoring categories of your league. If that's the case you need to make sure to update the code (primarily the xpath expressions) to map to the columns in your view. Definitely feel free to submit a pull request that makes the code a bit more flexible since my goal was to get something out quick in time for a draft later this week.  And if all you care about is the data, here’s the  projected 2014 data  as of August 25, 2014.",4,3,2014-08-26,3,"yahoo, fantasy football stats, scraper",215,Yahoo fantasy football stats - 2014 edition
27,0,Just a quick write up of how settings files can be handled in a Django project. Things are constnatly improving so this will be outdated soon.,"#python,#code,#devops",{% include setup %} I was helping a friend deploy a Django project over the weekend and we chatted about the best way to manage multiple settings files in a Django project. The primary reason is that you will typically have different settings between a production and development environment and but at the same time will have a lot of options shared between them. A production environment will typically be more restrictive and optimized for performance whereas a development environment will be setup to provide as much debug information as possible.  This got me thinking of the various configurations I’ve gone through over the years and what my latest approach has been. If you have additional variations and suggestions that you’ve been happy with I’d love to hear them.     My first real project had a single settings.py file that defined a different set of options based on the hostname. The benefit here was that every configuration setting was in one file and it was pretty easy to see what the differences were between development and production. The problem is that if the hostname ever changes you may break your entire application. This also makes it difficult to share you code with a team since the hostnames will be different and you end up with a massive file full of different configuration settings. {% highlight python %}import socket  if socket.gethostname() == 'ubuntu':     CONFIG_OPTION = 'prod-setting' else:     CONFIG_OPTION = 'dev-setting' {% endhighlight python %}     A simple improvement was splitting this single file into a common settings file along with separate files for each environment that were then imported based on the hostname. The common settings file would contain the shared settings while the individual environment files would contain the settings unique to each environment. This kept the files cleaner and made it clear what setting applied to which environment. {% highlight python %}import socket  if socket.gethostname() == 'ubuntu':     from settings_prod import * else:     from settings_dev import * {% endhighlight python %}     The previous options still suffered from relying on the hostname so a simple improvement was using symbolic links to point to the appropriate file. With this approach we can still have a common file as well as the individual files but the environment-specific files are importing the shared settings. The big advantage to this approach is that the the symbolic link command only needs to be run once on each server and will always point to the correct file. {% highlight python %}# settings_prod.py  from settings_common import *  # Now set the prod only options CONFIG_OPTION = 'prod-setting' {% endhighlight python %}  {% highlight sh %}ln -s settings_prod.py settings.py {% endhighlight sh %}     Another option that I started using is using the DJANGO_SETTINGS_MODULE environment variable to point to the appropriate settings file. I adopted this approach after reading  Two Scoops of Django  which has a ton of other useful tips that improved my development approach. This approach isn’t significantly different than the symbolic link one but it feels less hacky since it’s an approach supported by the official Django documentation and it’s easier to examine environment variables than looking at the symbolic links across your directory. {% highlight sh %}export DJANGO_SETTINGS_MODULE = project.settings.prod{% endhighlight sh %},1,3,2014-08-30,3,"Django, python, settings files",561,Managing settings files in Django projects
20,0,Startups that cater towards developers should adopt an API first approach and share it publicy with their potential customers.,#product,"{% include setup %} When building a SAAS product geared towards developers the quickest way to start is to build an API. One can even make the argument that the MVP should just be the API documentation. This benefits both sides. Potential users of the API will know exactly what to expect and have a clear understanding of the functionality and limitations and you can quickly see if there are any issues or inconsistencies in what you’re building. Some non-fiction authors will share a table of contents with potential readers in order to get feedback and this extends that idea to companies and their products. Especially when your primary users are developers this is a simple way to share your idea and approach without resorting to buzzwords or even relying on a beautiful site design.  The best example of this approach is  Stripe . At launch, they had a beautiful API that you were able to use without creating an account. After seeing how it well it worked it was an easy decision to register for a full account. Tons of companies adopt an API-first approach for their internal systems and it’s not a lot of work to extend this to the external world. There’s definitely a risk in doing it since you’re exposing more of your internals but if you claim to be developer friendly it’s the best way to actually prove it.",1,1,2014-09-01,4,"API first startup, product strategy, saas, product",239,API first startups
31,0,I'm less into the various tech services than I used to be and I decided to go through the common ones and see how much I'd care if they disappeared.,#meta,"{% include setup %} A  recent article  on our attachment to social media got me thinking about my most commonly used services and their relative importance. The goal is to answer the question of how I’d feel if various services suddenly disappeared. After going through this process it feels as if these services moved from being necessities to feeling like luxuries. They either have a substitute that will do what they do or only have value due to the network - clearly these are important but I just have no attachment to the product itself.  - Facebook: I don’t care much for it and at a certain level I want it to disappear. The best reason I use it is the only reason I use it - everyone I know is on it.  - Twitter: I enjoy Twitter and use it more frequently than Facebook but don’t think I’d feel the loss terribly. I use it as a content source primarily and in the end the stuff I’m interested in can most likely be found via Hacker News.  - Hacker News: Years ago, I remember participating in the discussions but lately I’ve just been using it as a source of news. My current use case would be taken care of via Reddit or any other tech news aggregator.  - Foursquare: This would be the biggest loss but purely from nostalgia. I started using it when it launched and saw it evolve through the various products. I’m not a fan of the recent split into Swarm and Foursqare but still suspect I’d be most upset if Foursquare disappeared.  It’s sad that I feel so cynical about the apps and I’m trying to understand why. I suspect part of it is that the novelty of these has worn off and part is that there’s so many new tech products out there that it may just be overload. We’ll see how I feel about them after the next five years.",1,1,2014-09-07,1,social media,327,Importance of various tech services
31,0,There have been rumors that Twitter will move to a curated model for the stream. I think it's interesting to see the impact this will have on their tech stack.,#product,{% include setup %} Apparently Twitter is considering curation user’s timelines. A perspective people haven’t really discussed is the impact on the tech side. Right now each user has a unique timeline that needs to be presented in near-real time in case they need to see it. This results in a massive storage operation using Redis where these timelines are  continuously generated and cached . By moving to a model where every user can be categorized into a group that sees a particular set of tweets Twitter can drastically reduce the amount of data they need to store per user. I’m sure Twitter already has a way of categorizing users in order to support the ad product and this approach would extend it to the “stream” product. In a way it’s akin to how compression works - find repeated patterns and replace every occurrence with something shorter. Then when you want to uncompress you just reverse the process.  I doubt this is the primary driver of the curation discussion and there are clearly more important issues at stake but this may be the proverbial “cherry on top” that will get Twitter to move to the curated model.,1,1,2014-09-08,4,"twitter, curation, social media, tech stack",212,Curated Twitter timelines and the tech stack
35,0,Two different styles of coding are top down and bottom up. I prefer the top down approach since it lets you identify and resolve issues before spending a ton of time on their implementation.,#code,"{% include setup %} Over the years, I’ve noticed two distinct coding styles. Some approach problems top down and will stub out the entire solution using dummy values and methods and come up with a naive solution before fleshing everything out properly. Others will instead take a bottom up approach and try to complete each method entirely before moving on to the next one.  Especially for larger problems, I prefer the top down approach. By stubbing out the various pieces it’s easy to see how everything fits together and makes it easy to identify and solve potential issues before investing a ton of effort into a poor implementation. The other benefit is that I start thinking at a systems level and come up with implementations that tend to be more extensible.  The only time I find myself taking a bottom up approach is when the problem is very well defined and I know exactly what the solution is or when I’m working on HTML and CSS. In that case, and especially with my limited skill, there’s so much coupling between the various components that I can’t avoid going linearly through the components. It does make me wonder whether people who have more frontend experience have also adopted the top down approach.",0,1,2014-09-16,3,"coding, development, engineering",212,Top down vs bottom up coding
31,0,I attended two bootcamp meet and greet sessions and came away impressed by how much people can learn in 12 weeks. It got me thinking about the future of software.,#meta,"{% include setup %} I recently attended two web development workshop “meet and greet” sessions where recent graduates presented their projects and chatted with potential employers. I’m honestly surprised by how polished the projects were. Sure there were a few simple ones but most were solid; they were good ideas, well designed, and had functional backends. It’s amazing what it’s possible to do in 12 weeks.  These programs focus on a single frontend framework, such as Backbone or Angular, and a backend framework, usually Ruby on Rails. With the number of plugins and public APIs available it’s easier to get an app up and running than ever before. Of course these programs won’t provide the same level of knowledge as a degree or years of experience will but for many projects that’s not important. Being able to get something functional and private is more important than perfect and private and these bootcamps provide enough skills to do that. More importantly, they make code accessible to an entirely new group of people and provide enough skills to allow them to continue learning on their own.  It does make you think where tech skills are headed. As it becomes easier to build a larger variety of apps it will be interesting to see where software engineering will end up. Software engineering is a young industry and I suspect it’ll become increasingly specialized as the base set of tools and knowledge become widespread.",0,1,2014-09-17,3,"web dev bootcamps, coding, software engineering",241,Web development bootcamps
13,0,We ran into an RDS replication issue that I've never seen before.,#devops,"{% include setup %} Earlier this week we encountered an odd RDS issue that I’ve never seen before. An AWS hiccup caused a database replication query to fail which stopped the replication process. We discovered this the following day when we saw weird results during after running an analysis query. The nice thing was that this wasn't a huge deal since our production system relies on the master database but we did have to spend time dealing with this.  When we discovered this issue we did a few online searches to see how to resolve the issue and resume the replication. Turns out there's a command, ""CALL mysql.rds_skip_repl_error"", that will skip the current replication error and move on. In our case, the errors occurred when creating temporary table for a legacy job so we were able to skip it. Otherwise, we'd run the risk of breaking the sync between our master and replica databases.  Unfortunately, running this query once wasn't enough since the error keep on reappearing. After speaking with an AWS rep, we realized we could keep on running that command until we skipped past the replication errors. Another useful tip was to look at the ReplicaLag CloudWatch metric to see how far behind the replica database was from the master. In our case after going through a couple of dozen of these skip error calls replication resumed but the replica database was still more than a day behind.  While the replication caught up, we made a quick update to our scripts to point to our master database instead of replica so that our jobs would reference the correct data. After replication caught up we simply reverted this change.  To prevent this issue in the future, we're going to revisit the jobs that were using the temporary tables. We've also added a CloudWatch alert to notify us if replica gets too far behind. In a way we got lucky since these errors were recoverable. Without that we would have had to recreate the replica database which may have had a performance impact on our master database.",0,1,2014-09-20,4,"rds, aws, database, devops",349,Dealing with an RDS replication issue
29,0,"In a 1980 talk,Steve Jobs talks about the need to improve hardware in order make software more accessible and usable. Usability is what's driven technology, not core functionality.",#meta,"{% include setup %}     Earlier this morning I watched a Steve Jobs talk from 1980 where he discusses Apple and the relationship between hardware and software. An interesting piece comes at the 12:30 mark where he addresses the question “Right now software is powerful enough, what impact will improvements in hardware have on software?” His answer is great: “[We will] start chewing up power specifically to help that one on one interaction go smoothly and specifically not to help the calculation...  start applying that power to remove that barrier”  Sure the response is very Jobsian but the underlying point is significant. It’s only a tiny bit about what the software actually does; the majority is realizing that people will actually be using the software to solve problems and building tools for that experience. I remember starting with DOS on the family computer and being blown away when I first used Norton Commander. Similarly to when I saw Windows for the first time and saw my first smartphone.  Most hardware improvements over the past 30 years led to improvements in usability, not functionality. Processors in 2014 are 100,000 times more powerful than those in the early 1980s and a majority of the improvement went into user experience - better UIs packed into smaller devices. Without usability improvements computers wouldn’t be nearly as ubiquitous as they are now and would primarily stay a hobby for engineers. Each usability improvement brings aboard a whole new set of people. You can make the case that the same thing occurs with programming languages - very few people were writing assembly code at its peak compared to C code, and fewer people were writing C code at its peak than JavaScript.  Usability improvements are still happening but they’re taking the form of cloud and background services - akin to the way Google Now provides contextual information and the way Siri handles voice recognition. As sophisticated as they are, they will only get better as hardware improves.",0,1,2014-09-21,3,"usability, functionality,",337,"Software is good enough, why improve the hardware?"
21,0,My first experience with Java left a bitter taste in my mouth but recent work has made it fun again.,"#devops,#product","{% include setup %} It’s amazing the impact tools have on productivity and enjoyment. I remember my first foray into Java using a combination of text editors and Ant. Setting up and configuring a simple project was a nightmare and without the internet I don’t know how I would have figured it out. This initial experience made me associate Java with an unnecessarily complicated approach that I wanted to avoid.  After Java, Python felt like a breath of fresh air. The code was simpler, more compact, and I was able to just dive in. Discovering pip and virtualenv made me enjoy it even more. But no language is perfect and with enough you uncover the imperfections. Performance became a bottleneck when I started working on serious code and I missed the benefits of static typing - especially when refactoring a large projects.  Recently, I started using Java again and it’s a completely different experience. I’m not sure whether it’s due to hardware or software improvements but Eclipse feels faster and more responsive. It makes Java nearly as fun as Python. The static, strong typing makes it easy to do large scale refactorings, Gradle and the open source ecosystem make it trivial to leverage all sorts of libraries, and the performance/coding ease is great - especially when dealing with concurrency. Good tools can make a world of difference to the accessibility and joy of writing a language. I read a while ago that Facebook has the strongest people  working on internal tools  and I’m not surprised; it may be one of the most effective way to make everyone happier and more productive.",1,2,2014-09-25,3,"java, python, developer tools",280,Dev tools matter
36,0,"One behavior I always found frustrating on smartphones is the way clicking in app links opens up a new window. I recently discovered a mobile browser, Javelin, that has a great solution to this problem.",#product,{% include setup %} Something that’s bothered me ever since I started using a smartphone is the link opening behavior. Whenever I’m in an app and click on a web link it would immediately open up that page in a browser window. And when I’m already in a mobile browser and click on a link it would open that page up in a new tab. Compare this with the desktop environment. Clicking on a link within an app does open up a new browser window immediately but since there are shortcuts to quickly switch between programs it’s not a huge deal. And when I’m already looking at a webpage and want to open a new link it’s possible to open it in the background using command+click.  The most common reason I click on a link is as a bookmark so that I can go through it after I’m done with what I’m currently doing. The default behavior is the opposite of that - it turns something that I want to consume asynchronously into a synchronous process. Sure there are times where I do want to switch gears but the vast majority of the time opening a new window is a distraction. That’s why it’s so surprising that mobile web browsers have adopted this behavior. Smartphones are both slower than desktops and make it more difficult to switch between programs. Why couldn’t the default behavior be to open all new links in the background?  I finally found a browser that lets me do just that. It’s called  Javelin  and has a feature called “Stacks” that lets you click links while allowing you to continue using the phone. These links are loaded in the background and are ready to be consumed whenever you want to go through them by clicking on a little overlaid icon. This is quickly becoming one of my favorite new apps since switching to Android and I’m hopeful we’ll see other desktop behaviors translated into a mobile-friendly versions. I think this is where Android has an edge by providing a more open environment for developers to work with. With that openness you do end up with more crap but also more gold.,1,1,2014-09-27,4,"browsers, mobile, javelin, android",368,Mobile web browsing and Javelin
15,0,I had a CSV of fantasy footabll stats and used MySQL to normalize it.,"#data,#sql","{% include setup %} As part of my preparation for the Intro to MySQL class I decided to put together a dataset we’d be able to explore over the course of the class. While trying to think of an interesting dataset to use I remembered I had a script that scraped Yahoo’s fantasy football projections for the 2014 seasons that I used to prepare for my draft. The only issue was that the script generated a CSV file so I had to go through a series of steps to turn it into a clean, relational database. I thought it would be useful to share the commands below and provide some context for those interested in learning more about MySQL and the data import/cleanup process.  The first step is to create the table that we'll be loading the CSV file into {% highlight sql %}create database stats; use database stats;  create table orig_stats (   week int,   name varchar(100),   position varchar(20),   opp varchar(50),   passing_yds float,   passing_tds float,   passing_int float,   rushing_att float,   rushing_yds float,   rushing_tds float,   receiving_tgt float,   receiving_rec float,   receiving_yds float,   receiving_tds float,   return_tds float,   twopt float,   fumbles float,   points float ); {% endhighlight %}  Now we load the CSV file into the table making sure to specify the options properly. In my case this took a few attempts to deal with the line endings. {% highlight sql %}LOAD DATA INFILE '/tmp/stats-2014.csv' INTO TABLE orig_stats FIELDS TERMINATED BY ',' LINES TERMINATED BY '\r\n' IGNORE 1 LINES ; {% endhighlight %}  Next step is to create the tables we want to end up with. In my case I wanted to normalize the data which required designed a new set of tables. A big assumption made here was that a player will not get traded from one team to another. This is definitely not correct in the real world but it is good enough for this exercise. If we wanted to allow for trades we would have a separate table that would map a player to a team by week. {% highlight sql %}create table teams (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(100),   PRIMARY KEY (id),   UNIQUE (name) );  create table positions (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(10),   PRIMARY KEY (id),   UNIQUE (name) );  create table players (   id int unsigned NOT NULL AUTO_INCREMENT,   name varchar(100),   position_id int,   team_id int,   PRIMARY KEY (id) );  create table schedule (   week int,   home_id int,   away_id int,   UNIQUE (week, home_id, away_id) );  create table stats (   week int,   player_id int,   passing_yds float,   passing_tds float,   passing_int float,   rushing_att float,   rushing_yds float,   rushing_tds float,   receiving_tgt float,   receiving_rec float,   receiving_yds float,   receiving_tds float,   return_tds float,   twopt float,   fumbles float,   points float ); {% endhighlight %}  Now it's on to the hard part. We want to take the data in the original stats table and convert into a properly normalized data set. The strategy here is to start with the simple tables and work our way up to the more complicated ones leveraging the normalized data we created at each step. The first two tables are teams and positions and we can derive them from the ""position"" field in the original stats table by splitting the position field into two and realizing that the left side is the team and the right side is the position of given the player. {% highlight sql %}insert into teams   (name)   select distinct(substring_index(position, ' - ', 1))   from orig_stats order by position;  insert into positions   (name)   select distinct(substring_index(position, ' - ', -1)) as pos   from orig_stats order by pos; {% endhighlight %}  To generate the players table, we get the player position and team from the stats table and then find the associated ids from the teams and positions tables. The key assumption here is that there are no two players with the same name, on the same team, and the same position. {% highlight sql %}insert into players   (name, position_id, team_id)   select p.name, pos.id, t.id   from (     select name, position,       substring_index(position, ' - ', -1) as pos,       substring_index(position, ' - ', 1) as team     from orig_stats     group by name, position, pos, team   ) p   join teams t on t.name = p.team   join positions pos on pos.name = p.pos; {% endhighlight %}  We can figure out the schedule by getting a list of the unique matchups in the original stats table. Since the games are symmetric we only need to look at the rows that are home games. {% highlight sql %}insert into schedule   (week, home_id, away_id)   select s.week, t1.id, t2.id   from (     select week,       substring_index(position, ' - ', 1) as home_team,       substring_index(opp, ' vs ', -1) as away_team     from orig_stats s     where opp like '%vs%'     group by week, home_team, away_team   ) s   join teams t1 on t1.name = s.home_team   join teams t2 on t2.name = s.away_team   order by s.week, t1.id, t2.id; {% endhighlight %}  Putting everything together we generate the new stats table by doing the relevant lookups in the tables we created. We can ignore the redundant fields (name, position, opponent) and the only thing we need to watch out for is duplicate players. In this case there are two names, Zach Miller and Alex Smith, that need to be made ""unique"" by also looking at their team. {% highlight sql %}insert into stats   (week, player_id,   passing_yds, passing_tds, passing_int, rushing_att, rushing_yds, rushing_tds,   receiving_tgt, receiving_rec, receiving_yds, receiving_tds, return_tds,   twopt, fumbles, points)   select s.week, p.id,   passing_yds, passing_tds, passing_int, rushing_att, rushing_yds, rushing_tds,   receiving_tgt, receiving_rec, receiving_yds, receiving_tds, return_tds,   twopt, fumbles, points   from orig_stats s   join teams t on substring_index(s.position, ' - ', 1) = t.name   join players p on s.name = p.name and p.team_id = t.id; {% endhighlight %}",0,2,2014-10-01,2,"mysql, cleaning data",958,Normalizing a CSV file using MySQL
21,0,I ran the famous movie quote auto complete experiment using Android after seeing it done on XKCD using iOS 8.,#meta,"{% include setup %} Earlier this week XKCD featured a  comic  where oft-quoted movie quotes are autocompleted by iOS keyboard predictions. I decided to do replicate the exercise using Android and Swype. Some are similar while others are completely different. I suspect a big part of the difference is that Swype uses my history when offering the suggestions and since I’ve been travelling recently many of them tend to be airport related.       Say hello to my little  brother and sister and the other hand.        Toto, I've a feeling we're not  going to be a good time.        Bond, James Bond  with the Eagles to the airport.        I'm a leaf on the wind. Watch the video game console and I will get there early.        Goonies never say never been to the airport.        You have my sword. And my bow. And my wife.        Hello, my name is Inigo Montoya. You can send you a call to discuss the details.        Revenge is a dish best served from the other side of the terminals.        They may take our lives, but they'll never take our word for it.",1,1,2014-10-04,4,"ios, android, autosuggestion, xkcd",198,XKCD movie quotes by Android
37,0,"I felt crappy after a cold so went for a walk to clear my head. Thinking about nothing inspired me to come up with 3 resolutions that have made me feel happier, productive, and more inspired.",#meta,"{% include setup %} This weekend I felt discouraged after getting cold but dragged myself outside to go for a walk and clear my head. I ended up getting a cup of coffee and just sat on a bench for 30 minutes and letting random thoughts go in and out of my head. It turned into a form of personal meditation where some ideas crystallized and stuck around while others quickly disappeared. It felt great (so great that I ended up going back to my apartment and doing a bunch of chores I’ve been putting of!), especially considering that I had to drag myself away from the couch to even go for the walk and would have much preferred to just sit on the couch and watch football.  While sitting on the bench I made three resolutions that are making me feel more inspired and productive. They’re all simple and only require commitment but I’m definitely benefiting from them.      Stop procrastinating . We all have a ton of things to do and I got into the habit of delaying simple ones like dropping off clothes for dry cleaning or selling some old electronics on eBay. Lately I’ve started going through my list of todos and it feels great. Each small achievement is something that I can cross off and encourages me to move on to the next one. For most of these priority doesn’t even matter since the important stuff happens anyway; it’s the small items that tend to get stuck in a state of never done.    Focus on one thing at a time . This is the hardest one for me but I realized that I tend to get distracted too easily. Whether it’s checking my email or browsing Hacker News it’s a huge inefficiency. I’ve also found myself trying to do work passively while watching a football game and it’s noticeable how little I get done that there’s no point in even attempting to work. It’s not possible to achieve  flow  without being able to concentrate on one thing and quality suffers. It’s not even a good use of time since you’ll just end up taking a longer time to do two things poorly. It’s better off to get the stuff that requires your attention out of the way and then spend the rest of the time relaxing.    Take distraction free breaks . Given how useful going for a aimless walk was I decided to make this a habit and try to go for at least three a week. So far I tried going before work but it wasn’t the same. Either the subconscious knowledge that I needed to track my time in order to get to the office or being in a highly trafficked city intersection made it harder to get lost in my thoughts. In any case I might end up making this a once-a-week activity and do it on the weekends in a quieter place where I can be distraction free.     I’d love to know of other resolutions that people have embraced to make them feel happier, inspired, and productive so if you have any ideas definitely let me know.",1,1,2014-10-08,4,"inspiration, productivity, happiness, depression",534,Clearing my head
27,0,I had a bunch of old useless files I didn't want to permanently get rid of so I started using AWS Glacier to back them up.,"#aws,#devops","{% include setup %} I’ve been trying to reduce the amount of stuff I have and a big part of it is old electronics. I’ve been selling off old headphones and random cables but the one thing that’s been more difficult to get rid of is older hard drives. I know that most of the stuff on them is junk that I’ll never see again but it’s still tough to just throw it away. They’re reminders of previous jobs and old projects that are a part of my identity that are tough to permanently delete with a click. Many of them are unique in the world and only exist on an old hard drive. I realize it’s foolish to keep them around but it’s tough to let go.  I thought about moving the stuff over to Dropbox but upgrading to a paid plan to store a bunch of files I’d never touch again seemed wasteful. I wanted something that was a one time upload, was cheap, and gave me peace of mind. I recently read about  Amazon’s Glacier  product and it fit the bill perfectly. It’s a penny a month for each GB stored but with additional fees for retrieving old files. Looking at the amount of files I want to store this would cost me less than 20 cents a month and allows me to get rid of a ton of old drives. I spent a couple of evenings this past week tarring up these old files and transferring them to Glacier using the  Simple Amazon Glacier Uploader  app.  Glacier is a great fit for data that’s difficult to throw away but unlikely to be accessed and is significantly cheaper than Dropbox. I’m in the process of going through more and more of my data and seeing which of it would be a good fit for Glacier.",2,2,2014-10-12,3,"aws glacier, dropbox, backups",317,AWS Glacier
26,0,Link bait titles are great at getting initial page views but cheapen your content and annoy your audience. This leads content to become a commodity.,"#meta,#product",{% include setup %} A couple of days ago I saw a mic.com article with the title “A European country is now offering free college education to Americans” but the only way to find out which country this was (Germany) was by clicking through to the actual page.      I understand that content sites make the bulk of their revenue through advertising but resorting to a link-bait approach seems like a terrible idea. It’s a shortsighted attempt that increases page views at the cost of insulting your audience and cheapening your effort that will not work as a sustainable strategy. Relying on headlines to generate traffic without any meaningful content is a great way to get to become a commodity. I hope that there are enough people out there that care about the content they’re producing and have a passionate audience that can be monetized based on quality of engagement rather than on quantity of page views. Otherwise we’ll all end up in a race to zero.,0,2,2014-10-18,3,"content, advertising, publishing",185,Link bait titles are a race to zero
26,0,I made my firstr contribution to an open source project and want to start doing it to every project that I use and find useful.,#meta,"{% include setup %} Despite being a huge proponent of open source I’ve never made a contribution to a  third party project  until this weekend. The project was a simple scraper that downloads each Jeopardy game from  j-archive.com , parses the data, and loads into a SQLite database. The project had one issue open that was to make the download code threaded in order to reduce the time of downloading nearly 4700 games from over 7 hours to less than 30 minutes. I gave this a stab on Saturday and submitted a pull request that was merged in by the author on Sunday.  The process was surprisingly simple and it felt good making an improvement to an already useful project. An idea I’ve been toying with is making a contribution to every open source project I use. Some would be simple and may not even be code while others might be significant improvements. The idea is to continue continue the cycle of improvement to projects that have made me more productive. I can only imagine what would happen if everyone who uses open source adopted this approach.",2,1,2014-10-19,1,open source,197,My first open source contribution
29,0,I get around one spam blog comment a week and wonder what the goal is. They're clearly spam and I don't understand how they add value to anyone.,#meta,"{% include setup %} Ever since I’ve started blogging I’ve been getting around one spam blog comment a week.  Disqus  does a nearly perfect job of flagging them so I don’t understand the motivation behind it. They’re obviously spam and my readers are suave enough to never click on any of the links. There’s also little, possibly none, SEO value since they’re all loaded asynchronously and every link has a rel=”nofollow” property. And if the goal is to spark a discussiong and raise awareness they're so poorly worded that no reader will take them seriously. The only thing I can think of is that these companies pay a third party service to grace tangentially related blogs with content on their behalf and these third party services go the cheapest possible route in both effort and quality.",1,1,2014-10-26,2,"spam comments, blog",140,Spam blog comments
21,0,I volunteer tough a four part Intro to MySQL class and wanted to share some of the lessons I learned.,#meta,"{% include setup %} Last Thursday was the last lesson of the four part Introduction to MySQL class I’ve been teaching at  Coalition for Queens  and I wanted to summarize my thoughts while they’re still fresh.  The diversity of the class was amazing and shows how useful affordable technology programs are. You get a mix of people from different backgrounds and different ages that all want to improve themselves and can all contribute in their own ways. Everyone has a unique experience and introducing technology into it may open up new opportunities.  It’s tough to get a curriculum that works for everyone but it’s important to try. Some people grasp concepts quicker than others. Some want to see more hands-on exercises. Some want homework assignments. Some want to be able to split up into groups and work with others on more complicated assignments. The dataset itself needs to be relevant and realistic or people will lose interest. I put the  curriculum up on GitHub  for suggestions but didn’t get any - hopefully others will use it in their classes.  Tools matter. For the first two sessions I used Keynote to generate the presentation and then exported it to a PDF. This approach lacked syntax highlighting which I wanted given the technical nature of the course. I switched to the wonderful  Remark.js  which allowed me to create slideshows using GitHub flavored markdown. This allowed me to integrate exercises into the lecture while incorporating syntax-highlighted examples. One issue was that it wasn’t as straightforward to export it as a PDF and I had to print it to a PDF file via Chrome.  Volunteer teaching is a great way to “teach to fish” rather than just giving a fish while helping develop public speaking skills and meeting a ton of awesome people. If you’re in New York City, work in technology, and believe technical skills are increasingly important you should take a look and try teaching a class at Coalition for Queens.",3,1,2014-10-27,3,"teaching, volunteering, mysql",343,Lessons learned teaching a MySQL class
25,0,I recently went to the library for the first time in nearly a decade to check out a book and felt pangs of nostalgia.,#meta,{% include setup %}      A.D White Library @ Cornell University by eflon    For the first time in almost a decade I checked out a book from a library. I don’t know why I ever stopped - the experience is extremely simple and you’re able to read a book for free. I had a book on my Amazon wishlist for a couple of weeks that I held off on buying but was able to read it over the past week after a quick visit to the library.  As a kid I used to go the library all the time and would go through multiple books a week and this brought back all those memories. I feel guilty for abandoning libraries in favor of Amazon and my iPad and think many people are missing out by consuming everything digitally. It’s not just the physical element but also the nostalgia and the knowledge that there were dozens of people who have read the exact same book you’re reading now. Reading a library book makes you feel that you’re a step in the book’s journey as it grows from home to home and person to person. That’s completely lost in a digital world and it will only get worse as libraries change to stay relevant. Until then I look forward to going back and checking out another book.,1,1,2014-10-28,2,"libraries, books",249,Libraries
28,0,I'm a huge fan of Markdown and love the way it's remained a tool rather than turning into a product. It's a lot more powerful this way.,#product,{% include setup %} I keep on discovering new use cases for  Markdown  the more I use it. My first exposure was when I migrated my blog to GitHub pages from Wordpress and Tumblr. Since then I’ve discovered GitHub flavored markdown which supports syntax highlighting which has been amazingly useful when blogging on tech topics or putting together notes for a tech talk. Just recently I wanted to include some MySQL snippets in a Keynote presentation and discovered the  Remark.js  library which lets you generate in-browser slideshows in Markdown with syntax highlighting.  There’s this desire to turn tools into products and it’s refreshing to see tools stay tools. They are able to stay much more flexible and evolve organically based on the needs of users rather than a preset direction. Markdown is a perfect example of this - it started simple but has evolved to have multiple variations and is used as the base for many other projects. At this point it has become such a standard that it can’t be co-opted by any single product or company.,2,1,2014-11-01,3,"markdown, product, tools",187,"Tools as tools, not products"
25,0,A recent FiveThirtyEight article on the sneaker resale market and getting luxury for cheap is a good guide for how to launch successful products.,#product,"{% include setup %} I recently read a FiveThirtyEight article on the  sneaker resale market . The concept is extremely foreign to me since I tend to not collect anything other than old notes and have a tendency of grossly mistreating my shoes and clothes. Nonetheless, I found it fascinating as it discusses the incentives of the various parties involved and comparing them against standard economic theory. One passage in particular was so insightful that I had to save it:    That differential allows people to buy something on the cheap but feel like they’re wearing a luxury item.    “So even if you paid $100, you’ve got $800 on your feet. It’s like having Gucci,” Taylor said   Everyone loves getting a deal but I think this is slightly different since it’s not so much about getting the deal as it is about being able to afford luxury and show it off. Sure it’s vain and has a bit of conspicuous consumption but if it gives someone self confidence and makes them feel like a million bucks I can’t complain.  We should strive to provide this type of experience when building apps and services. It’s not just adding some cheap gamification tricks or rewarding early users as much as it is making people proud to use your product. gamification tricks or rewarding early users as much as it is making people proud to use your product.",1,1,2014-11-05,2,"product, marketing",250,Luxury for cheap
22,0,To make it easier to connect to an AWS instance I had an old bash script that I've improved in Python.,#code,"{% include setup %} Last night I took an old bash script I wrote that simplified connecting to an EC2 instance in an AWS account and implemented the same code in Python. The old code worked by listing a set of AWS instances and then prompting to pick a single one to connect to. The problem was that it wasn’t always easy to find the index of the desired instance and the code took a bit of time to run.  The new code leverages the Python AWS library to pull down the list of instances for a given region and then filters it down based on the name, IP address, or public DNS. If it turns out there’s a match then it will only return the public IP address which makes it easy to connect using ssh. For example, to list all servers containing the name “web server” you would run the following:  {% highlight sh %}python list_hosts.py --region=us-east-1 --filter=""web server"" {% endhighlight %}  And if you know there will only be one you can connect to it directly by using ssh and running the script inside two backticks: {% highlight sh %}ssh `python list_hosts.py --region=us-east-1 --filter=""web server""` {% endhighlight %}  The code’s up on  GitHub  but at the moment there’s just this single script. I’ll keep adding more as I run into various issues working with AWS.",1,1,2014-11-09,4,"aws, ec2, python, ssh",239,Some simple AWS tools
29,0,Recently I've been thinking about the optimizations I've made when working on my computer. They're tiny on their own but can add up to a ton of time.,#meta,"{% include setup %} A pretty trivial post but something I’ve been doing for a while now is keeping my dock as a vertical bar on the right of my screen. I started doing this years ago when I was working on Windows and it was too difficult to track every single program that was running. At that point I was in finance and would have a dozen Excel workbooks open and needed to be able to quickly switch between them. The only way I could do this with a bottom toolbar was by making it extremely thick which would take up too much space. Moving it to the side solved that problem and I stuck with it as I moved to Ubuntu and now OS X.  These days I have significantly fewer programs running at once and I’m much better at hopping between programs but it’s surprising what a good dock location can do for efficiency. Getting everything right only saves a fraction of a second but doing this countless times a day adds up. As important as tools are, the way they are access and used is just as important. The challenge is getting stuck with a suboptimal routine that you’re used to and not moving to a more optimal one since the short term cost is high. The greatest example of this is probably the  Dvorak keyboard layout  - in theory it’s significantly faster than QWERTY and yet virtually everyone is using the slower QWERTY approach. It’s just good enough.",1,1,2014-11-13,4,"productivity, dvorak, keyboard, docks",257,Productivity optimization
27,0,JavaScript services are becoming increasingly powerful and allow a whole slew of applications to be built without touching a line of backend code. This is incredible.,#meta,"{% include setup %} I’ve been a fan of GitHub pages ever since I started using them to host my blog a couple of years ago and a thought that’s been constantly popping up is why there haven’t been any products or services that help small businesses host their sites on GitHub. GitHub’s  terms of service  forbid a third party from hosting pages on behalf of customers but it doesn’t seem as if there’s anything stopping someone from building a tool or documenting the set of steps to help someone create a simple site and have it hosted on GitHub. That way the business only has to pay a domain registration fee while still getting fast and robust hosting with a fairly solid CMS.  Going further this site can be made significantly more dynamic by integrating third party services via client side JavaScript. Use Facebook to handle authentication, Google Analytics to provide analytics, Disqus to provide comments, and Firebase to provide a data layer. There’s no backend to maintain and you get to use a set of free and powerful tools. This isn’t going to work in every use case but over time we’ll see more and more applications built using this approach.  The simplest code is the code that you don’t have to write and I think we’ll start seeing more and more of these third party services that provide specialized functionality via JavaScript snippets. Many of them will also be free when starting out since their marginal cost will be virtually zero for small projects and will encourage tons of people to create these pseudo static sites that provide dynamic functionality without a backend. I suspect that as these tools become more popular we’ll see them packaged together by other companies and services that will make web development accessible to a wider audience. One of these days I’ll see what kind of application I can build using free services and client side JavaScript.",1,1,2014-11-18,4,"javascript, github, static sites, client side",333,Backendless applications
29,0,Google is launching a service called Contributor which will allow users to pay a monthly fee to avoid ads. This fee will then be distributed to participating publishers.,#product,"{% include setup %} Google recently launched a program,  Contributor , that offers an ad-free monetization model to publishers. The idea is that a user pays Google up to $3 a month and in return Google will not show that user any display ads on a website that’s a participant in the program. The monthly payment will then be distributed across the participating sites - most likely based on how many times you’ve visited that site.  I like the idea - not because most ads are terrible but because it shows that both publishers and Google are willing to experiment with another approach. Ads, as much as we dislike them, are the primary way content producers make money since web users expect free content everywhere.  The biggest audience for Contributor will be those who currently run adblock but feel guilty about it. Most people want the content free on principle and refuse to pay but the people that are using adblock but do want to support the publisher may be willing to pay the $3 a month to feel noble - in fact they might keep on running adblock and treat this as a way to reward the sites they visit.",1,1,2014-11-20,5,"google, adtech, publishers, ads, adblock",203,Paying publishers without ads
18,0,Fab raised over $330M but is selling for $15. Just a reminder of how difficult startups are.,#meta,"{% include setup %} The recent news that PCH is set to acquire Fab reiterates how difficult startups are. So many startups strive to get an investment and believe that once they raise a round everything will get easier. That’s when things get difficult. Instead of focusing on achieving product market fit you start worrying about market share, competition, company culture, recruiting, process, which require a completely different skillset than what you started with. And to add to that you’re now accountable to a growing list of employees, shareholders, and customers. When I was working on my first startup I really thought that being able to get funding was the measure of success. Now I realize how naive that view was and how much more there actually is. Fab raised over $330M and wasn’t able to grow into a successful business despite undertaking massive pivots. There  are probably thousands of founders claiming that they’d be able to succeed with that kind of money without realizing how difficult it actually is.",0,1,2014-11-22,2,"fab, startups",171,Fab's fall from grace and the difficulty of startups
16,0,AWS is constantly shipping new products and features and it's incredible how productive they are.,#product,"{% include setup %}           I never thought about this until [Shaun](https://twitter.com/szach) brought it up but now I see it all the time: AWS ships code quicker than any other company that size. Some are simple feature improvements to existing products, such as an advanced instance search, additional configuration options for ELBs, and new instance types, but others are entirely new products, such as Lambda, EC2 Container Service, and the newly announced code management suite.  What’s even more impressive is that they provide infrastructure to thousands of other companies and the cost of failure is massive. A breaking feature will affect thousands of companies and cost a ton of money and hurt their reputation. Yet they keep on shipping and launching as if they were a newly launched startup.  I’d love to know the process Amazon has in place that drives these releases - both how they’re able to code at such scale as well as the test process they use to make sure no existing functionality breaks due to new features.  The half serious joke here is that Amazon is always shipping - physical items from Amazon.com and digital services from AWS.",1,1,2014-11-24,2,"aws, product",210,AWS: Always shipping
31,0,After reading Rob Ewaschuk's post I spent some time going through our alerts and figuring out how they could be simplified. This led to a discovery of latency based alerting.,"#devops,#aws","{% include setup %} A month or so ago I read Rob Ewaschuk’s  philosophy on alerting  and since then I’ve been trying to be more aware of the alerts we have and whether any can be improved. The most actionable insight was to start thinking in terms of “symptom-based monitoring” where the alerts should reflect what the users are experiencing rather than various issues along the tech stack. This aligns your alerts with user expectations and can also simplify alerting since they will all be running at a high level. It may take longer to diagnose what the underlying problem is but it will reduce the total number of alerts required.  One of our alerts checks for faulty instances that are attached to a load balancer. We're notified whenever one goes down with the goal of investigating the cause and getting it back up and running. While serious, it's not critical since there's a fair amount of redundancy and the user won't notice any impact unless a large enough number of instances fail. Using the symptom-based monitoring approach we were able to tweak the alert to monitor the  latency of requests  made by the load balancer to the instance and trigger a warning if it gets too high for too long. This reduces the number of non-critical alerts while making critical alerts more in line with customer expectations.  The larger an application gets the more essential it is to have a firm overview of the system with a solid set of alerts. Too many and you end up either missing important ones or wasting too much team dealing with false positives. Too few and you discover problems too late. Alerts are typically, and rightly, ignored on smaller projects but when you have multiple applications distributed across dozens of instances it's increasingly important, and simultaneously more difficult, to understand what's happening. Something I've started doing to identify potential improvements is tracking every single alert and tracking it's false-positive rate with the goal of finding alerts that aren't meaningful and either get rid of them or replace them with something more actionable. Over time we'll hopefully get the right balance.",2,2,2014-12-01,4,"aws, monitoring, alerting, devops",374,Symptom based monitoring
26,0,I worked on a side project while on a flight and it's a completely different experience without the internet. I think everyone should try it.,#meta,{% include setup %} Whenever I fly I try to be at least somewhat productive. This time it entailed finishing up an old blog post and messing around with Node.js on a side project. They say the only way to appreciate something is when it’s gone and that’s how I feel about developing without internet access. It’s such a common occurrence to need to look up the documentation for a particular function or library or search for novel error messages that my approach is completely altered without the internet. Where before a few visits to Google or Stack Overflow would have taken care of the problem now I get to rely on man pages as well as dozens small experiments to figure out what’s happening.  It’s definitely not as efficient as having everything at your fingertips but it’s definitely fun in small doses. In my case I get to brush up on some rusty skills and also get to be a detective when diagnosing bugs. Knowing that you will be coding without internet also forces you to set up a standalone development environment. This requires making sure all the static assets you use are available locally and can be served by your application as well as making sure you have a local database that contains realistic data.  It’s tough to avoid the internet when coding but it’s a worthwhile exercise to attempt a few times a month - it will make you appreciate what you have but also introduce you to a whole new set of skills.,0,1,2014-12-07,2,"coding, internet",258,Internetless coding
39,0,Piketty's Capital in the Twenty-First Century is a wonderful book but the biggest impact is showing how useful it is to have publicly accessible data. I'm hopeful we see more and more of this in the future.,"#meta,#data","{% include setup %} I read Thomas Piketty’s Capital in the Twenty-First Century a couple of months ago but have only organized my notes and thoughts now. It’s a simple, enjoyable read that provides an overview of the modern western economies and offers a compelling explanation of how wealth and income equality occur. I took a variety of economics classes in college but none of them felt as concrete as the book: Piketty does a great job introducing simple mathematical relationships and then simulating the results under different conditions. This allows the reader to get a feel for the data and makes the ideas much more tangible than an abstract formula. Piketty couples this with the economic data from the past two centuries to craft a persuasive argument for the causes of wealth accumulation.  Countless others have looked through the data, identified issues, and provided counterarguments so I don't want to get into that but I do want to highlight how important having data is for all types of research. If we're serious about these topics we should strive to collect as much data as possible while making it as accessible as possible. Piketty spent numerous hours collecting and transcribing the data from various paper sources and it's amazing what came out; I can only imagine how much other valuable research would come out if there was more publicly available data.  Governments should be responsible for collecting data and releasing it publicly. Many are starting to do this already although it still tends to be obfuscated behind a navigational maze and hidden in esoterically formatted PDFs. Over time we should see it become more transparent as the data formats standardize and we develop better tools to dig through the existing data.  Another issue we need to address is data correctness. On one hand it's great that people are going through Piketty's data and making sure it's valid but on the other if it's extremely confrontational and used to invalidate his work it serves as a warning to others that plan on releasing their data. Why would a researcher spend thousands of hours collecting data and making it accessible and then have to deal with the critics who find a few issues? Much easier to keep the data hidden and only provide the high level numbers that can't easily be challenged without doing the hard work. This perverse incentive needs to be resolved if we expect to see high quality researched being produced with open sourced data.  I'm hopeful that these larger scope theories with potential societal-impact become more common as we move into the 21st century. We have an increasing variety of tools to start making sense of this data with both individuals and institutions being more involved in organizing the world's data. No theory will ever be perfect or explain every case but having more data will serve as a guide for governments to hopefully improve life for their citizens. And if data is collected along the way it will fuel more analysis with actionable insights.",0,2,2014-12-09,4,"piketty, capital in the twenty-first century, economics, data",510,Piketty and the power of data
30,0,Constantly seeing something makes you oblivious to the changes and that's how I feel about technology. Traveling breaks that and lets us see how much tech is actually changing.,#meta,"{% include setup %} If you’re constantly watching something grow it’s hard to notice the magnitude while those further away see it immediately. This is well known for parents not seeing how quickly their children are growing but obvious for distant relatives and friends who get a glimpse once every few months.  I have the same relationship with technology. I’m surrounded by it each day that it’s hard to tell how much it’s changed but a way to combat this bias is by traveling, especially to developing countries.  I’m writing this blog post on a Macbook Air that has a ridiculous amount of battery life at the Delhi airport while tethered to a mobile hotspot running off of my US phone. It's not as fast nor as comfortable as what I get at home or in the office but it's incredible, especially when compared to my two prior trips, a year ago and four years ago. During last year’s trip I had to buy a cheap Android phone and spent countless  hours running around  trying to get a working SIM card. This time I didn’t even have change SIM cards due to T-Mobile’s global roaming plan. Four years ago I didn’t even bother doing anything on my phone and had a 3G USB dongle that required it’s own proprietary software to even connect to the internet.  New apps and products launching is just the surface, the biggest changes are happening to infrastructures that make these experiences possible. These aren’t visible day to day but are unmistakable when seen year to year and traveling makes them apparent.",1,1,2014-12-13,2,"traveling, technology",274,Travel to appreciate technological growth
30,0,Sometimes the best thing to do when working on a tough problem is take a break and think about something else - this allows the subconscious to do the work.,#meta,{% include setup %} Something I’ve encountered is being stuck on a difficult problem but then taking a break until an “aha moment” just materializes. This happened throughout college on difficult problem sets as well as countless engineering projects at work. Sometimes instead of getting hung up on a tough problem the best thing to do is to forget about it and go for a walk and let the subconscious take over. I don’t know why this works but it seems to be common with others as well.  Bill Gates takes an  annual reading vacation  where he reads books across a variety of disciplines in order to have their themes cross pollinate and spawn new thoughts and ideas. This isn’t a conscious process and he relies on his subconscious to do the organization and connect the different ideas together. This is akin to what happens when we stop thinking about a tough problem and focus on something else: the mind is free to wander and may combine them into something that’s useful - or at least inspire another thought that may be relevant.,1,1,2014-12-14,3,"thinking, coding, subconscious",187,Waiting for the aha moment
24,0,The Yahoo fantasy football app logs me out but doesn't force me to enter a password to log back in. What's the point?,#design,"{% include setup %} Now that fantasy football season is over for me and I have no risk of angering the fantasy football gods I can complain about an interface decision in the Yahoo Fantasy Football Android app. Every once in a while the app will sign me out, which I suspect is a security feature, but I can log back in without having to re-enter a password. The only effect this “feature” has is getting me annoyed. The app has clean and simple visual design but that shouldn’t be prioritized over actual usability. Hopping between apps is such a common task that developers should strive to make it as painless as possible. This may involve changing the views around to make them more light weight or figuring out a way to simulate behavior without having to show a loading screen but it definitely makes the app feel snappier and more responsive.",0,1,2014-12-16,3,"usability, ux, design",154,Why log me out?
20,0,Amazon's Fire TV Stick is great. I've been using it to replace my Xbox 360 and a Raspberry Pi.,#product,"{% include setup %}           Photo by  Rancho de la Luna      Earlier this week I set up Amazon’s [Fire TV Stick](http://www.amazon.com/Amazon-W87CUN-Fire-TV-Stick/dp/B00GDQ0RMG) and wanted to jot down some thoughts while they’re still fresh. For the $20 promotion price, it’s a great deal. My alternative was an Xbox 360 along with a Raspberry Pi running XBMC. The Xbox would be used for streaming shows on Netflix and watching older DVDs while the Raspberry Pi would let me watch various files off of a USB stick. I’ve never tried a Chromecast so don’t know how the Fire Stick compares but so far it’s been much quicker to startup and navigate than either the Xbox or the Raspberry Pi. When all you want to do is watch a quick show during dinner it’s a bit frustrating when you’re done eating by the time the Netflix app is ready to use on the Xbox.  Other than speed, the other big benefit has been the ability to use my phone as a remote. This allows both voice search, which Amazon has been doing a good job of transcribing, as well as a real keyboard. Having to type by navigating an alphabetically-sorted on screen keyboard with a laggy controller is not fun.  I haven’t had a chance to install any games or explore any of the other apps but I’m still impressed. It might be time to finally get rid of the Xbox and the old DVDs.",2,1,2014-12-18,4,"amazon fire tv stick, firestick, chromecast, xbox",267,Amazon's Fire TV Stick
21,0,As technology improves and we rely on it more and more passively it's going to cause huge problems for privacy.,#meta,"{% include setup %} I just discovered that Google launched a new  AdWords feature  to help brick and mortar store owners track the effect their online spending is having in the offline world. The way it works is that if a user sees an ad for a particular store or product on their phone and then ends up close (based on the location sharing option in iOS and Android) to the store in question, Google will use that information as a signal that the ad was the cause of the store visit. It’s not supposed to be perfectly accurate but the idea is that with enough data Google can come up with models that can estimate the actual numbers.  Mapping online spend to offline conversions has been the holy grail ever since advertisers started spending online and with the proliferation of smart phones that track everything we’re getting closer and closer to solving that problem. For centuries advertisers had to estimate and have faith that their spend in newspapers, magazines, and public spaces was having an impact with no good way of measuring the results. The ability to track a person’s location is immensely powerful and we’ll start seeing more and more use cases. Using a similar approach it may even be possible to see what effect a billboard ad has: monitor the locations of your ads and if anyone walks by them assign a probability that they’ve seen it. Then if they end up in your store you can assume they got there by looking at the ad. It’s significantly more difficult than that since people see hundreds of ads a day and the result is not always immediate but even having a tiny bit of data is better than none at all.  This should make everyone a lot more concerned about their privacy. In the past it was simple to have distinct lives - home versus work, inside versus outside, online versus offline - but with our attachment to modern gadgets the lines are rapidly blurring. Being aware is the first step in avoiding being tracked but it’s only a short term solution. As technology improves we’ll rely more and more on passive benefits which when coupled with better and faster data mining algorithms will make it very hard to live “off-grid.” We rely on government to preserve our privacy but I worry that we’re moving too quickly for the legislative process to have any real impact. Bitcoin and other distributed systems may be able to counter this decrease in privacy and I’m curious to see what sort of counter-systems they’re able to produce.",1,1,2014-12-20,3,"technology, privacy, society",448,Passive technology and the decline of privacy
35,0,DevOps is a necessary skill for all software engineers to know. It gets you closer to the hardware while letting you think at a more abstract level of how your application will actually work.,#devops,"{% include setup %} I’m becoming increasingly convinced that DevOps is a necessary skill for any software engineer to have. It gets you closer to the hardware and helps you understand the way your code will actually run and where it fits within the tech stack. It also provides independence when working on new projects since it gives you both the knowledge to understand the needs as well as empowers you to make them happen. This is especially important when working at a small company where there’s immense value in having general skills that can be used to make progress independently without need to disrupt others. My first job writing code I only had to worry about my small patch of software but over time I’ve slowly picked up a variety of DevOps skills that help me write better code. Below are the skills that every software engineer should know - they may not all fall under traditional DevOps but I believe they’re essential for anyone writing code. If you have any others let me know and I’ll add them to the list.  - **Shell commands**. Almost any task can be done efficiently using a series of shell commands. Being aware of the various commands and their options makes it simple to find specific files, summarize data, or just monitor a server. The most important ones are ls, cd, pwd, rm, wc, find, grep, head, tail, less, sort, cut, sed, and curl. - **Configuration of various applications**. This might be specific to Linux/Unix but knowing the default configurations of common applications and where the settings are is important when setting up an application and diagnosing problems. There have been countless times I installed something on my box only to run into issues with unexpected behavior. Being able to examine the configuration and grep through the logs is critical to understanding what’s actually happening. - **Package managers**. Both the ones by provided by the operating system (apt and yum) as well as the language specific ones (pip, npm, gem, CPAN). There already so many powerful open source tools and libraries available that it’s rare to find something that can’t at least serve as a starting point for whatever you’re building. - **Web architecture**. This is a big one but it’s crucial to understand how the internet works from the time you type in a web address to a browser to what’s going to happen on the server. A good starting point is to look at Amazon Web Services and be able to explain each of the services offered, how they’re used, and how they fit together. Even better is to play with them to get a sense of how they can be used. - **Setting up and deploying an application on a brand new VPS**. Using AWS or Digital Ocean it’s trivial to get a virtual private server (VPS) but it will come completely empty. Being able to log in, install the necessary software, and get it responding to external requests is vital for anyone writing web code. Even better is deploying multiple applications on the same VPS and configuring DNS to make it work. - **Back of the envelope calculations**. Having a sense of how long various types of requests and operations take is important in understanding the type of hardware you need and where optimizations can be made. Another good skill is thinking in terms of “bytes.” This helps you estimate how much memory your code is using and makes it easy to understand what solutions are scalable and which will end up causing problems.",0,1,2014-12-26,4,"devops, aws, vps, engineering",591,DevOps for the rest of us
31,0,Instead of focusing on making an AI act as a human it might be interesting to try a form of the Turing test where a human acts as an AI.,#meta,"{% include setup %} A friend sent me  an article  where the author discusses the recent news of an AI finally beating the Turing test and how he himself was clearly able to determine that the AI was not a human. The most common explanation of the Turing test is where someone communicates with both a human and an AI and is not able to tell which is the machine and which is the human. It’s almost always phrased in the way that a human will act normally and the AI will try to act as a human, mistakes, typos, and imperfect information.  Regardless of whether modern AIs can beat the Turing test I think it’s inevitable that an AI will conclusively beat the Turing test in the coming years. A more interesting question is whether a human can trick another human into thinking he or she is an AI. It’s similar to the Turing test in that it’s supposed to make the AI and human indistinguishable to a judge but instead of making the AI smarter we’re dumbing down the human.  The nice thing about this approach is that historically it was very easy for a human to act as an AI by making dumb mistakes and responding with non-sequiturs. I suspect it’s currently quite difficult to respond in a way that would convince someone you’re an AI, even after enough time speaking with one, and I’d love to see this attempted. In the end both of these approaches converge to the same goal of making AIs and humans indistinguishable and this is just another way of looking at it.",1,1,2014-12-29,2,"turing test, ai",283,A new Turing test
33,0,I've stumbled unto a way of doing recursion using a series of redirects. I can't think of a real use case but it's one of those fun hacks that's interesting to see.,"#code,#javascript","{% include setup %} I’ve stumbled onto what seems to be a solution without a problem but something that’s been fun to experiment with and might have an actual application. The idea is to replace a recursion step with a URL redirection. In this situation the base case will return a 200 response while the recursive step will do a redirection with a slightly updated URL. The sample node server below uses this idea to handle a three tasks - sum up to n, compute a factorial, and test whether an integer is prime.  {% highlight javascript %} var express  = require('express'),     port = 4000;  var app = express();  app.get('/sum', function(req, res) {   var n = parseInt(req.param('n'),10) || 0,       a = parseInt(req.param('a'),10) || 0;   if (n === 0) {       res.status(200).send('Sum: ' + a);   } else {       var url = ""/sum?n="" + (n-1) + ""&a="" + (a+n);       res.redirect(url);   } });  app.get('/fact', function(req, res) {   var n = parseInt(req.param('n'),10) || 1,           a = parseInt(req.param('a'),10) || 1;   if (n === 1) {       res.status(200).send('Factorial: ' + a);   } else {       var url = ""/fact?n="" + (n-1) + ""&a="" + (a*n);       res.redirect(url);   } });  app.get('/isPrime', function(req, res) {   var n = parseInt(req.param('n'),10),       f = parseInt(req.param('f'),10) || 2;   if (f > Math.sqrt(n)) {       res.status(200).send('Prime');   } else if (n % f === 0) {       res.status(200).send('Composite');   } else {       res.redirect('/isPrime?n=' + n + '&f=' + (f+1));   } });  app.listen(port); console.log('Server started on port ' + port); {% endhighlight %}  The only cases I can think of where it’s even remotely useful is if your servers are behind a CDN and you want to cache every intermediate result without having to write the application logic to do it or you need to reduce the amount of work done by a single HTTP request. It’s just not an efficient approach otherwise - the overhead of making new HTTP connections and handling arguments for every recursive step is usually more expensive than doing the actual logic within a single request.  The other use case I can think of is purely educational - it forces you to write your recursive code in a tail recursive style and forces you to think about the state you need to share between redirect requests. And if you’re ever told to solve a problem without using for loops or recursion you can violate the spirit of the request by using a series of HTTP redirects.  I’m genuinely curious if there’s an actual use case for this and whether anyone’s had to do this.",0,2,2014-12-31,6,"javascript, redirects, recursion, fun hacks, code, programming",443,Redirect recursion
28,0,"I've been working on a simple project, jsonify.me, that lets users create a JSON object of whatever they want and host it under their own domain.",#product,"{% include setup %} Over the past few weeks I’ve been experimenting with Node.js and wanted to share the project I’ve been working on,  jsonify.me . It’s an “API only” product without an interface other than a  documentation page . The idea is to allow anyone to have a publicly accessible URL endpoint that can contain whatever information they want as long as it can be stored as a JSON object. In my case, I have all sorts of semi-structured data that I want to make accessible and keeping it under my domain ( json.dangoldin.com ) makes it easy to access for whoever is savvy enough to figure it out.  The code is still in the early stages and needs to be cleaned up but the core functionality is there. It works by uploading each user’s JSON object to S3 and then doing a redirect whenever a GET request is made to that user’s subdomain. In my case I registered an account with jsonify.me, got an authentication token, set json.dangoldin.com as my subdomain, updated the CNAME record of json.dangoldin.com to point to domains.jsonify.me, and then made a POST request to json.dangoldin.com with my JSON object which uploaded it to S3 under my username.  I love the idea of a semi-structured format evolving over time as more and more people standardize around common field names as well as a set of third party apps that will migrate data from other services into these JSON objects. Imagine being able to have a service that will authenticate with LinkedIn and then extract the relevant data and put it as JSON into your object or a service that fetches new posts from your blog and adds them to your JSON object. There’s still a long way to go and the code right now is very minimal but I’d love to see people give it a shot and suggest improvements.",3,1,2015-01-04,3,"jsonify.me, quantified self, node",337,Introducing jsonify.me
21,0,Finally played around with the export of my Twitter analytics in Excel. Nothing too surprising but still an interesting exercise.,#data,"{% include setup %} I finally got around to exploring the Twitter analytics data and wanted to see whether I could find anything useful. My dataset contained 831 tweets, every single one since October 2013, as well as the text, the number of impressions, and the number of engagements. Just by loading the data into Excel, calculating a few values, and generating a pivot table it’s easy to investigate a few ideas. I’ve included some of the pivot tables below along with the various items that stood out.                              Has hashtag         No hashtag                             Has mention         171         251                     No mention         237         181               - Avg impressions vs hashtag and mention (excluding @replies): Idea was to investigate whether tweets with hastags or mentions end up being due to the fact that they are more likely to appear in search results. The results are a bit weird since it seems as if having a hashtag only helps if there wasn't also an @ mention. Otherwise it hurts.                     @mention         # tweets         Total engagments         Total impressions         Avg engagement rate         Engagements / Impressions         Avg impressions                             No         446         2192         89714         2.7%         2.4%         201                     Yes         385         914         58112         2.9%         1.6%         151               - I suspected that sending someone an @reply would reduce total impressions but increase the engagement rate since it's directed at someone. It does reduce the average impressions and only leads to a slight increase in engagement rate - and only when looking at the average of rates, not the total engagements over total impressions. I suspect most people don't differentiate between an @reply and an @mention which doesn't lead to a significant difference in actual engagement rates.                     Has mention         # tweets         Total engagments         Total impressions         Avg engagement rate         Engagements / Impressions         Avg impressions                             No         331         1524         62850         2.6%         2.4%         190                     Yes         115         668         26864         2.8%         2.5%         234               - If we exclude @replies and compare tweets with and without mentions, the tweets with mentions have both a higher average number of impressions and a higher engagement. Nothing surprising here - @mentions are good since they draw attention to a tweet while @replies hurt since they limit total reach. Still nice to have some data to confirm.                     Has Link         # Tweets         Total engagements         Total impressions         Engagement rate         Avg impressions                             No         426         990         59385         1.7%         139.4                     Yes         405         2116         88441         2.4%         218.4               - Tweets with links have higher engagement - most likely since there's a stronger call to action. Again this isn't surprising but nice to see it backed up by a bit of data.",0,1,2015-01-06,3,"twitter, analysis, excel",477,Some quick Twitter analytics analysis
28,0,How many languages should a codebase be? Some like the idea of choosing the perfect language for each project while others like standardizing around a few languages.,#meta,"{% include setup %} I’ve discussed the pros and cons of having a codebase out of a few languages versus having the choice made per project or application with a bunch of people and opinions differ. On one hand, having many languages provides flexibility in choosing the right language for the job and allows engineers to learn and explore new tools. Better habits are encouraged since the interface between components requires a well structured and tested structure rather than relying on code similarity. On the other, it prevents code and component reuse and makes it difficult for teams to standardize around a style and codebase. Also, engineers can’t switch projects as easily as they’d be able to under a common language and prevents the depth of knowledge one gets from working on a shared codebase with others.  I used to think the flexibility of being able to choose the right language for the job was the only thing that mattered but now prefer a limited language codebase. With modern languages, libraries, and tools it’s easy to write good code in any language and there are only a few applications that warrant a specific language, and even then only at scale.  My perfect mix is a static, strongly typed language for large, complex codebases that have high performance needs, a scripting language for one off tasks and processes and for quick experimentation, and JavaScript with a framework for the frontend. The static, strongly typed language makes it much easier to refactor code and improves performance while the scripting languages make it easy to quickly get something working or prototype an idea. My current favorites are Java and python - they’re both easy to write and complement each other’s weaknesses. Something I haven’t had a chance to explore in depth is moving to Jython or another JVM based scripting language - that would provide the benefits of the highly functional and robust Java code from a scripting context.",0,1,2015-01-09,5,"java, python, jython, limited language codebases, software engineering",325,Limited language codebases
34,0,I've been collecting all sorts of stats for 2014 and wrote a quick Python script to get some quick insights. The goal is to help me identify patterns and strive to live better.,"#data,#code","{% include setup %} At the beginning of last year I decided to do my part of the quantified self movement and started a daily log of how much I’ve slept, my mood during the morning, afternoon, and evening, as well as what I ate and drank. There were a couple of stretches where I forgot to fill in the details and did what I could from memory. My mood also had a pretty big impact on the way I filled in the subjective questions but hopefully it balances out over a year. I tracked the data via a Google spreadsheet and exported it as a CSV in order to analyze it via a  simple Python script . For now I’ve only pulled some summary stats but will take a deeper look in the next couple of days to examine the distributions and identify any patterns.  - After removing days without data, I’m left with 354 days for the year. - Out of these days, I was in a good mood for the vast majority (89%) with the remaining days a combination of being sick, getting sick, having a sore throat, or being congested. - I slept an average of 7.15 hours each night with the most being 11 and the least being 3. - The most popular breakfast items where cheese (53 days), eggs (45), smoothies (32), bagels (25), with the rest being a various combination of fruits and various dinner leftovers. - Sandwiches were the most popular lunch item (63 days), followed by soup (42), Chipotle burrito bowls (32), salads (28), and Sophie’s (23), a nearby Cuban restaurant. The remainder were food places that hovered around the teens. - Dinner was a lot more basic. I had salad at almost a third of my meals with the main dishes being dominated by protein: chicken (43 days), fish (33), turkey (20), salmon (16). My favorite starches were potatoes (34), pasta (27), and rice (16). - I drink too much coffee averaging 1.3 cups a day. The most coffee I’ve had on a single day was 3 cups which happened on 6 days. The goal is to drop this below 1 a day in 2015. - I drink too much alcohol also averaging 1.25 drinks a day. Beer (259) and wine (143) made up 91% of my drinking with the rest being a combination of mixed drinks, cocktails, and hard liquors. Similar to coffee, I’ll try to drop the average to below 1 a day in 2015.  Despite being a simple exercise there’s a lot of interesting information that can help me improve in 2015. By looking at the data more and examining relationships between various fields (mood and drinking, drinking and sleep, etc) I’ll hopefully find even more ways to live healthier and better.  On a related note, I’m repeating the same exercise in 2015 but am modifying the template a bit in order to collect more structure data. I’m going to be tracking the times I sleep and wake up rather than the duration and decided to put all the foods under a single column rather than per meal to make it easier to include snacks. The other big change was splitting my mood columns into a physical and a mood component to measure them separately. Looking forward to seeing what I discover in 2015.",1,2,2015-01-11,3,"quantified self, tracking stats, 2014 in review",556,2014 stats
31,0,It's easy to procrastinate and delude ourselves into ignoring simple tasks but all it takes is to take the first step. After that taking the next step becomes much simpler.,#meta,"{% include setup %} Numerous times I’ve procrastinated on doing something convincing myself that it would either take too long or just wasn’t worth doing but more often than not when I finally take the first step I’m able to quickly complete the task. I don’t know why our minds encourage procrastination but I suspect it’s not just me. I’ve been combating this tendency by recognizing that it’s happening and forcing myself to just do something, as simple as it is. A small task typically turns into a series of small tasks in which I’m able to make a significant amount of progress. In fact, there have been numerous times where I’ve even been able to achieve “flow” - despite being hesitant in the first place. It doesn’t seem like much but 10 minutes here and there do add up. Whether it’s coding up a simple feature, doing a quick data analysis, or just jotting down a few ideas, it’s infinitely more valuable than staring at a phone.",0,1,2015-01-17,2,"productivity, efficiency",168,Take the first step
20,0,I discovered that GitHub had a series of tools to render maps and decided to play around with them.,#code,"{% include setup %} After discovering  GitHub's map visualization  feature I needed to give it a shot on the only GPS dataset I had available, my runs from RunKeeper. Unfortunately, the RunKeeper files were in GPX while GitHub expects either geoson or topjson. A short  Python script  later and I was able to convert the GPX data into  geojson . The other hiccup I encountered was that the generated geojson file was too large for GitHub to visualize. My 232 runs contained 162,071 latitude/longitude pairs which turned into a 4MB file - not massive but large enough for GitHub to refuse to visualize it. The simplest solution was to generate multiple files but that made it impossible to see all my runs on a single map. The other solution was to see if converting to topojson would reduce the file size. That helped but I wasn't able to find the right balance between compression and quality and ended up with a hybrid approach - two files, one per running year, each in topojson.             -->  The entire process was painless and quick. The geojson format was straightforward to generate and GitHub does a great job rendering it. The entire process took an hour and I had to read the  topojson utility docs  to figure out how simplification worked. One thing I didn't get to do was explore GitHub's map diffs but will try to in the next couple of weeks.",4,1,2015-01-18,3,"github, tools,",283,Fun with GitHub's map tools
29,0,If you’re designing systems that collect and use data from the client you need to make sure your backend code is capable of dealing with the inevitable trash.,#data,"{% include setup %} At  TripleLift , we collect a variety of data - some on the client side and some on the server side. One thing we’ve learned is that you should never trust or make assumptions about client data, no matter how great your JavaScript is. You will always see odd data coming in and your data processing pipeline needs to be designed to take this into account. In our case, one of our jobs assumed (and the client side code confirmed) that particular events would be unique - this allowed us to write a much simpler query without having to worry about many to many joins. Unfortunately, we saw that the aggregate data didn’t match up with what we saw in the logs and after some investigating we discovered that we were seeing some duplicate rows generated on the client side. Taking a deeper look it turned out that there were some plugins and scripts that were making duplicate requests to our analytics server.  There may be ways to deal with this better on the client side as well as smarter backend logic to deal with potential duplicates but the easiest fix is to just assume you will have messy data and prepare accordingly. In our case it entailed writing more complicated queries that were robust enough to not require clean input. It took a little bit longer to write and design but our pipeline can now handle weird input without impacting the final results. If you’re designing systems that collect and use data from the client you need to make sure your backend code is capable of dealing with the inevitable trash.",1,1,2015-01-24,3,"analytics, data, pipeline",277,Don't trust client side data
28,0,I have a very loose memory of my childhood. I wonder whether kids these days will have much higher fidelity versions of theirs as they grow up.,#meta,"{% include setup %} A fun exercise is picking a random day in the past and trying to recreate it using the various tools at our disposal. In my case the most useful ones are my calendars, both personal and work, the photos I took, and Foursquare/Swarm. As long as I was vigilant in documenting the events it’s simple to figure out what I did. We lose a bit of the mystery when we document our lives and we no longer have long discussions trying to recreate events with friends. I don’t know whether this is better or worse but we’ll probably see more and more of this happening. Our phones are already collecting our location and video is becoming increasingly popular. Now we have high fidelity versions of recent events but only vague memories of our childhood. I wonder whether kids that are growing up now will have access to accurate memories of their childhood when they grow up and what the impact will be.",0,1,2015-01-25,2,"technology, memories",167,The changing fidelity of the past
19,0,"IBM's rumored to layoff more than 100,000 people so I wanted to see how it compared against others.",#meta,"{% include setup %} Given the  rumor  of the massive IBM layoffs I decided to pull some others and see how it compared. Surprisingly, the next highest was also at IBM - but in 1993. On one hand, it's odd to see this pattern as if they've learned nothing. On the other, it's a sign that they acknowledge the problem and are willing to adapt. Since the 1993 layoff IBM's stock price increased over 950% and this round may provide another burst.  On a side note, the largest layoffs are available online but I wasn't able to find a non ad-ridden, non-paginated table so hopefully others find these useful.                     Number         Company         Year         Layoffs                             1         IBM         1993         60,000                     2         Sears         1993         50,000                     3         Citigroup         2008         50,000                     4         General Motors         2009         47,000                     5         AT&amp;T         1996         40,000                     6         Ford         2002         35,000                     7         Boeing         2001         31,000                     8         USPS         2010         30,000                     9         Bank of America         2011         30,000                     19         HP         2012         27,000                     11         Daimler Chrysler         2001         26,000",1,1,2015-01-26,2,"ibm, layoffs",209,IBM's rumored layoff
17,0,We wrote a simple application from Node to Netty and this describes our migration on AWS.,#devops,"{% include setup %} A fun little exercise I had to do was rewrite a simple application from Node.js to Netty to fit into the rest of our stack. The rewrite took a couple of days but the deployment and testing was critical to get right so I wanted to share our approach. To provide some context, the application was an HTTP server that handled ~1,000 requests a minute with each request spawning at most three more to pull in more data.  Make it work. Make it right. Make it fast. The statement is attributed to Kent Beck but I've become a huge fan and try to approach all projects with this mindset. In our case, having a product deployment already available made it simple to test. We would just run some production requests against our new code and compare the responses. If they matched then we knew we did the right thing. Note that if you're on AWS and running an ELB, a simple way of getting HTTP requests without touching any code is through  access logs . Amazon will store each of the requests to the ELB to a file on S3 that you can then easy download and parse.  The next step was making sure it could handle the same load as the old application. The first thing was to run a series of  Apache benchmarks (ab)  so we can get a rough idea of the concurrency and performance on a single request. As part of this test we turned off the caching layer in our application to hobble it as much as possible since if it could handle that, it could handle anything. The final step was using a  script to simulate the requests in the ELB access against the new server and see how it behaved.  The deployment turned out to be the easy part. All we had to do was launch a new server behind a new ELB and swap the DNS record to point to it. We did this for a few short periods before swapping it back so we could do a few final checks before leaving the DNS record pointing to the new ELB. After a couple of days we eliminated the old ELB and instances completely. The chart below shows the transition between the two load balancers.",2,1,2015-01-29,4,"devops, migrating, Node.js, netty",419,Migrating a simple HTTP application on AWS
22,0,MySQL ignores case when doing the general ORDER BY. It's possible to make it case sensitive by using the BINARY keyword.,#sql,"{% include setup %} At  TripleLift , we have a migrations job that copies aggregate data from Redshift to MySQL so it can be accessed along the rest of the transactional data. As part of a test, I tried comparing that the data matched exactly but ran into an issue when exporting the data to select. Namely, to make the comparison as simple as possible I wanted to run the same select query in both tables and compare the results. Unfortunately, the sort order between MySQL and PostgreSQL (what Redshift is based on) acts differently for text fields. PostgreSQL takes case into account while MySQL does not. This has an especially weird results when you have values that contain characters with an ASCII code between the lower and upper case letters: [\]^-`. It took some research but I discovered that MySQL provides an option to do a case sensitive sort - just add a “BINARY” option before the field name.  The following   queries demonstrate this behavior - all but the BINARY one can run on both MySQL and PostgreSQL.  {% highlight sql %} CREATE TABLE test_table ( t varchar(5) );  INSERT INTO test_table (t) VALUES ('a'),('b'),('c'),('d'),('e'),('f'),('g'),('h'),('i'),('j'),('k'),('l'),('m'),('n'),('o'),('p'),('q'),('r'),('s'),('t'),('u'),('v'),('w'),('x'),('y'),('z'),('A'),('B'),('C'),('D'),('E'),('F'),('G'),('H'),('I'),('J'),('K'),('L'),('M'),('N'),('O'),('P'),('Q'),('R'),('S'),('T'),('U'),('V'),('W'),('X'),('Y'),('Z'),('0'),('1'),('2'),('3'),('4'),('5'),('6'),('7'),('8'),('9'),('['),('\\'),(']'),('^'),('_'),('`');  SELECT * FROM test_table ORDER BY t ASC;  -- MySQL only SELECT * FROM test_table ORDER BY BINARY t ASC; {% endhighlight %}",1,1,2015-02-01,2,"mysql, postgresql",219,MySQL vs PostgreSQL sort order
24,0,Switcheroo makes it easy to test development code on a production site by rewriting the URL but unfortunately it only works on Chrome.,#code,"{% include setup %} At  TripleLift , we’re big fans of the  Switcheroo  plugin and rely on it during development to test new versions of our code. It allows us to override a production hostname with one of our development boxes so we can see how our code works on a live site. So if a production site is referencing a JavaScript file at http://production-environment/script.js we use Switcheroo to have it reference the development file at http://dev-environment/script.js. Unfortunately, it’s only available for Chrome which makes it more difficult to run browser specifics tests on other browsers.  To deal with this problem we came up with a small redirection app that runs locally and is browser agnostic. Instead of entering the desired host to redirect in the extension, you add it to the local  hosts file , mapping it to localhost. This bypasses the DNS lookup and sends all requests to that domain to the locally running server which then serves a redirect to the desired URL. The  code’s up on GitHub  with a readme that should hopefully be easy to follow.",4,1,2015-02-07,3,"switcheroo, testing, client side testing",201,URL redirection app
37,0,I've decided to start learning Scala and have had great success learning it through the Project Euler problems. The biggest benefit has been being able to access other solutions once I'm able to solve a problem.,#meta,"{% include setup %} Over the past week I've been learning Scala. The initial motivation was that our API code is in PHP and in dire need of a rewrite. And since we've been rewriting our other critical applications in Java we want to leverage the JVM as much as possible. At the same time, we want to keep the code simple, expressive, and maintable while being fun and easy to write. I've heard great things about Scala so decided to give it a shot.  My first step was to install the  Play framework  and play around with the examples but I quickly discovered that while I could understand and tweak it, I needed a better Scala foundation to actually work on a real project. One of my favorite ways to learn a new language is to go through the Project Euler problems in a new language. The problems are a fun balance between mathematics and computer science and gradually build up in complexity which aligns itself well with the build up in my coding skills. The other big benefit is that solving the problems gives you access to other peoples' solutions which you can use to improve your code and knowledge. Normally looking at other people's code isn't the most impactful but in this case since you've already solved the problem you have the background to actually absorb the new patterns and styles.  So far, I've found Scala fun and expressive but tend to write it in a Java-like way. I'm definitely seeing significant changes in my style though. The very first solution looks just like Java while the most recent one is definitely functional. The plan is to work on a few more problems this weekend and then take another stab at the Play framework.",1,1,2015-02-13,4,"scala, programming, learning a new language, project euler",303,Learning Scala
33,0,For the first time I saw contextual behavior in the Google Hangouts app when my wife asked me where I was. The app gave me the helpful option of sharing my location.,#meta,"{% include setup %}            Yesterday I got a reminder of how deep smartphones and tech companies have gotten into our lives. After spending a day volunteering at the [C4Q](http://www.c4q.nyc/) office, I got a text from my wife asking me where I was. When I opened the Hangouts app I saw an option to share my current location. This is the first time I’ve seen a contextual behavior in Hangouts. I’ve seen it before in other apps - a Gmail alert telling me I forgot to attach a file when the text has “please find attached” or Google Calendar defaulting to a weekly repetition if I put “weekly” in the meeting title - but this is the first time I’ve seen it happen in Hangouts. Basic contextual behavior is relatively simple to support and can just require a simple word search but it has incredible potential as more and more data gets collected. Our smartphones are with us wherever we go collecting data each step of the way. Right now the behavior is formulaic and standardized but soon enough our phones will act as personal assistants - keeping track of everything in our calendars while understanding everything we have going on. This has the potential to drastically simplify our lives but we may be making a Faustian bargain in the process.",1,1,2015-02-16,5,"google, hangouts, smartphones, technology, privacy",237,Smarterphones
22,0,I played around with Node.js for the past couple of months and wanted to share some thoughts around my experience.,#javascript,"{% include setup %} I've decided to move on from  Node  after messing around with it for the past couple of months. And while the experience is still fresh I wanted to share my thoughts. I’m far from an expert so take all these with a grain of salt.  - Node’s powerful and in the right hands can make a developer extremely productive. I was able to write a few simple applications surprisingly quickly given my limited knowledge and I can see why so many opt to use it. At the same time it requires a commitment to the Node-centric way which can be tough depending on your background. JavaScript has functional scope and the benefit of Node depends on an asynchronous approach which can be difficult to write. - It’s drastically different from writing client side JavaScript. Instead of worrying about supporting multiple browsers you have to write code that’s maintainable and supports a growing number of use cases. This isn’t that much different from any other backend language but came as a surprise to me since I expected it to be somewhat similar to writing front-end code. - JavaScript is very difficult to write well. Despite (and possibly due to) JavaScript’s pervasiveness it’s tough to find good code. It’s so flexible that it’s easy to get started but that flexibility makes it critical to keep pruning and cleaning your code. Everyone has their own way of writing JavaScript which can be damaging when working as part of a large team on a large application. Many dismiss JavaScript as being an introductory language but a case can be made that it actually requires an expert to do well. Whereas other languages have rules that prevent new developers from making mistakes, JavaScript lets you do whatever you want. - Testing is paramount. Due to JavaScript’s flexible nature it’s important to test thoroughly. When writing Java I rarely have to worry about typos or scope issues since my IDE will let me know immediately but there’s no such luck with JavaScript. I discovered a ton of issues in my toy applications as soon as I started writing tests. - Lots of resources to learn about it online. After committing to working on some Node I was able to find a ton of useful examples and resources online. The community is large and there are a ton of useful libraries on npm but it’s tough to identify the best ones. There seem to be multiple versions of every library and for someone new it can be a bit overwhelming trying to pick the right one to use.  I enjoyed my experience with Node and learned a ton but it’s style and approach just don’t fit the way I work. JavaScript’s lack of structure makes it difficult for me to imagine using it on large, team-based projects. Of course there are best practices to make it work but that’s something that would need to be part of the engineering culture versus something that’s part of the language itself. Node is great for small, experienced teams who want to get an app up and running quickly but if the application has complex logic or will require a large team to maintain I would opt for a more rigid, higher performance language. I’m biased towards the JVM and have recently picked up Scala as my “experimental” language. The goal is to do a similar post on Scala once I get more experience.",1,1,2015-02-22,3,"node, javascript, engineering",579,Lessons from Node
28,0,I've only been managing an engineering team for 6 months now but something that I've been trying to adopt is the idea of a full stack developer.,#management,"{% include setup %} I’m a pretty new engineering manager but a philosophy I’ve adopted is to try to have everyone on the team be as full stack as possible. Everyone has their strengths and weaknesses but being able to grasp the entire stack improves code quality and reduces disruption. And it goes beyond technology and into the business and user world too. Understanding how these various components fit together allow you to make smarter decisions and provide the tools to test and verify your code. The other big benefit is that you’re not waiting on anyone and avoid having your flow disrupted by others.  As an example, imagine having an ecommerce website when you get the idea that you want to start tracking the amount of time people are spending mousing over your product images. The goal is to see whether this behavior is correlated with sales which will give you more data to drive an upcoming redesign. Clearly there will be front-end JavaScript involved since that will be triggering the event but there’s also a lot going on behind the scenes. Depending on the number of events you expect to see you can have a wildly different implementation. How do you want to handle multiple mouseovers over the same image? What data do you want to capture? What kind of analysis will you want to run? How will this data be tied back to the sales data? Where will this data be stored? Will there need to be any additional processing to make the data usable? How will you test the data flow? What needs to happen for you to deploy it? How much additional load will this put on the production system?  These questions can all be answered by looping in various people but understanding the business case and the full tech stack makes you more independent and increases the likelihood that the first version will be the final version. In addition to having the necessary language skills, I’d love to see every web engineer know how to set up a VPS from scratch, be comfortable with the command line, have a basic understanding of SQL and databases, and understand the various components of the web and how they fit together.",0,1,2015-02-23,3,"full stack developer, software engineering, engineering management",376,In praise of the full stack developer
19,0,I wrote a pretty basic set of functions to make it easier to work with localStorage and lists.,"#code,#javascript","{% include setup %} I recently discovered the localStorage functionality in HTML5 and used it on a quick internal tool at TripleLift. One hiccup I ran into was that while it provides the ability to set and get key/value pairs it stores everything as a string so I needed to write a few utility methods to get it to work with lists. They’re pretty straightforward but hopefully they inspire someone to improve on them.  {% highlight javascript %} // Also let caller specify max size of list function addItem(k, v, limit) {   var a = getItems(k);   a.push(v);   if (!isNaN(limit)) {     while (a.length > limit) {       a.shift();     }   }   localStorage.setItem(k, JSON.stringify(a)); }  function getItems(k) {   var a = null;   try {     a = JSON.parse(localStorage.getItem(k));   } catch(e) {}   if (a && Array.isArray(a)) {     return a;   }   return []; }  // Tests/Examples localStorage.setItem('test_list', null);  addItem('test_list', {""name"": ""Dan""}); addItem('test_list', {""food"": ""pizza""}); addItem('test_list', {""beer"": ""Newcastle""});  var l = getItems('test_list');  console.log('Lengths match: ' + (l.length === 3)); console.log('Value 0 matches: ' + (l[0].name === 'Dan')); console.log('Value 1 matches: ' + (l[1].food === 'pizza')); console.log('Value 2 matches: ' + (l[2].beer === 'Newcastle'));  addItem('test_list', {""size"": 2}, 2);  l = getItems('test_list');  console.log('List limit works: ' + (l.length === 2)); console.log('Value 0 matches: ' + (l[0].beer === 'Newcastle')); console.log('Value 1 matches: ' + (l[1].size === 2)); {% endhighlight javascript %}",0,2,2015-02-26,3,"javascript, localStorage, lists",243,Lists and localStorage
32,0,When doing Java development a huge win comes from using log4j effectively. Done right it can separate the signal from the noise and make it significantly easier to navigte your code.,"#code,#java","{% include setup %} Something that’s incredibly helpful when writing Java code is customizing  log4j . There are a variety of configuration options and learning just a little bit about them can make you notably more productive. I’ve found two features that have sped up my development cycles.  One was updating my PatternLayout to include the filename and line of each message. With Eclipse, this allows me to quickly jump to the relevant code block whenever anything looks odd rather than having to first open the file and then search for that particular message.  {% highlight properties %} log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d %p (%t) [%c] (%F:%L) - %m%n {% endhighlight properties %}  The other was to pick the appropriate log level at the package level. If I’m working on a single package I'll reduce the logging level of other packages to make the relevant messages stand out. This is especially handy when you incorporate eager third party packages that drown out your own messages with their own.  {% highlight properties %} log4j.logger.com.dan.package.one.logging=WARN log4j.logger.com.dan.package.one.logging.ClassName=INFO log4j.logger.com.dan.package.two=DEBUG log4j.logger.com.dan.package.two.working_on=TRACE {% endhighlight properties %}  My style of development is to rely on logs more than the debugger so these two have made my life a lot easier. In general. logging is an important tool for all developers and yet few tend to tweak the default settings. By understanding the available configuration options you’re able to tweak them for whichever problem you’re solving. This may not seem like a huge win but when you’re running the same program hundreds of times a day the small efficiencies add up.",1,2,2015-02-28,3,"java, log4j, logging",299,Getting the most out of log4j
29,0,Coding isn't for everyone but basic computer literacy should be. Knowing how to download an run an open source program can help protect against malware and increase productivity.,#meta,"{% include setup %} Earlier today I came across another reason why basic computer literacy is a necessary skill. It’s not just about knowing to code or understanding how computers work but getting access to a slew of tools that are orders of magnitude better than what you’d find on a sketchy site.            My realization came when I was trying to download some YouTube videos that could be watched without internet access. Doing a quick Google search I found dozens of sites with each one trying to force me to download some additional software to “speed up” my experience. I’m positive most, if not all, of these would fall into the malware category so I decided instead to do a simple search on GitHub for “youtube download.” Lo and behold the first result was the wonderful  youtube-dl  library. Within two minutes I kicked off a script that proceeded to download a dozen videos.  I can only imagine what someone without access to GitHub would have done - either given up or downloaded some ridiculous malware that may have worked but would have left their computer in a sorry state. Learning to code is one thing but having a little bit of familiarity with the command line and reading technical documentation would do wonders in helping people solve their problems, avoid malware, and even encourage them to learn more.  Replacing Google with GitHub for any computer related “how to” search may be overkill but I’m going to start doing it. The side benefit of GitHub is that it even comes with a built in review system due to the star and fork system which hasn’t yet been gamed.",1,1,2015-03-08,2,"computer literacy, malware safety",304,Computer literacy for protection and productivity
17,0,"Tools are essential to developer productivity and tools for static, strongly typed languages are significantly better.","#meta,#java","{% include setup %} Good tools are essential for developer productivity. Imagine how tough it would be to write code in an editor that didn’t have any of the features we use on a daily basis - syntax highlighting, smart spacing, shortcut keys, auto completion, etc. It takes time to get used to all the tools available but once we’re familiar with them we’re orders of magnitude more productive.  For the past year my primary language has been Java although I’ve gotten to do a fair amount of JavaScript, Python, and PHP as well. As great as Sublime is, I’m much more productive in Eclipse. It has nothing to do with the editor and everything to do with the language. Eclipse is able to provide a lot more functionality due to Java’s static, strongly type nature. Some examples are being able to quickly rename variables and methods, move packages, and quickly identify dumb mistakes in method signatures and typos. I suspect similar tools exist for weakly typed or dynamic languages but I can’t imagine them working as well as they do in Eclipse. Strongly typed and static languages are able to get rid of an entire class of errors that are common with scripting languages - typos, forgetting to add an argument to a method call, messing up a type - that the time saved typing gets replaced with the time spent debugging. For many tasks this tradeoff makes sense but larger projects that involve multiple developers and require higher performance would benefit from moving to a strongly typed, static language.  I’m learning Scala which seems to combine the flexibility and expressiveness of Python with the safety and performance of Java. So far I’m cautiously optimistic but we’ll see where I am in a month.",0,2,2015-03-10,5,"strong typed, weakly typed, static, dynamic, programming language",293,"Strongly typed, static language tools"
15,0,Today's Pi Day so this is a quick post highlighting it's interest and appeal.,#meta,{% include setup %} Since it’s Pi Day (at least in the US)I decided to jump on the bandwagon and contribute my own thoughts. Pi is fascinating. It’s such a simple definition - the ratio of a circle’s circumference to it’s diameter - yet it’s both irrational and transcendental and impossible to actually express as a simple number. People have been trying to get more accurate estimates for multiple millennia with multiple great mathematicians trying to derive their own approximation.            Srinivasa Ramanujan's Pi approximation     Looking at a Wikipedia article for [Pi approximations](http://en.wikipedia.org/wiki/Approximations_of_%CF%80) is itself overwhelming. How Ramanujan was able to come up with his approximations is tough to understand - they seem so ridiculously esoteric that it’s hard to imagine someone was able to come up with such a formulation. Since then there have been improved approximations and Pi’s been calculated to 12.1 trillion digits. I can’t think of any real reason why we’re spending countless computer cycles to get better approximations but that’s the allure of Pi: incredibly simple to explain while being infinitely expressive.,1,1,2015-03-14,2,"Pi, math",192,An ode to Pi
31,0,I'm a much worse speller than I used to be due to modern technology. To rectify this I'm forcing myself to retype every word correctly rather than rely on autocorrect.,#meta,{% include setup %} I recently discovered that I’m a much worse speller than I used to be. The culprit is obvious - computers make it too easy to correct our mistakes. In school when making mistakes we’d have to rewrite each word until it became ingrained but these days all we do is just click on the suggested fix without a second thought. There’s nothing there to help me retain the mistake so I continue making it.  This realization made me uncomfortable so I’ve adopted the hybrid strategy of retyping every word that I make a spelling mistake on. It’s never going to be as good as using pen and paper but it’s much better than picking a word from a dropdown menu. There are tons of behaviors that technology has made obsolete but that should not be the reason to abandon those skills. For the same reason that having math skills helps us in daily lives it’s important to hold on to the basic skills we learned as children.,0,1,2015-03-15,3,"spellchecking, education, technology",171,Learning to spell again
36,0,The biggest thing an engineering team can do to be productive is start imposing standards and conventions on everything they do. This will allow them to write less code that does more and increases quality.,#management,"{% include setup %} When it comes to productive coding, one of the most important things to do is to impose a set of standards and conventions. As long as you stick with them your code becomes significantly easier to write and maintain. Conventions range from having a standard way of declaring variables to the way files are organized within a project to the field names in database tables. The obvious benefit is that your code becomes significantly easier to navigate, both to you as well as to others on the team, since you don’t have to run through a series of searches trying to figure out whether a variable is called myVariable, MyVariable, or my_variable. The bigger impact is how much simpler your code becomes. By using a standard structure it’s possible to write code that’s further up in the abstraction hierarchy. This is a huge win for productivity and quality since  more code leads to more errors  and the best code is code that’s not written in the first place.  Two examples of how we’ve adopted conventions include:  - Making sure that every database table in our “log” schema has a timestamp column containing timestamps and every table in our “agg” schema has a ymd column containing dates. This allowed us to write an abstract job that aggregate the data from a log table to an agg table without having to worry about the underlying structure. All we had to do was specify the columns that were the keys and which ones needed to be aggregated - the job itself took care of the scheduling, the query construction, and the reporting. In addition, we were able to quickly write up a simple job that archived old log data. The job doesn’t care what table it takes as long as it has a timestamp column. - We use RabbitMQ for some of our asynchronous tasks and we’ve developed a standardized format that a majority of tasks share. These tasks take a name, a date, and an hour and then run a query for that hour. By imposing this structure, we were able to write a single block of code that would take tasks with a start and end date and republish them as a series of hourly tasks in the date/hour format. Since each task takes the same arguments, we’re also able to use reflection to automatically create an instance of the appropriate class for each task. For example, the task {“task”: “do_an_agg”, “ymd”: “2015-03-17”, “hour”: 10} automatically gets translated into new DoAnAgg(“2015-03-17”, 10). All we need to do is make sure the class DoAnAgg exists, has the appropriate constructor, and exists in the proper package.  Both of these examples are straightforward but the value comes in coming up with the proper abstraction that avoids unnecessary code. Standards make it easy to spot repeated patterns which can then be refactored upstream. This improves the leverage of everyone else on the team and makes every engineer more productive. People idolize the mythical 10 or 100x engineer but there’s more value in making the entire team more productive.",1,1,2015-03-18,3,"engineering culture, standards, convention",542,Power of engineering standards
25,0,"Despite being economically better, we're so used to the idea of owning items that it will take time for us to shift our mindsets.","#society,#pricing","{% include setup %} The sharing/rental economy is getting stronger and stronger and will have a massive impact on societies - especially cities. One thing that’s been on my mind is how it will fit into human behavior and biases. We’re so used to making infrequent or one time payments and then owning something that moving to a rental or sharing model might be difficult. For example, I don’t own a car and mostly rely on a combination of public transportation and CitIBike to get around. The rare times I need a car I’ll use either Zipcar, Hertz 24/7, Lyft or Uber depending on my exact situation yet each time I make the decision I can’t help but think about the cost. I realize that in the grand scheme of things it’s much cheaper than owning a car but during the moment itself it’s draining. It’s similar to the of unbundling TV - it’s much cheaper to just pay a dollar per episode to watch a TV show than pay more than $100 each month for cable but do people actually want to be thinking about spending the dollar each time? I suspect most would rather pay the premium for the entire bundle and the option of watching anything instead of feeling as if they’re being nickeled and dimed. I have no idea whether this is innate in human behavior or something that we’ve just grown accustomed to. I suspect it’s the latter - there are countless items we pay for individually and don’t think twice about it. What will make the sharing economy universal is when we start treating the majority of our purchases as per use rather than a lifetime subscription.",0,2,2015-03-21,3,"sharing economy, rental economy, human behavior",281,The sharing economy and human behavior
17,0,I can’t figure out why nearly every website forces you to login after resetting your password.,"#design,#product",{% include setup %} I can’t figure out why nearly every website forces you to login after resetting your password. It’s an extra step that adds nothing to security and introduces friction into the experience. The fact that I just entered my password into a form field should be enough to trigger the authentication flow and get me back into the app. The only reasons I can think of that it’s a way to confirm that the person actually remembers their new password or that the functionality just hasn’t been built. The former case doesn’t make sense - the fact that they forgot their password indicates they rarely use the site and will just forget it again by their next login attempt. It’s easier to just give them the immediate access and have them reset their password later. An even better approach would be to just have them enter the same password twice to make sure they match. The latter reason is just sloth - the engineering effort would be minimal and it would improve the experience and mood of the users who are already frustrated after multiple failed login attempts.,0,2,2015-03-24,6,"design, ux, authentication, password, reset password, forgot password",190,Why login after resetting your password?
22,0,It doesn't bother me when apps launch on iOS before Android. What bothers me is that I can't reserve my username.,#product,"{% include setup %} Both Meerkat and Periscope launched on iOS first. That doesn’t bother me despite have an Android phone. They’re running a business and it’s up to them to decide where they want to invest the time they have. What bothers me is that I’ve been using a specific username across the various services, dangoldin, and now run the risk of losing it on these newer networks. A simple fix would be to at least allow me to preregister my username without requiring an iOS device. This would also encourage me, and a lot of other Android users, to download the app when it finally does make its way to Android.  This is a first world problem but as more and more people start building their brands it’s useful to have a single name that spans across services and networks. It makes it easier for your audience to find you and lets you transfer audiences from one to another. There’s always going to be a way to login with Twitter or Facebook but I like having my networks divorced from one another with my username serving as the only link.",0,1,2015-03-29,3,"iOS first, Android, usernames",193,iOS first and username claiming
15,0,Tools are incredibly important to be productive so here are the ones I use.,#meta,"{% include setup %} Great tools have the potential to make us significantly more productive and I wanted to share my existing setup. A huge part of that productivity is our comfort with our tools since over time we learn the shortcuts, understand the capabilities better, and develop processes to solve common problems. The challenge is that there is always a tool that might be better but the learning curve is too steep to warrant a time investment. Here's what I have so far.  - Google Chrome Canary + FirefoxDeveloperEdition: I like being on the bleeding edge so use the nightly builds of both browsers. My preference would be to use Firefox for everything but I'm more familiar and comfortable with the Chrome dev tools.  - Google Calendar: I'll use this for both scheduling meetings as well jotting down deadlines and todos. It's been working great and I haven't felt a need to use anything else. I tried using a few apps but wanted something that had a better integration with the rest of the Google suite.  - Fastmail + Gmail: We use Gmail at work but my preference is for Fastmail. It's cleaner, simpler, and faster than Gmail and reminds me of what Gmail was when it first launched. Within both Fastmail and Gmail I strive to achieve ""inbox zero"" with varying success.  - Google Docs: Whenever I need to write anything non code I'll reach for Google Docs. The interface is simple to use and I like the cross device sync. The only time I'll move away is when I don't have internet access or I'm taking random notes.  - Excel: I tried using Google Spreadsheets but Excel is significantly better for larger scale projects and I'm too comfortable with the keyboard shortcuts I picked up during my maangement consulting and finance days. My ideal solution would be a spreadsheet interface built on top of a language such as R. Then I'd be able to use the spreadsheet component for the quick and dirty work, pasting data into it, doing simple transforms, etc and dive into the R for the more serious quantitative work.  - Terminal with zshell and ohmyzsh: Being comfortable with the terminal is vital for developers. It's the primary way to interact with external servers and knowing the various commands and scripts allow us to quickly diagnose and fix problems. The add on I use is zshell with the ohmyzsh configuration since it comes with a nice set of bells and whistles - git integration, useful highlighting, ..  - Sublime Text 3 + SFTP: For scripting progrmas my editor of choice is Sublime Text. It's surprisingly snappy and lightweight while providing a lot of flexibility for third party plugins. One of these plugins is SFTP which allows me to sync local files over to a remote server. I do a lot of my development work on an EC2 instance so being able to save them remotely is a huge productivity boost. I used to use Evernote for note taking but have switched to using text files in Dropbox. This allows me to organize them the way I want and leverage the command line to find exactly what I'm looking for.  - EC2: At TripleLift, each developer gets their own EC2 instance to be used for development. This both mirrors the production environment better than OS X would and allows us to make our sites publicly accessible to other developers. The other nice piece is that it interfaces nicely with the other AWS products, namely S3. Transferring files from S3 to EC2 is much quicker than going from S3 to a local computer. The two major constraints are that it's command line only and tends to be less performant than our local machines.  - Eclipse + IntelliJ: For Java I'm using Eclipse and for Scala I'm using IntelliJ. I've been coding Java for a lot longer and am much more familiar with Eclipse. It's possible that I'll move to IntelliJ at some point but for now my projects allow me to keep them separate.  - Git on GitHub with Hub: No surprise here. GitHub makes it easy to collobarate with others and I'm a big fan of the interface. The only annoyance I had was being unable to open pull requests from the command line but I've since found Hub which provides a command line interface to GitHub.  Would love to hear of other tools that people find useful.",0,1,2015-04-03,3,"productivity, tools, development",732,My tool setup
30,0,The best efficiency wins come from using the right tool for the job. This is a quick example of combining Redshift and Excel to identify a missing data issue.,#code,"{% include setup %} As part of our data pipeline, we have a Redshift agg job that takes low level data and rolls it up to an hourly aggregate. A latter job takes the hourly data and rolls it up to a daily level which is used for high level reporting and summary statistics. Earlier this week we ran into a hiccup that caused some of these aggregate jobs to fail. After fixing the issue we had to figure out what data was affected and rerun it. We wrote a simple query to count the numbers of rows per day per hour in order to spot any gaps.  {% highlight sql %}select ymd, hour, count(1) as cnt from hourly_agg_table where ymd >= '2015-04-01' group by ymd, hour order by ymd, hour; {% endhighlight %}  This gave us a dataset with three columns that we wanted to then “pivot” in order to quickly spot the gaps. Using the pivot table functionality in Excel, it was simple to put date along one dimension and hour along the other to quickly spot the missing agg periods. All that was left was rerunning the job for those hours.            This investigation reminded me how important it is to be familiar with your tools and choose the right one for the job. Redshift and Excel are antithetical - Redshift is massively parallelizable and built for terabytes of data while Excel slows to a crawl when dealing with tens of thousands of rows. But by mixing them together we’re able to use each for what it’s best for: Redshift for very quick, large scale queries and Excel for the quick and dirty investigative work. This approach is useful in all sorts of problems - from mixing command line scripts with fleshed out programs to using a script or Excel to generate commands that you can then paste into the terminal or an editor. The key point is understanding your workflow and tools well enough to come up with an optimized process.",0,1,2015-04-06,3,"redshift, excel, data analysis",355,Redshift meets Excel
19,0,I had to deal with some requests off of Google's CDN taking close to a minute to succeed.,#devops,"{% include setup %} I’m not sure whether this is a recent issue but earlier this week I started noticing that many HTTP requests to  Google's CDN  were taking close to a minute to complete. In particular, this blog would take almost a minute to render since it uses two fonts and an old version of jQuery both hosted by Google.            After some investigation it turned out that the issue seemed to only happen on Chrome Canary (43.0.2351.3 canary (64-bit)) and even occured when visiting the URL directly. Neither standard Chrome, Firefox, Firefox nightly, nor a simple curl requested had this issue - it seemed to be a purely Chrome Canary issue.  I didn’t spend a ton of time investigating the root cause since it seemed to be browser specific but ended up implementing two simple solutions to deal with the problem. One was self-hosting jQuery (via GitHub pages) and the other was loading the fonts asynchronously using Google’s JavaScript implementation. This allows the content to load without having to wait for the fonts or jQuery to be available. This will occasionally cause a bit of a flicker as the text gets redrawn with the newly loaded font but I prefer this to have my site not load for nearly a minute.",1,1,2015-04-12,2,"google cdn, ajax.googleapis.com",242,Dealing with an unresponsive Google CDN
30,0,There's an uncanny valley when it comes to representing humans but there's a similar item at play when it comes to advertising as it gets more and more native.,#product,"{% include setup %}            Old Spice ad mocking the uncanny valley (Credit:  Gizmodo )     The  uncanny valley  is this idea that although we keep getting better at depicting people through technology, a few small kinks ruin everything and make people feel repulsed compared to an obvious imitation. Another way to explain it is that we’re a lot more comfortable with cartoon characters that are obviously fake than pseudo-realistic video game characters that look real but have non-human behavior or expressions.  I think this also exists in advertising. I work at TripleLift which allows us generate ads that look and resemble a publisher's website. This means that if a publisher's site has a feed layout where each image is 300x300 with a particular font and typography we’ll use the same style for our ad. At the same time, we make it a point to include a ""Sponsored"" overlay, apply a brand logo, and redirect to the advertiser's website upon a click. Our goal isn't to obfuscate the ad but rather give a publisher an effective way to monetize their content without having to resort to traditional, distracting banner ads that take up the entire page and hurt the consumer experience. The goal is to complement the user experience by providing great looking ads that are relevant to the audience.  It’s likely we’d be able to increase short term performance if we start obfuscating the fact that it's an ad but in addition to being immoral it will fall into some form of uncanny valley. People aren’t stupid and will see something’s off if an ad is pretending to be content. The end result benefits no one and sets advertising back. The site visitor is pissed off by the experience, the publisher loses integrity, and the advertiser may get better vanity metrics without deriving any value. Native is a much better ad format when done right but it comes with risks that need to be properly handled by the industry or we’ll end up with a  depleted field .",3,1,2015-04-12,2,"native advertising, uncanny valley",378,The uncanny valley of advertising
19,0,I ran a data scraping script over two days that ended up clobbering my Dropbox folder. Exciting times.,#data,"{% include setup %} Thursday night I kicked off a data scraping project for a friend. Since I was going to be out of town until Saturday night I decided it would be a good idea to run the job on my beefy home computer and write the results into a Dropbox folder so I’d have it accessible on my other computer while traveling.  Unfortunately, when I finally looked at my Dropbox Friday night it was completely busted. In addition to being over my 6 GB limit, the syncing was completely stopped and Dropbox was using up my entire CPU. I had to figure out a way to deal with this while holding on to the scraped data.  Since Dropbox was entirely unusable, I disabled it on my travelling machine and did a bit of investigation with the data I had with the hope of running it on the complete dataset when I got back home to my primary computer. When I finally got back home I saw that the scraping job was still running and had downloaded around 791 thousand files into one folder that totaled 11.7 GB.            The solution seemed straightforward - move the files out of Dropbox into a separate directory and then let Dropbox recover itself. Sadly that didn’t quite work. First, doing a “mv * targetfolder” ended up causing an issue with the globber since there were too many files for bash to handle. The fix was simple - move the entire folder and then rename it to the destination folder - but it took me a few attempts until I stumbled unto it. Second, Dropbox was in such a wretched state that it refused to do anything. The solution here was a bit more involved. I had to log in to the Dropbox site, remove the data from the UI, unlink Dropbox from my computers via the website, and then relink them via the app on the computer.  Two lesson here: do not save your results in Dropbox and when downloading hundreds of thousands of files do not save them in a single folder.",0,1,2015-04-19,3,"scraping, dropbox, lesson learned",372,Don't scrape into a Dropbox folder
13,0,"Adding a database column in SQL is easy, except when it's not.","#devops,#code","{% include setup %} A frequent event when working with a SQL database is adding a column. Ideally, you’d want to add this column before or after another one that makes sense rather than all the way at the end. MySQL makes this straightforward since you can use the AFTER keyword when adding a column to specify exactly where it should be added. PostgreSQL and Redshift make this difficult since all new columns are automatically added at the end.  Normally, this isn’t a problem in most cases since you just write a query to specify the desired column order but it makes doing a simple “SELECT *” more annoying and will break naive jobs that rely on a particular column order.  The accepted solution is to create a new table with the proper structure, migrate the data from the original table while using a default value for the new column, drop or rename the original table, and then rename the new table with the original name. It’s a lot easier to see this in code:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Migrate the data INSERT INTO test_new   SELECT id, name, 0   FROM test;  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  One issue with this approach is that if the table is very large, it will take a long time to migrate the data from the original to the new table. In this case a possible approach is to do it piecemeal - if you know you only need recent data you can migrate a subset of the data first to get the table ready, do the rename, and then migrate the rest. In code again:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Migrate some of the data INSERT INTO test_new   SELECT id, name, 0   FROM test   WHERE id > 1000000;  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- Migrate the rest of the data INSERT INTO test   SELECT id, name, 0   FROM test_old   WHERE id <= 1000000;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  If you know for a fact that it's not important to have the old data immediately available you can opt to rename the original table, create the new table, and only then migrate the data. This allows queries that need to write data to this table to complete immediately. The risk is that the legacy data will only be available after the migration completes. The last code block:  {% highlight sql %} -- The original table CREATE TABLE test (   id int not null primary key,   name varchar(20) not null );  -- Let's say we need to add an age column -- Create the new table CREATE TABLE test_new (   id int not null primary key,   name varchar(20) not null,   age int not null );  -- Backup the old table RENAME TABLE test TO test_old;  -- Rename the new table to have the original name RENAME TABLE test_new TO test;  -- Migrate the data INSERT INTO test   SELECT id, name, 0   FROM test_old;  -- If all looks good, drop the old table DROP TABLE test_old; {% endhighlight %}  The first scenario handles having columns in the right order but the other two can be useful on MySQL as well when tables are large and performance is critical. For high load databases, transactions can be used to make sure that the renames are done atomically - this will avoid an intermittent query writing to a non existent database. It's surprising how a task as simple as adding a column can evolve into a large problem with a variety of solutions when running into a variety of constraints.",0,2,2015-04-23,6,"postgresql, redshift, mysql, sql, database, create column",739,Adding columns in PostgreSQL and Redshift
23,0,Using AWS to scale is great but make sure to be aware of your service limits or you might be caught unaware.,#devops,{% include setup %}            Something I haven’t seen mentioned much is that AWS has  service limits . The only way to find out that you’re hitting one is when an instance fails to launch with the error message “Your quota allows for 0 more running instance(s)” with a link to open a support ticket and request a higher limit.  This is a serious problem when you’re at the instance limit and depend on auto scaling for high loads. The instance limit prevents you from scaling to meet the demand and the only reasonable option is to file a support ticket and hope someone is able to get to it in time. The doomsday option is to temporarily shut down secondary or tertiary instances to make room for the critical ones.  I understand why Amazon chose to implement it but I wish they had better support around messaging when you’re close to the instance limit or whether your current setup can push you over. There’s nowhere in the UI where you can see the limit but luckily there’s a simple command to do it via the  command line interface  - aws ec2 describe-account-attributes --region=us-east-1.  If you’re running a quickly growing stack you should make sure to monitor your service limits so you’re not caught unaware. Also make sure to monitor the limits per region and within a VPC as they each have their own.,2,1,2015-04-26,4,"aws, devops, service limits, cloud computing",270,AWS service limits
29,0,I've been using Google Contributor for a couple of days now and it's interesting. The biggest effect has been making me more aware of the content I consume.,#meta,"{% include setup %} Near the end of last year, Google announced the  Contributor  program - a way to pay a monthly fee which would then be distributed across the websites you visit. In return, you’d start seeing fewer ads. Earlier this week I got off the waitlist and decided to give it a shot. The signup process was amazingly simple - choose a monthly dollar amount and you’re good to go. The effect is noticeable - on many sites I’ll see a blank spot where an ad should have been. The best part is being able to see how much I’ve contributed to the various sites I visit. Over the past couple of days I’ve spent a little over 60 cents removing 51 ads. An unforeseen effect is that I’m more aware of the content I consume and the sites I visit - seeing that some of my money is going towards shady sites makes me more conscious of my browsing behavior. I’m definitely curious to see where this approach goes.",1,1,2015-04-28,4,"google contributor, publisher, content, digital advertising",190,A few days with Google Contributor
30,0,Just a thought experiment I've been having on what the world would look like if there was a programming language that truly had a single way of doing things.,#meta,"{% include setup %} A thought experiment I’ve had on my mind is this idea of a programming language that only has a single way of solving every programming problem. Regardless of the problem, multiple people writing code independently would end up with the same exact code. No such language actually exists (yet) but it’s fun to think about extreme cases in order to understand where we stand now. With this programming language the only differentiation between developers would be time since the end result would be the same. Beyond that, if there was always a unique solution to every problem this language would be able to write the code itself.  On the other extreme you have current languages which provide a ton of flexibility with dozens of ways to solve a simple problem. In this world developer skills are paramount. You want to make sure you find the approach that solves the current problem but is also written in a way that’s flexible enough to be easily modified for whatever the future brings. Enforcing a structure that’s based on best practices makes it easy to write code that grows with the team.  A great example of this is the rise of JavaScript frameworks. JavaScript is extremely flexible and gives the developer a wide range of paradigms to choose from. This leads to the same problem being solved hundreds of different ways depending on the style and mood of the author. The fact that it actually has a book dedicated to the “ good parts ” highlights how flexible the language is and how easy it is to go off track. Despite being close to twenty years old, only now are we seeing frameworks being developed that take a very opinionated view of how JavaScript should be written. There’s nothing in the language itself to enforce a standard so each framework takes on the responsibility. This allows large teams to collaborate on large projects without having to worry as much about individual styles and decisions.  Software engineering is a new industry and I suspect we’ll see more and more standardization as it evolves. The current approach is to use general languages for a wide range of problem domains but I think we’ll start seeing more and more languages that are specialized by problem domain. This won’t get us to the language with a single way of doing things but it will it a lot simpler to solve problems in a well defined and standard way.",1,1,2015-05-02,3,"programming, software engineering, javascript",422,A unique solution to every problem
29,0,I found a bunch of fliers from a computer show that I attended in the mid 1990s. It's crazy to see how much better computers are these days.,#meta,"{% include setup %} While doing some spring cleaning I discovered a bunch of fliers from various computer shows I attended in the mid 90s. Bsed on the Windows 95 and Windows NT promotions I suspect this must have been in 1995 or 1996. What’s striking is how much better our computers are. It’s one thing to be abstractly aware of Moore’s Law but shocking to actually see it. The top of the line model in 1995 was $2,500 and came with a 4 GB hard drive, 64 MB of RAM, a 200 MHz processor, and a 33.6 kbps modem. Adjusting for inflation, this is equivalent to $3,700 in 2015 dollars. With that budget you can a top of the line computer with an order of magnitude more of everything and still have enough leftover for a smartphone which is also an order of magnitude more powerful than a computer from the mid 1990s.",0,1,2015-05-06,4,"computers, windows 95, windows nt, tech history",462,Computer show fliers from the mid 1990s
23,0,Various thoughts I've had about driving before GPS when we had to use an atlas to figure out where we were going.,#meta,"{% include setup %} I’ve been thinking about driving before GPS. I remember my family having an atlas in the backseat that we’d reference for long trips and actually map out our journey - which roads to take, which exits to get off of exits, and the distances involved. My clearest memory was constantly trying to figure out whether we missed an exit or not. The usual solution was to just pay attention for the next couple of minutes and try to use the signs along with the road atlas to figure out where you were on the map. Now, you just type in the destination on your smartphone as soon as you get the car and just start driving. Even if you make a mistake the directions automatically update to correct your course. The amount of time saved by GPS for every trip that no longer needs to be preplanned or adjusted enroute must be incredible. I suspect it’s also changed the type of trips we’re making - rather than going to the same old nearby spots that we know we can get to, we’re confident enough to go beyond that and discover something new, knowing that our phones will bail us out.",0,1,2015-05-09,3,"driving, gps, atlas",203,Driving before GPS
28,0,After getting disconnected one too many times due to crappy internet service and losing my SSH sessions I discovered Mosh which makes remote development a lot easier.,"#meta,#devops","{% include setup %} Since I do a fair amount of web development having flaky internet is a big hit to my productivity; especially when I have a half dozen open SSH sessions that bulk disconnect every few minutes. After being thwarted one too many times by spotty internet at the office I decided I had enough and started looking for alternatives. One of the tools I discovered was  Mosh . Mosh allows you to open a remote session just like you would do with SSH but unlike SSH it’s robust enough to handle networking disruptions. In fact, I can start a Mosh session on Friday afternoon before leaving the office for the weekend, let my computer go to sleep, and then have it automatically resume as soon as I get back to the office on Monday and wake my computer up. I’m still amazed at how well it works and only wish I discovered it sooner.",1,2,2015-05-12,3,"mosh, shoddy internet, flaky internet",161,Mosh trumps shoddy internet
22,0,"I wish where was a way to capture those fleeting thoughts, especially at times where a phone or notebook aren't options.",#meta,"{% include setup %} Many of my thoughts come serendipitously - whether it’s an errand I need to run, an idea for a blog post, or a feature I should build into one of my projects. But unless I’m able to jot it down soon after it slips my mind until I have another serendipitous thought to bring it back. It’s frustrating when I know I had something but can’t recall what the actual thought.  A smartphone makes it a lot easier since I can always jot down some words to trigger the thought later on and I’ve also started carrying a small notebook for times I'd just like some pen and paper. Unfortunately, there are still a few cases where this approach doesn’t work. One is when I’m in a group and don’t want to bust out my phone and start taking notes. The others are where a phone just isn’t practical - whether I’m out exercising and don’t want to take a break, in bed where I don’t want to stare at a bright screen, or just in the shower. In those cases I’d love something akin to mind reading where I’d be able to just back up a thought to a place that I can reference later. The only idea I’ve had is a braille-like system that lets you enter the worlds tactilely. Imagine having a small device in your pocket that you can run your fingers over to type whatever you’d like. You’d be able to do this regardless of the location - whether you’re in bed or in a crowded subway car with your hand in a pocket. I’d love to see a Kickstarter for this.",0,1,2015-05-17,3,"notes, notetaking, productivity",280,Taking spontaneous notes
28,0,Working in software it's clear that there's not a strong relationship between time spent and quality yet when we evaluate other service providers that's our natural tendency.,#meta,"{% include setup %} We live in a world where it’s impractical to be a generalist so we specialize in a subset of skills and go to others for everything else. This works well but there’s still imperfect information - it’s tough to gauge someone’s skill level when you’re not an expert. In fact, when we lack awareness we end up using time spent as a proxy for skill when comparing across service providers. Imagine going to two barbers that charge the same amount for a haircut but one takes 10 minutes and the other takes 30 minutes. Even if we can’t tell the difference between the two haircuts we’d value the 30 minute one more due to the time difference. This seems backwards. The barber that was able to achieve the same result in 10 minutes is the more skilled one but instead we feel swindled when we back the price into an hourly rate. We should be willing to pay more for the 10 minute haircut since it gives us more time for our own pursuits. Yet when we lack knowledge we opt for the shortcut of equating time and skill. I’m trying to break this tendency by thinking about the end result rather than the effort and time involved. It’s interesting to compare this to software development. I know that just because someone spent more time on a project doesn’t mean it's better than someone who knocked it out yet I still view other skilled professions from a “time equals quality” perspective. It makes you wonder whether professionals in other industries have a similar mindset where they realize time spent isn’t an indicator of quality in their own profession yet view it as a sign of quality in others.",0,1,2015-05-18,3,"experts, time versus quality, estimating productivity",292,"Experts, time, and quality"
29,0,As general tools get better and better we'll see a decline of niche tools for specific use cases. This will further entrench the position of the market leaders.,#meta,"{% include setup %} I have a few sites that are “first stops” for specific use cases. I’ll go to Google Maps for directions, Foursquare for ideas of where to go, and Amazon whenever I need to buy something. They’re great most of the time but what’s interesting is what happens in the failure case. At that point my primary tool is no longer sufficient and I need to move on to secondary options. In these cases I tend to not have a well defined set of fallback options - for most of them I’ll fall back to the general case of using Google and then exploring from the search results. The only clear exception is Foursquare in which case I’ll go to Yelp before moving on to a general Google search. What’s surprising is that the fall back option usually leads to a successful outcome. Maybe I should switch my approach to start with the general search first and only move on to the specific tools when it fails. I wonder if we’re converging to a world run by fewer, smarter, and more powerful apps.  Data begets data  and as we supply more of it to the leaders we entrench their position, making it significantly harder for new companies to launch. We need regulation that enforces data mobility and allows people to export all the data that they’ve contributed and share it with whoever they’d like.",1,1,2015-05-22,4,"data, monopoly, niche tools, google",243,The decline of niche tools
28,0,The PSEG site isn't up to date with the latest HTTPS best practices so Google Chrome prevents me from accessing it. We'll see how this plays out.,#meta,{% include setup %} Earlier today I wanted to check up on my electricity bill but ran into an issue trying to login to my PSEG account. Turns out that my nightly version of Google Chrome is preventing me from logging into their site since it has a poor HTTPS configuration. Instead of seeing the login page I get the following message: “Server has a weak ephemeral Diffie-Hellman public key”. Luckily for me this only happened in the nightly build and I was able to login using both the nightly version of Firefox and the standard version of Chrome.            I wonder whether Google’s making the right decision here. What happens when they propagate these changes down to the standard versions of Chrome and countless people start having issues paying their bills. I understand that Google wants sites to upgrade their security but there will be a ton of disruption in the interim. Especially on sites people use to manage their lives. I’m just hoping that the sites that need to upgrade their security do so before Chrome updates itself.,0,1,2015-05-25,3,"chrome, https, security",203,Google Chrome knows what's best for me
34,0,I ended up stripped the head of a screw when assembling a piece of furniture and needed a way to finish the job. Luckily I stumbled on an easy fix using the driver.,#meta,{% include setup %} Note that this is straying a bit far from my usual posts but I thought it would be helpful for anyone that’s had to deal with a stripped screw or a broken screw head. In my haste I used the wrong driver bit and completely stripped the screw head. It was deep enough that I wasn’t able to extract it using pliers while being so stripped that none of my screwdrivers had enough grip to finish screwing it in. After a bunch of failed ideas I finally stumbled unto a solution that worked and could have helped me over the years. The idea is to use a drill/driver but instead of using a bit in the head you tighten it around the stripped screw. Then when it’s tight around the screw you drive it in until it’s where you want it to be. The other option is to use this approach to get the screw out and replace it with a brand new one to make sure it’s able to removable in the future.,0,1,2015-05-26,3,"screw, stripped screw, hardware",179,Dealing with a stripped screw
22,0,I added another tool to my JavaScript tools page that lets you generate a date range from a series of inputs.,#code,"{% include setup %} I finally had the chance to go back and add  another quick tool  to my JavaScript arsenal. This one lets you specify a start date, an end date, a step size and interval, along with a desired date format and it will generate the dates in between. This is a surprisingly common activity for me. Every time I need to split a query into multiple date ranges or come up with a series of arguments for various jobs I end up using Excel to come up with the appropriate date ranges. By having it available via the web it makes it a lot easier to generate exactly what I need as well as provides the flexibility to keep on improving. If there are any improvements you’d like to see or if anything is unclear definitely let me know.",1,1,2015-05-30,4,"javascript, date range, automate date calculation, date format",170,Date range generation
20,0,Combining RelayRides data with the Edmunds API provides an interesting look into the best car to list on RelayRides.,"#data,#dataviz","{% include setup %} After discovering and browsing [RelayRides](https://relayrides.com/) I noticed that there were some users that had multiple cars available for rent. Clearly they weren’t using each of their cars and were using RelayRides exclusively as a revenue generating business rather than renting a car out when it wasn’t being used. This got me thinking about what the best car would be to rent on RelayRides if my goal was solely to maximize my return.  There were only a couple of factors at play here: the initial cost of the car, the price the car will rent at, and how often the car is rented. By combining these values we can come up with a ratio of car price to expected revenue per day. The challenge was in getting this data but it turned out to be surprisingly easy.  My first attempt was to simply scrape RelayRides but I ran into a variety of issues with the authentication process and not being able to evaluate JavaScript via a Python script so I switched gears. My second attempt was a lot simpler but got me what I wanted - after doing a search I opened the network tab in Google Chrome and examined the HTTP requests being made. One of these was to the /search endpoint which gave me a JSON feed of the 200 most relevant cars as well as their make, model, year, the daily rate, the listing time, as well as the total number of trips taken. All I needed to do was dump it into a file and start writing a quick script to start parsing and analyzing the data.            The next step was actually getting the price of a car. Once again I thought this would be a problem but it turns out the  Edmunds API  was perfect for this. It’s entirely free and amazingly worked in nearly all cases with the data I was able to get from RelayRides. The API was smart enough to take the make, model, and year from the RelayRides and provide an estimated price without any sort of data cleaning of transformation. The only issue I ran into was a few rate limiting errors when I decided to parallelize my script but the fix was to just introduce a delay between consecutive requests and retry if I ever encountered an issue.            Combining the RelayRides data with the Edmunds API and doing some simple math gave me the answer I was looking for - a 2008 Toyota Prius. There were a few cars that had a better expected return but they also had very few trips taken and weren’t listed for long which leads me to believe that their return won’t last. For the most part, the rental rate ends up being highly correlated with the price of the car - the most expensive in my dataset was a 2011 Mercedes G-Class which is listed at a $550/day and has an estimated cost of $80k while the cheapest was a 2003 Ford Taurus that’s listed at $32/day and has an estimated cost of $4k. In general, the market seems pretty balanced in terms of price but there’s a wide variance in how often different cars get rented out - it’s clearly proportional to price but there’s definitely something else there. Unfortunately, this approach only lets us examine the cars that are actually listed and won’t let us predict how a random car would do. In the future I might take a stab at running a regression to generalize this approach but the challenge will be in figuring out the relevant factors.            As usual, the code’s up on [GitHub](https://github.com/dangoldin/relay-rides-analysis) and I’d love to hear ideas or thoughts on how to improve the code or the analysis.",3,2,2015-06-07,4,"relayrides, edmunds, maximizing return, data analysis",693,Finding the optimal car to list on RelayRides
32,0,Be careful when using the GROUP BY in MySQL on a derived field with the same name as a column. Instead of the derived field MySQL will use the column name.,#sql,"{% include setup %} I discovered a nuance with MySQL's GROUP BY statement earlier today that I’ll share with the hope that others can learn from it. It’s fairly common to use a coalesce statement to handle null values while keeping the resulting field the same name. For example:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY user_id; {% endhighlight sql %}  The nuance is that we want the GROUP BY to apply to the entire coalesce expression but as it’s written it only applies to the user_id column from table_a. This has potential to give odd results in more complicated queries. The only fact I even discovered it was that it was causing a duplicate key constraint violation in another table. The solution is quite simple but annoying - you have to use the entire coalesce expression within the GROUP BY statement:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY coalesce(a.user_id, b.other_user_id); {% endhighlight sql %}  The reason this solution is messy is that it’s very easy to update the SELECT but forget to update the GROUP BY. This won’t throw an error and MySQL will execute the query just fine - the results just may be unexpected. What I’ve started doing is renaming the resulting column and using that within the GROUP BY:  {% highlight sql %} SELECT coalesce(a.user_id, b.other_user_id) as final_user_id, sum(s.num) as total_nums FROM table_a a LEFT JOIN table_b on a.some_id = b.some_other_id LEFT JOIN stats s on a.stat_id = s.id GROUP BY final_user_id; {% endhighlight sql %}  This makes the query a bit more complicated but it’s being explicit about what we want and avoids hidden errors.",0,1,2015-06-09,3,"mysql, group by, querying",342,A MySQL “GROUP BY” nuance
31,0,People are doing tons of different things on their phones but I can't get over the frustration of how much less efficient I am on a phone than a computer.,#meta,{% include setup %} Smartphones are supposed to be the next big wave but I can’t get myself to be productive on them. Every action takes an order of magnitude longer than it would on a regular computer which prevents from me from starting it in the first place. The challenge is that I’m a power user on a computer able to leverage shortcuts across a variety of programs to be extremely productive. The cost is that when I switch to a phone it’s impossible for me to attain that level of speed which is extremely frustrating.  I see that others are spending a ton of time on their phones and are using it to replace a variety of activities they’d normally do on a full fledged computer but I’m not able to do the same. I wonder what impact this has. Smartphones are great for consumption but I wonder whether they actually improve people’s productivity. It’s much better than not having any digital device and is definitely helping get more people digitally connected but I can’t imagine them replacing actual computers in the short term. What will make smartphones more productive is when they start understanding our intent and predicting what we actually want to do. Something akin to Google Now but instead of showing us what we want to see allowing us to create what we want to create.,0,1,2015-06-13,3,"smartphones, productivity, google now",232,Smartphone productivity
26,0,I had to use the Edmunds API a couple of weeks ago and was pleasantly surprised by how easy and simple it was to use.,#product,"{% include setup %} As part of the  RelayRides analysis  I needed to estimate the price of a car and stumbled across the  Edmunds API . I came in with some low expectations but was pleasantly surprised by how well it worked. I thought I’d need to go through a data cleanup process to make sure I was using the correct arguments in the HTTP requests but somewhat remarkably the Edmunds API was able to properly handle nearly every request.  It’s unbelievable how happy a good API makes me. Dealing with various edge cases is a huge time suck so having an API that works as expected the first time you try it is incredibly refreshing and highlights the amount of crappy APIs we’ve all had to deal with. I’d expect this to come from a small, product focused company or at least be built in house but it turns out Edmunds partnered with  Mashery  to develop their API. It definitely makes a case against keeping development in house.",3,1,2015-06-19,2,"edmunds, api",185,The Edmunds API
25,0,When doing text message based two factor authentication it's important to actually be secure by not showing the unlock code on a lock screen.,#product,"{% include setup %} The purpose of two factor authentication is to prevent unauthorized access to your accounts by requiring a device other than a password to verify that it’s actually you. Usually this is a text message to a phone or an app such as Authy or Google Authenticator. Being paranoid and despite the inconvenience I chose to do it for the vast majority of my accounts that support it but some are significantly more secure than others.  In particular, developers need to be careful when doing text message based authentication and make sure the code is not visible during a lock screen. Twitter includes the login code as the first word in the message whereas Bank of America does it right and makes sure the code is not visible without unlocking the screen. It’s a seemingly tiny difference but highlights how important it is to get security right.    	  		  			   			 Bank of America obfuscating the code  		  	  	  		  			   			 Twitter including the code on the lock screen",0,1,2015-06-20,4,"security, two factor authentication, twitter, bank of america",203,Properly handling text based two factor authentication
31,0,Amazon rewrote the boto library to take advantage of their domain specific models that represents the AWS API. It's wonderfully clever and I'd love to see standardization around API definition.,"#meta,#code","{% include setup %} Yesterday, Amazon  announced  a major update to their Python client, boto3. The core functionality is unchanged but they used a clever solution to make it easier to add, modify, and remove endpoints. By coming up with a  standardized representation  for each of the endpoints they’re able to write wrappers in different languages that generate the API calls programmatically. For example, I've included a subset of the  EC2 definition  below. It contains the information necessary to programatically generate the API wrapper to hit the appropriate EC2 endpoints.  {% highlight json %} {   ""service"": {     ""actions"": {       ""CreateDhcpOptions"": {         ""request"": { ""operation"": ""CreateDhcpOptions"" },         ""resource"": {           ""type"": ""DhcpOptions"",           ""identifiers"": [             { ""target"": ""Id"", ""source"": ""response"", ""path"": ""DhcpOptions.DhcpOptionsId"" }           ],           ""path"": ""DhcpOptions""         }       },       ""CreateInstances"": {         ""request"": { ""operation"": ""RunInstances"" },         ""resource"": {           ""type"": ""Instance"",           ""identifiers"": [             { ""target"": ""Id"", ""source"": ""response"", ""path"": ""Instances[].InstanceId"" }           ],           ""path"": ""Instances[]""         }       },       ...     }   } } {% endhighlight %}  This domain specific approach is great when working with APIs and I’m surprised more libraries don’t adopt it. The benefits include being able to keep the actual code the same and only updating the definitions as well as having definitions shared across various language implementations. An additional benefit that can be gotten is actually downloading the latest definitions at runtime. This way you’re always running against the latest version of the API and don’t have to worry about upgrading versions.  I’d love to see more companies adopt this approach and even come up with a standard API declaration language. Then a single set of scripts can be used to wrap any API. Imagine how much simpler it would be to integrate with third party APIs when all you need to do is read the docs and have everything else wired. In fact the docs themselves can be generated from the base definitions.",3,2,2015-06-23,5,"apis, amazon, aws, boto3, domain specific languages",333,Domain specific API definitions
26,0,When running raw SQL make sure to use table aliases in your queries even when they're not required. This insures you against future breaking changes.,#sql,"{% include setup %} One of the best habits to develop when working with SQL is to always refer to fields through an alias. Numerous times I decided to just take a shortcut and ended up regretting it later. Even if you’ve tested your query to make sure it works there’s no guarantee that a future change to a table schema won’t break it.  Let’s say you have the following two tables - with items.category_id corresponding to categories.id  {% highlight sql %} create table items (     id int,     name varchar(20),     category_id int,     owner_id int );  create table categories (     id int,     code varchar(4) ); {% endhighlight %}  It’s straightforward to join the two tables to get some basic info:  {% highlight sql %} select i.id as item_id, name, category_id, code from items i join categories c on i.category_id = c.id; {% endhighlight %}  Let’s say we test the code and deploy to production. It works perfectly until someone adds a “name” column to the categories table. All of a sudden our query stops working with a helpful “Column 'name' in field list is ambiguous” error. The reason is that the query doesn’t specify which source table for the name column. The solution is to simply prepend the items table alias to the name field and we’re back to a functional query.  {% highlight sql %} select i.id as item_id, i.name, category_id, code from items i join categories c on i.category_id = c.id; {% endhighlight %}  This issue is tough to check against since it requires searching your entire codebase every time you need to alter a table. A better approach is to always specify the schema and avoid the issue altogether. Especially in a quickly growing engineering team where multiple people are working on the same code base it’s very easy to run into these sorts of issues that may only get discovered in production. Although most ORM frameworks abstract this away it’s sometimes necessary to dive down into raw SQL and this is one of those small best practices that is a tiny bit of additional effort to significantly reduce a future risk. Avoid learning this lesson the hard way.",0,1,2015-06-27,3,"sql, queries, best practices",370,Ambiguous SQL queries
22,0,Being productive on the command line is crucial for a developer and zsh with Oh My Zsh makes it significantly easier.,#devops,{% include setup %} I spend a fair amount of time in the command line and one of my biggest wins in productivity has come from adopting Z shell along with the wonderful  oh-my-zsh  framework. I initially installed it when looking for better git integration but have been discovering tons of new tricks and features since. In addition to the standard autocompletion for both paths as well as commands there are various plugins to support a variety of other scripts. Just a few days ago I enabled a plugin to allow for autocompletion for Python’s fabric commands. The advantage for a single command is tiny if you’re quick on the keyboard but when you’re running hundreds of commands each day it’s nice to get your typing speed to be as quick as your thought process. Zsh comes close.  If your bash environment is optimized for your flow after years of tweaking zsh is not for you. Otherwise you should give it a shot - it’s a superset of what you get in bash and you can easily migrate your bash configuration to zsh and can easily switch back. I have dozens of apps installed right now that I’m sure I’ll forget to reinstall when I get a new computer - until I realize I need them. Zsh on the other hand will be one of the first things I install - it’s that critical to my productivity.,1,1,2015-07-06,5,"zshell, zsh, oh my zsh, bash, shell",242,Zsh and Oh My Zsh
37,0,I read a book explaining that the reason Apple didn't use Intel chips since the very beginning was due to Steve Jobs not paying Steve Wozniak fairly. Turns out it's not true but an interesting hypothetical.,#meta,"{% include setup %} I recently finished  The Intel Trinity  which detailed the history of Intel and its rise from a small memory manufacturer to the leader in microprocessors. The entire book is worth a read if you’re interested in startups and the rise of Silicon Valley but one anecdote that immediately stood out was about the reason Apple didn’t use Intel chips  until 2005 . Before then Macs relied on  MOS Technology, Motorola and PowerPC  chips. The Intel Trinity makes the case that the reason Apple waited so long to adopt Intel chips was due to the fact that Steve Wozniak didn’t have enough money to build the Apple I prototype using Intel and had to resort to the cheaper option - a MOS 6502/Motorola 6800. And the reason Steve Wozniak didn’t have enough money was because Steve Jobs didn’t split the Atari payment fairly between them and took the lion’s share without even telling Steve Wozniak about it.  After a tiny bit of digging around the  concensus seems  to be that this is most likely a fabrication and even if Wozniak had the money he would have still gone with the Motorola chip - he was more familiar with the technology and the end goal was to make an affordable personal computer which would have been impossible with the Intel chip. It does make one wonder what the history of Apple and the computer industry would look like had they adopted Intel at the very beginning rather than in 2005.",4,1,2015-07-08,5,"apple, intel, steve jobs, steve wozniak, microprocessors",283,Apple and Intel
14,0,A quick JavaScript tool to make it easy to compare two SQL schemas.,"#sql,#code,#devops","{% include setup %} During development it’s common to get your dev database out of sync with the one in production. Sometimes it’s due to an additional column in development you added before realizing it wasn't necessary and other times it’s just creating a few temporary tables on production that you forget to drop. In both cases it’s useful to reconcile the schema differences every once in a while to keep your database in a clean state. In the past I would just run a simple query (select table_schema, table_name, column_name from information_schema.columns;) on each environment and then use either Excel or Google Sheets to spot the differences. This takes a bit of time so this weekend I put together a quick  JavaScript tool  to automate the process. You simply run the schema query on each of the environments and paste the resulting rows into the two text areas. The result is a JSON based diff showing the additions, deletions, and modifications to each of the tables and fields. The next step is to modify it to also identify differences in the column types.",1,3,2015-07-12,3,"sql, schema, compare sql schemas",193,Comparing SQL schemas
37,0,I realized that our brains work just like Bloom filters. They don't always know the details and they have some false positives but they're great for at least telling us that we ought to know something.,#meta,"{% include setup %} As many at TripleLift will tell you I have a fondness for  Bloom filters  but only recently did I realize that our brains work in a similar way. We don’t always know every particular detail or have perfect recall but what we do have is the ability to realize that something is familiar and that we might have encountered it before. This triggers enough additional thoughts that we’re able to dig up the actual thought or reference. For example I can’t always recall the exact Java library I need to use for a particular problem but I know that I’ve solved similar problems before and can quickly rediscover my previous solution, whether through an online search with the appropriate keywords or even by going through some old code.  I’d even argue that it’s more important to have awareness of everything you’ve done and seen in the past than to have a perfect recollection of a smaller subset of items. Knowing that you’ve seen something before takes care of the fear of the unknown - very similar to how  George Dantzig  was able to solve an “unsolvable problem” as a student since he didn’t know it was considered unsolvable.  Unknowingly I’ve even developed an approach to take advantage of this mental model. I dump interesting notes and links into text files that I “tag” with a bunch of additional thoughts or keywords I think of at the time. Then whenever I run into an issue and realize that one of my notes might be useful it’s a simple text search to find exactly what I’m looking for. Rather than rely on a structured approach such as Evernote I rely on my own adhoc system and am rarely unable to find what I was looking for. In the extreme cases I can even resort to a regex search and some piping to deal with too many results or a very scattered document. Every once in a while I’ll even write a quick Python script to provide a semblance of order although almost always I just resort to a text search.",2,1,2015-07-16,2,"bloom filter, memory",360,A Bloom filter in my head
28,0,"Despite it's shortcomings Excel is still the only Office product I use. There just hasn't been a replacement that has that combination of speed, performance, and simplicity.",#meta,"{% include setup %} Other than the usual developer tools the only desktop based app I use is Excel and every few months I try to wean myself away. I love being able to keep all my text docs and slideshows online and have them accessible and sharable anywhere. The best part is updating the content without having to worry about bombarding people with yet another email.  I tried doing the same with Google Sheets and it works for smaller tables but as soon as you get tables with thousands of rows it’s noticeably slower than Excel. It’s amazing for what it can do but it feels as if the browser just can’t handle the rendering nor the calculation that a large spreadsheet entails. Some of the time closing and reopening the table fixes the problem but this is too reminiscent of Windows in the 90s and ends up in a glacial pace after a few minutes of work. I continue to use Google Sheets for small, collaborative files but for anything larger or anything that will need heavy computation I’ll switch to Excel. I’ve also been using R for more repetitive analyses but for the quick and dirty analysis that comes from the result of a SQL query Excel is still king.",0,1,2015-07-19,3,"excel, google sheets, R",214,Excel wins
22,0,It's easy to get so obsessed with a technical problem that by solving it you introduce a litany of new ones.,#meta,{% include setup %} A common behavior when solving a coding problem is focusing too much on the solution and not enough on the general context. If this is a software problem this may manifest itself as a very quick turnaround on a task that inadvertantly breaks an existing behavior or even something that ends up causing a headache months from now when a slightly more nuanced use case needs to be supported. Experienced developers will not only solve the task at hand but will also understand the limitations of their solution and are able to identify the areas that will be adversely affected by their solution. Nearly every software decision comes with tradeoffs and strong developers can think through this maze and pick the most appropriate one given the situation.  Part is experience and learning from our mistakes and part is knowing our applications and how the various components interact and fit together. A big driver of this is curiosity - some developers will stop as soon as they find a library that solves a particular problem and will maybe even read the docs but great developers will step through the actual code to understand how it works. Imagine two people working on the same tasks for a year - one who’s curious enough to read through the source code of open source libraries they used and one who doesn’t. I’d bet that the one who was curious enough to read through third party source code learned and retained significantly more than the one who didn’t. Another one is relentlessly thinking in abstractions - thinking at a higher level makes it much easier to spot patterns and identify code that needs to be refactored. This ends up paying massive dividends in the long term when massive scopes of work can be eliminated. One way to get better at this is to constantly reevaluate your work and identify code that’s been duplicated since it may be a sign you chose the wrong level of abstraction. Another one is thinking through what you’d have to do if you needed to make some tweaks in the future - would you be able to reuse the code in a clean way? Which part would be the easiest to modify? Which parts are too coupled to separate easily? The simplest way may be to just look and the code and see if it’s ugly - that may be a sign that you did something wrong.,0,1,2015-07-20,3,"programming, coding, software engineering",407,Tunnel vision
26,0,One of the best ways to onboard as an engineer is to understand the database and how various tables and fields relate to one another.,#meta,{% include setup %} I’m convinced that the best way to ramp up as a newly hired engineer is to go through the database. Rather than relying on outdated documentation or discovering undocumented features the database is the actual source of truth and defines both the limits and the capabilities of the application. You can examine the relationships between the various objects as well as the litany of features and options that are supported. It’s definitely more difficult to get up to speed on a database rather than documentation or a demo of the UI but the knowledge gained is significantly deeper. Especially when you’re going to be working on features that depend on the database it’s incredibly useful to know how the database is laid out and set up. On its own a walk through of the UI provides a high level overview of how it works but coupling that with the database allows you to internalize the connections and actually understand how the user interactions feed the data and vice versa.  One of the most interesting benefits is seeing the progression of a company’s products and features. A typical database will contain a litany of fields and tables that stick around after the underlying feature or product is antiquated. These features have no documentation and the only way to know what they’re used for is to spend hours going through version control history or talk to someone who was actually around. Despite these fields and tables no longer being used they’re valuable for the context they provide. Seeing the evolution of a product allows you to identify what worked and what didn’t work and serve as as a springboard for new ideas.  Databases are rarely part of an engineer’s onboarding process and are mostly on a “need to know basis.” Only when you’re working on a particular feature do you have to understand the relevant schema and even then you’re not expected to go beyond what you’re working on. This is the wrong approach and there’s a ton of implicit knowledge in our databases that make the entire team more productive. Coupling this with a walkthrough of the UI and the API is a great way to learn and relate the various concepts.,0,1,2015-07-25,3,"engineering, management, onboarding",377,Use the database Luke
16,0,Go takes a simple and novel approach to interfaces that's commonly found in dynamic languages.,#code,"{% include setup %} I’ve only been playing around with Go for a couple of weeks but one of the language design decisions I’ve really enjoyed is how interfaces are handled. Coming from a traditional object oriented background it’s typical to define an interface that defines a few method signatures and then explicitly implement that interface in a new class. Below’s a trivial example of this approach in Java:  {% highlight java %} interface Animal {   public boolean isFurry();   public String speak(); }  class Dog implements Animal {   public boolean isFurry() {     return true;   }    public String speak() {     return ""Woof"";   } }  public void aRandomFunction(Animal a) { ..  } // Can take anything that implements Animal {% endhighlight java %}  With this approach a compiler immediately identifies cases where you choose to implement an interface but forget (or mess up) implementing one of the underlying methods.  Go’s approach is different. In go you would define the interface as usual with the expected methods and you would write functions that accept the interface as the argument. But instead of explicitly specifying that a particular object implements an interface you just do it. Then if it turns out you’ve successfully implemented the methods you can use that object wherever the interface is expected. The compiler is still able to point out signature issues since it can tell when you’re trying to use an object with a required method but it’s done in an implicit way. Below’s the equivalent Go code:  {% highlight go %} type Animal interface {   isFurry() bool   speak string }  type Dog struct { }  func (d Dog) isFurry() bool {   return true }  func (d Dog) speak() string {   return ""Woof"" }  func aRandomFunction(a Animal) { .. } {% endhighlight go %}  Dynamic languages frequently use this “duck typing” approach since the variable types may only be discovered during run time so it’s neat seeing it implemented this simply in a static, strongly typed language. The simplicity and novelty of Go’s interfaces make me eager to keep digging and see what else I discover.  https://en.wikipedia.org/wiki/Duck_typing",0,1,2015-07-29,4,"golang, java, duck typing, intefaces",348,The Go interface
17,0,Code is not done when a pull request is submitted but when it's deployed to production.,#meta,"{% include setup %} As a developer, it feels wonderful to commit some code and knock an item off of the ever growing to do list. Unfortunately, until that code is deployed it’s not delivering any actual benefit. It’s easy to open a pull request and move on to the next task but to create high quality products we need to only consider our code complete when it’s deployed and running issue free. So many things need to happen between writing the code and deploying it - handling conflicts with other database changes, updating database schemas, and monitoring the actual code to make sure it’s working as expected on a production system. Calling something done before it’s deployed is a lazy shortcut.  This approach also encourages developers to care more about their code and take a big picture view of the product. By taking an active role in the deployment we’re forced to think through the dependencies and design a release process that avoids downtime and occurs in the right order. For simple features it’s straightforward but larger, coupled ones require an approach that may even end up in rewriting code in order to simplify or stage a complicated deployment process. And if you know your features aren’t complete until they’re deployed you’ll make an effort to actually get them deployed. This is a huge risk reduction since the code and ideas are still fresh in our minds and can deploy code in small batches rather than massive monoliths.  The holy grail is continuous deployment which couples code commits and deployments but it requires significant effort to get it working smoothly that may not be worth it for early stage companies who need to focus on building their product. For them iterating is crucial and every developer needs to take ownership of getting their code into production.",0,1,2015-08-01,4,"engineering management, deployment, quality, engineering teams",307,It's not done until it's deployed
36,0,It's very easy to want to be involved in every decision and through in your two cents but it's more important to focus on what actually affects you and trust others to do what's right.,#management,"{% include setup %} A critical component in communicating between various teams is knowing who has what responsibility. Especially with driven people it’s easy to have overlap between various functions - product and design; design and frontend engineering; and frontend engineering and backend engineering. This is both good - because it’s able to focus more eyes on a particular problem and provides a new perspective - and bad  - because people may feel that they can’t move quickly enough and don’t want to cede decision making power. Great teams thrive in this environment while poor teams degenerate into a Dilbert cartoon.  One approach that I’ve been preaching is to standardize on the edge points that can act as a form of “contract” between the teams. At those edges it’s great to have the debates and argue the merits of various implementations but beyond that the ownership should lie with the respective team.  An example is to image two engineering teams - one is a full-stack team responsible for the UI and the corresponding API endpoints for a customer facing application and the other is a backend team that uses this information to run the hidden part of the application - the data collection, the web server, and the various third party integrations. In this case a good intersection point would be the database - both teams leverage it and have their own thoughts on what to store and how to structure the schema. The debate should be centered around these questions rather than how each team builds their own components. Once there’s agreement on the database structure each team can go ahead and work independently of the others.  Similarly, a designer can create a series of mocks that can then be debated with the frontend team. The frontend team may push for a different design that will simplify their code and a designer may push for a certain approach that significantly increases the product’s usability. After both teams settled on an approach they can focus on what they’re great at - a designer may focus on getting the visual details perfect while the front end team can start writing the HTML, CSS, and JavaScript.  By focusing on what we actually need to do our jobs and trusting others to do the same we’re able to skip the politics and move quickly. It’s human nature to be curious and want to know everything that’s going on but it’s a massive hit to productivity. Especially at a startup when speed is critical being able to skip the unnecessary meetings, debates, and politics can make the difference between success and failure.",0,1,2015-08-03,3,"management, politics, efficiency",432,Debate what's necessary and no more
31,0,I'm a huge fan of static sites and think it would be straightforward to implement a simple version of search. I plan on giving it a shot on this blog.,#meta,"{% include setup %} I’ve written  previously  about the appeal of static sites and recently came up with another example of how powerful the setup can be. The gist is that the site’s content is static HTML, CSS, and JavaScript but the relevant underlying content is refreshed on a recurring basis with a separate job. This allows you to host the entire site on S3 and avoid maintaining your own web server.  Normally, implementing a site search requires a backend to accept queries, break them down into the appropriate keywords, and hit a search index to find the matching documents. A simple implementation would index the site using a server side script and store the results in a JSON file that could then be access on the client side. The client side JavaScript would need to be intelligent enough to parse the query string and reference the right index file but a simple solution is easily doable. I’m surprised more simple sites haven’t adopted this approach and I’ll give it a shot with this blog to see what issues I run into.",1,1,2015-08-09,2,"javascript, static sites",189,Static site search
34,0,Sometimes it's necessary to reprioritize tasks on a RabbitMQ queue. One way to do it is to get every task on there and use shell scripting to requeue then in the correct order.,#code,"{% include setup %} Earlier today we had a hiccup where we had a bunch of messages piled up on a RabbitMQ queue that were not being consumed. Some of these tasks were very quick data loads while others were more involved jobs that could take multiple minutes to run. Normally these are distributed relatively evenly across the day so it’s not a problem but in this case we had hundreds of tasks in a random order and we wanted to shuffle them around such that the data load tasks executed first so that the data would be quickly accessible to other higher priority jobs.  Luckily, we remembered we had some old shell commands that helped us backup and restore a RabbitMQ queue so it only required a bit of scripting to come up with a sequence of commands to do exactly what we wanted. The script works by dumping the contents of the queue into a file, extracting the message field, filtering the messages into the desired buckets, turning them into queue addition commands, and executing the resulting files.  {% highlight sh %} # Dump the contents of the queue to a file. # To be safe requeue the messages and do a manual purge when # we confirm the data looks right. ./rabbitmqadmin get queue=data_queue requeue=true count=2000 > tasks.log  # 1 - Get the appropriate field (in our case the fifth one) # 2 - Remove the header rows # 3 - Trim the line # 4 - Prepend the publish command and turn the task message string into an argument cut -d'|' -f5 tasks.log | sed '$d' | sed '1,3d' | sed 's/^ *//;s/*$//' | sed -e ""s/^/.\/rabbitmqadmin publish exchange=data_queue.task routing_key=standard payload='/"" | sed -e ""s/$/'/"" > tasks.clean  # Split the tasks into two pieces cat tasks.clean | grep log > tasks.clean1 cat tasks.clean | grep -v log > tasks.clean2  # Queue the tasks in the appropriate order sh tasks.clean1 sh tasks.clean2 {% endhighlight %}",0,1,2015-08-12,3,"rabbitmq, tasks, devops",335,Reprioritizing a non priority RabbitMQ queue
25,0,It's possible to generate data using either a generative or filtering approach. Both are useful and the skill is knowing when to use each.,#meta,"{% include setup %} While playing around with Scala I rediscovered streams - a list-like structure that's lazily evaluated - meaning that only when you access a particular value is it evaluated. This makes it possible to create infinite streams since all you need is a function that's able to compute the next value. In such a way we can create a stream of all numbers, just the positive even numbers, or just the prime numbers. Calculating each successive prime number will become more difficult but it is possible.  In the case of the positive even numbers it's possible to generate the stream in two simple ways - one is to take each positive integer and double it while the other is to take every positive integer and filter them down to those that are divisible by two. Both of these will generate the exact same numbers in the same order but do it in opposite ways. One generates the numbers from a base list and the other filters a larger list down.  In this example the efficiency of the two approaches is similar: the first goes through each element once and does a bit shift while the second goes through two elements and does a bit comparison. But on real code the differences between the two approaches can be significant. It's also likely that one of the approaches may not even be possible or be too arduous - imagine generating a list of prime numbers.  Both are useful depending on the problem and the skill is figuring out when to use each. The generative approach feels as if it should be the more efficient one but there are many cases where filtering is easier and quicker.",0,1,2015-08-15,5,"scala, streams, lazy evaluation, generating, filtering",285,Generating vs filtering
26,0,I'm having my phone repaired and am feeling what I expect are withdrawal symptoms. It's scary to thinh how addicted we've become to our smartphones.,#meta,"{% include setup %} As they say you don't appreciate something until it's gone and I got to experience first hand when I cracked the screen on my phone and dropped it off for a quick repair. Unfortunately, the repair wasn't so quick due to a screw up and I'm still phoneless more than a day later.  I find myself reaching for it despite knowing it's gone and even feel it vibrating in my pocket without it being there. It's both amazing and frightening how significant my phone has become in my life and I'm am actually glad that it's missing. In many ways I feel like an addict that missed a fix and it's a rude awakening. For the first time in years I had to ask a stranger for the time and had to find an open barber shop  without a map or an online search. I also went to bed without my usual habit of checking up on Twitter or catching up on some blog posts and I woke up without immediately reaching for my phone.  It's shocking how hooked we've become - nearly every person at my train station is staring at their phone while waiting for the train, on the train itself, and when they leave. Of course phones make our lives better but we have to realize the price we're paying and sacrifices we're making. I plan on being more mindful when I do get my phone back and will try to keep some of my days phone free - who knows what sorts of adventures I'll have.",0,1,2015-08-16,2,"smartphone, phone addiction",263,A phoneless day
35,0,"Scripting languages make it easy to move quickly with small steps. On the other extreme some languages let us take slow, large stpes. The goal is to find a language that maximizes actual speed.",#meta,"{% include setup %} The most common way of making sure code works is by going through the “develop-run-test” loop. We write some code that we expect to have a certain behavior, we run the code and trigger that behavior, and then we confirm that the results are what we expected. And we keep iterating, hopefully making more progress with each new iteration.  One thing I’ve noticed is that this pattern varies drastically for me depending on the language I’m working with. I’ll cycle through iterations much quicker in Python than I will with Java. Part of it is that my Java projects are larger and take a longer amount of time to start but I suspect the bigger benefit is that Java’s strong and static type system makes it easier to take larger coding steps than I’d be able to with Python. For example, if I need to write a method to extract data from a JSON object I’ll approach it very different if I’m doing it in Python than I would if I were doing it in Java. With Python I’d jump into the REPL and walk through a few examples and make sure I handle the the various edge cases whereas with Java I’d place a lot more faith in the IDE and it’s litany of warnings.  Each language comes with it’s own pros and cons and it’s impossible to find a single language that fits every use case. The goal is to pick the appropriate language for the job at hand - and this may involve starting with one and moving to another one as the problem domain changes or the team grows. The ideal language is one that’s able to maximize the product of the iteration speed as well as the step size. Taking frequent, small steps is equivalent to taking fewer, bigger steps - the aim is to maximize the resulting speed - not the individual inputs. A great language paired with a strong development environment achieves both speed and step size.",0,1,2015-08-20,3,"programming, Python, Java",338,Development cycles across programming languages
30,0,"In addition to undertanding a database, looking at the AWS account and understanding how the various components fit together is a great way to ramp up as an engineer.",#devops,"{% include setup %} Last month I  wrote  that one of the best ways to ramp us a new engineer is to start going through the database schema and understand how the various tables fit together and what the various values mean. That provides a great view around the engineering product - the various fields indicate the options and functionality available and the tables indicate how the components work together as well as what and how data is collected.  The flip side is that this doesn’t actually provide any view into the application architecture - what’s the hardware used? What are the applications and how do they fit together? How do the applications work? What’s the load on the various components and what’s done to address it?  If you’re on AWS or another cloud provider a neat way to answer these questions is to look at the relationship between the various components and the appropriate stats. For example you can start with Route 53 to see the subdomains used and what they’re mapped to. Some may be mapped to EC2 instances while others may be mapped to a ELB, S3 buckets, or Cloudfront. Each of these provides a view of how the application is used - if it’s on S3 then the application is going to be static HTML, CSS, and JavaScript but if it’s hitting a load balancer then you can expect the application to be under heavy load and be supported by multiple EC2 instances. Beyond this you can look at the amount of requests being made and the volume of data going in and out as well as whether there’s any pattern throughout a day or week. There’s a ton of monitoring tools in AWS and each provides an additional data point that provides insight into the application architecture. The various options and dashboards available highlight how important devops is for every engineer - and how valuable it is for every engineer to have at least read-only access to AWS. It’s tough to write good, scalable good unless you understand how it will be used and how it will fit in with the rest of the stack.",1,1,2015-08-23,4,"aws, devops, learning, application architecture",363,Learn the application architecture through AWS
35,0,I had an interesting dinner at a restaraunt that allows you to pay what you want for your meal. The best part was seeing the subtle ways the restaurant gets you to pay up.,#meta,{% include setup %} This past Wednesday I had dinner at Blu - a restaurant that’s  adopted  a “pay what you want” pricing model. Customers have an incentive to underpay the final check so I was curious to see how Blu handled it throughout dinner. I noticed three tactics they used to get people to pay fairly and am sure they utilized a bunch more that I didn’t even notice:  - Anchoring: Before sitting down to eat the waitress explained that it was pay what you want and most of the dishes are estimated to be priced between $10 and $12. This sets the expectation early so if you do decide to pay less you’re making an explicit decision to underpay. - Reminder: At the end of the meal we were told how many dishes we ordered. This was also helpful but I can’t help but think that this is a way to give you an estimate of how much you should pay - especially when paired with the fact that the expectation is $10 per dish - a very easy number to multiply. - Shame: I found this the most interesting one. Instead of giving you a blank receipt and allowing you to write what you want to pay you have to tell the waitress what you want them to charge. This forces you to explicitly vocalize your payment to another person rather than quickly writing something and slinking away. And no one wants to be judged as cheap face to face so we’re encouraged to pay well.  I’m a huge fan of behavioral psychology experiments that shed some light on the way our minds work and it was a great experience to partake in one. I only wish I could have spotted more behavioral cues that I’m sure they employed.,1,1,2015-08-29,3,"dining, game theory, behavior psychology",309,Game theory dining
24,0,Despite not having a functioning phone I managed to survive pretty easily without Gmail but definitely learned to handle two factor authentication better.,#meta,"{% include setup %} While my phone was being repaired I ran into a predicament. The only way I could log in to my Google accounts was by authenticating via an SMS code which I wasn’t to get without an SMS code. Additionally, I never bothered to actually write down the backup codes thinking I’d never need them so I was stuck in the envious position of being Google account free for 4 days.  Luckily, I had two things going on that made the loss easily manageable. One was that I shared my personal calendar with my work account so was able to see (and create) everything I needed through my work account. And two - I’ve been forwarding all of my email from Gmail to Fastmail  since March  of last year. The only real frustration was not being able to search through my email history nor use the chat. Otherwise it was barely noticeable.  I definitely got lucky so the lesson here is that it’s impossible to predict what’s going to happen and you should just deal with the annoyance of the backup codes. Google also provides the Authenticator app which is another way of supporting two factor authentication. None of these is as simple as just not having two factor authentication but I think it’s a must have - especially for a primary email account which is linked to every other account - including your financial and social media accounts. Losing access to your emails makes it very easy to reset the passwords on those and there’s no excuse in not enabling two factor authentication.",1,1,2015-08-31,4,"two factor authenticaton, 2fa, security, google",269,Two factor authentication hell
21,0,I'm a big fan of Twitter and wish they actually focused on getting their product right rather than on monetization.,#product,"{% include setup %} Eugene Wei  published a great post  on the power of networks and how Twitter hasn’t been taking advantage of their core product - a public messaging protocol. Given this thesis, Twitter should move away from the artificial 140 character limit and innovate on top of the protocol rather than be bound by it.  I’m not nearly as eloquent but I also have my gripes with Twitter that his post motivated me to write. It just feels they don’t care about the user experience. Cross device sync is still a problem - if I clear a notification on my phone why do I see it again on my computer? People are still complaining about the OS X app not being as functional as the other versions. Even on my phone the navigation feels inconsistent - sometimes I get taken out of the app and sometimes a screen is loaded inside. This causes me to hit the back button at the wrong time and randomly leave the app which resets my location. I’m a big fan of Twitter and it’s pretty much the only social network I actually use but I’m frustrated by how poor it is.  It feels as if Twitter has decided to focus purely on monetization rather than evolving the product. It’s okay to do this when you’re in a dominant market position (see LinkedIn) but Twitter should be focused on making sure they’re getting new users that stick around. The only way to do this is to make a product that’s useful, fun, and easy to use.  Disclosure: I own a bit of Twitter stock.",1,1,2015-09-03,2,"twitter, product",277,Come on Twitter
34,0,Google has amazing integration between their tools that allow them to do a ton of search inference. I want to see this behavior develop in our operating systems to link various apps together.,"#meta,#product","{% include setup %} It feels as if Google has been getting better and better at what I call search inference. I’ll oftentimes do a search for a particular place on either Google Maps or the general site and see it automatically show up in Google Now. Or I will start with a simple search query that needs to be refined with Google able to offer perfect suggestions. Given how much data they’re collecting it’s not a surprise but it’s an easy way to realize I’m not that unique.  The fact that I can even sense improvement highlights how significant the improvements have been - gradual ones wouldn’t be as noticeable. What I want is for these innovations to become part of the OS. I currently use OS X and while it’s easy to use with a lot of neat utilities and applications it’s not smart. I constantly go back and forth between apps - referencing some notes in Sublime, writing some SQL queries, messing around with Excel - and would love the OS to be smart enough to understand my intent. The simplistic version of this is a smarter auto complete that transcends apps - rather than going back and forth between Sublime and Sequel Pro copying and pasting queries it would be nice for the OS to allow me to autocomplete fields and table names that are being actively used. The more advanced version would detect patterns in my workflows and allow me to skip numerous steps. When working on most tasks we have a mental model of how we’ll proceed - first I’ll write a query to pull this data, then I’ll dump it into Excel and run these calculations to figure out some values, then I’ll use these values to make a few updates in the database. These tasks are abstract with a lot of context locked up in our heads but I can see our computers getting smart enough to help us skip the majority of these steps. Given how often we do these trivial manipulations an intelligent OS can make us strikingly more productive.  Google has a huge advantage given both the massive data they have as well as controlling the entire ecosystem. This gives them the ability to know how apps fit together on a technical and behavioral level. Modern OSes separate themselves from the applications and run in isolation to the rest of the world so they don’t have Google’s key advantages. Despite this, I think it’s inevitable we’ll see these smart OSes develop - especially if we want to be as productive on our smartphones as we are on our desktops.",0,2,2015-09-05,3,"smart os, google, artificial intelligence",435,Smart OSes
30,0,There was no easy way to see the parking zones of Jersey City on a map so I decided to use the PDF they provided to generate a map.,"#code,#dataviz,#python,#javascript","{% include setup %} A big part of owning a car in Jersey City is dealing with the street parking. Unfortunately, Jersey City does not make it easy to see what the zones are - instead there's a  PDF  that lists the streets and address ranges that are part of each zone. After getting frustrated with this annoyance for too long I decided to just take matters into my own hands and visualize the zones through some scripting. This is a relatively simple project that still involved some false steps so I wanted to document the process and provide a peek into my development approach.  The first step was extracting the address ranges from the PDF into something more digestible by a program. I tried using a PDF converter but that ended up mashing up the addresses together so I took a step back and came up with a very simple script that took copy and pasted text from the PDF and cleaned it up into a list of addresses.  To do a quick proof of concept I started with a single zone for now and see whether I could get it visualized the way I wanted to using Google Maps. After converting the address range into a starting and end address I attempted to use the Google Maps API to do the geocoding (going from an address to a latitude/longitude). Unfortunately, due to the volume of addresses I wanted geocoded I quickly hit the rate limit cap. I introduced a throttle between calls but that ended up causing the page to take too long to load.  Even then, the geocoding wasn't 100% accurate and I needed to figure out how to visualize the resulting zones from a set of coordinates. The first visualization atttempt was to just connect the coordinates with a series of lines but as expected that led to just a jumbling of lines. As a quick fix I sorted the coordinates clockwise by figuring out the center of the coordinates, converting each coordinate as an angle from the center, and then sorting the resulting points by angle. This led to a ""starburst"" shape that was neater but still didn't represent the actual zone.  It's not done just yet and I'm working on two improvements - one is moving the actual geocoding work to an offline script so I don't have to deal with the rate limiting issue and two is using a convex hull algorithm to come up with a polygon that encapsulates each of the addresses in a zone that should improve the visualization. Feel free to follow along on  GitHub  and offer any feedback, suggestions, or even a pull request.  Writing good code on the first try is tough and part of the process is attempting an approach that may require backtracking. The challenge is realizing when something isn't working and being able to take a step back and revisit the actual goals and understand the constraints. Some projects do end up perfect on the first try but the vast majority require multiple iterations to get right. Experience helps us understand the constraints and tools we're working with but as the popular saying goes: ""Wisdom comes from experience. Experience comes from bad judgement.""",2,4,2015-09-12,3,"jersey city, parking zones, 07302",548,Mapping the Jersey City parking zones
38,0,It's surprising to see our users hack around our products to solve this problem. This is a great way to help us understand the limitations of our products and build new features to support new use cases.,#product,"{% include setup %} As engineers, it's easy to get focused on technical problems and lose sight of the business. We realize our code will be used externally but we have a tendency to focus on what's close to home rather than the actual real world usage. One of the biggest eye openers for me has been seeing people interact with our products.  We like to think of ourselves as ""hackers"" but it's amazing to see the length people go to ""hack"" our products to do what they want. Whether it's someone keeping multiple tabs open to be able to reference information back and forth and avoid losing data or someone registering multiple accounts to bypass a database uniquness constraint - it's a way for people to bypass the intended design and I'd argue that these ""hackers"" are a sign of a useful product. In fact, I'd argue that if people aren't hacking around a product's limitations it's not a good one. These workarounds are a sign that the product is so useful that people are willing to go through additional manual effort to use it for a different use case. If that's not a sign of new functionality to support I don't know what is.  And the best way to understand these workarounds is to talk to our customers and collect as much information as we can. Guessing people's intentions isn't helpful but being able to identify an anomalyous flow and then talking to that person is a great way to understand the intention and the workaround. This insight can then help drive product direction and innovation.",0,1,2015-09-13,2,"product management, product development",268,Workaround driven product development
27,0,Rather than offloading work to other teams engineering teams should strive to internalize the externalities by supporting business cases as soon as they become technically feasible.,#product,"{% include setup %} Recently I've adopted the practice of having the engineering team support other team when the core technology can support it - even if hasn't been fully built in to the product. This may require manually adding entries to a series of database tables that or manually pulling reports that aren't yet availabe via the UI. Despite being an inefficiency for the engineering team it provides a variety of benefits that outweigh this minor inconvenience.  - Engineers understand the business better. By being closer to the actual use cases engineers understand how the product is used and the problems that other teams are solving. This can have huge wins in the future when there are multiple implementation options available and the developer needs to pick one. Being able to pick the right one can significnatly change the cost of doing future development work.  - Work gets done faster. Engineers love their efficiency and as soon as they end up having to do the same thing twice they'll think of ways to automate it. This is the perfect way of shifting the problem to the person best suited to solve it. A support person may need to come up with a series of inefficient workarounds while an engineer will solve the underlying, core issue.  - Improved quality. Being closer to the actual use case will improve the quality of the code since the developer will know how their code will be used and will understand the options available. The other benefit is that it will become a lot easier to see usability issues as well as improvement opportunities that can be implemented in the future. Bugs will also get caught earlier since the developer will be more likely to monitor the behavior if they were the ones that worked on it. The fact that they used it will provide more motivation to monitor it when it's out in production.  - Prioritization makes sense. Oftentimes it's not obvious why one feature is prioritized over another. Being able to work through some business cases and experience user frustrations is a great way to get a look at the product from a different perspective.  - No workarounds that end up being ""grandfathered"" in. It's common to see users come up with workarounds to support a use case not officially supported. This places a big burden on the engineering team that may end up needing to support it since a particular workflow now depends on it. By exposing the problem to engineers earlier it's more likely that this situation is avoided.  Years ago I read how Kayak has their  customer support phone number get routed  to the engineering team some percentage of the time. The goal is to expose engineers to customer problems with the idea that an engineering can actually solve a problem once it bothers them enough times. I view this as ""internalizing externalities"" - rather than offloading engineering costs to other teams (product managers, QA, customer support) we should address the problems as soon as they arise. This ensures the quality stays high while aligning the engineering team with the rest of the company.",1,1,2015-09-15,4,"engineering, management, product management, code quality",531,Internalizing externalities
37,0,As rote work gets more and more automated it's important for people to keep learning and making connections between various fields and topics. This improves our creativity and lets us focus on what we're good at.,#meta,"{% include setup %} I'm a strong believer that one needs to keep learning and to not get content with the knowledge they have. This can come in the form of new experiences or challenges but should be seen as a learning opportunity. Throughout school we have a structure in place to help us learn but after we graduate we have to take the responsibility ourselves. Unfortunately, many people don't and even take pride that they haven't read a book since college.  As more and more of the rote work becomes automated it's important to develop the creative mind set that can take topics from a variety of fields and blend them together to create something new. The irony is that our education and professional systems are becoming more and more specialized. As our knowledge of an area becomes deeper it's harder to be a generalist and we end up focused on only a few areas and skills.  My solution is to read as much as a I can across a variety of fields: fiction and non-fiction, classics and modern fiction, even the occasional textbook. The goal isn't so much to recite everything from memory as it is to plant some remnant thoughts in the back of my mind that may end up being useful in the future. It's the difference between not knowing that there's an answer to a question and knowing that an answer exists. The latter gives you the ability and confidence to find the answer whether the former gives you an exuse to give up. The other benefit of this cross-disciplinary approach is that you get great at identifying patterns. These act as a shortcut and make it easy to absorb new information as well as spot patterns in the wild.  These days it's too easy to get our knowledge bite-sized through the various social networks, articles, and headlines but that information is fleeting. The way to really absorb information is through focus and being able to spend more than a few minutes on a particular topic. Otherwise it's just wasted time and effort.",0,1,2015-09-20,3,"learning, knowledge, automation",352,Keep on learning
13,0,Finally finished the first pass at visualizing the Jersey City parking zones.,"#code,#dataviz,#python,#javascript","{% include setup %}                                    I finally had the chance to finish up the Jersey City parking zone mapping project from a couple of weeks ago. The goal was to take a PDF of valid addresses for each zone and visualize it on a map. The result can be found at  http://dangoldin.com/jersey-city-open-data/  and includes the zones that had enough geocodeable addresses to generate a valid polygon.  As expected, most of the work was going from the PDF to a set of valid geocoded addresses. The biggest challenge was extracting the text from the PDF and transforming them into addresses that could be accurately geocoded. Once I had that it was simply modifying the Google Maps  polygon example  to generate a list of polygon and finding a library to overlay the zone labels.  The two things I want to change are to modify the visualization to also include a street level visualization rather than relying on a polygon since it’ll make the information bit more useful as well as incorporate the street cleaning hours. If anyone has that data I’d love to get it.  As usual, the code is up on  GitHub  and pull requests are welcome.",3,4,2015-09-24,4,"jersey city, parking zones, 07302, open data",243,Mapping the Jersey City parking zones II
36,0,The iPad Pro will have performance similar to the Macbook at a fraction of the price. I wonder if Apple is purposefully taking a margin hit to get more and more people over to iOS.,#meta,"{% include setup %} Supposedly the performance of the recently announced iPad Pro will rival that of the Macbook and encourage tons of people to buy it with productivity in mind rather than just consumption. And at a starting price of $799 it’s significantly cheaper than the Apple laptop options. Apple is known for achieving large product margins but I wonder if the iPad Pro is sold at a lower margin to get people to switch to iOS and get even more tied to Apple’s ecosystem. I currently use a Macbook Pro for my work but can switch to any Unix based environment without any hit to my productivity. I’m not sure if this is me being too cynical but I can definitely see Apple taking a long term view here and taking a much lower profit margin on the iPad Pro in order to get people to actually make the switch to the walled garden of iOS. Among the developer community Safari is already seen as the  reincarnation of IE  given Apple’s lackluster support. These may just be coincidences but for a company as detail oriented as Apple this feels like a strategic decision to shift away from the open web and into the walled app garden.  During the 90s Apple nearly failed due to the dominant Windows ecosystem and Windows’ ability to run on commodity hardware. At least for now, mobile isn’t at the commodity hardware stage and Apple has taken the lead in smartphone hardware. This position allows Apple to impose its ecosystem and software but I wonder what happens if smartphone hardware get commoditized - similar to what happened to PCs in the 80s and 90s.",1,1,2015-09-26,4,"apple, ipad pro, hardware, software",287,The iPad Pro and Apple's walled garden
44,0,There's this idea that media consumption is zero sum and as we consume more online we'll consume less on traditional channels. I'm not convinced this is the case and view it as more of an increase in the total size of the pie.,#meta,"{% include setup %} I’ve seen the point made across a variety of articles that media companies see media consumption as being zero sum - there’s only a fixed amount of consumption that people have and they’re allocating it between a variety of options. The typical example is people abandoning legacy television for YouTube. Thus, the rationale goes, the time people would have spent watching television has been replaced by them watching YouTube.  I understand the concern. People are watching less and less standard television with only time sensitive and restricted content (ie sports) being viewed on television but there’s a world of nuance. People may be watching less and less broadcast television but the content that’s being consumed is growing. Cordcutters pride themselves on not paying for cable television yet they’re still subscribing to Netflix and Hulu. The same shows are being watched - just not on the usual devices. In addition, there’s the rise of multi-screen consumption - we’ll be on our phones or laptops while watching TV. Whether it’s a distraction from a commercial, a way to follow an event on Twitter, or just browsing Wikipedia to get a bit more information, we’re parallelizing our media consumption and increasing the size of the media pie rather than solely switching between platforms.  I found the following graphic which highlights the shift in media consumption over the past century.                               The BrandBuilder Blog                Part has been the rise of new technology to fill in more and more of our days. Radio gave way to the rise of television which started taking up more and more of our time. The internet then came along and gave us another way to access and consume content that. The smartphone blew that out of the water and gave us the internet and media consumption wherever we are. Whereas in the past we’d be watching TV or using the internet at home or at the office with smartphones we can consume content wherever we are. The pie of “total available media consumption minutes” has been growing over time and will only keep increasing. What will people do when we’re being chauffeured around by our self-driving cars? We’ll be using our future devices to consume and entertain ourselves the exact same way we do now. That’s a whole new timeslot for us to consume content that we’ve never had the ability to before.  As more and more of our responsibilities get automated that time will become available. Some will fill it in with creative pursuits while others will view it as another timeslot for media consumption on an ever increasing number of screens and devices. It’s definitely depressing but I’m not optimistic given recent trends.",1,1,2015-10-04,3,"media consumption, television, internet",491,Is media consumption zero sum?
27,0,We like to think in terms of black and white of large companies being evil and small companies being good but it's more complicated than that.,#meta,"{% include setup %} It's become a popular idea that big companies are evil and we should only be supporting small and local businesses. There’s some truth to it - smaller companies are much more aligned with the incentives of the community whereas larger companies may be managed from thousands of miles away via a spreadsheet. When the only goal is to make more money it’s very likely that morality and honesty will suffer.  At the same time, we should differentiate evil companies from evil actions. Even Walmart has done some good. A story that comes to mind is the reduction in the size of  detergent bottles . There was an arms race by detergent manufacturers that were diluting their detergent in order to sell large bottles for the same price. Sure, it cost more to produce the larger bottles, but then they'd be able to generate significantly more sales when people saw that you were offering “twice” the volume at the same price as your competitors. Sure enough competitors followed suit and we ended up with a  Prisoner’s Dilemma  spiral. It took Walmart, along with a few other large retailers, to reverse the trend and get the manufacturers to start producing bottles in smaller, more concentrated sizes.  And on the other extreme there’s Starbucks. Despite not being a trendy new coffee shop they have the size and resources to launch massive initiatives. One of the recent ones is a way for current employees to get a  full cost covered college degree  from Arizona State University. Despite this being isolated to a single college and only classes no local coffee shop would be able to make this happen. And the cynic may see this as a marketing ploy but if it’s still able to help more people get a degree I’m all for it.  In both of these cases the benefits came from the size of the company. Larger companies have both the resources and the clout to push change that may not have been possible by smaller company. Instead of dismissing them as permanently broken we should be focused on getting them to use their clout and money for societal gain.",3,1,2015-10-10,5,"corporate good, corporate evil, walmart, starbucks, society",374,Not every big company is evil
40,0,There are tons of tools to learn a new programming language from simple exercices to walkthroughs to large open source projects but the best way is to have your own project that you've implemented across a variety of languages.,#meta,"{% include setup %} At the beginning of the year I wanted to learn a bit of Node.js and decided the best way was to code up a simple project. The idea was  jsonify.me , a simple API only app that provided people a simple way to generate their own JSON profiles that they would then be able to map to any domain name, for example  http://json.dangoldin.com . The primary goal was to get some real experience with Node.js rather than rely on some walkthroughs and tutorials. Since then I’ve used it as the starter project to learn new languages. I’ve coded it up in Scala and have just finished up the Go version.  The project has a few nice properties that force me to gain a pretty good understanding of the language and how a typical project plays out. Despite being a pretty simple program it touches a bunch of modern web components. The code needs to be able to parse and modify HTTP requests and headers in order to support redirection and authentication. In addition, the code comes with a working LinkedIn OAuth example and gives an opportunity to incorporate an OAuth library. The other big thing is integrating the AWS S3 client library which provides a simple way to get exposure to the AWS ecosystem.  Everyone who’s learning new languages should have a “go to” project. It’s okay to go through a series of tutorials to get the basics of a language but nothing beats working on a project you’ve already done across a variety of other languages. In addition to coding up a project in a new language you get a feel for the way the program is structured and laid out. Over time you start getting an intuitive feel for how one language works compared to another and can understand the tradeoffs between them. Having your own project also allows you to optimize towards the skills you want to learn - in my case I wanted them to be focused on the web and allow me to work with the various HTTP elements as well as a few third party libraries. Tutorials and walkthroughs are great to get a feel for the language but they don’t force you to think through the design or architecture which are critical when working on larger projects. It’s amazing how much effort that takes up when learning a new language and the only way to learn it is to experience the frustration of doing it.",2,1,2015-10-11,3,"learning to code, new programming languages, software design",426,Have a 'go to' project when learning a new programming language
19,0,The way publishers should fight adblock is by focusing on high quality content and building an engaged audience.,#meta,"{% include setup %} Ever since the release of iOS 9 and it’s support for adblocking apps I can’t go a day without seeing some article about adblock. Some condemn it and claim it’s stealing from publishers while others make the case that ads are so intrusive that they deserve to be blocked. I don’t want to dwell into either of these but something that’s been on my mind is that publishers aren’t doing enough to differentiate themselves based on the quality of their audience.  Top tier publishers that have unique and high quality content should focus on building a community. This passionate group of users will engage with the content on the site as well as provide valuable information to the publisher. By signing up for an account users provide valuable demographic information as well as interests based on what they see and what they do. This is also something that adblock won’t be able to easily block since it will be such a core part of the experience and hosted on the publisher’s own domain.  It’s true that publishers with low quality or commodity content will suffer as users move on to something with a better experience but this will arguably make the web a better place. The fact that some people even run adblock implies they have no respect for the publisher’s effort - and pursuing a lowbrow approach just turns that into a death spiral.  Disclosure: I work at TripleLift which provides a much better advertising experience for users, publishers, and advertisers.",0,1,2015-10-15,4,"adblock, publishers, quality, content",256,"Adblock, publishers, and content quality"
31,0,When writing code it's important to think about how it will be deployed. That leads to higher quality code and improves the rate at which teams can deploy new code.,"#management,#devops","{% include setup %} The goal of every bit of code should be to make it to production. Code that’s not deployed is wasted effort as well as a loss to the business. And a big part of making sure code is deployed is thinking through the deployment plan as we write the code. Some code is deployed simply by pushing the new application while other code may require updating the database schema. More complex code may depend on other applications which will need to be tweaked and deployed beforehand. Large companies and teams have dedicated ops teams that handle deployments but small teams need to do this on their own.  Thinking through the deployment also leads to better code. By going through the steps of how the deploy will work you end up breaking your code down into a series of changes that end up being significantly safer and reduce the risk of a large failure. For example, we may want to write an update that will add an additional feature to our application based on a flag in a database. A safe way of doing it is to create a new column first that will have no effect on the existing application and then roll out our the new code that starts using this column. Future releases can then remove the code that uses legacy columns with latter releases dropping those columns entirely. None of this is shocking news but it’s surprising how rarely we think about deployments when we set out to write code. Especially as a team grows it’s important for everyone to be thinking about the way their code will work and the way it needs to be deployed.",0,2,2015-10-18,3,"deployment, coding, dev ops",285,Writing code? Think about the deployment
22,0,I recently discovered how powerful the date shell utility is. Definitely an improvement over a Python script for simple date logic.,#code,"{% include setup %} The longer I code the more I appreciate the power of the shell. Getting familiar with common commands is a great way to improve your productivity and over time you amass a massive collection of scripts that allow you to do nearly everything. The most recent utility I discovered was “date”. As expected, it displays the current date and time but it can easily be adapted to display the current datetime in nearly any date format but also allows you to offset the current date in a variety of ways.  {% highlight sh %} ➜  ~  date Mon Oct 19 22:35:37 EDT 2015 ➜  ~  date +%Y-%m-%d 2015-10-19 ➜  ~  date +""'%Y-%m-%d'"" '2015-10-19' ➜  ~  date -v+3d +%Y-%m-%d 2015-10-22 ➜  ~  date -v-3d +%Y-%m-%d 2015-10-16 ➜  ~  date -v-3y +%Y-%m-%d 2012-10-19 ➜  ~  date -v+3y +%Y-%m-%d 2018-10-19 ➜  ~  date -v+3y +""%Y-%m-%dT%H:%M:%S"" 2018-10-19T22:39:18 ➜  ~  date -v+3m +""%Y-%m-%dT%H:%M:%S"" 2016-01-19T22:39:24 {% endhighlight sh %}  In the past I’d resort to a JavaScript utility or a quick Python script when I needed a simple date calculation but lately I’ve been able to do nearly everything solely by using the built in date utility. It’s still a bit cumbersome for generating date ranges or when requiring complicated logic but for the basic stuff it’s surprisingly powerful and expressive. It’s amazing how full featured the shell is and how often we avoid it and use more fleshed out languages. Instead of trying to find new languages it’s worth taking the time to actually explore and understand the shell - it’s one of the better investments an engineer can make.",0,1,2015-10-19,3,"shell scripting, date, shell date",303,Dates in the shell
14,0,I rewrote jsonify in go and would love to see what people think.,#code,"{% include setup %} A couple of weeks ago I wrote about the idea of having a  “go to” project  that you use to pick up a new language and earlier this week I finished the bulk of the rewrite of  jsonify.me . It went through a Node.js phase, a Scala phase, and is currently in the go phase. The idea is to give people an open ended and simple way to generate a personal JSON object, similar to how people may have an about.me page but in JSON. This object can then be mapped to any subdomain (mine is at  json.dangoldin.com ) and be referenced by any third party code. For example, you can construct your personal jsonify.me object based on the information in your various social media profiles and then make that information accessible to a variety of sites or pages that can generate it in a variety of ways. One site can turn it into a simple resume while another one can turn into a visual timeline of your history. At the moment it’s entirely open ended with the vast majority of the functionality provided solely through an API. Over time I’ll add some more bells and whistles but I’d love to see the community come up with their own unique JSON format that can then get adopted - similar to the way the hashtag system on Twitter evolved. I suspect it’s going to be significantly more complicated since there’s no 140 character limit but am still interested to see where this goes. Play around with it and let me know what you think!",3,1,2015-10-21,3,"json, personal json, quantified self",288,Jsonify.me 2.0
35,0,Just because your code works on a development environment doesn't mean it will work on production. The scale and infrastructure may be completely different and you nede to keep it mind when writing software.,"#devops,#management","{% include setup %} The biggest development lesson I learned over the years is that production is a completely different beast from development. Code that works perfectly in a development environment can fail catastrophically in production and cause a severe impact on the business. Issues can stem from bits of inefficient codes to database schemas that just don't scale on production. Ideally your development environment  mirrors production and has the same load and hardware but that's rarely the case. For the other cases cases I’d go through the following items to make sure your code is ready for production:  - General code efficiency: Your code may pass unit tests and work fine when you’re running it on development data but you should make sure the code itself can scale to production data. Inefficient code may be fine to push to production if it’s not being hit often or you have the hardware to back it up but you need to make sure this is the case. This also extends to UI applications: if your development environment has a few rows for a customer while in production a customer will have hundreds, you need to make sure that the UI is responsive and that the design actually fits the production use case. - Query performance: This is the most frequent problem I’ve seen when new code is deployed. A query may run fine in development which can have a magnitude less data but as soon as it’s pushed to production queries that used to take milliseconds while developing start taking multiple seconds. The simplest way to deal with this is to just run your queries on production and confirm they work - especially on the datasets and filters you suspect will be problematic. The results may lead to solutions such as adding new indices to a table or generating new summary tables to speed up the code, neither of which would have been easy to discover during development. - Deployment plan: Part of writing code is thinking through the deployment and a big part of deployment is making sure you’re avoiding down time. In addition to making sure your application rolls over gracefully to the new code you should be thinking about the database migrations you’ll need to make and confirming they will run as expected on production. I’ve encountered cases where adding a column took a few seconds on a development database but multiple hours on production. If that’s the case you should rewrite the migration to avoid downtime - for example creating a new table and population it with legacy data and only then renaming it to the original name. - Rollback plan: As much as we like to think our code is perfect mistakes happen and we should write code that’s easy to rollback. Ideally it’s as simple as just pushing the older code but it may need a bit more work if it depends on database changes or other applications.  None of these should be earth shattering and over time they become a habit but until then it’s important to go through each one to ensure a successful production deployment.",0,2,2015-10-25,3,"software, development, coding",518,Production makes fools of us all
23,0,I took a walk down memory lane and thought about the old projects I've worked and what I've learned along the way.,#meta,{% include setup %} This past weekend I was going through some old projects and got a bit nostalgic. Some were my first foray into web programming and startups while others were just me messing around and trying to learn a new framework or language. Each of them have taught me valuable lesson and I thought it would be fun to go through each one and jot down a quick background as well as the lessons learned. I’m doing a high level pass so if any of these are interesting definitely let me know and I’ll do a deeper dive.  - scenepeek.com: I started this with a close friend back during my finance days when I really didn’t know what I was doing. The goal was scrape the web and identify various events that could then be easily surfaced and discovered. This was right before smartphones became popular so it does make one think of what could have been. This was my first real time doing “devops” and working with various instances and configuring Apache. The other big lesson learned was that we probably should have started with some framework to get our project out sooner. Instead we ended up writing raw PHP and building everything from the ground up. -  getpressi.com : Applying the lessons learned from Scenepeek I left a a full time job at Yodle to cofound a startup (initially called Glossi) that would create social media mashup pages. We were accepted into an accelerator and ended up making significant progress but were never able to figure out whether our core customers were consumers or larger companies. We couldn’t commit as a team and ended up floundering until selling to a small advertising agency. At the peak we had a dozen customers and most likely could have turned it into a lifestyle business had we had the maturity and focus. -  makersalley.com : After breaking up with Pressi went in the opposite direction and built decided to build something with a concrete business model rather than waiting for one to fall into our laps. We both liked Etsy and wanted to do something with a community element as well as having to do with physical goods. This was a two sided market play and we were never able to get people to buy expensive furniture online. We got so enamored with our vision of how awesome the furniture and designers were that we focused on getting them rather than on getting customers. -  better404.com : This was a small side project I started to help websites improve their 404 pages. I wanted something that was more passive than building a marketplace and catered to my strengths which were more on the tech side. I don’t have too much time dedicated to this but every once in a while I’ll make some updates with the idea of making it a small passive income generating product. -  jsonify.me : Scratching an itch here but I love the idea of every person having a JSON page that’s a representation of what they are and what they care about. It’s also my “go to” project when learning a new language. It’s a proof of concept more than anything else right now but I’d love to see where it goes. I’m passionate about people owning their data and lending it to third parties as needed and view this is a way to achieve it using existing methods.  On one hand I want to polish some of them off and see what I can do but on the other I’m curious about trying new things. People glorify this idea of a purely passive income but I suspect nothing is that easy and every project will require some ongoing maintenance and improvement to stay relevant.,4,1,2015-10-29,4,"nostalgia, old websites, startups, technology",640,Some nostalgia
38,0,My old projects are impossible to get running. Not just because the code is worse but because I never thought to document the deployment process or any of the requirements. This is an valuable habit to develop.,"#devops,#meta","{% include setup %} A clear pattern emerged as I was digging through my old projects. Other than the code quality and approach improving over time what stood out was the way I approached deployment. My earliest projects didn’t have a set of requirements and the configuration was all over the place. The more recent projects have a clear set of requirements as well as the command lines needed to get them running. In fact, I’m able to build and run my recent projects within a few minutes by running “pip install -r requirements.txt”, updating the configuration file, creating the database, and running the database migration script. This is a massive improvement when compared to my initial projects where there was no documentation and my setup involved a ton of adhoc, undocumented work directly on the production server that’s now lost.  I’d argue that this is one of the better habits to adopt as a developer. We do a surprising amount of duplicate work over the years and being able to reference a prior solution is immensely useful, especially when it’s easily discoverable. It’s also a great way of identifying patterns and similarities between projects and understand why some approaches worked and why some failed. This retrospective approach is an active way of improving rather than relying on the “osmosis” approach of just waiting for information to get absorbed.",0,2,2015-11-01,4,"code, code styling, devops, deployment",231,Good code is easy to build and deploy
44,0,There's a ton of discussion as to how Airbnb should be regulated with many people for it and many people against. I propose a new zoning category and to rely on the market to come up with an elegant solution to the problem.,#meta,"{% include setup %} I’ve been meaning to share some thoughts on regulating Airbnb for a couple of months now but kept putting it off. The  recent news  was motivating enough for me to finish it off.  I’m a huge fan of Airbnb and it’s my first step whenever I’m traveling. Nearly all my experiences have been great and I’m contemplating getting rid of my Starwood card since it’s just not as useful anymore given that I gravitate towards Airbnb first. At the same time I understand the impact renting a place Airbnb has on the neighbors and can imagine myself hating it if my neighbors were listing their places.  The challenge is that the host is benefiting while passing the cost to someone else. The host is able to get above market rent while the neighbors have to deal with the potential noise and the risk, albeit a low one, of a stranger. The obvious solution seems to come up with a new zone category between residential and hotel commercial that Airbnb as well as other home rental places would be able to fit in. These locations can then be rented out with the city’s blessing as well as contribute to the tax revenue of the city. This relies on the market to come up with a fair price for the location. If you’re interested in renting a place on Airbnb you should be willing to pay more for the property and if you decide you’ll never be renting your place on Airbnb you should have neighbors that share the same belief.  I don’t have much knowledge on zoning laws and what goes into it but this feels like a solution that should work once in place. Getting there is the hard part - what happens to buildings where half the tenants want their units to be Airbnb eligible and half don’t? Who ends up having the final say? Maybe the solution would be to keep existing places the way they are and make sure new construction goes through this zoning process. This will ensure that over time more and more buildings have a clear definition of rental eligibility.",1,1,2015-11-05,4,"airbnb, sharing economy, zoning, real estate",365,Zoning Airbnb
32,0,A couple hundred years ago Napoleon or Frederick the great stated that an army marches on its stomach. The modern day equivalent would be that a company marches on it's data.,#meta,"{% include setup %} A couple of hundred years ago nearly every European country was engaged in some sort of military conflict which led either Napoleon or Frederick the Great to state that “ an army marches on its stomach .” The point being that logistics are the most important when it comes to having a successful army. These days the corporate equivalent would be that a company marches on its data.  Every company claims to be data driven and there’s a slew of data collected about us each day. The most successful companies are able to leverage this data and use it to derive insights that drive direction. Unsuccessful companies may collect the same data but don't leverage in an impactful way. It’s easy to collect information but it’s a huge challenge to turn into action. There are many options just for storing the data: one approach may make it easy to store tons of data while making it hard to run large scale analyses while another allows for a distributed computation approach that's too slow. Beyond data storage there’s the actual analysis piece: what’s the appropriate model to use that can represent the relationships between the variables while being true to life? All these are questions that will become increasingly critical and separate the winners from the losers. Data itself has potential for massive  monopoly feedback loops  - companies that succeed are able to collect more and more which improves their product which collects more data. Right now it may only seem as if larger companies should care but I suspect within the next 10 years we’ll see more and more small and local businesses adopt a truly data-driven approach, whether through internal tools or through external services",2,1,2015-11-08,3,"data, strategy, business",300,A company marches on its data
36,0,An interesting behavior I've adopted is to use apps when I'm on LTE and mobile web when on wifi. My gut tells me apps are faster and use less data but are also less flexible.,#meta,"{% include setup %} Lately, I’ve noticed an interesting trend with my smartphone usage. When I’m on wifi I’m much more likely to use the mobile web, click links, and read various articles whereas if I’m on LTE I’ll stick to dedicated apps. I noticed this at my apartment which has a narrow layout with my living room having wifi and my bedroom stuck on LTE. Despite me being in the same mindset regardless of which room I’m in my behavior changes dramatically.  I don’t recall making a conscious decision to change my behavior so I suspect this behavior evolved to deal with the increase in my data usage. I’m still not sure that apps are more data efficient than the mobile web but it definitely feels that way due to the improved speed and responsiveness. If you’re concerned about data usage it’s obvious that you’ll want to do as much as you can on wifi rather than go through your data plan but what’s interesting is that in my mind I’ve concluded that apps and mobile web are differentiated by my data plan. I’d love to know if others do something similar or I’m the anomaly.",0,1,2015-11-09,5,"apps, mobile web, data usage, lte, wifi",198,"Apps on LTE, mobile web on wifi"
32,0,I went through each repo on my GitHub account which was a nice walk down memory lane and wanted to go over each of the projects briefly talk about each one.,#code,"{% include setup %} Writing up my old projects got me browsing through my GitHub account to see what else I've worked on. Some I'll update when I get a good idea while others I completely forgot until going through the list. I noticed two big themes when going through the list. The first is how much nicer it is to have projects that are in static HTML/CSS/JavaScript since they can be hosted publicly on GitHub and don't require any setup or configuration to start using. The other is how many third party libraries or APIs I've used and how much more difficult everything would have been had I had to build everything from scratch. If anyone is interested in forking and ressurecting some of these I'll be glad to polish it up.  - [Twitter archive analysis](https://github.com/dangoldin/twitter-archive-analysis): At some point Twitter announced that they would allow you to export your entire Tweet history and this was a quick pass at a toolkit to analyze the data and do a few simple visualizations. I wrote a blog post about it [here](/2013/01/19/making-sense-of-my-twitter-archive/).  - [Instagram download](https://github.com/dangoldin/instagram-download): A couple of years ago Instagram changed their policies so I decided to close my account. Before that I needed a way to export my photos. This was a simple app/script that spawns a very basic OAuth web application in order to authenticate you with the Instagram API which allows you to export all your photos.  - [Yahoo fantasy football download](https://github.com/dangoldin/yahoo-ffl): I've been in a fantasy football league for almost a decade now and every year I update this script to scrape the data from the Yahoo fantasy football site. The goal was to use this data to develop a statistical model to help me manage my team but I haven't gotten around to starting that yet. Maybe next year!  - [Runkeeper stats](https://github.com/dangoldin/runkeeper-stats): A pretty simple R script to analyze and map the data that can be exported from RunKeeper. I wrote a blog post about it [here](/2014/01/04/visualizing-runkeeper-data-in-r/).  - [Site analysis](https://github.com/dangoldin/site-analysis): I was frustrated by the slowness of various sites and decided to write a script to see what was taking sites so long to load. This analyzes the top Alexa sites and figures out how much data they're loading and of what types - CSS, JavaScript, images, etc. I wrote a blog post about it [here](/2014/03/09/examining-the-requests-made-by-the-top-100-sites/).  - [Relay Rides analysis](https://github.com/dangoldin/relay-rides-analysis): This script analyzes the JSON search results of Relay Rides (now Turo) and combines it with data retrieved using the Edmunds API to identify the cars that have the best financial return. The return is calculated by looking at the estimated price of the car and dividing it by average money earned per day. The obligatory blog post is [here](/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/).  - [Jersey City parking zone mapper](https://github.com/dangoldin/jersey-city-open-data): Jersey City has a ridiculous PDF that lists the streets and addresses that belong to each zone. I painstakingly extracted, cleaned, and geomapped the data in order to visualize the zones on a map.  - [JS tools](https://github.com/dangoldin/js-tools): Probably my most commonly used code. This is a series of tools hosted on ... that provide some basic utilities that help me throughout the day. The most useful lately has been a way of comparing SQL table schemas but it has a bunch of others.  - [Citibike station directions](https://github.com/dangoldin/citibike-station-directions): A web app that breaks every trip down into a walk to a Citibike station, biking from Citibike station to Citibike station, and another walk to the final destination.  - [Meerkat crawl](https://github.com/dangoldin/meerkat-crawl): To help a buddy out I started mapping out the network relationships between users on Meerkat but quickly ran into a scaling issue. I got to around 5 million connections and wasn't able to figure out how to actually visaulize it in a clean and timely way.  - [Yet another Hacker News reader](https://github.com/dangoldin/yahnr): My attempt at modifying the Hacker News experience to show the top stories over a rolling 24 hour period. This was a good exercise in messing around with pseudo static sites where the content is solely hosted on S3 with a script to push new files every few minutes.  - [Python tools](https://github.com/dangoldin/python-tools): A series of Python scripts that I've writtent to deal with various minor issues. I have a ton more that I'll add to this repo when I find them.  - [MySQL class](https://github.com/dangoldin/mysql-class): I taught a MySQL class at [Coalition for Queens](http://www.c4q.nyc/) and this is the series of slides used.  - [Redirector](https://github.com/dangoldin/redirector): A tiny Node.js app that acts similar to the ""Switcheroo"" Chrome browser extension but able to work across other browsers. It requires a bit of manual set up but then uses the hosts file to intercept web requests and redirect them to another host. A quick write up [here](/2015/02/07/url-redirection-app/).  - [Oyster books crawl](https://github.com/dangoldin/oyster-books-crawl): This was a series of scripts that crawled the Oyster API to pull the available books and then analyzed them to find patterns. A bit sad that this script outlived Oyster itself. I wrote a blog post about it [here](/2014/03/16/fun-with-the-oyster-books-api/).  - [Taxi pricing](https://github.com/dangoldin/taxi-pricing): The goal here was to compare the pricing of taxis across various cities. The two primary dimenisons used were cost per a minute waiting and cost per a mile of driving. Using this information one can then see how different cities and countries value labor costs. The analysis is written up [here](/2013/12/29/taxi-pricing-in-nyc-vs-mumbai/) and [here](/2014/01/09/taxi-prices-around-the-world/).  - [Meeting place finder](https://github.com/dangoldin/meeting-place-finder): A simple script that uses the Google Maps API to come up with an ideal meeting place for a group of people that ensures everyone has the same commute time.  - [Lincoln text analysis](https://github.com/dangoldin/lincoln-text-analysis): An old project that read in the text of Abraham Lincoln's speeches and did a few visualizations of the text. I wrote a blog post about it [here](/2013/02/12/analysis-of-lincolns-words/).  - [Lawdiff](https://github.com/dangoldin/lawdiff): I participated in a journalism meets tech hackathon and this was my team's entry. We looked at proposed state laws and compared them against other states to identify laws that were most likely written by a special interest group. We had a number of false positives but were able to find a bunch of laws that were nearly identical despite being introduced in multiple states.  - [IMDB](https://github.com/dangoldin/imdb): Another early analysis project where I scraped some IMDB data in order to analyze the average age of actors and actresses over time. This came after I watched Misrepresentation and wanted to show that actors and actresses are treated differently in the movie industry. I wrote a blog post about it [here](/2012/05/23/trend-of-actor-vs-actress-age-differences/).  - [Jeopardy parser](https://github.com/dangoldin/jeopardy-parser): I found an open source crawler of Jeopardy clues and made a few updates to make the code multi threaded and able to crawl significantly faster. I then worked with my wife to turn this data into a simple web app that displayed random Jeopardy clues for us to test our knowledge.  - [Map fun](https://github.com/dangoldin/map-fun): Similar to the RunKeeper analysis above this was another pass at summarizing my running data over multiple years but this time leveraging GitHub's map tools. I wrote a blog post about it [here](/2015/01/18/fun-with-githubs-map-tools/).  - [Node toys](https://github.com/dangoldin/node-toys): This was the start of me messing around with Node.js and getting a feel for the framework. One of the fun projects I used it for was evaluating recursive functions using HTTP redirects. I did a quick write up of it [here](/2014/12/31/redirect-recursion/).  - [AWS tools](https://github.com/dangoldin/aws-tools): A super simple script that downloads a list of EC2 instances and then prints the IP, name, and address. The end goal was to make it simple to connect to an instance without going through a manual process of figuring out the appropriate address to use. I ended up not using this that much since it was easier for me to maintain a list of aliases and hosts in a text file. A very basic write up [here](/2014/11/09/some-simple-aws-tools/).  - [Wikilearn](https://github.com/dangoldin/wikilearn): This was one of my favorite projects. The goal was to analyze a Wikipedia article and come up with a visual timeline of all the dates and events that occured. I used an open source library for the visualization piece but ended up running into all sorts of issues analyzing the Wikipedia text. This is where I got a bunch of exposure to NLP but still wasn't able to make it work.  - [Mixergy mp3 download](https://github.com/dangoldin/mixergy_mp3_download): I subscribe to the Mixergy feed and this was my attempt at a script that would just download the available mp3 files and store them for future listening. I'm sure the HTML code of the page has changed since then so the code is most likely broken.  - [Geo data](https://github.com/dangoldin/geo_data): A one of script I wrote to crawl a site and generate a mapping of ZIP codes to counties. I'm not sure why I needed this but I suspect it was for some sort of data analysis project.",41,1,2015-11-12,5,"code, projects, analysis, open source, github",1543,My old projects
29,0,I've recently started using a calendar to track and plan every adhoc task I need to do. This has been a huge help in keeping my productivity high.,#management,{% include setup %} A recent trick I’ve picked up to manage my time a bit better is to take all the adhoc tasks I have to do and scatter them into my calendar for the next few days. This allows me to actually get to working on the tasks and I can make sure none of them are forgotten. Using a calendar also forces me to think about the time I expect these tasks to take and plan around that. I’m nearly always running behind and am constantly shuffling tasks around but it’s much better than my previous system of a text file with a constantly growing list of todos. A side benefit of this approach is that I can split my day into [maker versus manager](http://www.paulgraham.com/makersschedule.html) chunks rather than be at the whim of meeting invites.,1,1,2015-11-19,4,"management, time management, planning, prioritization",142,Adhoc task management
37,0,Doing a group by on a derived column can be dangerous if it has the same name as one of the originals. It's much better to be explicit with the columns and group by you're using.,"#sql,#code","{% include setup %} I had a bit of fun with MySQL earlier this week when trying to explain a non obvious “group by” behavior. It’s fairly common to want to manipulate a field in order to transform it into something more useful. The difficulty arises when you want to keep the original name. Below is some SQL code that highlights the odd behavior.  {% highlight sql %} drop table if exists dan_test;  create table dan_test (   id int not null,   id2 int not null );  insert into dan_test (id, id2) values (1,1), (2,2), (3,3);  select * from dan_test;  select id, case when id = 1 then 2 else id end as id, id2 from dan_test;  select id, sum(id2) from dan_test group by id;  select case when id = 1 then 2 else id end as id, sum(id2) from dan_test group by id;  select case when id = 1 then 2 else id end as new_id, sum(id2) from dan_test group by new_id; {% endhighlight sql %}  With the second to last query it’s not obvious which id field the group by is referring to: the original from the table or the derived field? It turns out it’s the original field which can cause problems if you’re unaware of this subtlety. There are a few different ways to deal with this situation, including grouping by the derivation formula, but my favorite is to use a brand new field as in the last example above.",0,2,2015-11-21,2,"mysql, query issues",244,More MySQL fun
22,0,Despite being extremely similar on a technical level - streaming a digital product - Netflix and Spotify have completely different profiles and performance.,#product,{% include setup %} The news that Adele was not going to put her new album on the streaming services got me thinking about the differences between the way music and video are consumed. Just last week Rdio announced that it’s selling its assets to Pandora which is a reminder of how hard it is to start a music company - music labels wield all the control and are able to dictate the terms they want. Even Spotify is not yet profitable despite having millions of subscribers.  On the flip side we have Netflix which on the surface provides a very similar product - streaming video rather than streaming audio. Yet they’re profitable and are quickly expanding internationally and even developing critically acclaimed shows such as Narcos and Master of None.  I find it fascinating that although the two industries are so similar on a technical level they’ve played out so differently. Part of it is that audio consumption is just drastically different than video. Most people will stream music throughout the entire day at work and not mind repeats of a favorite song while shows and movies are watched in shorter bursts and I like to think that most people want to avoid repeats. I’m not a huge music listener and the music I enjoy tends to be available on every service but I suspect most people who are passionate about music want access to a band’s entire catalog as well as having immediate access to new releases - this is something that Spotify needs to provide that Netflix doesn’t have to worry much about. Netflix can survive on the back catalog alone while Spotify needs to bend over backwards to make sure they have the most recent releases.  Netflix has moved into producing their own shows which is allowing them to get ahead of the back catalog problem and move into the HBO model while still having access to a slew of old shows and movies.These are divorced from their creators and can stand on their own while music has extremely strong ties to the artist. This makes it extremely difficult for Spotify to apply a Netflix model and start producing albums - the only way they’d be able to make it work is by becoming a music label. Netflix on the other hand can pay top directors and actors to develop a show that can succeed or fail - but in either case it’s only loosely coupled with the creators.  I can’t find the blog post now but I read something a few days ago about how hard it is to build a successful music startup. The root cause is that the music labels have so much control and power that they’re charging a license fee that prevents startups from having any money to spend on innovation or product. Instead they’re transferring money from venture capitalists into the hands of the labels. The labels are basically the rentiers of the music industry and prevent innovation by sucking up investment that can be used to launch new products. My gut is that this won’t last since there’s just too much happening in adjacent industries but I’m crossing my fingers.,0,1,2015-11-22,3,"netflix, spotify, streaming",528,Why are Netflix and Spotify so different?
28,0,I started a project with the hope of analyzing a bunch of Meerkat data but ran into issues dealing with the size and scope of the network.,#data,"{% include setup %} While going through some old repos I came across an old [project](https://github.com/dangoldin/meerkat-crawl) I started to analyze the Meerkat network. The idea was to crawl the network and come up with a list of users as well as who they were following and who they were followed by in order to then analyze the network. The crawling was pretty easy to do and after running it over a weekend without any parallelization or threading I was able to get around 200,000 user profiles with a little over 4 million network connections. The challenge became actually analyzing this data to derive something useful. I tried a few tools - including [Gephi](http://gephi.github.io/), [Cytoscape](http://www.cytoscape.org/), and [NetworkX](https://networkx.github.io/) - but was unable to get anything more useful than a few simple summary stats. I was hoping to get a neat visualization of clusters to see the various cliques on the network but visualizing that data either broke the programs or took too long to even complete. I made the most progress when using a simple script to filter out the “tail” of the data which allowed the remaining data to be visualized but I felt that the filtration may have eliminated a bunch of interesting information. If anyone has some experience dealing with the analysis of large networks I’d love to hear some ideas.",4,1,2015-11-26,3,"big data, network visualization, data analysis",229,Analyzing large networks
33,0,It's easy to get bogged down and focus on implementation but a nicer approach is to think about the interfaces. This leads to a  hgiher quality codebase that's easier sustain and adapt.,#code,{% include setup %} An idea I’ve been preaching over the past few days is to start thinking in terms of interfaces when thinking about writing code rather than the actual implementation. It’s a higher level of abstraction that leads to a higher quality and more scalable product. Rather than focusing on the details it’s better to think about the components and how they’ll interact with another - this also makes it easy to put in a crappy implementation for now while making it easy to modify and rewrite in the future. As engineers there’s a strong desire to obsess over the perfect code which can lead to a significant amount of refactors and rewrites without translating into actual business value. Thinking in terms of interfaces and components forces you to get the design and architecture right and leaving the implementation details for later. A side benefit for me has been being able to take pride in the design and flow and not worry about the code itself - allowing me to write code at a much faster place and sprinkle a series of todos for the parts of the code that I know need improving.,0,1,2015-12-02,4,"coding, software engineering, development, engineering management",195,"Think interfaces, not implementation"
34,0,It's amazing how far we've come. I remember allocating a better part of a day to setting up a new computer but now it can be easily done in less than an hour.,#meta,{% include setup %} In the past I’d be wary of setting up a new computer knowing that every time I’d need at least a couple of hours to get everything into a workable state. These days I actually look forward to setting up a new computer. Nearly every file I care about is hosted online and a large chunk of my productivity apps are online as well. The only tools I need to run locally are the various IDEs as well as a variety of open source tools and libraries that my code depends on. Even then I’d bet it takes less than an hour to get things to an 80% state at which point I’ll only discover what’s missing by just going through my day.  And this is as a developer who needs to build various applications from source and deal with potential library conflicts. For someone who doesn’t have to deal with these issues it must be incredibly quick to get a workable setup these days.  At the same time it seems as if we need to upgrade our computers less frequently since for most tasks computers from a few years ago are good enough. In fact I’m still on an early 2011 MacBook Pro with some upgraded RAM and a new SSD drive. I don’t even notice any performance difference between it and a newer MacBook Pro at the office. And for most tasks why even bother upgrading a computer when you can get nearly everything done on a tablet? Just attach a keyboard and you’re good to go. I can’t imagine us ever going back to the pre cloud days of computers and I can only imagine what kind of digital productivity tools we come up - especially with the rise of VR and the constant improvement in the performance and battery life of our existing digital devices.,0,1,2015-12-06,3,"computers, cloud, technology",313,Setting up a new computer - then and now
26,0,It's an odd situation when a museum has a suggested donation amount as an entry fee yet you still get in via a corporate sponsorship.,#meta,"{% include setup %} Yesterday I attended a concert at the Newark Museum and ran into a fairly common situation when lining up to get in. They had a suggested donation amount, which is entirely optional, while at the same time they provided free admission to anyone with a Bank of America card due to Bank of America’s sponsorship. I’ve seen the same sort of setup at museums in New York and I suspect it’s common elsewhere in the United States as well as abroad but the entire concept strikes me as odd.  I understand that it’s a way for Bank of America to reward its customers but because the admission was a suggested amount it made me feel as as I’m neither contributing towards the museum nor as getting any value from Bank of America. Without my debit card I would have felt noble contributing when I didn’t have to but with the card it feels as if Bank of America is giving me a way to avoid feeling guilty.  I’m sure these thoughts are irrational and I’m overthinking it but the process struck a weird chord with me and I’m surprised I haven’t noticed it before.",0,1,2015-12-07,3,"museums, corporate sponsorships, economics",199,Optional museum fees and corporate sponsorships
10,0,Visualizing the routes of garbage trucks in Jersey City.,"#code,#dataviz,#javascript","{% include setup %}                                    A couple of months ago I took a stab at plotting the Jersey City [parking zones](http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/) after getting frustrated that the only place to see them was a PDF of streets and addresses. Last week someone left an awesome [comment](http://dangoldin.com/2015/09/24/mapping-the-jersey-city-parking-zones-ii/#comment-2385514530) pointing out that Jersey City has a bunch of open data available, including a near-real time feed of [garbage truck locations](http://www.jciaonline.org/gpsMap.php?view=map), a general [open data portal](http://data.jerseycitynj.gov/), as well as the ability to [request custom data](https://jerseycitynj.seamlessdocs.com/w/records_request). As a first project I decided to capture the movement of the garbage trucks every minute and then plot the results on a map. The results are interesting - some trucks remain local to Jersey City while others end up venturing as far as Newark Airport. The final visualized routes are at [http://dangoldin.com/jersey-city-open-data/garbage-trucks/](http://dangoldin.com/jersey-city-open-data/garbage-trucks/) and the code is up on [GitHub](https://github.com/dangoldin/jersey-city-open-data).  The approach I took was straightforward. After going to the real time map I opened the network explorer in order to see the HTTP requests being made to update the map with the latest truck locations. It was a single URL call that was returning a pipe delimited file containing the location of each truck. By writing a simple wget script and setting it as a cronjob I was able to capture the truck locations every minute. After a day’s worth of data I combined the files and removed duplicate lines (for when the trucks stayed in a single location). After that it was simple to use the Google Maps API to draw a route for each individual truck. The neat thing here is that 90% of the work was done through simple shell commands. One command to fetch the data every minute, another to combine them into a single file, and then a few others to sort and dedup the data. By the time I got to coding all I needed to do was convert the data from a pipe delimited file into something that could be consumed by the Google Maps API.",7,3,2015-12-12,4,"jersey city, open data, 07302, garbage trucks",397,Jersey City garbage truck routes
23,0,Every company and team should have components that are mission critical that need to be maintained no matter how quickly they move.,"#management,#product","{% include setup %} The accepted belief is that startups should move quickly and err on the side of speed rather than quality. This makes sense. Startups are so risky that they won’t fail due to making a few mistakes but will fail if they get out maneuvered and out innovated. The big advantage startups have is speed and that needs be leveraged.  The one caveat I’d make is that every company, big and small, should have mission critical elements that need to be maintained when pushing new features and updates. I was reminded of this last week when an unnamed corporate feedback startup sent out the private one-on-one notes people jotted down in preparation for their meeting to everyone within the company. This was a huge betrayal of trust and ruined the good will people had for the company and the product. If they weren’t able to get this basic piece right how are they expected to do the rest? Every company has these mission critical components that everyone needs to be aware of and great care must be taken to ensure they work before every deploying or change. In the adtech case it’s serving ads - if ads aren’t working then publishers aren’t making any money and losing money during each impression. For cloud productivity applications it’s critical that they don’t lose your data - downtime is annoying but at least you can switch to another task while they get back up. If you lose your data and documents you have to figure out exactly what you lost and decide whether it’s worth recreating. Everyone in the company should know what these these mission critical components are and it’s everyone’s job to make sure they’re working as expected since failure carries existential risk for the customer relationship. It’s unlikely that a single bad event will ruin things but as soon as it becomes a pattern it’s likely that that customer will be lost forever and never return due to the faulty first impression.",0,2,2015-12-15,5,"management, product, engineering, quality, software development",336,Know what's mission critical
35,0,Nearly every API library is the same general steps. I expect we'll see API libraries that will be automatically generated based on a data model or even an AI process that 'reads' the documentation.,#meta,"{% include setup %} A large part of modern software engineering is working with external APIs and services. Whether you want to automate a deployment on AWS, collect payments via Stripe, or track various behaviors using MixPanel, the process is the same - go through their documentation to figure out the available endpoints, the request requirements, and what the response will be. The next step is writing a simple API wrapper around the relevant endpoints that can then be accessed by the rest of the application. Given all the investment in AI research I’d love to see an application that’s able to generate API wrappers in any language for an API based solely on the documentation. Amazon has taken the first steps by [developing a data model](https://aws.amazon.com/blogs/aws/now-available-aws-sdk-for-python-3-boto3/) to represent their API which is then used to generate the actual libraries in a variety of languages. By changing something in the definition they can quickly rebuild the libraries in every language. One can also imagine using this data model to generate the actual documentation. This documentation can then be used to go back to the data model which can then be used to go back to the documentation.",1,1,2015-12-19,3,"api, ai, amazon",206,Automatically generating APIs
38,0,Going to the gym every day will get you stronger and stronger but having a workout plan will make it even better. Now apply that philosophy to work where we spend more than 8 hours a day.,#management,"{% include setup %} If you go to the gym you end up getting a lot more out of it if you approach your workout with a plan in mind. The same thing happens with work. If you go every day you will inevitably get better but if you come in with concrete goals and ways to push yourself you’ll be in a much better position. It’s not as easy to measure your performance at work compared to the gym but just taking the first step and realizing that you want to improve is already beyond how most people approach work. Just by thinking about your performance you improve your ability to identify your strengths and weaknesses. Doing this on a consistent basis gets you into the habit of being introspective and improves your self-awareness, which is necessary to improve.  I’ve approached this problem by jotting down notes and thoughts throughout the day and then having a non-disruptive, scheduled time each week for me to go through and digest it. Some weeks are slower than others but more often than not I gain valuable insight on what I did well and what I can improve. Writing these notes has become a habit and I don’t even need to focus on it anymore. The value of these is that over time you end up having a log of your improvement and can acknowledge how much you’ve grown as well as how much is still left. I’ve also discovered themes from week to week that have helped me improve as an engineer and a manager. If you haven’t been approaching work the way you approach the gym you’re losing valuable time and opportunity to improve yourself. You’re already spending at least 40 hours a week at the office so you should make it as valuable as you can.",0,1,2015-12-20,4,"progress, self improvement, self awareness, working out",309,Have a work “workout” plan
26,0,Engineering teams constantly struggle with the speed vs quality tradeoff. I think both should be achievable but if they're not then quality should be prioritized.,#management,"{% include setup %} Recently I’ve found myself have similar conversations with various members of the engineering team regarding the tradeoff between speed and quality. Every situation is different but without going into project details I've found that quality come first, speed second. Not because I think speed is unimportant but because I think quality is underrated. In the desire to push the next feature and launch the next product quality tends to be sacrificed. This is fine as long as we understand the tradeoffs but in most cases those are externalized to others. For example, if an engineering team ships a buggy feature, the engineering team only incurs the cost of fixing it, and even then only if they end up fixing it. Simultaneously, the cost is passed on to the users who are powerless to fix it. And then it goes through multiple tiers - first the end user who becomes inefficient and may lose work, then the support person responsible for dealing with these issues, the product manager who has to context switch to both understand and prioritize the issue, and finally the engineer. During each step time is lost but most importantly is the interruption of [flow](https://en.wikipedia.org/wiki/Flow_%28psychology%29) for multiple people, each of whom gets distracted from what they’re doing in order to deal with a problem that could have been prevented in the first place.  I'm also skeptical of the quality versus speed tradeoff and believe that both can be achieved. I've worked with many people who have been able to deliver both and believe it's a skill that can be developed just like any other. Some situations do force a tradeoff but I suspect these are in the minority for a good engineer. Even then I would push for a refactor after it's deployed in order to bring the quality up to par. Focusing on quality also builds better habits - you'll get quicker naturally over time but if all you do is prioritize speed your quality won't improve. By focusing on quality first your speed will improve on it's own.",1,1,2015-12-25,4,"management, code quality, speed vs quality, software engineering",347,The true cost of low quality
24,0,It's interesting to look at my family's smartphone usage and see if you can guess who's who. Lots of interesting behaviors and patterns.,#meta,"{% include setup %} By examining my family’s phone usage you get an interesting representations of how different generations use their phones. The five of us - parents, younger brother, and younger sister - are all on the same plan and T-Mobile breaks down the usage into phone minutes used, messages sent, and total amount of data used. I played a game with my friends to see whether they’d be able to decipher who’s who but it turned out to be surprisingly difficult and unintuitive. Turns out that my teenage sister uses least talking minutes and data but consumers average number of texts. At the same time, my mom runs a business and has close to 2000 minutes of talk time with the most number of texts sent while only using a moderate amount of data. My brother and I have a similar usage pattern - low minutes and messages but the highest data usage out of the entire family. The biggest surprise is my sister’s data usage that reinforces how much time teenagers are spending via apps and on separate social networks. That stereotype I grew up with of teenagers being constantly on their phones is still true - texting and talking have just bee replaced by siloed apps.",0,1,2015-12-28,3,"smartphones, technology, generations",208,Smartphone usage by generation
32,0,Over 2015 I've been a lot more self aware and have been thinking of life lessons to adopt. This post is a summary of the key lessons I've discovered and adopted.,#meta,"{% include setup %} Part of my 2015 goals was to have a weekly retrospective where I’d be distraction free and force myself to just sit and think. I usually did this on a Sunday morning by going outside and sitting on a bench overlooking the river or inside a quiet park. At the end of each of the retrospectives I’d sit down and jot down my thoughts in order to consistently revisit the list in order to keep improving. Below are the lessons of 2015 that I’m adopting going into 2016.  - Scheduling time for a task rather than just a goal. In the past I’d add tasks as a day event to my calendar. The better approach is to block specific time for a task - this ensures I’ll at least get something done and makes it more difficult to push things back. - Minimize the amount of physical things I own and focus quality over quantity. Maybe this was due to the apartment move but I’ve come to the conclusion that I would rather have fewer things of higher quality. This is a bit tough for me to act on since I tend to like getting deals and am pretty cynical towards trends and fashions - I can’t tell what’s actually high quality and what’s just marketing. - Sleeping more and better. Some people can get away with little sleep but I’m unfortunately not one of them. I need to get at least 7 hours to be productive. - Tracking my time usage better. It’s amazing how much time we actually have and how much of it we waste. For me it’s due to a variety of distractions and I need to be better at understanding how I spend my time in order to improve my behavior. - Don't procrastinate. A simple lesson here but I need to stop pushing things to tomorrow that I can do today. Especially when delaying something ends up snowballing and delaying a bunch of other things. - Focus on one thing at a time. Multitasking doesn’t actually work and I end up doing multiple things poorly and slowly rather than one thing well. I typically fall into this habit when watching some TV while doing some work - in those cases I’m almost always slower at my work and it would have just been better to finish the work and reward myself with some leisure time. - Distraction free walks to think. This is a rephrasing of the introductory paragraph but it’s important to get away from distractions and just force your mind to wander and think. It’s difficult at first with the desire to look at a phone or a random website but it’s worth it. - Knowing at every point why I'm doing something. Another lesson here in understanding how I use my time better. If I’m doing something I should know exactly why I’m doing it since everything comes with an opportunity cost. This doesn’t mean that I need to be productive at all times and can never relax but I should understand the tradeoffs I’m making. - Having and evaluating short and long term goals. I wish I did this when I was younger but it’s important to have goals we’re constantly working towards since it provides direction and allows us to measure our progress. - Watching less TV. A no brainer here but TV is a pretty big waste of time and I should watch less of it. I already don’t have cable but still find myself wasting time watching Netflix or some football games. - Focus on making versus consuming. This is all about productivity but I need to get into the habit of not consuming as much (TV, blogs, games, etc) and instead using that time to create. I’m already decent at this but need to get to the point where creating actually gives me more pleasure and relaxation than consuming. - Focusing and dedicating time to finance/investing/routine/research. As I’ve gotten older I can’t help but think about my later life and a big part of is figuring out how to invest my savings now to prepare for the future. I need to be more active in my investments and make sure the money I have isn’t just sitting around depreciating. - It’s okay to not have any new insights. During one of my walks I just wasn’t able to think of anything new and that’s perfectly okay. Not everything is about productivity and novelty and it’s fine to just relax and enjoy the moment. - Having a behavior consistent with views. A philosophy of life one here but if there are certain things you feel strongly about you need to make sure you act in alignment with it. It’s tough to do given outside constraints but something I’ve been more keen on. This sounds a bit abstract but an example is fighting peer pressure - sometimes it’s better to just skip an event and focus on what you want to do. - Just get started with something small, work your way up. Oftentimes embarking on something new feels like a gargantuan undertaking but it’s better to just start and take it one step at a time. The point above on scheduling time for tasks rather than goals helps address this. - Identifying bad habits and working on eliminating them. This is all about self-awareness but we all have bad habits and if we acknowledge them and work on eliminating them we’ll all be better off. - Abstinence versus moderation. I don’t recall where I read this but it rung true to me. The point was that we’re all wired differently and that some people have a hard time doing moderation and for them abstinence is necessary. A lot of my bad behaviors fall into this territory and I’d be better off completely abstaining rather than walk the fine line of moderation. - Thinking about personal brand. I’m not sure this is relevant to everything but I think it’s important to think about the personal brands we have and fostering it. Who knows how the world will look in the future but it’s important to have a good reputation and understand how you’re seen and perceived. - Having constant list of todos. I maintain an ever-growing list of todos that I will try to knock out when I have some spare time. It helps take care of a few items while keeping me productive. - Finding entertainment from within, not outside sources. Rather than rely on the outside world to entertain us we should find that within - that way we can always be entertained and don’t need to be blocked by anything. - 1% better each day. Just a thought here but if we all got 1% better each day and that compounded then at the end of a single year we’d be nearly 38 times better. This is tough to achieve but there’s just so much potential that we at least have to try. - Expectations are oftentimes better than the reality. Many times I’ll do something because i have the expectations and thought that I’ll enjoy it but after the fact I realize that it was a waste of time. The biggest example of this for me is drinking - I come in with the notion that it’ll be fun but more often than not it’s the same as any other time. It would have been better to save the money and calories and just have a fun time with friends. - When making spelling mistakes, retype the entire word. A small one here but my spelling has gotten worse with the advent of built in spellcheckers and my way of fighting it is to retype the entire word without using the spellchecker whenever I make a spelling mistake. This at least gets me into the habit of spelling words properly. - Investing time and value into things that compound. Similar to many earlier points but we should be focused on investing our time into things that matter and help lay the foundation for the long term. In my case these are knowledge and health - investing in both of them now provides compounding effects for nearly everything later. - Taking care of the small things. These days it’s easy to get inundated with tons of small things that all eat up small amounts of time. It’s easy to dismiss these but I still strive to take care of the small details. - Figure out habits and rituals. Rather than trying to do too much at once it’s better to focus on a few things and do them until they become habits and rituals. Only then should we pick up new habits to adopt. - Running in the morning changes mood the rest of the day. It may be tough to wake up early in order to go for a run but it sets the tone for the entire day so I need to just do it. - Exceptions are never exceptions. It’s easy to skip something you don’t want to do by writing it off as an exception but it never is. It’s just a rational trick to make us feel better but it’s easy to destroy a habit by constantly thinking of exceptions.",0,1,2016-01-01,3,"2015 lessons, productivity, self-awareness",1522,Lessons from 2015
20,0,I had a nice vacation in Paris and wanted to compare the two while the thoughts were still fresh.,#travel,"{% include setup %} I just got back from a 10 day vacation in Paris and couldn't help but compare it against New York. That's what traveling does - forces you to compare what you're comfortable with the novelty you're exposed to. Some make you appreciate what you have while others make you want more. In any case I wanted to share my thoughts while they're still fresh.  - Public transit: One of the first things you notice after living in New York are the public transit systems in other cities. New York has a reputation for having one of the best (one of the best?) in the world and I was curious to see how Paris handled it. The first thing I noticed was how short the platform was - rather than the multiple block stops in New York the Paris platform is enough for a 5 car train - and sure enough that's the size of the Paris trains. Each station I've been to had accurate time estimates and it felt as if the trains ran frequently and I never had to wait longer than 6 minutes although I've only taken it during the day. One thing that's struck me as odd was that it seemed as if every train had their own method of opening the door. In New York the doors open automatically but in Paris you need to either hit a button or pull some sort of level to get the doors to open. The way the stations were labeled felt friendly to tourists as well - each time you had to decide on an uptown or downtown train it would list each of the stops along with the potential transfers along each route which made it very easy to orient ourselves. The last thing I want to mention is price: the NYC subway costs $2.75 right now and you have to pay a fee for the metrocard itself. In Paris the fee is €1.80 which is just under $2 at current rates and you can buy 10 at a time for €1.40 each - significantly cheaper than the NYC subway. - Bike and car share programs: New York has Citibike and Paris has an equivalent version called [Vélib](http://en.velib.paris.fr/). I didn't get a chance to use it so don't have much of an opinion but the rates they offered were significantly lower than a non-annual Citibike pass. A daily Citibike pass is close to $10 whereas you can get day of Vélib for €1.70 and a week for €8. In addition to a bike share program, Paris has an electric car share program with stations prevalent across Paris. I didn't get a chance to use these but it seemed like a really neat idea that reminded me of ZipCar without the burden of needing to return the car to the original destination. - Neighborhoods, not districts: This might be entirely due to where I stayed and wandered but each neighborhood felt like it's own little city. We'd walk around in a neighborhood and it would have everything one would need - a bakery, a cafe, some grocery stores, a few restaurants and bars, a dry cleaning place, and a few boutique shops. It made it seem that one only needs to walk a few blocks to have everything they need. In New York it feels as if there are districts - the flower district on 28th, the diamond district in midtown, the theater district near Time Square, the rug district on 31st, the lighting stores in chinatown - but it didn't feel as if Paris was structured the same way. Paris of course is known for the shopping on Champ-Elysees but that's more the exception than the rule. The only other area that felt like a district was a series of falafel shops in the Marais. Of course this may be completely wrong and only visible through my tourist lens. - Architecture: Compared to New York Paris is ancient and its architecture and layout reflects that. Due to [Baron Haussmann](https://en.wikipedia.org/wiki/Georges-Eug%C3%A8ne_Haussmann)’s work during the 19th century Paris has a consistent look and feel which adds to the beauty. Paris barely has any skyscrapers since the majority of the buildings were construct before the elevator era. I was also struck by how mixed use the buildings were - many of them were businesses on the ground floor while the higher floors were residential. New York definitely has a bit of that but still feels as if it has some areas that are resident focused while others are commercially focused. - Price: Based on my conversions and research I expected Paris to be a lot more expensive than it actually was. The biggest reason was that the exchange rate was hugely in my favor ($1.1 per €1) but even then the cost felt offset by the listed price including tips and taxes. For example, if I go to a restaurant in NYC and have a $14 dish it’ll end up costing me close to $18 due to the tax (8.875%) and tip (~15-20%). At an exchange rate of 1.1 dollars per euro that’s equivalent to a €16.37 dish. We went to a few grocery stores and the prices for fresh food felt reasonable and only a tad bit higher than what we were used to. We also got a chance to look at some posted real estate listings and they seemed cheaper than NYC - but the apartments are generally smaller. This is a pretty biased view since we spent it as tourists and didn’t have to buy clothes or any real house items but I suspect all in all it would be pretty comparable, if not cheaper, than New York. - Panhandlers: In NYC it’s typical for people to look away and rush by someone panhandling but what struck me about Paris was that people would stop and have conversations with them. Even more, people were stopping with their children to chat and seemed to be engaging in meaningful conversations. My French wasn’t good enough to pick up the contents but the fact that people actually stopped and had conversations struck a chord with me. We talk about treating poverty and homelessness but unless we treat them as people and provide proper respect it will be for naught. - Restaurants: Not too much here but one thing I wanted to point out was how diverse the streets of Paris were compared to the “front” of the restaurants. The host and waitresses at nearly every restaurant we ate at had the “classically French” look - I’m not sure whether this was intentional but it struck me as odd given how much diversity we have in NYC. - Public restrooms: I haven’t seen this anywhere yet but Paris has free, public, self cleaning restrooms. It’s a bit slow since you have to wait through the washing cycle for each person but the fact that it’s publically available and free amazes me. - Cabs: For the most part we used the subway but we had an interesting experience when we used a cab. The driver suggested an alternate route to the one provided by his GPS and it took us a bit longer than expected to get where we were headed. Instead of charging us what the meter showed he admitted fault and told us to pay a lower amount. Despite our protests he stuck to the lower amount. I’m not sure if this is a common experience but I’m extremely doubtful something like this would ever happen in NY.  Combined, these make it seem that I prefer Paris to New York but I honestly haven’t figured that out. Paris seems to have more progressive policies than New York but I’m basing that purely on my 10 day trip and actually living and working there may be entirely different. It feels as if Paris takes public services more seriously - the public transit is cheaper, more frequent, and more robust since I didn’t experience a single stall or failure which is sadly a common occurrence in New York. I’m also aware that I’ve only spent 10 tourist days in Paris and may be approaching it through rose-colored classes. I’d love to get thoughts from people that have lived for significant periods in both.",2,1,2016-01-03,3,"paris, new york, travel",1382,Paris versus New York City
24,0,One of the most effective ways to reduce risk is to make sure that your development environments mirror production as closely as possible.,#devops,"{% include setup %} An important lesson I’ve picked up is to have a consistent development environment across your computers. These days it’s common to have a home computer, a work computer, as well as a series of VPSs that we use for development. The more similar they are the easier life gets. Having the same code and libraries reduces the risk of an application working on one machine but not the other and avoid the hassle of upgrading esoteric libraries. I’ve run into numerous issues where small version difference led to weird behaviors that ended up taking a long time to debug. Consistent tools help as well - using emacs on one machine but vim on another slows you down when you have to context switch and figure out which one you’re using. By committing to one you become more efficient as you develop the shortcuts and flows that are possible. Using virtual environments and containers helps get at this point - they’re both ways to ensure that the code you’re writing and testing is going to be the same code that’s running on production. Without this every time you deploy new code you’re risking failure. More often than not it will work as expected but it’s those rare cases that will be problematic and anything that can be done to avoid them should be done. One of the simplest ways is to align your development environments with your production ones.",0,1,2016-01-09,4,"dev ops, production, development, deployment",241,Have consistent development environments
18,0,Using namedtuples and the csv library make it incredibly easy and clean to read a file in Python,#code,"{% include setup %} Python’s my goto language for doing quick tasks and analyses with the majority of them being quick scripts to analyze a file or pull some data. I’m constantly looking to improve my code and lately have developed the following approach. The goal isn’t to make it as short as possible but to make it as expressive and clean as possible. They're related but not synonymous.  {% highlight python %}#!/usr/bin/python  import csv from collections import namedtuple  # Can add whatever columns you want to parse here # Can also generate this via the header (skipped in this example) Row = namedtuple('Row', ('ymd', 'state', 'size', 'count'))  with open('file.csv', 'r') as f:     r = csv.reader(f, delimiter=',')     r.next() # Skip header     rows = [Row(*l) for l in r]     # Do whatever you want with rows {% endhighlight %}  The reason I like this approach is that it’s obvious what’s happening and it’s being done in a Pythonic way. There’s no traditional for loop that spans multiple lines and it’s simple to update the loop to manipulate the values during the handling of reach row. This approach also leverages the namedtuple collection which is one of my favorite types - a class-like structure that's significantly more memory efficient but provides easy named access the fields (row.ymd, row.state). With this basic structure in place we can add all the bells and whistles that manipulate and tweak the rows. One thing to be aware of is that the namedtuple generates if immutable so you either need to manipulate the values before construction or use additional structures to transform the data.",0,1,2016-01-10,3,"python, reading a file, clean code",273,Cleanest way to read a CSV file with Python
18,0,First pass at a summary of the personal stats I've been collecting over the course of 2015.,#stats,{% include setup %} Over the past year I’ve been collecting personal stats nearly every day in order to see if I can spot any patterns and just understand myself better. These ranged from the time I spent sleeping to my mood (both physical and mental) to what I ate and drank. Over the weekend I hope to dive deeper into them and work out some relationships and patterns but for now I wanted to share just some basic summary stats. The script to analyze the data is up on [GitHub](https://github.com/dangoldin/annual-stats-analysis) but note that it’s designed for my file format.         Avg  Min  Max  Std Dev      Sleep (Hours)  7.35  3  11.5  0.96    Alcohol (Drinks)  1.79  0  14  1.97    Coffee (Cups)  1.38  0  2.5  0.66      I also have data for my physical and mental moods three times each day. I haven't gotten the chance to get anything meaningful out of it yet but for the most part I'm a pretty happy person! I've had a few colds and headaches but I categorized over 90% of the days as being in a good mood but just under 80% where I'm a good physical mood due to some congestion or allergies which end up improving by the end of the day.  These summaries are interesting despite being simple and I can't wait to see what I discover when I take a deeper look. The goal is also to use this exercise to tweak the what and how of what I'm going to collect in 2016. Definitely let me know if you have ideas.,1,1,2016-01-12,3,"stats, quantified self, personal tracking",255,2015 Stats: Part 1
33,0,It feels as if Google Voice is still in the dark ages of the web. Both the frontend and the backend seem as if they haven't seen any improvements in a while.,,{% include setup %} Nearly all the conversations with my family is in Russian and phone calls are no different. The fun happens when I miss a call and it goes to voicemail. Turns out that despite the amazing job Google does in transcribing English calls it fails terribly at Russian. Instead of realizing that it’s not English it ends up with transcriptions such as “douche nozzle booster.”      Given Google’s expertise in machine learning and their massive data sets I’d expect them to at least be able to identify a non-English language. My guess is that Google Voice is no longer a priority and may not even be under development at all. I had a little over a hundred unread messages I needed to mark as read. With Gmail you get the option of applying an action to the entire selection - not just what’s visible - but with Google Voice you have to go through it page by page. And there’s no way to include more items per page. A tiny bit of modern web functionality did make it through though and I was able to use shortcuts to get the job done relatively quickly. I realize self driving cars are both more exciting and have more potential but I wish there was something being done to improve Google Voice - there’s a ton of us still using it.,0,0,2016-01-17,3,"google voice, google, language transcription",245,"Poor, neglected Google Voice"
26,0,To write great code engineers need to embrace end to end ownership and not think their work is done when they open a pull request.,#management,{% include setup %} Great engineers assume end to end ownership of their products. Rather than focusing on one feature at a time they understand how it fits in with the rest of the product and think about the impact it will have on users and the business. This leads to code that scales with the product while being able to be maintained and developed by a small team. But you can only have this with everyone embracing full ownership over a product.  This idea can be expressed via an ownership hierarchy. The idea is that all engineers are responsible for writing code but the best ones want their code in products that’s loved by the end users. By moving up this hierarchy you develop a larger sense of ownership than someone who just wants to knock out some tasks.  - I opened a pull request: This is the start for every engineer. We all write code and some may consider it done when they open a pull request - leaving the rest up to someone else. - My code’s merged into master: The next level is a tiny bit beyond - in this case it’s not just that the code was written but that it has also been merged into the main branch. - My code’s deployed to production: At this point we’re at least aware that the code isn’t the end goal and that we want to make sure it’s out in the real world. - My code is being used in production: We’re finally at the point where we care that our code is actually being used. Code that’s deployed but unused doesn’t matter and we strive to write code that’s actually used. - Users love my code: The peak is building products that are loved by users. This is what drives great products and should be the goal for every bit of code that’s written and deployed.,0,1,2016-01-23,3,"software engineering, product ownership, engineering management",315,The ownership hierarchy
26,0,We've all heard of the the 10x developers but it's not about the code. It's about the decisions made that provide leverage in the future.,#meta,{% include setup %} If you’re in the software engineering world you’ve probably heard of the 10x developer. They’re an order of magnitude more productive than everyone else and can make all sorts of problems go away. The 10x number is completely arbitrary but I’ve worked with numerous developers who were notably more productive than others. A big part of it is just being able to write more code - a combination of knowing the right tools for the job and moving quickly while avoiding mistakes. But a bigger part in the productivity comes from making the appropriate decisions that are able to stand the test of time. If your code needs rewriting every time a new feature comes out it’s going to be tough to be as productive as someone whose code can be easily expanded and maintained as the product evolves. Great developers make design decisions that are able to solve the immediate problem but also leave a clear path for the improvements that will inevitably come. If you know what’s coming in a couple of months or in a year it’s simple to account for it in the current design but the real skill comes in being able to think of the unanticipated cases and be able to support them with minimal effort. Beyond that some choices end up unlocking opportunities that would have been difficult to fathom in the first place. Imagine coming up with an elegant implementation that solves an urgent problem and a couple of months later you realize that with minimal tweaking that implementation can turn into something that is transformative to the product. It’s impossible to think through every decision since you’ll end up stuck in a world of “analysis paralysis” but great engineers either have a gut feel or enough experience to make these high leverage decisions more frequently than others.,0,1,2016-01-24,3,"software engineering, software development, productivity",310,The famed 10x developer
31,0,It does seem as if the big tech companies are going all in on a few key industries. I wonder if this is a sign of a new Gilded Age.,,"{% include setup %} I just saw that Apple has [acquired](http://techcrunch.com/2015/11/24/apple-faceshift/) Faceshift, a VR based startup that makes it easier to create realistic animated characters. Two years ago Facebook [acquired](https://www.facebook.com/zuck/posts/10101319050523971) Oculus VR and Google soon followed by an [investment](http://venturebeat.com/2014/10/13/google-counters-facebooks-oculus-buy-with-500m-investment-in-vr-startup-magic-leap/) in Magic Leap. Apple is rumored to working on a self driving car and we all know Google is doing the same thing. And around the time that Facebook acquired Oculus Uber was [poaching](http://www.nytimes.com/2015/09/13/magazine/uber-would-like-to-buy-your-robotics-department.html?_r=0) a good chunk of the robotics department at Carnegie Mellon.  VR and self driving cars have been hailed as the next big thing but it’s still shocking how so many of these large tech companies that started off in different industries are converging and out spending each other on these new technologies. It seems as if in their desire to own a new market they’re all joining the fray hoping to become monopolies in these new industries.  Coupled with the news of how [dominant](http://www.wsj.com/articles/the-only-six-stocks-that-matter-1437942926) the larger tech companies have been in the stock market compared to the other players it’s hard not to think that we’re moving into a world dominated by a few big companies that can outspend others and take over brand new industries. We love the idea that a small group of people in a garage can become the next Google but I wonder whether that’s likely to remain true. I hope so but maybe we really are just setting us up for another Gilded Age.",5,0,2016-01-29,6,"technology, Apple, Google, Facebook, Uber, market",279,Unification in tech and a new Gilded Age
32,0,It's easy to focus on what problems we're solving but it's equally important to focus on what our code won't do. This ensures we're able to ship a high quality product.,#management,{% include setup %} When starting to spec out a new feature a good habit is to think about what it won’t do. This forces you to focus on the problems that aren’t being solved and makes you aware of the tradeoffs you’re making. Rather than focusing on the problems being solved it’s equally important to know what you’re not doing as well as what your implementation will preclude you from doing in the future. To be effective we need to make tradeoffs or we’d never be able to launch anything but we shouldn’t make them blindly. We need to be aware of the tradeoffs we’re making and understand the paths that will be closed off by a given implementation. By thinking of the negatives of a particular approach we’re able to surface many of these dormant issues. This helps avoids surprises later on and ensures the code has been dissected and thought through in a variety of ways.  Another great thing to do is to share this list with the end users of the product. We’re known for having a variety of biases and a common one is risk aversion. In this case if we just list the problems we’re solving everyone’s glad to endorse it but as soon as we highlight the negatives people will start speaking up. We’re never going to ship perfect code but it’s something that we should strive for and getting actionable feedback early in the process is one of the best ways to get closer to that goal.  Everyone picks up this skill naturally through experience after being bit too many times by a crappy implementation but imagine how much better we’d be if we understood the tradeoffs we’re making with every decision. Focusing on the cases our solution doesn’t work for and prevents us from handling is a great way to get this experience earlier.,0,1,2016-01-31,2,"software quality, engineering management",314,Describe what your code won't do
37,0,In a Simpsons Homer's brother decides to build a car for the average man and comes to Homer for ideas. The end is that he builds a ridiculous car that no one wants. Don't do that.,#product,"{% include setup %} Years ago, one of my projects at Yodle involved building out an automated reporting system that would consolidate all the existing reports being run via SQL queries and consolidate them into a unified application that would take care of the execution and the delivery. During the design process I spoke with existing users to see what else they’d like and it quickly morphed from a cron-job like application that just emailed CSV files based on SQL queries into a full fledged business intelligence tool that users could use to pull arbitrary data formatted in a multitude of ways. While thinking through the design of this application I spoke with the CTO and he gave me a phrase I keep going back to: “To get the expressiveness of SQL you have to write SQL.”                      While simple and glib I like how relevant this statement is to building software. When asked users will push for the most flexible and powerful system that comes with all the bells and whistles but at that point it’s equivalent to them just writing the code themselves. We have to know where to draw the line and understand what the use cases our product needs to support and not just everyone’s wishes. Otherwise we run the risk of building a [Homer](http://simpsons.wikia.com/wiki/The_Homer).",2,1,2016-02-07,3,"homer, product management, software",244,Don't build a Homer
30,0,Retargeters work by showing you products you've already seen with the hope that you buy it. But why should I see an ad for a product I've already bought?,#meta,"{% include setup %} Retargeting ads work by checking to see a product you’ve looked at and then showing you that product over and over again with the hope that at some point you buy it. There are entire companies dedicated to this with extremely sophisticated algorithms so I’m surprised when I see inefficient behavior. In my case it was an Amazon ad that kept following me around even after I already purchased the product, a precision cooker. Given that Amazon knows my purchase history and sees that I’ve already bought the cooker it makes no sense to keep showing it to me. It seems that their algorithm figured this out as well and started showing me the same product in different packages and at different price points. The fact that they have logic that’s smart enough to show me different variations of the same product but not take into account my purchase history shocks me. What makes this even worse is that I own some Amazon stock and realize that this inefficiency has an impact, albeit a tiny one, on my shares.",0,1,2016-02-10,4,"retargeting, advertising, adtech, amazon",184,Retargeting gone wrong
25,0,With the rapid change of pace in technology we need longer government term limits to provide more stability in order to create better policies.,#meta,"{% include setup %} Many have written about society’s inability to enact laws quickly enough to deal with the current pace of technological innovation. Governments are still trying to figure out how to regulate the sharing economy with both AirBnB and Uber being reacted to rather than being effectively regulated. This leads to different treatment in different locations and causes confusion for consumers, the businesses, and regulators.  A potential way to rectify this is to actually increase term limits for people in government. With politicians focusing on reelection every few years and constantly moving in and out of office it’s tough to develop a consistent regulatory approach. This worked well a hundred years ago when new industries would take a decade to develop and you could regulate them as they grew. Now it can take a year for entirely new businesses to be created before governments can react to what’s happening. By then the new consumer behavior has become entrenched and becomes difficult to change. Rather than worrying about reelection and undoing prior policies politicians should be focused on the future and how to adapt government for an increasingly changing society.  It’s counterintuitive that to deal with rapid change you want slower government turnover but it makes sense. Imagine a football team changing coaches for every game or a company switching CEOs every year. They’d be too busy dealing with the leadership change to win games or grow as businesses. Stability is necessary in rapidly changing environments and governments need to adapt to provide that in modern times.",0,1,2016-02-13,4,"government, politics, policy, society",259,Longer terms in government?
28,0,People worry about tech debt but database debt is worse since it's so hard to change and tweak. Spend the extra time desiging a flexibile database schema.,#devops,"{% include setup %} One of the biggest lessons I’ve learned is to spend extra effort thinking about the database when setting out to build something new. Compared to changing a database schema, changing code is trivial. The database structure defines how you think about your business and either provides the flexibility as you grow or impedes you when forced to support something it wasn’t designed to handle.  With code you can do a deploy which can replace all behavior at once while with data you’re forced to acknowledge and handle the data you have. If this is a large table you have to figure out how to migrate the data to a new schema. The simple way is to deal with the downtime and hope the migration works. The more complex way is to support two database schemas at once with your code while the migration occurs. Neither of these would be necessary if you think through the database design choices you’re making. It’s going to be impossible to address every future need but there’s incredible value in at least thinking through potential changes and how they’d be supported.  A simple question is the relationship between tables - are you ever assuming a one-to-one relationship that may be one-to-many in the future? If that’s the case you’re probably better off designing the database to support the more advanced case but having your application only support the one-to-one case. This keeps the flexibility in place if you need but doesn’t complicate the code too much.  Another question to ask is whether there’s anything redundant. It may be easier to denormalize your data a bit for the sake of improving a query but don’t. If a database can support an inconsistent state it will support an inconsistent state. Whether due to a bug, a timing issue during a deploy, or someone making a manual update you’ll end up with an inconsistent state in the database which will likely lay dormant for too long. Avoid this issue entirely by removing all redundancies and potentially conflicting fields.  Beyond the tactical questions, thinking about your business and product roadmap a year from now is a great way to influence your schema now. If you suspect you’ll need to support a particular feature or flow you should imagine what your data would need to look like. It’s important to do this when writing code but it’s more important to do this when designing your database. Code can be changed with a deploy but database changes require more.",0,1,2016-02-15,3,"database, tech debt, database design",428,Design your database for flexibility
28,0,Self driving cars are inevitable but we're just scratching the surface of the changes they'll bring. The imemdiate effects are obvious. The secondary ones not so much.,#society,"{% include setup %} Nearly every major tech company is pursuing a self driving car future and it’s inevitable that at some point most cars on the road will be completely autonomous. Cheap and easy transportation is the immediate change but there will be massive secondary effects to the shapes of cities and society.  A [college professor](https://www.johnson.cornell.edu/Faculty-And-Research/Profile?id=lvo2 ) used the example of the invention of the car to highlight these sort of effects - if told that cars would be successful most people could have guessed that they’d replace horses and clean up cities. But very few would have been able to predict the rise of highways which led to the development of suburbs and the current structure of the United States.  Self driving cars have that same potential and it’s an interesting exercise to think through the impact. The short term is that fewer people will own cars, our roads will be safer, and that there will clearly be some disruption among the auto manufacturers. The medium and long term are where it gets tricky.  The increases in safety will lead to faster cars which may lead to another shift in where people live. One idea is that people will be able to live further and further away from cities which will lead to a decline in suburbs with more people opting to live in more remote areas while also leading to a boost in urban living.  Self driving cars will not just be ferrying people and one can imagine nearly everything being able to be delivered by self driving car or truck. I like the image of a “carrier truck” that drives around neighborhoods with a series of drones taking off and landing to deliver items along the way. In this sort of world there’s not a great need for physical stores. Taken to the extreme this means that cities will be designed to focus on the social elements. Convenience stores will disappear but restaurants will thrive. Most people aren’t buying everything online but I suspect it’s only a matter of time.  Public transit will have to change. I worry that if the price of self driving cars drops low enough to appeal to most people but high enough to not be affordable by some it will lead to a decline in public transit. At that point since most people wouldn’t care about public transition it would end up in a self destructive loop as more and more people decide to go for the self driving car route which in turn leads to less and less funds being allocated to public transit.  These are just scratching the surface and we’ll have to wait to see what happens.",0,1,2016-02-17,5,"self driving cars, society, culture, futurism, futurist",455,The impact of self driving cars
14,0,Let's Encrypt made it surprisingly simple to support HTTPS on my old projects.,#devops,"{% include setup %} I’ve been meaning to mess around with [Let’s Encrypt](https://letsencrypt.org/) since they launched their public beta but haven’t had the chance until earlier today. As an proof of concept I had a bunch of old projects running on a Digital Ocean instance and decided to try converting them to HTTPS using the Let’s Encrypt project.  Despite the usual complexity of getting and integrating an SSL certificate Let’s Encrypt made it extremely easy. It was smart enough to go through each of my Apache configuration files and prompted me to see which domains I wanted to switch over to HTTPS. After selecting a few and continuing to the next step it generated new configuration files with the appropriate setting to enable SSL support.  The only issue I ran into was handling a WSGI configuration properly. Let’s Encrypt works by copying an existing configuration file and adding a few lines to specify the SSL certificate. This works great for simple configurations but can lead to an issue when you have the same WSGI configuration across two files. The fix was straightforward - temporarily comment out the conflict lines, run the Let’s Encrypt script, and then uncomment the lines in the new SSL version of the file.  Overall an extremely simple way to enable HTTPS on your projects. In the past I would have never set SSL up on toy projects due to both the cost of buying one as well as the cost of dealing with a bunch of esoteric commands to set it up. Let’s Encrypt makes it incredibly easy - especially if you’re running Apache.",1,1,2016-02-20,3,"ssl, https, Let's Encrypt",268,Let's Encrypt
29,0,I got an Amazon Echo yesterday and have been playing around with it. Despite the gimmicky factor it really is a new way of interacting with our devices.,#meta,"{% include setup %} After reading the positive reviews I got past the gimmick factor and jumped aboard the Amazon Echo train and got it set up yesterday. After going through the obvious examples (what’s the weather, tell me a joke, add x to my shopping list, play song y) and playing around with it I’m past the gimmick stage. The always on listening is really a different way to interact with our devices. Conceptually it’s no different than using Siri or Google Now but in practice it’s a world of difference. I don’t always have my phone with me and for some things it just feels more natural to start speaking and see an immediate effect. Whether that’s playing some specific songs or playlists, changing the volume, or adding items to a shopping list it feels more natural than having to go through a phone. One of my favorite use cases so far has been using the Echo to keep track of my shopping list. In the past I’d be in the kitchen and realize we needed something and would forget as soon as I switch tasks. With the Echo I can immediately call out what to add and have the list readily available next time I go to buy something.  To be honest, 99% of our Echo usage has been playing music and adding things to a shopping list but I can see the potential there. There are a ton of apps, that Amazon calls skills, with new ones constantly being developed and I look forward to seeing what kind of cool stuff gets developed.",0,1,2016-02-21,2,"amazon echo, connected home",268,Amazon Echo
38,0,As we get larger platforms it's going to be increasingly important that they stay open and allow any third party to integrate. Otherwise we run the risk of helping these big companies entrench themselves more and more.,#society,"{% include setup %} I [set up](http://dangoldin.com/2016/02/21/amazon-echo) the Amazon Echo over the weekend and have been an active user of my wife’s Spotify account which comes integrated with the Echo. I would have preferred to use my Apple Music account but the Echo currently only supports Spotify. I suspect the biggest reasons are competitive - Amazon and Apple are competing for the home and it’s likely that either Amazon doesn’t want to integrate Apple or Apple is preventing Amazon from getting the integration done. At the same time Amazon has a music offering yet they specifically call out the Spotify integration. Is this because Spotify is only a competitor for music and the value of an Echo trumps this? Is it because Spotify has more reach and this is a necessary integration? I’m sure the answer is a bit of both but it’s fascinating to see how these partnerships develop.  Ideally every company would provide an open way for others to integrate their apps but we live in a competitive, capitalist world where every company wants to get an edge over their competitors. As consumers it’s up to us to push for the integrations we want and make sure these platforms stay as open as possible - otherwise we’ll end up making the rich richer and prevent new entrants from even having a chance.",1,1,2016-02-22,5,"amazon, apple, competition, monopoly, business",225,Platform partnerships
24,0,I will watch lectures and listen to podcasts at twice the original speed. Why don't we speak and listen at twice the speed?,#meta,{% include setup %} Whenever I watch some online lectures or listen to a podcast one of the first things I do is change the speed to either 1.5x or 2x the original. Sometimes I’ll have to skip back or reduce it back to the normal speed but for the most part this approach saves me tons of time and I like to think that I absorb the same amount of information. But the fact that I can absorb and process information at twice the speed makes me wonder how much more productive I’d be if every conversation I had occured at twice the speed. Is there some physiological reason we don’t speak at twice the speed? Is there a cultural factor? Does this information density vary based on language?  We can train ourselves to get faster and faster at processing aural information but there must be some limit and I suspect there’s a wide range in the information density of various languages. If this is the case I wonder if there’s some conclusion that can be drawn about that culture or society. Most likely the bottleneck is on the transmission side - the effort to produce language is more than listening and it requires both our brains to form thoughts and our mouths to turn them into sounds which are the limiting reagent.,0,1,2016-02-27,3,"language, speech, information processing",225,Aural information density
32,0,The first version of most tech companies is a scrapped together product but over time they grow and evolve and start coming up with specialized tools and solutions to new problems.,#meta,"{% include setup %} It’s obvious in hindsight but incredible when you experience it but every successful company has to iterate through a variety of tools as it, and its problems, grow. A typical modern tech startup starts by identifying a problem and using a common web framework to quickly come up with the first pass. But as this company grows new problems and situations arise that the initial solution no longer supports. They may end up having a series of asynchronous tasks and need to start using RabbitMQ with that use case. MySQL may no longer be enough and they start offloading their data to Redshift. That off the shelf web framework is no longer performant enough so they have to split it into multiple components and start embracing strong, statically typed languages.  This tool specialization also goes hand in hand with team specialization. A single engineer doesn’t have the time to do everything so startups need to make the short term decisions and focus on the next couple of months in order to grow. Only when they grow does t make sense to find the critical problems and dedicate time to fixing them. And hopefully by that point you have a larger team that can focus on the deeper problems.  For most of us the problems have already been solved and it’s about figuring out how to adapt the solutions for our systems. Sometimes this requires getting an open source library to work. Other times it may require implementing the code from an obscure academic paper. But the real success comes when you run into problems that no one else has encountered. At that point you’re dealing with extremely specialized problems that you were the first to encounter. These are the scale problems that Google and Facebook are solving and every startup hopes to get there some day. In fact, that is the ultimate mission engineering based companies - solving problems that haven’t been encountered yet.",0,1,2016-02-28,3,"tech startups, companies, engineering",328,Tool specialization and growing companies
22,0,It seems possible to make money by buying reserved instances and then selling them at a discount on the AWS market.,#aws,"{% include setup %} While reserving some EC2 instances earlier this week I discovered that Amazon allows you to [sell](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-buying-guide.html) reserved instances you’re no longer using. Usually the prices the third parties are offering are very close to the fair market value but I wondered if there was an arbitrage opportunity by reserving a longer term instance and selling it for a series of shorter term leases. The [typical discount](https://aws.amazon.com/ec2/pricing/) for buying a 1 year reserved instance is 30% while buying one for 3 years can get over 60%. The idea being that if you can get an instance for a 60% discount over 3 years and then sell it for 3 one year terms at a 25% discount you end up coming out ahead. Of course the challenge is that Amazon constantly drops prices so a 60% discount now may be equivalent to something much smaller three years later. There’s also the risk of no one purchasing your instances but that seems unlikely since you can always undercut Amazon’s official price. The other factor is the discount rate since you’re paying up front for 3 years worth of an instance. During that time you could have taken that money and invested it elsewhere which could have led to a better return but which would have been unlikely when you’re getting a 30% discount over the course of a year.  Unfortunately, based on the Amazon seller [documentation](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-selling-guide.html) it looks as if you can't actually split a 3 year reservation into 3 single year reservations and you'll be charged a 12% fee. There's always the option of using an instance and then selling it at a premium for a term that Amazon is not offering but I doubt it's worth it given the risks and the restrictions placed by Amazon.",3,1,2016-03-05,5,"aws, ec2 instances, reserved instances, ec2 marketplace, arbitrage",316,AWS EC2 instance arbitrage
29,0,Airbnb claims that it's too hard to localize their product but on the flipside you have companies such as Uber that built their entire product around local markets.,"#product,#society","{% include setup %} Yesterday I read an [interesting piece](http://www.nytimes.com/2016/03/06/business/airbnb-pits-neighbor-against-neighbor-in-tourist-friendly-new-orleans.html) on Airbnb in New Orleans. The content itself isn't new - it highlights the typical conflict between those that view Airbnb as violating local ordinances and ruining the city and others who believe that Airbnb brings value and is helping New Orleans rebuild after Katrina.  But what was interesting was the repeated claims of Airbnb and the other rental marketplaces that it’s just not scalable to follow local policies for every city and it’s up to the users to know their local regulations and follow them. I understand it’s difficult to localize complex products but these responses just feel like an excuse.  > Representatives of the larger home-sharing companies have met with New Orleans officials, but they are seldom heard from in more public forums. Officials of Airbnb and VRBO (Vacation Rentals by Owner, a HomeAway brand that is popular in New Orleans) point out that they operate in so many places they cannot possibly get into the specifics of local policy; they are merely private businesses offering services to consumers. So it is up to New Orleans and other cities to devise their own regulations, and up to users to follow them.  > According to Mr. Rivers, Airbnb and VRBO told his staff that it would be too onerous to adjust their software to accommodate every regulatory arrangement for thousands of municipalities around the world. Spokesmen for Airbnb and VRBO confirm that rewriting their platforms in this way is not practical.  Contrast this with Uber. They also run a marketplace that’s highly sensitive to local regulation but work within the confines of the law (including pushing to change legislation). Both Uber the company as well as the Uber app have adapted a localized view. When I open up the Uber app in New York City I see a variety of options that I don’t see in other places. In fact, Uber can even push idiosyncratic updates that may only last a couple of days - for example a special [“De Blasio” ride option](http://techcrunch.com/2015/07/16/uber-launches-de-blasios-uber-feature-in-nyc-with-25-minute-wait-times/) that came with a 25 minute wait time.  The goal of technology companies is to come up with elegant solutions to real world constraints. Uber has embraced it by building their company and product to embrace local differences while Airbnb adopted the attitude of a single product for the whole world. I’m confident if Airbnb wanted to build a flexible product that worked for local markets they’d be able to and it would actually be a fun and interesting product and engineering challenge. Startups need to embrace their challenges and this feels like Airbnb being complacent. I understand there’s a high cost to localize Airbnb and it comes with a world of risks but if they do it right they’ll be able to capture significantly more share and markets.",2,2,2016-03-06,5,"airbnb, uber, local laws, economy, startups",498,"Airbnb, Uber, and local laws"
32,0,I've been hesitant to use the MySQL enum field after reading some stories but it turns out to be pretty useful and stable. Not for everything but has it's use cases.,"#sql,#code","{% include setup %} The MySQL enum field provides a nice compromise - the space efficiency of using an integer, the human readability of text, and basic type safety. Yet I had this vague recollection of reading something that made it seem enums carried a ton of risks when changing the column definition so wanted to see if I could “break” it. Turns out it’s a lot more resilient than I thought. I went through a series of combinations - ranging from changing the order of the enums in the definition to trying to insert values that didn’t exist but in every case it handled it as expected. Doing a bit of research I discovered how MySQL represents the enum type. Rather than storing the values in a specific order MySQL supposedly creates a map-like structure to relate the integer values with their enum counterparts. This allows you to change the order of the enum definition without changing the underlying map or any of the stored values. I still wouldn’t use enums for anything that would require a join but for storing small and simple sets of data it works great.  {% highlight sql %} drop table if exists test;  -- Create the toy table create table test (   id int auto_increment primary key,   e enum('a','b','c') );  -- Populate it with some sample values insert into test (e) values ('a'), ('b'), ('c');  -- Confirm they look good select * from test;  -- Now let's add another possible enum value alter table test modify column e enum('a','b','c','d');  -- Looks good select * from test;  -- Add some more values insert into test (e) values ('d'),('a'), ('b'), ('c');  -- Looks good select * from test;  -- Change the order around alter table test modify column e enum('a','b','e','c','d');  -- Looks the same select * from test;  -- Change it again alter table test modify column e enum('a','b','c','d','e');  -- Looks the same select * from test;  -- Add and change the order alter table test modify column e enum('b','c','d','e','f','a');  -- Looks the same select * from test;  -- Fails since 'g' is not a valid value insert into test (e) values ('g');  -- Replace 'a' with 'f' update test set e = 'f' where e = 'a';  -- Now get rid of 'a' alter table test modify column e enum('b','c','d','e','f','g');  -- Now add 'a' back in alter table test modify column e enum('a', 'b','c','d','e','f','g');  -- Now swap 'f' back with 'a' update test set e = 'a' where e = 'f';  -- Looks just like before select * from test; {% endhighlight %}",0,2,2016-03-10,4,"mysql, code, enum type, enum data field",409,The MySQL enum type
29,0,We have a routine and goal when going to the gym and yet for something we spend 8 hours at each day we do not. This is crazy.,#meta,"{% include setup %} To get the most benefit from working out it’s important to have a plan and consistently measure yourself and keep pushing your goals and yet it’s surprising how rarely that’s done in a professional setting. We spend over 40 hours a week working but the majority of us view it as a chore and something that we just have to do. Imagine if everyone approached work the same way they approach the gym. People would have much clearer ideas of what they want to do and what challenges they face. They would be able to measure how well they’re doing and understanding what they need to start doing to get to the next level. Instead most approach it as something that they need to do rather than something they want to do.  Consistently going to the gym without a plan will definitely improve your shape and is better than not going at all. But it pales with what would happen if you went to the gym with a plan in mind. It took me a long time to realize this and I suspect most people approach work passively - they’ll just put in the time, do a good job, and see where things will go. But true success and joy come from constantly reevaluating your goals and thinking through the means to achieve them.  It’s depressing to think that most people spend a third of their day on something that they’re not actively engaged with when an attitude change can change the entire perception of work. Rather than putting in the bare minimum we should be thinking of what we want in our lives and how work can make that happen.  Many people seem to idealize retirement but I want to be so engaged in my work that I never want to stop. This is the vision we should all be working towards. I understand that this is a privileged perspective and not everyone has this choice and there are always constraints but it’s something we should all strive to attain.",0,1,2016-03-13,3,"life goals, work, success",345,Approach work like the gym
14,0,Google's AlphaGo victory is a passing of the torch from IBM to Google.,#meta,"{% include setup %} Last week the big news was that Google’s AlphaGo was able to win 4 of the 5 games against Lee Sedol in Go. As we’ve gotten better and better hardware it’s not surprising that an AI was finally able to win in a well defined environment. AIs will continue to improve and we’ll start seeing more and more of this behavior across a wide range of problems and not just games. The most significant part for me was that this was achieved by Google and not by IBM. IBM had two recent notable achievements in AI - one was building Deep Blue in 1997 which beat Gary Kasparov in chess and the other was building Watson in 2001 which dominated at Jeopardy. Yet just five years later Google has claimed the AI victory with AlphaGo.  It’s tough to not see this is as the passing of the torch. Google has tons of incredible AI that’s used to power the business but building an AI to focus on Go was taking something out of IBM’s playbook. Luckily, we’re seeing a ton of AI work being open sourced - from Google’s [TensorFlow](https://www.tensorflow.org/) to Facebook’s [FAIR](https://research.facebook.com/blog/fair-open-sources-deep-learning-modules-for-torch/) to [OpenAI](https://openai.com/blog/introducing-openai/) - and I’m excited to see what we come up with.",3,1,2016-03-20,6,"AI, AlphaGo, Deep Blue, Watson, IBM, Google",221,Passing the torch: IBM to Google
25,0,Data analysis needs to be fun and enjoyable to do. Otherwise there's enough rote and frustration to make it too easy to give up.,#meta,"{% include setup %} In order to do any meaningful data analysis you need to have fun doing it. Otherwise it becomes a chore that’s extended by each additional analysis you run and each additional failed attempt at an insight. This requires a positive attitude and enjoying the slow, methodical process of discovery and appreciating each iteration while getting closer to the end goal. The vast majority of analyses lead to no new insight, especially when all the easy stuff has already been figured out, and it’s critical to remain the optimist while appreciating the present.  A key part of this is tools. I have a set of tools I’m intimately familiar with and can manipulate them without much thought. It’s this passive approach and behavior that lets me go through the rote work while simultaneously focusing in on the challenging elements of the problems I’m facing. Fast tools are also a requirement. Tools that allow you to quickly get a result prevent you from leaving the zone and leave you ready for the next attempt.  When 80% of the work is rote data manipulation it’s important to not burn out while getting to the 20%. To be successful you need to find the fun in both the data manipulation and the analysis.",0,1,2016-03-21,1,data analysis,214,Data analysis needs to be fun
17,0,Political parties are just like product bundles. By looking at one we can understand the other.,#meta,"{% include setup %} I rarely write about politics but it’s an election year and I had an interesting realization. Political parties are just like product bundles. We each have our own issues and policies we’re passionate about but it’s impossible to find a politician, less a party, that has the same views we do. Instead we have political parties that take a few issues and policies and try to wrap them up in a bundle hoping to appeal to enough people to win an election.  Reading the Wikipedia [article for product bundling](https://en.wikipedia.org/wiki/Product_bundling) makes it obvious how closely it fits political parties. From Wikipedia:    Bundling is most successful when:    There are economies of scale in production.   There are economies of scope in distribution.   Marginal costs of bundling are low.   Production set-up costs are high.   Customer acquisition costs are high.   Consumers appreciate the resulting simplification of the purchase decision and benefit from the joint performance of the combined product.   Consumers have heterogeneous demands and such demands for different parts of the bundle product are inversely correlated. For example, assume consumer A values word processor at $100 and spreadsheet processor at $60, while consumer B values word processor at $60 and spreadsheet at $100. Seller can generate maximum revenue of only $240 by setting $60 price for each product—both consumers will buy both products. Revenue cannot be increased without bundling because as seller increases the price above $60 for one of the goods, one of the consumers will refuse to buy it. With bundling, seller can generate revenue of $320 by bundling the products together and selling the bundle at $160.       Each of these is a perfect fit for politics. There are huge economies of scale and distribution for political parties. They’re purely information so there’s no marginal cost and the brunt of the cost is in the formation of a party which is incredibly difficult due to the massive network effects and infrastructure required. People have diverse beliefs with great variance on the most important issues and don’t have the depth to know every issue.  It’s no wonder that political parties are so entrenched but this also provides insights on how to dismantle these bundles. We need to examine history and see how previous bundles have been broken down and see whether those solutions can apply to our political system.",1,1,2016-03-26,2,"politics, bundling",404,Political parties are product bundles
22,0,Politics is akin to economic bundling so we need to see how those were disrupted in order to revamp modern politics.,#meta,"{% include setup %} Yesterday I made the case that the current political system consists of a [series of product bundles](/2016/03/26/political-parties-are-product-bundles/) and I’ve been thinking of ways unbundling would work. And what better way than to look at existing products and industries that have been unbundled.  As numerous people have pointed out, the music industry is a clear example. Initially music was sold on CDs and there was no idea of buying solo songs. But with the launch of the iPod, iTunes, and internet proliferation it became possible to buy individual songs. Lately we’ve been back in the bundling phase with the various monthly music subscription services, such as Spotify and Apple Music.  Music debundling was driven by technological changes. It made no sense to package individual songs for sale when they required physical packaging. But as soon as the majority of households got reasonably fast internet it became possible to start selling individual songs.  But how does this apply to the political system? It’s not really a technology problem. We have the ability to share and disseminate information to anyone with an internet connection. We have the ability to allow everyone to vote through a smartphone. We have the ability for anyone to start a cause and share it with millions of people. Unfortunately, having the ability doesn’t mean much without follow through.  In the case of politics there’s so much entrenchment (think recording studios) that change occurs at a glacial pace. The reason the recording studios signed with Apple was because of the rampant piracy - not due to their desire to improve the consumer experience. We need the political equivalent of piracy to spur this unbundling. Ideally it comes dressed as a white knight ready to save the system but leads to unexpected secondary effects that lead to significant changes in the system.  My gut is that we need a few small changes that open to the door to these unintended effects. Something akin to allowing people to request or report services via an app which leads to people asking what else? That will open the door to voting for issues and politics from our phones and maybe even filing taxes.",1,1,2016-03-27,2,"politics, bundling",368,Unbundling politics
31,0,To innovate companies need to make short term decisions that increase their optionality in the future. Making the right decision now can set you up for huge success later on.,"#product,#meta","{% include setup %} Last weekend I finished the [The Everything Store](http://www.amazon.com/The-Everything-Store-Bezos-Amazon-ebook/dp/B00BWQW73E), which details the rise of Amazon from a fledgling online book retailer to its current form. One pattern that stood out for me was how Amazon was able to continuously push into new business areas due to the infrastructure that they had in place based on previous decisions and commitments.  They started with books but were able to grow into other smaller products once they figured out the logistics behind shipping smaller items. Once Amazon had that in place they kept tweaking their distribution system to expand the variety of products offered while improving the speed of delivery. This allowed them to keep amassing a list of products which they used to open up their platform to third party sellers. And as Amazon improved their infrastructure they were able to open that up to these third party sellers as well. In parallel, they built AWS to provide computer services to internal Amazon teams but were able to turn it into a brand new line of business that powers the majority of new startups. And now their are rumors of Amazon building out a shipping service to bypass FedEx and UPS.  Ben Thompson coined this the ""[ladder-up strategy](https://stratechery.com/2016/snapchats-ladder/)"" and I’d argue it’s the only way companies can keep consistently growing. Relentlessly focusing on a few things and then using them to attack adjacent markets is how you grow from a struggling startup to a powerhouse. The challenge is making the short term decisions that set you up for success in the future as well as knowing when to leverage that infrastructure to move into the next thing. The former is incredibly difficult - it requires thinking beyond the immediate step and understanding the opportunities that become available after a successful execution of the initial step. Then follow up by thinking of what the next strategic step will be and what doors that will open up. After a few iterations of this exercise you may get a glimpse of your company’s future. But the ideas are the easy part - the execution is an order of magnitude more difficult. Scenarios will consistently come up that necessitate changing your approach but each change will pull you further and further away from your initial vision. You then need to either steer the company back towards the original direction or adapt your plan based on these new directions.  It’s incredible seeing this successfully execute though. Companies that are able to do this consistently increase the size of their moat and become nearly impossible to dislodge. Each of their activities complements and reinforces the others which when coupled with their benefits of scale grant them monopoly-like powers.",2,2,2016-04-03,4,"laddering up, business strategy, ben thompson, innovation",463,Ben Thompson's “laddering up” and building bigger moats
41,0,The more I code the more I believe that code should be minimized and ideally never even written. Good engineers are able to do a ton with well thought out and abstracted code which is easier to manage and improve.,#management,"{% include setup %} The best code isn’t code that’s elegant or code that’s brilliant it’s code that doesn’t need to be written. One of the best feelings is when you can take a new problem and turn it into an existing problem that already has a solution. Sometimes that requires making a few tweaks and compromises to the problem or the code but the time and effort saved can be massive. This requires a deep understanding of the problem being solved as well as the existing code. Someone knowing the code but not the problem won’t be able to transform the problem into something applicable. And someone having a deep knowledge of the problem but not the code won’t be able to see how the code can be adapted to solve this scenario. The optimal result comes from someone who can strip away the cruft from both of them while still maintaining the spirit of both in order to combine them.  To make this work you need code that’s clean, well architected, and accessible. Such code is a pleasure to work with and is transparent enough that a decent programmer can see how it can be tweaked to solve new problems that arise. This requires massive amounts of discipline to go back and refactor your code when necessary to keep it in a pristine state so it can be easily transformed when needed. And that transformation with introduce wrinkles that will need to be ironed out to set it up for the next wave of changes.  While writing this post I was reminded of a joke that emphasises this idea of minimizing work by focusing on what you’ve already done:     A mathematician was interviewing for a job. The interviewer asks him - ""You are walking towards your office and running late for a very important meeting and you glimpse a building on fire with people screaming for help. What will you do?"". The mathematician thinks for a while and replies : ""People's lives are more important than an office meeting. I would immediately call for a fire brigade and help the trapped to the best of my abilities"". The interviewer seems to be impressed with the mathematician's answer and moves on to the last question. Just to check his sanity, she asks: ""And what if the building is not on fire?""  After a moment of thought, the mathematician replies with confidence: ""I will set the building on fire. Now, I have reduced it to a problem that I have already solved before!""    -  ScottElliot  on  reddit",2,1,2016-04-05,3,"code, productivity, management",440,The best code is no code
30,0,Engineers always have to balance the quick and dirty work with something more elegant that will take longer. Using a rule of generalizing when n=3 strikes the right balance.,"#management,#code","{% include setup %} Engineers strive to write code that’s general and flexible enough to adapt to support a variety of cases with minimal changes. Unfortunately, writing general code isn’t easy and requires significant thought, effort, and experimentation. The challenge is figuring out the appropriate time to generalize your code.  If you do it too early you may spend unnecessary time writing generalized code that will never be used again. Even worse you may write code that you think is generalizable but ends up collapsing under its own weight under future scenarios. In this case writing minimal code would have served you better since it would have been much easier to adapt or throw away to support the new case.  If you do it too late you most likely spent time doing repetitive work that could have been better spent building a scalable solution that you may end up doing anyway.  My rule of thumb is to generalize at n=3. The first two times I have to support a new scenario or process I'll just do it manually or hacked together. But as soon as I need to do it for the third time I'll start looking for a more generalized solution. At this point it's likely that the third is not the last time I'm going to have to do it and I also have 3 cases to base and test my solution on.  This isn’t a trivial approach but works surprisingly well. It’s incredibly difficult to predict whether a simple script will morph into something more or end up being used once. The easiest way to predict whether it will be repetitive is to wait until it is repetitive - for me that magic number is 3. High enough to weed out the edge cases but low enough to get enough value from being generalized.",0,2,2016-04-07,6,"coding, management, short term, long term, programming, program design",307,Generalize at n=3
28,0,MaxMind made a decision in 2002 that has led to some unfortunate unintended consequences. It's important to think about the societal impact of the code we write.,"#code,#society","{% include setup %} Earlier today I read an [article about MaxMind](http://fusion.net/story/287592/internet-mapping-glitch-kansas-farm/), a company that offers an IP address to geographic location mapping service, making a seemingly minor decision in 2002 that that led to unintended consequences that have been going on since then. The article goes into detail about the decision and the effect but the main idea is that it’s not a prefect system and they needed a way to approximate some IP addresses to particular locations. Lo and behold these locations are now seeing tons of harassment from law enforcement and various strangers online.  This is a perfect example of how a quick fix to a seemingly simple problem can lead to a world of problems that can impact others without you even knowing. I can imagine myself running into that problem and making the same decision. It’s unlikely I would have thought about the people that may have lived at those coordinates or that people would actually be using this information to track people down.  There’s a lesson here for everyone who’s writing software: at the end of the day all the code we write will have some effect on people and we need to be mindful of that. We’re not going to stop making mistakes but we should take the time to consider the impact of every line of code we write.",1,2,2016-04-10,3,"buggy code, maxmind, geolocation",233,Unintended consequences
40,0,There's a new design pattern I've seen in B2B startups that allow you to sign up with a company email address and do all the setup work in the background. This is great and everyone should be doing it.,"#product,#design","{% include setup %} One of the latest trends I’ve noticed is B2B companies is allowing you to sign up with a company email address and automatically linking you with the rest of the organization. This is a definite no-brainer and a really simple way of getting new users setup without having to be bottlenecked by a burdensome administrative process. No one on the HR team has to enter employees into the system nor send anyone their username or account info. Instead they just provide a link to the service and have people sign up with their company email address. Once this is done they immediately have access to whatever the base employee account should have. Only later one does an admin need to grant additional permissions and privileges.  The companies off the top of my mind that have done this are [Slack](https://slack.com/), [Greenhouse](https://www.greenhouse.io/), and [Tallie](https://tallie.com/) but there are countless others. If you’re building a B2B product that’s designed around teams working together this should be at the top of the product queue. It’s a great way to get on the good side of the HR team while getting your users onboarded quicker.",3,2,2016-04-13,7,"b2b, company signups, corporate email address, company email address, slack, greenhouse, tallie",199,The corporate email signup design pattern
39,0,Amazon announced that they'll be offering Prime as a monthly service and this contributed to a price drop in Netflix. I find this irrational - it's a tiny decision that was expected and should have been taken into account.,"#product,#meta","{% include setup %}      Last night, Amazon [announced](http://www.nytimes.com/2016/04/18/business/amazon-challenges-netflix-by-opening-prime-to-monthly-subscribers.html) that in addition to the annual plan they’re going to start offering Prime as a monthly service. Sure enough, investors interpreted this as good move by Amazon (up 1.51% at end of day) while hurting Netflix (down 2.79% at end of day and even more post earnings). These percentages translate into a $1.34B decrease to the Netflix valuation and a $4.49B increase in valuation for Amazon. As a shareholder of both I find this behavior interesting for its irrationality.  Companies are constantly innovating and have a constant stream of ongoing initiatives and experiments. I’m surprised such a simple move can impact the markets so much - it seems like an obvious move that would have happened at some point and should have been baked into the current price. The fact that there was such a sudden stock price move attributed to the news strikes as proof in the irrationality of the markets - trivial decisions shouldn’t be moving the needle and people should be investing in long term plans and visions.  I’m bullish on both and view them both as compelling replacements to cable and legacy TV consumption. Netflix has better content portfolio and is worth the $7.99 I pay each month. Amazon provides some new shows but I’m a Prime member for the free 2 day shipping. I’d love to see the numbers but I suspect there’s a large overlap between households that have Netflix and those that have Prime. The real competition is existing cable networks that are going to get punished as the younger cord-cutter generations move out of their parents’ homes.",1,2,2016-04-18,6,"amazon, netflix, stock market, media, tv, cable",304,"Irrationality of the markets: Amazon up, Netflix down"
24,0,Given the hubbub around bots I decided to write a super simple one that works with Telegram and responds with random blog posts.,#code,"{% include setup %} A combination of bots being in vogue and Telegram offering $1M in [bot prizes](https://telegram.org/blog/botprize) got me to spend a little bit of time writing a bot last week. To get my feet wet I created a simple, self-serving bot that would reply with a random blog post when sent a /blogme command. The code itself is extremely straightforward and most of the time was spent going through the Telegram bot docs and getting the deployment and HTTPS setup. A nice feature that Telegram has is the ability to write a bot that can respond to both polling and webhooks. The polling approach is a much trivial to get started with since you don’t need to worry about any of the devops work and can work on the core interaction. The cons are that it won’t respond immediately and you need a way to track messages your bot has already replied to. Changing it to a webhook provided real time responses but made it a bit more difficult to test and wrapping everything inside a minimal web framework. The biggest hiccup was the requirement of HTTPS for a webhook integration but [Let’s Encrypt](https://letsencrypt.org/) made it simple to get up and running. A year ago I wouldn’t have bothered prototyping anything that required HTTPS but these days it’s incredibly easy to set up. The [code is up](https://github.com/dangoldin/bots) on GitHub and if you’re interested in bots definitely take a look. And if you have Telegram installed try messaging “danblog” with /blogme to get a random blog post.",3,1,2016-04-23,4,"telegram, bot, messaging, let's encrypt",263,A Telegram blog bot
35,0,It's easy to get carried away and start rewriting all parts of your code. Don't do this and treat it like an ill patient that requires multiple transplants - do one transplant at a time.,#management,"{% include setup %} When working on new features it’s easy to keep increasing scope until you end up doing a full rewrite of your code. Don’t. It’s healthy to refactor code as you go but you need to be wary of how many things you’re changing and the risks those changes carry. Code will get stale unless it’s constantly maintained and updated as the rest of the product evolves but trying to change too much at once will make it difficult to diagnose issues and increase the odds of bugs in production.  The analogy is that of an extremely sick patient. That person may need a variety of transplants but it’s dangerous and stupid to replace multiple organs at once. Instead you should find the most critical one to replace and do that. After the body adjusts to that transplant you move on to the next most critical one. Otherwise the body will go into shock and reject the organs.  Bad code is similar to this patient. There are countless things that can be improved but if it’s doing a critical job keeping a product alive you need to treat it carefully. Replacing everything at once may end up working but more likely it will cause a slew of problems that will be tough to diagnose given the various changes. It’s much better to approach code like a sick patient - make a change, release, and monitor to make sure everything is going well. Once you’re confident that the code is functioning as expected you can move on to the next most critical item. Over time you end up replacing the critical components while reducing risk.",0,1,2016-04-27,4,"engineering management, coding, software development, software engineering",277,Avoid full body code transplants
32,0,Despite being bullish on tech I tend to manage as much as I can through text files. They're extremely powerful while providing the flexibility to change to suit my evolving needs.,#meta,"{% include setup %} As many people know despite being bullish on tech I’m spartan and utilitarian with my technology usage. This expresses itself as a strong bias for text above nearly another format. There are tons of apps that try to help me organize my tasks and todos but I prefer simple text files and an intelligent folder structure. This is true when it comes to blogging as well - rather than using a fancy CMS or hosted application I rely on Jekyll which exposes my content in Markdown.  On the surface this seems inefficient - why build your own tools when perfectly good apps exist that will be maintained and improved over time? Unless I spend a ton of time there’s no way I’m going to be able to build a blogging platform that competes with Medium or Wordpress nor will I ever make a to do application that is better than Todoist, Wunderlist, or Google Calendar.  For me it’s less about the tool and more about the problem. Sure, a tool helps with that but I’m more about figuring out a process that works for me. Despite how great an app is it’s extremely unlikely that it will change to accommodate my evolving needs. Having my own process optimized around text gives me the flexibility to do things my way as well as easily change both the process and the underlying data.  Just last week I realized that I forgot to add metadata to a few of my blog posts. Had the content been squirrelled away in a web app there’s no way I would have been able to easily find which posts were affected other than writing a crawler and examining the DOM. But having everything in simple structured Jekyll text files made it as simple as writing a simple command line regular expression to identify these posts. And this can easily scale to any other blog maintenance task I have - whether it’s adding some additional information to a subset of posts or just searching for various words or phrases.  The success of this system depends on building out and committing to a structured approach when dealing with text. Text is innately extremely flexible but by imposing a semi-structured system of tags and folder structures it makes it extremely easy to navigate and manage. And if anything does change it only requires a small script to update everything to fit the new format. Replication is also simple - I can either keep it in a version control system or have it synced via Dropbox. If you’re undisciplined or have a static workflow definitely leverage an existing tool but if you’re constantly trying to improve your system and want the ability to go back and analyze content you produced there’s not much better than text. It unlocks the power of the command line while giving you the option to write whatever esoteric script you need to solve your own problem. And if you do want to export your data anywhere else it can be as simple as turning your simple, semi-structured text into an API request to whatever service is in vogue at the moment.",0,1,2016-04-30,5,"text, todo list, calendar, blogging, utilities",528,Text is king
29,0,Google's done an incredible job analyzing our photos and making them easily searchable. What happens when every bit of digital content we produce is analyzed and data mined?,#meta,"{% include setup %} Last Friday, Fred Wilson [wrote a post](http://avc.com/2016/05/feature-friday-photo-search/) lauding Google’s photo search. I’ve had the same experiences. In the past couple of months I’ve made numerous searches without expecting a useful result but in nearly every case I was pleasantly surprised. Just in the past week I wanted to search for a short story I wrote while in middle school that I digitized at some point over the past few years. My first attempt was to search for “paper” which got me too many results to parse through. But for my second attempt I tried “essay” and was able to find a photo of one of the hand-written pages. It was simple to look at the date I uploaded that one page to find the others. A couple of days ago I was out of town but needed my passport information to fill out an online government form. Turns out that I have a photo of my passport on my Google account - I backed it up years ago as I was traveling so I had proof in case anything happened to it.  On one hand I’m clearly impressed by how accurate the searches are but does make me worry about how much information we inadvertently share that can be indexed. It’s hugely convenient now but it’s impossible to predict the future and see how it will be used. I wonder how many photos we’re currently sharing that we assume are indiscernible to these automated systems. The vast majority are safe for now but given the pace of technological progress I’ll be shocked if software isn’t more accurate and faster than humans in 20 years. And photos are just a small piece of the puzzle - every bit of digital content we produce will be data mined until it can’t reveal any more. All this will benefit us in the short term but I wonder the world will look like when everything we produce can and will be analyzed and understood by machines.",1,1,2016-05-08,5,"google, photo search, machine learning, ai, google photos",340,Google's photo search is eerily incredible
35,0,Keep a database clean is underrated. One of the easiest things to do is to drop unused tables. I have a few tricks I've picked up over the years to amke this process easy.,"#sql,#code","{% include setup %} When writing code it’s very easy to accumulate deprecated database tables that end up as zombies - they’re still around and may even be populated and used by a variety of side scripts but if they disappeared and the dependent code was removed nothing would be different. In fact you’d have a smaller code base, a smaller database, and would hopefully improve everyone’s productivity a tiny bit.  Dealing with the tables are are still being populated and read requires a bit of investigative work and knowledge of the product since there’s no simple way of identifying them. But there are a simple ways to identify tables that are no longer updated.  ### 1. The metadata Some databases provide a last updated flag as part of the metadata tables. For example, MySQL contains an update_time field inside the information_schema.tables table for MyISAM tables. Reading the MySQL documentation it also looks as if recent versions will have this set for some InnoDB tables as well.  ### 2. The temporal columns In the case where there’s no metadata for a table you have to resort to a bit of trickery. If your table has any form of a time column then you can write a very simple query - **select min(timestamp), max(timestamp) from table** - to spot the most recent data in a given table. If this date is old you may be able to safely assume that this table is no longer being populated or maintained. Combining this quick trick with data from the informatino_schema.columns table and you can write a very neat query that can run this check across the entire database. For example, you can first run **select table_schema, table_name from information_schema.columns where column_name = 'timestamp'** to identify every table that contains the timestamp column. Then you can automate the creation of a monster query that will generate a checking query for each of the tables and then union them all together. So then you end up with something akin to **select 'table_1' as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_1 union all select 'table_2' as table_name, min(ymd) as min_ymd, max(ymd) as max_ymd from table_2 union all...** The query may take a while depending on the indices but once it does you can quickly sort by the max timestamp to quickly spot the potentially unused tables. A small adjustment you can make to deal with tables that may still be getting populated is to look at the number of rows that exist by day. If you see a huge decline it can be a good indicator that this table may just be getting some noise from an older job and is safe to remove, but only after removing the deprecated job.  ### 3. Snapshot and monitor But what do you do if there’s no metadata and no timestamp column? Ideally you’d have created and updated timestamps in every table. If not you can either add these to be automatically set and see whether anything changes over time or you can just take a manual snapshot today and a few days or weeks later to see whether anything changed. If the table is too large you can compare the number of rows or some summary statistics. The general idea is to compare it against multiple periods of time to see if, and how much, it’s changing.  These are just a few tricks I’ve picked up over the years trying to keep database schemas clean. Most companies do a good job managing the deployment process when generating new tables and writing new code but it’s rare to find companies that tend to their database garden. I believe maintaining a clean database is underrated - it’s valuable to know that everything in your database is used and that you don’t have to worry worry about an obscure script touching an obscure table you’ve never heard of. I’d love to know if people have other tips that can be used to both keep, and get, a database clean.",0,2,2016-05-11,5,"sql, mysql, database cleanup, database schema, metadata",671,Identifying unused database tables
1,0,,#design,{% include setup %}         Wanting to avoid a busy lunch rush but hankering for Chipotle I decided to download their app to order ahead. It’s a straightforward app and everything went as expected until I had to enter the expiration date for my credit card. The way the app is set up is that you’re expected to choose the month first followed by the year. Unfortunately it prevents you from picking a month in the past. One can probably guess what problem this leads to: if the expiration date is in the future but the expiration month is before today’s month the app rejects the month change until you change the year. The screenshot illustrates the design.  I tend to be more passionate about usability issues than most - especially ones that are obviously wrong and trivial to fix. I suspect in this case in the desire to make the user experience better by not allowing a user to select a date in the past it actually had the opposite effect and decreased the usability.,0,1,2016-05-14,5,"design, usability, date selection, user experience, chipotle app",196,Expiration date selection design anti pattern
18,0,I'm running an experiment and will cross post to Medium in order to compare the engagement numbers.,#meta,"{% include setup %} Despite my [aversion](http://dangoldin.com/2014/02/02/why-i-manage-my-own-blog/) to walled gardens and platforms I’ve seen a ton of people make the switch to [Medium](https://medium.com/). Within the past month I’ve seen a variety of bloggers move over to Medium, both big and small: [Mark Suster](https://bothsidesofthetable.com/finding-a-new-medium-aa0f882815d#.s4y1c45ky), [Semil Shah](http://blog.semilshah.com/2016/04/30/medium-rare/), [Andrew Parker](http://thegongshow.tumblr.com/post/143602596745/corporate-governance-dictatorships-vs-democracy), and a former coworker, [Dillon Forrest](https://medium.com/@dillonforrest). I’m still not convinced that Medium is for me but it definitely feels as if it’s at that inflection point with more and more people moving to Medium. And from what I’ve heard it does wonders for reach and promotion - something that I’ve been relying on Google search and Twitter for.  To that end I’m going to try an experiment and start publishing on Medium ([https://medium.com/@dangoldin](https://medium.com/@dangoldin)) as well as on my primary blog. The goal is to experiment with Medium and see how much engagement it can actually drive. To start I’m going to copy some of my posts over to Medium and see how they fare.  So far, one of the nice things about Medium is that it comes with a simple API that allows you to take either Markdown or a subset of HTML and turn into a Medium post via a quick API call. In fact, earlier today I wrote a [small script](https://github.com/dangoldin/medium-tools) that that takes the raw Jekyll markdown and posts it as a draft to Medium. It won’t work on every single post yet but for the ones that are pure markdown it works perfectly (example: the [original](http://dangoldin.com/2016/05/11/identifying-unused-database-tables/) vs on [Medium](https://medium.com/@dangoldin/identifying-unused-database-tables-f1e969039f6c#.1n6p1g1jw)).",10,1,2016-05-15,2,"medium, blogging. content",287,Experimenting with Medium
27,0,I got my hands on a dump of IMDB data and wrote up the process required to get it into a state that's useful for analysis.,"#sql,#code,#data","{% include setup %} In 2012 I did a [simple analysis of IMDB](http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/) to analyze the change in actor and actresses’s ages over time. At that point I limited the analysis to the top 50 movies each decade and hacked together a quick script to crawl and scrape the IMDB analysis. A couple of weeks ago I came across a great [post by CuriousGnu](https://www.curiousgnu.com/imdb-age-distribution) that did a similar analysis across a larger set of movies but limited to movies since 2000. I reached out and they were kind enough to give me a DigitalOcean instance containing the data already loaded into MySQL. The analysis should be finished up tomorrow but I wanted to write this post up to share the mundane parts of the process. The janitorial part is critically important to an analysis and it’s important to get it right or the results will may be meaningless or even completely wrong. The [NY Times interviewed](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0) a variety of data scientists and came away with the conclusion that 50 to 80 percent of a data scientist’s time is spent cleaning the data. This is no exception and I wanted to provide a sense of the effort and thought that goes into getting data into a state that’s actually useful.              Lucky for me I already had the data loaded and queryable in MySQL. Most of the time the data is scattered all over the place in a variety of different formats that require a slew of scripts to wrangle and manipulate the data into a useful format.  The first task was to get familiar with the data and I started by looking at sample rows from each of the tables. The table names were descriptive but it turned out that some of them were empty. Running a query that calculated the size of each provided a good idea of where the valuable data was - for my analysis the useful data lived in the title, name, cast_info, and person_info tables.  {% highlight sql %} SELECT TABLE_NAME, table_rows, data_length, index_length, round(((data_length + index_length) / 1024 / 1024),2) ""Size in MB"" FROM information_schema.TABLES WHERE table_schema = ""imdb"";{% endhighlight %}              The next step was figuring out the way the tables related to one another. Since the field names were obvious this was extremely straightforward. The only nuances came due to an unconventional naming scheme - for example the title table contains the list of movies but the other tables map to it via a movie_id column. Similarly, the name table contains people but it’s referenced via person_id in other tables. They key part here was starting with a movie I know and confirming that the results made sense. In my case I chose my favorite movie, The Rock, and made sure that the results of my query made sense.  {% highlight sql %}select * from title t join cast_info ci on t.id = ci.movie_id join name n on ci.person_id = n.id where t.id = 3569260;{% endhighlight %}  After getting a feel for the data it was time to actually think about the data necessary for the analysis. To see what was possible I examined the person_info table which contains a variety of information about each person - anywhere from birth and death dates, to spouse, to various names, to height. In my case looking at the birth and height gave me some ideas but I needed to extract these to make them useful. I ended up creating a table for each one and writing a series of queries to populate each one. This required looking at the format of the data in each of the rows and leveraging various combinations of the locate, substring, and cast commands to transform the text fields into something numeric. The birth date was straightforward since it came in two styles - one was just a year and the other was the full birth day with day and month.  {% highlight sql %}insert into person_birth     SELECT person_id, cast(info as UNSIGNED)     FROM person_info     WHERE info_type_id = 21     AND length(info) = 4;  -- Birthdate is full date so just take the year insert into person_birth     SELECT person_id, cast(substring(info, locate(' ', info, 4) + 1, 4) as unsigned)     FROM person_info     WHERE info_type_id = 21     AND length(info) > 4;{% endhighlight %}  Height was a bit more difficult since it came in a variety of formats. Some were in centimeters, while others were in feet, while others were in feet and inches, with a small fraction having partial inches. Each of these required a complicated series of MySQL commands to convert to inches.  {% highlight sql %}insert into person_height     SELECT person_id, cast(replace(info, ' cm','') as unsigned) * 0.393701     FROM person_info     WHERE info_type_id = 22     AND info like '%cm';  -- No inches insert into person_height     SELECT person_id, substring(info, 1, locate('\'', info) - 1) * 12     FROM person_info     WHERE info_type_id = 22     AND info not like '%cm'     AND info not like '%/%'     AND info not like '%""%';  -- No fractional inches (would also work for no inches but playing it safe) insert into person_height     SELECT person_id, substring(info, 1, locate('\'', info) - 1) * 12 + substring(info, locate('\'', info) + 1, locate('""', info) - locate('\'', info) - 1)     FROM person_info     WHERE info_type_id = 22     AND info not like '%cm'     AND info not like '%/%'     AND info like '%""%';  -- Fractional inches insert into person_height     select person_id, cast(base_height as decimal) + cast(numerator as decimal)/cast(denominator as decimal)     from (     SELECT person_id, info, substring(info, 1, locate('\'', info) - 1) * 12 + substring(info, locate('\'', info) + 1, locate('""', info) - locate('\'', info) - 1) as base_height,         substring(substring(info, locate(' ', info, 5) + 1, 3), 1, locate('/', substring(info, locate(' ', info, 5) + 1, 3))-1) as numerator,         substring(substring(info, locate(' ', info, 5) + 1, 3), locate('/', substring(info, locate(' ', info, 5) + 1, 3)) +1 ) as denominator         FROM person_info         WHERE info_type_id = 22         AND info not like '%cm'         AND info like '%/%'         AND info like '%""%'     ) temp;{% endhighlight %}  Finally it was time to dive into the data. The first query I decided to write was to look at the average age of actors and actresses by year. Writing the query and doing a quick explain caused me to add a few indices to improve the performance but even then it still took over 20 minutes to execute. Having used Vertica and Redshift in the past I knew a columnar database would help but I wanted to keep it free. This led me to [MonetDB](https://www.monetdb.org/).  Somewhat remarkably, installing and setting up MonetDB was a breeze but I had a two hiccups migrating the data. One was creating the equivalent tables in MonetDB which had a slightly different syntax from MySQL and required a bit of trial and error to work through. The other was the actual export of data from MySQL in a way that was also easy to load into MonetDB. I ended up settling on a CSV export that also took into account the various ways to delimit, escape, and enclose the different fields. After getting the migration to work on one table it was just a series of copy and pastes to get the other tables over.  {% highlight sql %}-- MySQL export select * from title into outfile '/tmp/title.csv' fields terminated by ',' enclosed by '""' escaped by ""\\"" lines terminated by '\n';  -- MonetDB import COPY INTO title from '/tmp/title.csv' USING DELIMITERS ',','\n','""' NULL AS '\\N'; {% endhighlight %}  I had no experience with MonetDB and didn’t know what to expect with this entire series of steps being a waste of time. I expected some improvement and it turns out the query that took over 20 minutes to run in MySQL was able to run in just over 30 seconds in MonetDB. I was off to the races. I spent the next bit of time QAing the data and dealing with outliers and edge cases. Some were due to mistakes I made - for example not filtering cast members to only include actors and actresses which manifested itself in an actor that lived to be over 2000 years old. This turned out to be a movie about [Socrates](http://www.imdb.com/title/tt1560702/) with one of the writers being Plato. Some simply uncovered weird data - there's a movie, [100 Years](http://www.imdb.com/title/tt5174640/), which is scheduled to be released in 2115 and led to some old actors and actresses. While others were clearly data mistakes - actors who were born after they died, for example [Walter Beck](http://www.imdb.com/name/nm2917761/) who was born in 1988 but passed away in 1964.          Dealing with these was an iterative process. I ended up settling on removing all non actors and actresses from the queries as well as limiting my dataset to movies produced between 1920 and 2015 while also eliminating all combinations where a movie was produced before a birth. These edge cases are infrequent enough that they most likely wouldn’t have had any impact on the results but going through this process gives us confidence in what we’re doing. The next step is actually going through the analysis which I hope to finish up tomorrow.  If you’re interested in the code, it’s up on [GitHub](https://github.com/dangoldin/imdb); and if you’re interested in the data contact me and I can share a snapshot of the DigitalOcean instance that contains the data in both MySQL and MonetDB.",8,3,2016-05-21,6,"sql, data analysis, imdb, movies, actors, actresses",1629,Analyzing IMDB data: Step 1 - Cleaning and QA
29,0,The results of analyzing the IMDB data in order to compare actors and actresses. The metrics include looking at age and height and how they change over time.,"#data,#code,#dataviz","{% include setup %} After getting the [IMDB data loaded](http://dangoldin.com/2016/05/21/analyzing-imdb-data-step-1-cleaning-and-qa/) it was time to dive in and start looking at the data. In 2012, I did an [analysis](http://dangoldin.com/2012/05/23/trend-of-actor-vs-actress-age-differences/) to examine the way actor and actress ages have changed over time. Unfortunately I did not have a large dataset and had to write a quick crawler to look at the top 50 movies during each decade. This time around, and with the [help of CuriousGnu](https://www.curiousgnu.com/imdb-age-distribution), I was able to get my hands on a much larger dataset. After cleaning and filtering the data I was left with over 208,000 unique actors (~65%) and actresses (~35%) spanning over 371,000 movies. The code is up on [GitHub](https://github.com/dangoldin/imdb) and contains both the queries used to pull the data from MonetDB, the R code to generate the charts, and a small script that generated the animation below. If you have suggestions or ideas definitely let me know.                              A replication of  CuriousGnu's chart  as a sanity check to make sure the data was loaded correctly. As one can guess, actresses skew younger compared to actors with an average of 34.6 compared with 41 for actors.                                       The previous chart examined the distribution across the entire date range but we can see how this shift occurred over time. Before the 1940's actors and actresses were much closer in age. Another interesting point is that both actors and actresses have been getting older on average. One theory is that this is a function of the movie industry being new at the beginning of the 20th century with very few actors and actresses at the start that have aged along with the industry. Another reason may be lack of accurate data prior to the 1940s in the IMDB dataset which skews the results toward more recently-born actors and actresses.                                       Similar to the above but focused on actors and actresses that have appeared in at least 100 movies. The goal here was eliminate some of the noise and focus on the high volume actors and actress. This tells a similar story to the previous chart.                                       Combining both the distribution and trend over time we can look at the distribution changes over time. This also highlights the bias in the early years - in the 1920s it looks as if no one was older than 30 according to the IMDB data. After some digging around it's due to the lack of birth dates for many of the early 20th century actors and actresses. For example, for movies produced in 1920 we have close to 19,770 actor/actress movie combinations but only 1,060 (~5%) with a birth date. For 2010 the respective numbers are 269,645 and 52,262 (~19%). This causes our distribution to look heavily truncated but ends up correcting itself once we get into the 30s and 40s. In this case the average ages are inaccurate until the 1940s but I suspect the relationship between the genders still holds.                                                        This is just a timelapse of the data above that makes it much easier to see the shift of the average actor getting older at a faster pace than the average actress.                                       In addition to birth date the data also contained the height so I decided to have some fun and see how that looked. This is just a plot of actor and actress height by year of production. My takeaway is that actor heights stayed roughly flat while actress heights have been increasing. Note that since I only had a single height for each person this wouldn't be able to accurately represent children growing up but I imagine those are a small fraction and wouldn't influence the results.                                       This is an interesting one. Instead of looking at the heights by movie production year this examines heights by birth date of the actor and actresses. In this case we see that actors have stayed roughly the same height while actresses have increased in height over time. There's also a huge looking drop at the end - going from a bit over 70 inches to less than 65 for actors and from 65 inches to less than 63 for actresses. This drop off is in the late 90s which also indicates these are teenagers just growing up.                                       For the last one I wanted to get a sense of whether actors are more likely to be in more movies than actresses. The chart here is a bit tough to read but it looks at the distribution of actors and actresses by the number of movies made. in this case the scale was massive since there were tons of people who've only been in a few movies so I had to normalize by taking the log. The effect is subtle but the fact that the tail for actors goes wider than the tail for actresses indicates that an average actor is more likely to appear in multiple movies than the average actress.",5,3,2016-05-22,5,"data analysis, data visualization, actors, actresss, imdb",1047,Analyzing IMDB data: Actors vs actresses
22,0,I don't understand why Uber is pursuing self driving cars so aggressively. It only carries risks and hurts their compettive advantage.,"#meta,#society","{% include setup %} Self driving cars are inevitable and yet I’m surprised by how aggressive Uber is in contributing to the space. Uber is winning right now due to massive network effects. For most drivers and passengers Uber is the primary option and they only switch when Uber is either in surge if you’re a passenger or if you’re a driver when no passengers are available. Self driving cars eliminate half of the market. They won’t need to balance multiple apps on their phones and won’t need to go back and forth trying to find a passenger. It will all happen behind the scenes and do a much better job than any human would. They’d be as likely to work with Uber as any of their competitors. In fact, the entire protocol may evolve to be open with owners setting up their cars to start picking up and dropping off passengers when they’re not in use. The equivalent of how you can sell electricity back into the grid without having to do a ton of extra work. Imagine being able to own a car and just let it roam so it starts earning.  It’s unclear why Uber is driving this change - self driving pose a risk and diminish their competitive advantage. Maybe the outcome will eliminate individual car ownership and Uber wants to own a fleet of these cars. In that case pushing for this result makes sense but carries a world of risks - why wouldn’t car manufacturers both produce the car and have it part of a fleet? The other option is that they accept it’s not ideal but feel as if they have no choice since if others achieve it first they’ll be in an even worse position. Or maybe Uber does think they’ll own the market by the time self driving cars are a reality and at that point no one else will even bother to compete.",0,2,2016-05-28,2,"uber, self driving cars",322,Uber and self driving cars
19,0,I've bene blogging actively since 2013 and wanted to share a few of the small highlights I've experienced.,#meta,"{% include setup %} I started working on a project to investigate my blog posts and see how my writing has evolved over time. I’m still working on it and will definitely write up the results but the entire process got me thinking about my blog and some of the highlights. I started blogging to improve my writing, improve my thinking, and grow my personal brand. Despite being a large time commitment I enjoy doing it and there have been a variety of small episodes that have made it even better:  - In 2013 I wrote a [short post](http://dangoldin.com/2013/04/12/why-dont-cellphones-have-a-dialtone/) with an excerpt from a book I was reading about the lack of a dial tone in cell phones. This took off on Hacker News and ended up being covered in [Gizmodo](http://gizmodo.com/5994589/why-your-cell-phone-doesnt-have-a-dial-tone), [Mental Floss](http://mentalfloss.com/article/50185/why-don%E2%80%99t-cell-phones-have-dial-tones), and even made an appearance in the NY Times tech ticker. - I built a small community. I have a small number of repeat visitors who will comment on the occasional post and I actually ended up meeting up with a frequent contributor, [Ted](https://twitter.com/tedder42), when I visited Portland for the first time. - When Turo was called RelayRides I did an [analysis](http://dangoldin.com/2015/06/07/finding-the-optimal-car-to-list-on-relayrides/) to figure out the optimal car to get that will generate the biggest return. This led to a few people reaching out and me doing a little bit of consulting work to analyze their market. - I’m a big fan of Citibike and came up with a [small web app](http://dangoldin.com/citibike-station-directions/station-to-station.html) to that translates every New York City trip into a walk to a Citibike station, a station to station bike ride, and then a walk to the final destination. After posting this a few people reached out to ask whether a smartphone app was available as well as ideas to make it even better. I unfortunately haven’t had the chance to work on it but it’s great seeing people finding value in something I’ve done. - Cities opening up their data and I had some fun visualizing the [routes of Jersey City’s garbage trucks](http://dangoldin.com/2015/12/12/jersey-city-garbage-truck-routes/). This led to me connecting to our councliwoman who then introduced me to the head of Jersey City’s tech innovation team. - An interesting one was when a journalist from [FiveThirtyEight](http://fivethirtyeight.com/) reached out to ask about an old GItHub project I was working on. This ended up not leading anywhere but did provide a glimpse into modern journalism and the desire to highlight and surface content from the tail. - You know you’ve made it when you have “SEO experts” reaching out and either offering their site optimization services or a payment to post an article with a link to another site. I’ve received dozens of offers so far but haven’t accepted any yet!  These are just the highlights and at this point I’m happy to receive any inquiry. None of these have been massive but they’re all small highs that are a reminder that what I write is being read. Their lack wouldn’t stop me from blogging but it’s always nice to receive a surprise comment or email.",8,1,2016-06-05,3,"blogging, branding, personal brand",545,Blogging: The small wins
20,0,My blog analysis is taking a while but I discovered a simple way of generating word clouds in R.,"#meta,#code,#dataviz","{% include setup %} Analyzing my blog is taking longer than expected but my goal is to have something meaningful over the weekend. In the meantime I wanted to share a [quick script](http://www.r-bloggers.com/building-wordclouds-in-r/) I discovered to generate a word cloud in R. I remember doing this years back in D3 and having to spend a bunch of time figuring it out. Compared to that doing it in R is a breeze. In this case I have a CSV dump of my blog in /tmp/out.csv and am generating two word clouds - one for keywords and the other for tags of my blog posts.  {% highlight R %} # Install and the libraries install.packages(""tm"") install.packages(""SnowballC"") install.packages(""wordcloud"")  library(tm) library(SnowballC) library(wordcloud)  # Read the file df = read.csv(""/tmp/out.csv"", header=TRUE)  # Now generate the two word clouds. # Most of the work here is removing the unncessary and common words # as well as optionally stemming each of the words. In my case since # I'm plotting the keywords and tags I ignore this step.  corpus     Word cloud of the tags I use.       Word cloud of the keywords I use.",1,3,2016-06-06,3,"word clouds, R, data visualization",311,Word clouds in R
23,0,I received an offer for a website optimiziation offer and decided to take them up on it and see where it went.,#meta,"{% include setup %} I’ve been getting a stream of offers to help “optimize” my site and decided to follow through with one and see where it went. The general pitch is to call out existing errors and problems and offer a service to help fix the variety of errors and improve my search ranking. Here’s the text of the most recent email:     Dear business owner of dangoldin.com,   How is it possible that your website is having so many errors? Yes, most of the people share their anger and frustration once they get my email.   Now, I will show you the number of broken links, pages that returned 4XX status code upon request, images with no ALT text, pages with no meta description tag, not having an unique meta description, having too long title, etc., found in your dangoldin.com.   I have a large professional team who can fix all the above issues immediately at an affordable price. I guarantee you will see a drastic change in your Google search ranking once these are fixed.   If this is something you are interested in, then allow me to send you a no obligation audit report.   Best Regards,   XXXXXX     Clearly this is not personalized as every mention of dangoldin.com can be replaced with another domain and have the same effect. The language doesn’t feel natural and is awkward but the author does include a series of technical words and phrases to showcase his knowledge. I wonder if they have A/B tested the hell out of different copies and ended up coming up with this. I recall reading that Nigerian scammers purposely use non-standard English as a way to identify even better marks. If they wrote in perfect prose they’d end up luring many more people into the top of their funnel that would end up backing out later. Much better to get a smaller set of people hooked that have a higher conversion rate.  The day after my reply I received a PDF titled “ Website Analysis for dangoldin.com .” It’s surprisingly well-fleshed out and contains a series of best practices and stats that my site is ranked on. It has the obvious ones such as number of pages indexed by Google as well as some esoteric ones, such as whether it’s listed on “DMOZ.” The analysis ended with a search ranking plan as well as the pricing page with 3 potential plans ranging from $300 to $900 a month. My gut is that this was a dual effort between code and humans with the bulk automatically generated and a human polishing it up. I’m confident that the human component was outsourced given the language and the fact that the firm has presence in India. Generating this was probably cheap but not insignificant and does make me wonder what their conversion rate is. The $300 price point seems high but is in line with the website optimization services out there so maybe these guys have figured out their customer acquisition model.",1,1,2016-06-11,3,"spam, site optimization, marketing",518,Following up on a website optimization offer
1,0,,,"{% include setup %} I started actively blogging in 2013 and have been consistently writing 2 posts a week. There’s a ton of information here and I spent some time learning R all over again in order to analyze and visualize my blogging history. I started with a simple [Python script](https://github.com/dangoldin/blog-analytics/blob/master/analyze.py) that went through each post and dumped it into a CSV file with a series of columns that would be easy to [analyze via R](https://github.com/dangoldin/blog-analytics/blob/master/analyze.R). The columns ranged from numeric stats - such as how many words, tags, images, and links - to the actual text of the post itself. The goal was to put in a structured enough shape that the rest of the analysis could be handled in R. I started by collecting some summary statistics and looking at them over time but got carried away and ended up digging deeper into my evolution as a blogger.  Some high level stats to start it off:  - 412 total posts with 54 of them before 2013 - 725 total links - 537 total tags - 1,379 total keywords - 9,705 total words in the meta descriptions - 145,499 total words of content                              As mentioned I started actively blogging in 2013 so there's no surprise here.                                       Given that I've written the same number of posts in 2013, 2014, and 2015 it looks as if my posts have gotten shorter and shorter.                                       Similar to the point above - I'm sharing fewer and fewer links.                                       Yet I'm still tagging the posts at roughly the same rate. This makes sense since I'll do anywhere from 1 to 3 tags per post.                                       By month there's a bit more noise due to vacations but am keeping pace with 2 a week.                                       Nothing obvious here.                                       Just for fun but this is the total number of words by week. I also did this by day but it was even noisier.                                       Clearly I write more during the weekend. Note that I had to prepend a number to the day of week to get the sort working.                                       Similarly, the number of words is also higher on weekends.                                       Another way to look at it is to see the distribution by year. In 2013 I was actually pretty on-point with my Tuesday/Friday writing schedule but since then have regressed to mostly writing on the weekends.                                       The same information as above but switching the X and Y axes. I find this one not as easy to interpret as the previous one.                                       This examines the various companies I mentioned over time. Google's dominant and it looks as if I haven't written about microsoft since 2014. You can also see the rise of Uber and Snapchat.                                       Looks as if 2015 was the year of languages with Python and JavaScript dominating the others.                                       Word cloud of the various tags I used on my posts. Clearly I like engineering and startups.                                       Tag wordcloud for 2013. All about startups and design here.                                       Tag wordcloud for 2014. This gets deeper into technology with strong representation by AWS, devops, coding, as well as a variety of programming languages.                                       Tag wordcloud for 2015. Welcome to engineering management. In 2015 I developed into a manager and start writing about the various lessons I've learned on the journey.                                       Tag wordcloud for 2016. Nothing significant yet and looks like a pretty healthy mix of the prior years. We'll see how this looks after the year is over.",2,0,2016-06-12,1,,964,Analyzing my blog
29,0,Some people are making the case that WeWork deserves it's massive valuation and I just don't see it becoming the foundational layer the way AWS and Stripe have.,#product,"{% include setup %} I have been subscribed to [Stratechery](https://stratechery.com/) for almost a year now but have recently started listening to the [Exponent podcasts](http://exponent.fm/). One of them, titled [Pickaxe Retailers](http://exponent.fm/episode-071-pickaxe-retailers-2/), makes the case that WeWork has an appropriate valuation due to their ability to leverage their strong brand and become the utility layer for real estate as well as provide a slew of products to their tenants. Similar to the way AWS has eliminated the need to run your own data center and Stripe has eliminated the need to acquire merchant accounts and negotiate with vendors, WeWork may do the same for physical space - both commercial and [residential](http://www.fastcompany.com/3055325/from-wework-to-welive-company-moves-members-into-its-first-residential-building).  While the explanation is reasonable it’s tough for me to buy into it. The decision to use any product boils down to how easy is it to switch and what’s the cost/revenue potential. In the case of AWS it’s incredibly costly to switch. You have to incur the cost of updating your code and deployment to make sure it will run on a new platform, retraining your team, and if you plan on switching to your own datacenter then hiring for roles you’ve never had to deal with. Added to this you have Amazon constantly cutting costs while innovating on new products. The value in switching only comes at massive scale - even Dropbox is getting beaten up over their move away from Amazon instead of focusing on building a more compelling product.  Stripe is in a similar situation. Despite providing a seemingly simple service it’s difficult and costly to replace. Stripe contains customer data and has a slew of products, for example subscriptions, that make it tough to switch. Imagine having to ask every customer to re-enter their credit card information. At the same time, Stripe gets cheaper and cheaper as your volume increases which makes it less and less compelling to replace.  I just don’t see these sort of arguments holding true for WeWork. AWS and Stripe both run services that start of cheap and get even cheaper as you scale. They’re both unbelievably sticky and have a growing cost of switching. WeWork has neither of these. The actual office space may be great but over time it gets easier to make the decision to rent your space directly rather than pay WeWork’s margins. WeWork does provide additional services that make their space great when you’re small. Unfortunately, these same services can easily be outsourced as you grow. For unlimited coffee you go with [Joyride](http://www.joyridecoffeedistributors.com/service/page/cold-brew-iced-coffee-kegerators-coffee-kegs/). For office cleaning and maintenance you go with [Managed by Q](https://managedbyq.com/). For food you can go with one of the hundreds of delivery startups. Every service WeWork provides can be had via a separate company..  Picture a small company starting out on AWS, using Stripe, and renting an office at WeWork. As they grow it’s easy to imagine them still using AWS, still using Stripe, but no longer at WeWork. Netflix is the perfect example. They’re a public company with a current market cap of just under $41 billion. Yet they’re still on AWS. And Amazon is a competitor! I can’t imagine any public company using WeWork as their primary office space solution.",6,1,2016-06-18,6,"aws, stripe, wework, coworking space, scale, business",558,"AWS, Stripe, and WeWork"
13,0,I wrote two blog posts via phone and it went surprisingly well.,#meta,"{% include setup %} A few weeks ago I had to run some errands at the mall and ended up having some free time. I was also a few blog posts behind so decided to see how much I could actually do via phone. Surprisingly, I got a fair amount done. The posts still required a fair amount of editing when I was back on my computer but for getting the bulk of the content and structure down on my phone was nearly as good as via a real keyboard. What it lacked in speed it made up for by not having real multitasking which made it more difficult to get distracted. It wouldn’t work for posts that require search or significant research but for quick blurbs or jotting down thoughts it works remarkably well and I suspect it will only improve with time. Years ago I viewed phones and tablets as being purely designed for consumption rather than creation so this has been a pleasant surprise and I’m coming around to the idea that one can be productive without an actual computer. Next is to try attempting to write a blog post via voice dictation.",0,1,2016-06-19,4,"blogging, phone, smartphone, productivity",197,Blogging from my phone
14,0,Messaging apps are extreemly fragmented and it's fascinating to think about the space.,#product,"{% include setup %} The messaging space is fascinating. There are probably hundreds of apps available with pretty massive fragmentation. Onavo collected the following data to indicate the reach of the various messaging apps by country and while WhatsApp (owned by Facebook) is clearly dominant there are some countries that WhatsApp is a fringe player, especially among Asian countries.       via TechCrunch    It’s shocking how dominant the local companies are in Asia. WeChat is the behemoth in China. In Japan it’s Line. And in Korea it’s KakaoTalk. I don’t know whether it’s as simple as nationalism or that the local companies just have a much better understanding of the market and were able to build better products.  I just view messaging apps as utilities.There’s no need to restrict myself to a single app and I use them reactively. If someone messages out via iMessage I’ll use that. If someone uses WhatsApp I’ll use that. And so on. If a friend asks me to use a particular app I have no problem downloading it and giving it a shot. They have limited network effects and there’s no reason to restrict yourself to one.  I suspect most people feel the same way. They probably have an app that’s the goto with their most frequently messaged group but if they’re part of another group that has their own principal app there’s nothing stopping them from using it.  The most dominant apps will be the ones that are able to leverage them to become utilities. Tencent has built a massive business on top of WeChat which acts as the digital hub in China. WeChat is not just for messaging but is essentially the operating system for mobile in China. It can be used to interact with a litany of services in china - including payment at physical stores, booking ridesharing services, and serving as an authority on identity. Nothing like this exists in the US or Europe and it’ll be interesting to see what comes out.",1,1,2016-06-22,4,"messaging, wechat, facebook messenger, whatsapp",356,Messaging app fragmentation
25,0,Brendan Eich launched a new browser that's taking a new approach to advertsing. It's definitely faster but it's going to be a tough journey.,#product,"{% include setup %} Trying to launch a new browser seems like a fool’s errand and yet if there’s anyone that can do it it’s [Brendan Eich](https://en.wikipedia.org/wiki/Brendan_Eich), who in addition to creating JavaScript also ran Mozilla. Given his pedigree I decided to give his new browser, [Brave](https://brave.com/), a shot. It’s definitely a bit on the rough side compared to the mainstream browsers but it’s surprisingly fast. The speed improvement comes from a built in adblocker rather than having it implemented via slower browser extensions. At the same time Brave wants to pay publishers for their content by partnering with higher quality advertisers in order to serve benevolent ads that should also be priced at a premium.  The difficulty with this approach is that tracking users is what makes the advertising so valuable. Being able to track users allows advertisers to see what users care about, their purchase intent, as well as a whole slew of demographic information based on their consumption behavior. Eliminating this will cause advertisers to be shooting in the dark. There’s a reason Google and Facebook are eating up nearly 80% of every advertising dollar - they’re leveraging their data to provide extremely targeted and effective advertising that will be tough to do without the ability to track users.  I can see the case that Brave can centralize the tracking and allow users to opt into sharing this data with various partners. The challenge is getting advertisers on board with this as they would have to trust Brave for their reporting and getting users to opt in to this. I know very few people who’ve used adblock and then decided to switch back to a full ad experience. Brave has a tough road ahead.",2,1,2016-06-23,4,"brave, adblock, browsers, Brendan Eich",290,The Brave browser
29,0,Chinatown produce vendors are able to sell their produce much cheaper than the alternative despite smaller scale. They've set upa system designed for high turnover and low prices.,#product,"{% include setup %} The Wall Street Journal had a [great piece](http://www.wsj.com/amp/articles/why-fruits-and-veggies-are-so-crazy-cheap-in-chinatown-1466762400) on why produce is so cheap in Chinatown. The conclusion:       Her discovery: Chinatown’s 80-plus produce markets are cheap because they are connected to a web of small farms and wholesalers that operate independently of the network supplying most mainstream supermarkets.     Most of the city’s fruits and vegetables come from wholesalers at the Hunts Point Produce Market, the South Bronx distribution hub boasting all the color and accessibility of La Guardia Airport. Chinatown’s green grocers, in contrast, buy their stock from a handful of small wholesalers operating from tiny warehouses right in the neighborhood.     Because the wholesalers are in Chinatown, they can deliver fresh produce several times a day, eliminating the need for retailers to maintain storage space or refrigeration, said Ms. Imbruce.     I love this. It runs counter to the common belief that cheaper prices can also be achieved through massive scale. Yet in what I suspect is one of the hardest industries, food distribution in NYC, small scale seems to be doing the better job. Produce has an extremely short shelf life and combined with the cost of real estate in NYC it must require some incredible management to be able to sell it for the half the price of produce found at the supermarket. And everyone involved ends up winning - consumers get cheap prices and a great selection, the stands are able to turn around a ton of inventory due to the low prices, and the farms benefit from the variety of crops they’re able to grow.  This is a perfect example of being able to build a successful business by focusing on activities that complement each other (à la [Michael Porter](https://hbr.org/1996/11/what-is-strategy)): they have their own local wholesalers that get constant replenishment that can then be priced incredibly cheaply which encourages high turnover and feeds back into the need for quick replenishment. This also allows them to focus on produce that doesn’t need to be kept on the shelf as long and is expected to be sold and eaten within a short amount of time. They embraced the idea of “making it up in volume” by setting up every activity to drive that goal.",2,1,2016-06-26,5,"chinatown, startups, produce, michael porter, five forces",390,Low cost at small scale
16,0,I believe Snapchat will be huge if they keep executing the way they have been.,#product,"{% include setup %} The more I use Snapchat the more obvious the potential. The way the product has evolved reminds me of Facebook’s history. Facebook started simply as a profile page for Ivy League college students but due to strong execution and brilliant product decisions has grown into the current behemoth. Snapchat is on a similar path - the initial version was a simple ephemeral photo sharing app but the recent updates seem frequent and massively impactful.            Movie tickets within Snapchat     Earlier today I was messing around and spotted a movie trailer ad for Swiss Army Man that was followed by an option to swipe up to buy the movie tickets that felt native to Snapchat. I didn’t have to click to go to another app and it felt natural to just swipe to get to the next step. And this was a simple case of buying movie tickets. I can imagine this flow expanding to other scenarios that follow a powerful ad with an immediate transaction. This echoes WeChat - the powerhouse app in China that’s an unholy mix of a [social network, a payments platform, and an app ecosystem](http://a16z.com/2015/08/06/wechat-china-mobile-first/). There’s no equivalent of WeChat outside of China and I suspect most think the replacement will look like a WeChat clone. Snapchat feels completely different yet has the potential to be more. WeChat’s foundation is a third party app ecosystem that’s built on top of text while Snapchat is almost entirely visual and asserts a high bar for third party experiences.  This is huge. Interacting with Snapchat is a joy, whether it’s taking photos, playing with the filters, checking our your friends’ stories, or watching the Discover videos and this is due to the masterful job they did with the interactions. They’re not immediately obvious but once discovered are intuitive and consistent across the variety of experiences. Want to go next? Just tap. Want to dig deeper? Swipe up. Want to go back? Swipe down. This is incredibly powerful. By learning these shortcuts Snapchat is able to offer a variety of adventures that users can easily engage with without taking up any additional screen space. This allows every experience Snapchat offers to take up the full screen which keeps us in the moment and makes it easy for us to keep going.  Snapchat is already taking the baby steps of becoming a platform by enabling external parties to build on top of Snapchat. The obvious case are the content producers Snapchat is partnering with but a more telling example is the way they’re approaching geofilters. [Geofilters](https://snapchat.com/geofilters) are created offline, are then uploaded to the Snapchat site, and after approval become accessible in the app. This is a foreshadowing of the Snapchat formula - build out a compelling in-app experience and then follow it up with tools for outsiders to craft their own.  The movie trailer ad can be extended to highlight products - a compelling video of a product that can then be followed up with an option to buy. This can extend into multi-touch - imagine being able to tap on different sections of the screen that drive different experiences. If I’m watching the Olympic trials I can tap on the different players to get some more information about each one or if I’m watching some NBA highlights I can tap on LeBron’s jersey or sneakers to get taken to the Snapchat-integrated Nike Store. And this is just scratching the surface - by focusing on new highly engaging user experiences and setting up the tools to create these compelling stories Snapchat can transition into an incredibly powerful platform.  Snapchat should have no problems monetizing. The advertising industry can be broken down into two major types - brand advertising and direct response. Brand advertising is the typical TV ad that’s focused on building awareness and selling a story and lifestyle. Direct response, on the other hand, is about getting the customer to “convert” and requires every dollar spent to back out into a measurable return. Think of Google Adwords - you search Google for a book, click the sponsored Amazon link, and then buy it - Amazon will then know how much you paid for the book as well as how much the click cost. Using this data they can then optimize their campaigns to maximize profit. Snapchat may be able to sit at that intersection. It’s the perfect platform for high quality brand videos that take up the full screen but can also drop down into transactions - think of my movie trailer/ticket experience earlier today. This will help them get closer to the holy grail of advertising by attributing purchases to brand advertising.  Snapchat is tiny compared to Facebook but major shifts in the tech world have never been direct. New platforms start at the fringes but keep growing until they supplant the incumbents. Google superseded Microsoft by becoming the entry point to the web. Facebook is supplanting Google by bypassing the open web and providing app experiences on every platform. How would Snapchat unseat Facebook? I don’t know but I’m sure Facebook’s watching.",2,1,2016-07-03,3,"snapchat, facebook, google",863,Snapchat's massive potential
28,0,Good code is just like a physical system with a high potential energy - it can quickly turn into kinetic energy to solve a variety of problems quickly.,"#management,#code","{% include setup %}        Potential energy : the energy of a body or a system with respect to the position of the body or the arrangement of the particles of the system.     Dictionary.com              Kinetic energy : the energy of a body or a system with respect to the motion of the body or of the particles in the system.     Dictionary.com       I’m constantly striving to discover new ways of thinking about code and my latest is thinking about it through what many of us learned in high school physics - potential and kinetic energy. The definitions are above but a simple way to think about it that potential energy is what your system is capable of while kinetic is exercising that option. One can look at code the same way. Code that has a high potential energy can be turned into a vast amount of kinetic energy that can deliver new products and features at an amazing pace. This is code that is well architected and tested and is designed in such a way that it can be easily modified to handle whatever it comes its way. Code with low potential energy, on the other hand, is poorly designed with small changes leading to unintended side effects such that most of the time is spent fixing the code up. The comparison here between a rocket and an old, rickety car is appropriate. The rocket expends the bulk of its energy in minutes and travels hundreds of miles. The car breaks down every couple of miles and requires a skilled mechanic just to keep it going for another few miles.  But in physics we have the law of the [Conservation of Energy](https://en.wikipedia.org/wiki/Conservation_of_energy) stating that energy can’t be created nor destroyed. This also applies to code! Taking code with a high potential energy and quickly modifying to solve a need may reduce its potential energy. In this case it’s up to us as developers to exert effort to bring it back up its high potential energy state.  This metaphor is an obvious exaggeration but it does strike at what makes for good code and something we should all strive to write. It’s not about being brilliant or elegant or simple but about being flexible enough to support whatever the world throws at it and it’s up to us to keep it at that level. To keep pushing the physics - every new feature adds [entropy](https://en.wikipedia.org/wiki/Entropy_(order_and_disorder)) to our codebase and unless we actively clean it up it only gets worse.",4,2,2016-07-04,4,"coding, software engineering, design, architecture",437,Maximize the potential energy of your code
29,0,My mortgage got transferred to a new servicer that offered a worse experience. Why do we have no say in something that will affect us for multiple decades?,#society,{% include setup %} I haven’t seen much written about how consumer protection relates to a product’s user experience but it’s a topic that’s worth exploring. I was reminded of this when my mortgage loan was sold to a new servicer. I came home to find a letter in the mail notifying me that my loan has been sold and that going forward I’d have to use a different payment portal and system. It was simple enough to register but the payment process became less efficient and there was no support for a Mint integration.  This is clearly a first world problem and there are a lot of benefits that come with being able to buy and sell loans. It’s the foundation of our financial system and allows companies to specialize across the entire loan business - some are designed for loan origination while others focus on servicing. This also encourages companies to improve their loan valuation models since if they’re able to identify an arbitrage opportunity they can trade on it and profit.  At the same time it’s frustrating that as a consumer I have no say in what happens and it’s a commitment made on my behalf for multiple decades. I don’t know what the right answer is here. User experience is highly subjective and what works for me may not work for someone else and products should hopefully improve over time yet I think there is something here. As an engineer the simple answer would be to force every consumer facing company to expose all functionality and data via an open API which would allow any experience to be crafted around it but I can’t imagine that actually happening. And maybe none of this will actually matter since AI will get to the point where we won’t need to interact with any of these services directly.,0,1,2016-07-09,3,"consumer protection, ux, loans",309,Consumer protection for UX
36,0,Earlier today I was catching up on Euro Cup and encountered a content not available in your country error message. This is Sloppy and Twitter needs to do a better job with their flagship product.,#product,"{% include setup %}           I’m a huge Twitter fan so it’s especially frustrating when I encounter issues. The latest one was discovering a ""This content is not available in your country"" message when trying to catch up on some Euro Cup highlights in a moment. I understand that in today’s digital rights world there’s always a chance for some content to be unavailable but there’s no reason it should have been included in Twitter’s flagship product that’s supposed to attract and engage new users. The fact that it’s manually curated makes it even worse - how could this have slipped through? One explanation is that the curator was not based in the US and had access to the video. The other is that the video was available initially but was pulled later on. In both cases Twitter should have had the appropriate safeguards to identify this was happening and amend the moment. An even better approach would have been to have different versions of the moment depending on the user's location. The current implementation just feels sloppy and I can’t stand to see it in a product I love using.",0,1,2016-07-10,2,"twitter, product",211,A Twitter Moments fail
34,0,It used to be a best practices to automatically redirect users to the logged in version of a site but I've noticed two sites that are forcing the homepage to be seen first.,#product,"{% include setup %} When I started building sites one of the accepted principles was to give customers what they want as soon as you can. This manifested itself by taking users to the logged in view whenever they navigated to the site’s homepage. This makes sense - if you know a user’s logged in why waste their time by showing them a homepage that’s designed to sell the product?  Yet recently I encountered two sites, [Greenhouse](https://www.greenhouse.io/) and [Tallie](https://tallie.com/), that will default to the homepage and only load the logged in view when I click the sign in link. One argument is that they both have separate domains for the logged in experience - app.greenhouse.io rather than www.greenhouse.io and usetallie.com rather than tallie.com - but there’s nothing stopping them from redirecting to those as soon as they recognize that a user is logged in.  One explanation is that they’re using different domains for the landing page versus the app but it’s still odd. Greenhouse can set cookies at the wildcard domain (*.greenhouse.io) and Tallie can make the necessary redirect or client side check to see whether a user is logged in. In fact if you actually go to usetallie.com first it will redirect you to tallie.com which, after clicking on “Client Login”, will take you back to usetalie.com.  Another explanation is that they want to save on hosting costs and serve a purely static webpage at first. This way they don’t need any dynamic content and only need to have the dynamic logic for when a user wants to login. This seems like a reach though - these are both enterprise apps and can’t possibly have the traffic load to warrant this degradation of the user experience. Even then the cost of doing a simple login check should be enough for any modern web application to handle.  The last explanation I can think of is that there’s something on the homepage that they want every user to experience. And the only thing that would make sense is tracking and advertising. One potential reason is that the content is so sensitive that they either legally can’t or just don’t want to drop third party trackers inside the app yet still want the ability to target and track users who’ve landed on the home page. A preliminary look using Ghostery bears this out - the homepage for Tallie drops 20 trackers while the in app page drops 6, most of which are for analytics. For Greenhouse it’s not as direct with the homepage dropping 17 while the in app page drops 13, most of which are advertising related. If this is the case I’m disappointed, but not surprised, that user experience was sacrificed to drop some third party JavaScript trackers.                              Ghostery trackers: Tallie home + in app, Greenhouse home + in app               I’m searching for other explanations but can’t think of anything that else that would encourage this “anti-pattern” to make a comeback. If anyone has any ideas I’d love to hear them.",2,1,2016-07-16,6,"user experience, login, signin, homepage, greenhouse, tallie",543,Whatever happened to automatic login?
52,0,A neat coding puzzle I heard is to find the shortest path from one word to another where you're only allowed to change a single letter at every step and each step needs to be a valid word. I spent some time this morning writing the code to make it happen.,#code,"{% include setup %} A fun engineering puzzle I heard this week was to write an algorithm that finds the shortest path between two words of the same length where you’re only allowed to change a single letter each step and every word needs to be valid. This morning I decided to have some fun with it and wanted to jot down my thought process going through the exercise in the hope that it provides a bit of perspective on how I approach code.  The first step was to just do an example in my head to visualize the problem. I started with two short words, dog and cat, and went through the manual transition. The optimal solution is where each letter changed is the final letter - in the case of dog to cat it was simply dog -> dot -> cot -> cat. Now that I had a baseline (and a test), I decided to dive into the actual code.  The immediate realization was that since this was asking for the shortest path I’d need to do a breadth first search, something I haven’t had to touch since some early job interviews. The other realization was that the graph would need to be constructed on the fly. With these two in mind I dove right in.  I broke the problem down into three parts - one was loading the dictionary, two was writing a function that would get the “adjacent” words, and three was doing the search itself. The first function was straightforward since I just loaded in the built in OS X dictionary:  {% highlight python %} def load_dictionary(path = '/usr/share/dict/words'):   dictionary = set()   with open('/usr/share/dict/words', 'r') as f:     for line in f:       dictionary.add(line.strip().lower())   return dictionary {% endhighlight python %}  While thinking about the adjacent word function I thought back to [Peter Norvig’s spell checker](http://norvig.com/spell-correct.html) and remembered how simple yet powerful it was (if you haven’t seen it yet you should take a look - one of the most elegant code examples I’ve seen). All his code needed was a tiny tweak to filter the list of generated words to those in the dictionary.  {% highlight python %} def adjacent_words(word, alphabet):   splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]   replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]   return [r for r in replaces if r in dictionary] {% endhighlight python %}  Now it was time to do the actual search which took me a bit of time. I knew the theory but it took me a bit of time to translate it into code. And even then I wasn’t happy with how it looked so ended up finding a pretty simple [Python implementation](http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/).  {% highlight python %} def bfs_paths(source, target, dictionary, alphabet):   queue = deque(((source, [source]),))   while queue:     v, path = queue.popleft()     for n in [w for w in adjacent_words(v, alphabet) if w not in set(path)]:       if n == target:         yield path + [n]       else:         queue.append((n, path + [n])) {% endhighlight python %}  The last part was cleaning up the code and improving its efficiency. The key parts here were using string.lowercase as the universe of letters, replacing a standard list with a collections.dequeue to significantly speed up the “pop” operation, and making the dictionary and alphabet variables locally scoped. As a final test I ran through the dog to cat example and got two additional transformations: dog->cog->cag->cat and dog->cog->cot->cat. The complete code is below but note that I left it open-ended so it will print every path it finds rather than just the shortest one.  {% highlight python %} #!/usr/bin/env python  import string from collections import deque  def load_dictionary(path = '/usr/share/dict/words'):   dictionary = set()   with open('/usr/share/dict/words', 'r') as f:     for line in f:       dictionary.add(line.strip().lower())   return dictionary  # Peter Norvig's spellcheck code is amazing: # http://norvig.com/spell-correct.html # Just use the replace part of it def adjacent_words(word, alphabet):   splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]   replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]   return [r for r in replaces if r in dictionary]  # Had to remember how to get this working again # Took a bunch from http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/ def bfs_paths(source, target, dictionary, alphabet):   queue = deque(((source, [source]),))   while queue:     v, path = queue.popleft()     for n in [w for w in adjacent_words(v, alphabet) if w not in set(path)]:       if n == target:         yield path + [n]       else:         queue.append((n, path + [n]))  if __name__ == '__main__':   alphabet = string.lowercase   dictionary = load_dictionary()    for x in bfs_paths('dog', 'cat', dictionary, alphabet):     print x {% endhighlight python %}",2,1,2016-07-17,3,"word transformation, coding puzzle, breadth first search",812,Coding puzzle: Word transformation through valid words
31,0,Given that the modern world allows zero distribution costs and global reach brands are incredibly important and companies should be leveraging them as much as they can in their growth.,#product,"{% include setup %} Pokemon Go is huge right now. People across all age ranges, demographics, and geographies are getting involved and it’s hard to imagine this sort of adoption for any other game. What I find fascinating is that Pokemon Go was based on the same augmented reality mechanics as Ingress, another game developed by Niantic Labs. Ingress has a loyal following but pales in comparison against Pokemon Go when looking at the user numbers, despite Pokemon Go being less than 2 weeks old while Ingress has been around for almost 4 years.  The Pokemon brand has a huge following and it’s incredible what a strong brand can do in the modern world. With zero distribution costs and instant global reach an existing brand can grow faster than at any previous point. Now more than ever do brands matters. Modern technology has made it easier than ever to enter new markets and quickly launch apps but in this world of commoditization and heavy competition a strong brand can do wonders.  The companies best positioned to take advantage of this new world are the ones with strong intellectual property and brands that can leverage whatever innovation comes along. Right now it's augmented reality but in the future it will be something else. The business will be adapting new innovations that allow companies to magnify and enhance their brands.  This model reminds me of the pharmaceutical industry. I started my professional career working for a pharmaceutical consulting company which gave me a crash course in how the industry works. One of the more interesting insights was that the biggest advantage large pharmaceutical companies have is their sales force rather than their R&D. This allows them to just acquire small biotech companies for their newly developed drugs and have their own sales force selling it. This approach makes sense - you find what you’re great at and focus on applying that as much as you can. This is what Nintendo is doing with Pokemon Go and every brand with global scale IP should be doing.",0,1,2016-07-22,5,"Pokemon go, branding, marketing, business, intellectual property",343,Double down on your brand and IP
41,0,Our location tells a lot about us and is actively used in advertising but rather than being treated as a single point in time it should be treated as something dynamic that grows and keeps providing more information about us.,#product,{% include setup %} Targeting is one of the best ways to improve the return on an advertising campaign. By identifying potential customers you're able to focus your advertising on them rather than someone random. And one of the best simplest ways is to set up your advertising campaigns to focus on a specific geography. Maybe your product is only sold in the United States and advertising it elsewhere is a waste. Or maybe your product is sold everywhere but the messaging and copy needs to vary by region. Or maybe it's sold everywhere with the same exact copy but the price varies by region. Being able to change your campaigns by geography is a simple way to improve the performance of any campaign.            Yet most geographic targeting is dumb. Earlier today I was on Twitter and noticed an ad for a Dodge Ram sponsored by Ram Trucks Canada. It's true that I'm on vacation in Canada but it's definitely not the case that I'll be buying a car in Canada. The solution to this isn't complicated. Every social network should have a good idea of my patterns and where home and work are. And if I'm outside those areas it should be easy to determine whether it's a quick trip out of town or a longer vacation. For all I know these platforms have this information but they should be exposing it to advertisers. Of course these inaccurate mismatches are a tiny percentage of the total advertising spend but it adds up and more importantly having more fleshed out profiles will improve the ad optimization.  Imagine being able to determine whether someone drives to work or takes the train. Every heavily used social network has the data to derive this but I suspect few have. We're already placed in various customer segments based on our behavioral and consumption history yet geography is still assumed to be the current location. I suspect the more advanced companies are using geographic information to craft better profiles but I'd love to see this opened up to advertisers.,0,1,2016-07-26,4,"advertising, targeting, geo targeting, adtech",364,Smarter geographic ad targeting
21,0,One of my favorite programs that had a huge impact on the way I approach code was Peter Norvig's spellchecker.,"#code,#meta",{% include setup %} While working on a [small programming puzzle](/2016/07/17/coding-puzzle-word-transformation-through-valid-words/) I remembered Peter Norvig’s [spell checker](http://norvig.com/spell-correct.html) and how blown away I was after seeing it for the first. It’s one of my favorite examples of code that’s clean and elegant while being extremely expressive and powerful. If you haven’t seen it yet I encourage you take a look and step through it since he does a much better job of explaining both the code and theory than I ever could.  I don’t want to attribute my improvement as a coder to a single program but this program forced me to think much deeper about the code I write and provided a glimpse of good code. It serves as a goal and pushes me to be more aware of the code I write and whether it’s as simple and expressive as it can be. It’s not easy but approaching development through a lens of self improvement has been instrumental in helping me become a better coder. Good programmers are never happy with the code they wrote a year ago which is a sure sign that they’ve improved over the past year. Dissatisfaction is what drives people to improve and code is no different. It’s rare to find code that’s shocking in its brilliance and I’d love to see more examples so please share.,2,2,2016-08-06,1,code,233,One of my favorite programs
41,0,As managers we need to make our teams as productive as we can be. To do that we need to figure out what to measure since that's a clear way of improving. But how can we find the appropriate metrics?,#management,"{% include setup %} A key part of management is getting out of the way and building out processes that help your team be as productive as possible. At the same time, you can’t change what you can’t measure. Combining these two makes it clear that to improve, whether people or process, you need to start measuring and tracking the appropriate metrics.  In software engineering, some things are easy to track: how many bugs there are, how quickly they’re resolved, how much code are written - but rarely tell the whole story and may lead to perverse incentives. The common example is measuring developer productivity through number of lines of code written: a smart developer would purposefully write verbose and long winded code to get their metric up.  Then there are the items that are hard to measure but actually drive productivity: improvement as an engineer, simple and expressive code, code that’s easily changed. These are incredibly difficult to measure, especially at scale, but if you’re able to focus on improving these you’ve found the holy grail.  By being creative it’s possible to come up with proxy metrics and approximations despite not being able to find easy ways of measuring the actual performance drivers. Think of these as traits that productive teams have and should be encouraged. There will always be exceptions and many are susceptible to gaming but they’re much better than nothing.  Besides the usual suspects (velocity, bugs, test coverage), I plan on tracking the following proxy metrics. Individually they don’t tell the whole story but taken together I hope they’ll be a good way to help improve the productivity of an engineering team. Note that a requirement for these was that they would be easy to collect, ideally automated.  - Pull request size: I believe pull requests should be as small as they can be. Larger pull requests are harder to code review and carry more risk. - Pull request file variance: Not a 100% sure about this one but I suspect there’s a difference in pull requests that are isolated to a small set of files rather than dozens. It may indicate that our code is not as cleanly laid out or architected as it should be and may be worth cleaning up. - Pull request activity: Another soft one but I want to see whether the amount of comments and changes a pull request has carries any meaning. I think junior engineers tend to have more feedback on their code versus more senior developers and measuring this may be a good way of discovering that. The challenge is that this one is easily gamed and we should all want to encourage discussion of code in order to come up with as high quality code as we can. - Deploy frequency: The more we deploy the more useful code makes it out into the real world and we should strive to deploy as often as we can while maintaining a high quality bar. We’re not at continuous deployment yet but hopefully this will help us get there.",0,1,2016-08-07,3,"engineering measurement, metrics, productivity",507,Engineering management: Measuring the unmeasurable
30,0,Each new cconsumer technology category eclipses the former. At the same time they get closer and closer to our bodies. The inevitable outcome is that we will become cyborgs.,"#meta,#society",{% include setup %} The iPhone is the top selling consumer product of all time and a variety of podcasts and articles makes it seem that this is the peak of consumer technology and we’ll never see anything as popular. This is shortsighted. Every new technology achieved wider and wider adoption and eclipsed the previous generation - [laptops eclipsed desktops and smartphones eclipsed laptops](http://ben-evans.com/benedictevans/2014/4/25/ipad-growth). One thing that’s clear is that each generation of tech gets closer and closer to us. Initially we were exposed to computers when we went into the office. Soon we started buying desktops for our homes. After that we decided we wanted laptops that we could carry around with us. Smartphones gave us the ability to carry computers around in our pockets with a full day’s worth of charge.  Smartwatches aren’t very popular now but cellular connectivity may make them even more popular than smartphones. The interactions and designs will need to improve to handle the novel form factor but that itself is an opportunity to get closer to to the senses other than sight. Smartphones we carry but smartwatches we wear. Beyond smartwatches we may end up with technology that gets us closer and closer to becoming cyborgs. At this point we get very close to sci-fi territory with chips that are implanted under our skins or technology that can interface directly with our brains. At that point I can’t even imagine what sci-fi novels will be - everything will seem possible.,1,2,2016-08-08,5,"society, cyborgs, consumer technology, tech products, iphone",251,Consumer tech leads to cyborgs
16,0,Updated my Yahoo fantasy football stats script to pull data for the 2016-2017 season.,#code,{% include setup %} This is an annual tradition now but I just updated my old script that crawls and extracts the projected fantasy football data from Yahoo to work with the 2016-2017 season. The changes were incredibly minor: Yahoo broke the the login page into two steps and there was a minor change in the order of the columns. Both of these were trivial to implement and the code is up on [GitHub](https://github.com/dangoldin/yahoo-ffl). If all you care about is the raw data you can just download the [CSV](https://raw.githubusercontent.com/dangoldin/yahoo-ffl/master/stats-2017.csv).  Every year I intend to use the data to come up with a drafting algorithm yet I’ve failed to do anything with it over the past couple of years. I’m hoping this year is different.,2,1,2016-08-13,4,"fantasy football, yahoo, fantasy sports, data",133,Fantasy football stats: 2016-2017 edition
31,0,Open Source is incredible but integrating undocumented libraries is tough. Seeing how others have used it through GitHub search is a simple way to see how it should be used.,#code,"{% include setup %} Open source is great: if you find the right library you’re able to save a ton of time and get code that’s been through the gauntlet that you can confidently incorporate into your system. Unfortunately many open source libraries are partially baked with documentation that doesn’t always accompany the rapid development of the code. This leads developers to repeatedly cross reference their code with some archaic documentation and then wonder why it’s not working as expected. This is proportional to the obscurity of the library - popular libraries will have most of their kinks worked out but esoteric ones that are likely maintained by one person won’t have the polish.  Yet it would be nice to take one of these libraries and build off of it. The simple answer is to reach out to the maintainer and ask questions. I always get excited when someone reaches out with a question about how to use one of my open source libraries; I’m not at that scale where this is burdensome and it’s encouraging that someone is actually using my code. When this doesn’t work a neat trick is to [https://github.com/search](search GitHub) for usage of that code. Most documentation provides a simple starting tutorial and assumes the user can take it from there. More often than not this doesn’t work well and you have to look at the source code to understand how the code works, what arguments the methods expects, and the order in which they should be called. By looking at actual usage of the code you can see how others have integrated these libraries in actual applications rather than toy examples. This works incredibly well for open source libraries in that middle area where they’re not popular enough to have everything figured out yet are useful enough to have had numerous developers wrangle them into their code. Many new and popular libraries fall into this bucket so if you want to use code that’s just becoming popular leveraging GitHub’s code search is a great way to start.",1,1,2016-08-14,5,"open source, documentation, free software, libraries, code",343,Integrating poorly documented Open Source libraries
25,0,"Turo doesn't provide a way to download your ride history so I wrote a scraper that does it via Python, Chrome's WebDriver, and Selenium.",#code,"{% include setup %} I've been using [Turo](https://turo.com/) to rent our car out for the past couple of months and have been using a simple spreadsheet to track the revenue. Being a lazy engineer doing this manually became a bit tiresome so I finally automated it. Unfortunately Turo does not have a simple way of downloading the data and there’s no open API so I had to resort my usual solution: [scraping](https://github.com/dangoldin/turo-automation). Luckily for me I just came off of updating my Yahoo fantasy football scraping script and was ready to do the same for Turo.  The entire process took a few hours and the [result](https://github.com/dangoldin/turo-automation) is decent - it goes through every one of your completed trips and scrapes the receipt page for the total paid, total earned, the various reimbursements, and the start and end times. As of this writing it still doesn’t handle cancelled trips or trips that have not yet been taken. Another thing I noticed when writing the script is that Turo changed the representation of a trip - some of the older receipts had reimbursements in a different section from the newer ones so that needs a bit of tweaking. I’m sure there are some other edge cases I’m not handling properly since I could only code against the data I have; if it ends up not working for you let me know and I’ll see what I can do.  The process to build the scraper was standard: use Chrome’s source inspector to examine the structure of the page and then try using a few different selectors in an interactive Python section running [Selenium](http://www.seleniumhq.org/) to see whether they worked as expected. Once I had the various selectors and code figured it out it took a little bit of refactoring to get into a somewhat clean state.",4,1,2016-08-21,4,"turo, download history, car rental, relayrides",308,Downloading your Turo ride history
30,0,A neat idea is to write every scraper you code as an API. This provides a nice separation of concerns and turns boring scraping problems into interesting engineering challenges.,"#code,#meta","{% include setup %} While building the [Turo scraper](http://dangoldin.com/2016/08/21/downloading-your-turo-ride-history/) I became annoyed that there was no API to make my job significantly easier. Then I wouldn’t have had to go through a variety of hoops and iterations to get the data I needed and would also not have to worry about changes to their page design breaking the script. This got me thinking about an idea to write my scraper in such a way that it’s exposed as an API. In that case I can architect the code so that the retrieval and manipulation of the ride data is completely separate from the scraping code. Then if and when Turo does decide to release an official API all I’d need to do is swap my unofficial implementation out for the official one.  This chain of thought led to me to the challenges of building this on the engineering side. There’s something neat about being able to specify a bit of data through a series of steps. For example, to get the details for a ride the steps may be: 1) login to Turo, 2) navigate to that ride’s receipt page, 3) parse the details, 4) return them as JSON. Another API endpoint may be to retrieve all the rides. This one would be 1) login to Turo, 2) navigate to the first page, 3) fetch all the rides, 4) if there’s a next page, go to it and repeat step 3, otherwise 5) return the list of rides as JSON. For almost every request the first and last steps will be the same but the intermediate step will vary. This becomes even more interesting since we can now start to think about caching the results at the intermediate levels so you can avoid the steps if you’ve already done them in the past. This way we’re incrementally building a “shadow” version of the site and use that for everything we need but keep augmenting it when needed.  Pushing this further we can imagine a scraping specific language that represents the steps involved during a scraping session. The goal here is to replace the code that does the DOM traversal and instead come up with a cleaner and more expressive way that can be applied through code. Sometimes the application will be going to our cache but other times it will require actually navigating to the appropriate page.  I’m excited to try this approach out since it turns a rote scraping exercise into a higher order solution that can scale to other scraping jobs. I only wish I thought of it sooner since by the time I went down this rabbit hole I was mostly done with the actual code so I’ll have to give this a shot on the next scraping job.",1,2,2016-08-24,6,"scraping, apis, domain specific language, dsl, dom manipulation, meta programming",468,Writing scrapers as APIs
28,0,"I set up wallabag, an Instapaper alternative, earlier today and it got me thinking about open source and how it doesn't work well for cross platform projects.",#meta,{% include setup %} I’ve been a happy Instapaper user for years but the news that it was being acquired by Pinterest got me thinking about some alternatives. Not because I have anything against Pinterest; in fact I think this is a great fit and they’ll be able to complement each other but because it’s a reminder that no third party product is guaranteed to last and I wanted to see what open source alternatives are out there.  I discovered [wallabag](https://www.wallabag.org/) and got it setup earlier today. The documentation to install and get it running was incredibly straightforward and I was able to get it operational within an hour. Unfortunately it took a bit of wrestling to understand the various configuration options and I’m still unable to get it working across both the web and an iPhone. There’s a series of steps you need to do - from generating a unique RSS token to setting up an oAuth application that make it difficult to just get up and running. I understand that it’s designed for developers and offers a ton of customization but it should be simpler to get get the base installation - every user would want an extension to easily add articles and a way to access them offline on a phone and automatically generating the necessary settings would make it much easier to get started.  Trying out an open source alternatives is an eye-opening experience. You don’t realize how much polish it takes to build something usable. We love claiming that we can build anything in a day but it’s the relentless polishish that makes a successful product. I suspect this is why it’s incredibly hard to find open source products that require a cross platform approach. It’s difficult to think of successful open source applications that span across multiple environments. That requires multiple developers each agreeing on a unified vision and then making sure each of the components fits together. This is a tough combination and may be why so many popular open source projects are incredibly focused: it’s a lot easier to get multiple people working on a single product when it’s simple and they all share the same pain. But as soon as the scope expands there’s no single vision holding everything together and it shows in the final product.  The nice thing about open source is that anyone can add functionality and I’m already thinking of ways to improve wallabag. Hopefully I’ll have some time over the next few weeks.,1,1,2016-08-28,4,"instapaper, wallabag, open source, cross platform",419,Giving wallabag a shot
19,0,I tried using Google's Cloud Vision to identify the food in my fridge. It didn't go very well.,"#code,#meta","{% include setup %} Something that I haven’t quite figured out is how to avoid wasting food. I like to think I keep good track of everything in my fridge but too often I end up finding something in the corner that spoiled and needs to be thrown out. Earlier today I was talking to someone at the office about this problem and how nice it would be if you could just have something that knows everything that’s in the fridge and can track how long it’s been there and an estimate of how long it will last. I’m sure refrigerators in 10 years will have this built in but I wanted to see what I could cobble together in an evening.  Luckily for me Google released a Cloud Vision API and I decided to give it a shot. Turns out implementing it was extremely straightforward, despite Google’s poor documentation, with a quick code search on GitHub that led to me [https://github.com/ramhiser/serverless-cloud-vision](https://github.com/ramhiser/serverless-cloud-vision). Unfortunately, the results were not promising. I ran on three images and while the categorization was surprisingly accurate it was too general. I expected to at least accurate identification for the bottles and cans - milk, ketchup, yogurt but the closest it got was food, ice cream, and gelato. Granted, the photos weren’t staged well and it took me about 15 minutes to get it working but I was still disappointed. The Cloud VIsion service doesn’t offer much customization so I’m going to see how much better I can make it by improving the photos. I’ve included the original photos along with the classification results below. As usual my code is up on [GitHub](https://github.com/dangoldin/fridge-vision) although it was really just a straight up copy and paste from [ramhiser’s code](https://github.com/ramhiser/serverless-cloud-vision) above.                              {% highlight json %}[         {           ""score"": 0.90114909,           ""mid"": ""/m/02wbm"",           ""description"": ""food""         },         {           ""score"": 0.88251483,           ""mid"": ""/m/02phwj2"",           ""description"": ""display window""         },         {           ""score"": 0.81870794,           ""mid"": ""/m/0cxn2"",           ""description"": ""ice cream""         },         {           ""score"": 0.76996088,           ""mid"": ""/m/0270h"",           ""description"": ""dessert""         },         {           ""score"": 0.75129372,           ""mid"": ""/m/02fz11"",           ""description"": ""gelato""         },         {           ""score"": 0.69974077,           ""mid"": ""/m/02rfdq"",           ""description"": ""interior design""         },         {           ""score"": 0.57035172,           ""mid"": ""/m/02q08p0"",           ""description"": ""dish""         },         {           ""score"": 0.54961139,           ""mid"": ""/m/0191_7"",           ""description"": ""retail store""         },         {           ""score"": 0.53331912,           ""mid"": ""/m/031bff"",           ""description"": ""window covering""         },         {           ""score"": 0.51523668,           ""mid"": ""/m/01_bhs"",           ""description"": ""fast food""         }       ]{% endhighlight %}                                     {% highlight json %}[         {           ""score"": 0.87785435,           ""mid"": ""/m/07yv9"",           ""description"": ""vehicle""         },         {           ""score"": 0.78110605,           ""mid"": ""/m/0k5j"",           ""description"": ""aircraft""         },         {           ""score"": 0.77443254,           ""mid"": ""/m/0cmf2"",           ""description"": ""airplane""         },         {           ""score"": 0.7131173,           ""mid"": ""/m/02pkr5"",           ""description"": ""plumbing fixture""         },         {           ""score"": 0.71218145,           ""mid"": ""/m/015y8h"",           ""description"": ""jet aircraft""         },         {           ""score"": 0.66169083,           ""mid"": ""/m/06ht1"",           ""description"": ""room""         },         {           ""score"": 0.5944497,           ""mid"": ""/m/01lgkm"",           ""description"": ""recreational vehicle""         },         {           ""score"": 0.54411179,           ""mid"": ""/m/041x_j"",           ""description"": ""public toilet""         },         {           ""score"": 0.53394085,           ""mid"": ""/m/017_cz"",           ""description"": ""major appliance""         }       ]{% endhighlight %}                                      {% highlight json %}[         {           ""score"": 0.78413528,           ""mid"": ""/m/02phwj2"",           ""description"": ""display window""         },         {           ""score"": 0.647836,           ""mid"": ""/m/02rfdq"",           ""description"": ""interior design""         },         {           ""score"": 0.59498113,           ""mid"": ""/m/0c_jw"",           ""description"": ""furniture""         },         {           ""score"": 0.57692927,           ""mid"": ""/m/0191_7"",           ""description"": ""retail store""         },         {           ""score"": 0.54954523,           ""mid"": ""/m/08790l"",           ""description"": ""boutique""         }       ]{% endhighlight %}",3,2,2016-08-29,4,"computer vision, google, cloud vision, food identification",612,Food identification with Google's Cloud Vision
26,0,Social networks each have their own differentiated offering so it's interesting to see people sharing and posting items that bleed from one to the other.,#meta,{% include setup %} Social networks carry extreme network effects and have massive winner-take-all dynamics. This makes it impossible for two social networks that have the same pitch to co-exist and leads to pretty strong differentiation. Facebook owns relationships. Twitter owns interests. Instagram owns lifestyle. Snapchat is starting to own experience. This is why I find it fascinating when the content from one social network or medium bleeds into another. Twitter doesn’t allow for tweets longer than 140 characters so people overcome that by sharing screengrabs of long form text. I’ve seen the same on Imgur - it’s primarily used for images but often you’ll see someone posting an image of a long story. We have our own unique relationships across each of these networks so it’s not surprising that we’ll sometimes want to communicate something that’s best expressed with a specific medium yet it’s still fascinating seeing it in action. I get the feeling that they’re publicly exploiting a loophole and adding a tiny bit of chaos to the universe.,0,1,2016-09-01,1,social networks,174,Violating the norms of a social network
24,0,I just finished migrating my blog to AMP and am happy with the outcome. The site's snappier and feels much better on mobile.,#meta,"{% include setup %} Ever since AMP was announced I’ve been meaning to migrate my blog but hesitated due to the fear that it would take an inordinate amount of time and would be laden with edge cases. But over the Labor Day weekend I decided to give it a shot and see how far i could get. A quick GitHub search showed two promising repos - [amp-jekyll](https://github.com/juusaw/amp-jekyll) and [amplify](https://github.com/ageitgey/amplify) - and I gave them both a shot. They approach AMP integration in two different ways - amp-jekyll creates an AMP version of every post and has it live in a separate folder structure while amplify is a comprehensive theme. This made the amp-jekyll integration much easier since it’s designed to work parallel to the existing blog but I wanted to do a full rewrite.  I ended up cloning the amplify repository and manually importing a few of my blog posts to see how they’d render, handle the images, and look under the different style. After playing around with amplify I realized it would actually be straightforward to integrate directly into my blog as an additional theme. After copying over the design files and the libraries required to inline the SCSS I was left with making a few changes to the CSS to get it to resemble the previous design. All in it took a few hours to get my blog migrated to AMP and it’s incredibly quick - especially on mobile (if you haven’t given it a shot please do).  There are still a few things I need to take care of but I’m pleasantly surprised by the ease and simplicity of the transition and the resulting performance. The major problems I need to take care of before I call this a success:  - Disqus integration. I’ve been using Disqus to manage comments and it would be a shame if I had to ditch it. Based on a few StackOverflow and forum posts it looks as if it’s possible to get Disqus working by forcing an iframe with the comment section but I’ll have to figure out how this works. - Various styling fixes: Since I ended up starting with the amplify CSS there are a few inconsistencies that I still need to take care of - especially on some of my older posts that have some ugly inline CSS. - Img to amp-img: To be AMP compliant you cannot have any img tags and instead must use amp-img. This sounds straightforward but amp-img requires you to specify the dimensions of the image which I have not been doing. It looks as if there’s a plugin for this in amp-jekyll and I got it working locally but need to get it working on GitHub pages. - JavaScript heavy posts: I have a few older posts that depend on D3 for visualizations and I’m going to have to rewrite those posts to include the animations as amp-iframe elements. This seems straightforward but I’m sure I’ll run into some hiccups when I actually sit down to do this. - Build times take forever: This is my biggest issue so far. Since AMP requires all CSS to be inlined it means that every CSS change causes the entire site to be regenerated. Before AMPifying, Jekyll would build the site in around 10 seconds and now it takes nearly 2 minutes. I don’t have a good fix for this but a simple solution may be to have different CSS for the different types of pages to avoid a full site regeneration with every style change. While developing I solve this problem by moving all but a few posts out of the _posts folder in order to reduce the number of pages that need to be generated. Then when I’m happy with the outcome I’ll move the other posts back and let it go through the full generation. This is extremely hacky and I wish there was a better solution here.  I’d love to know what the readers of the blog think and whether they’re noticing any improvement so if you have any feedback please let me know. And I’m aware that I have yet to get Disqus set up to work with AMP but in the meantime let me know via [Twitter](https://twitter.com/dangoldin).",3,1,2016-09-05,5,"AMP, Google AMP, jekyll, blog design, mobile design",712,AMPifying my blog
18,0,A few weeks ago I scraped some fantasy football data from Yahoo and decided to visualize it to,#dataviz,"{% include setup %} In honor of the upcoming NFL season I thought it would be interesting to actually take a look at the scraped fantasy football projections and visualize it in a few different ways. The data contained the weekly projections for that week’s top 100 scorers which amounted to 1700 rows - note that this means the dataset only includes the top performers rather than every single player. I ended up using R since it makes it incredibly easy to process data and get some nice looking visualizations in only a few lines of code. As usual, the code is up on [GitHub](https://github.com/dangoldin/yahoo-ffl/blob/master/analyze.R) and I’ll keep updating it as I keep adding newer visualizations and analyses.                              Pretty simple here but highlights how much more valuable the QB position is compared to the others.                                     The  box plot  is a quick way of looking at distributions since it highlights a few metrics at once - the median, the quartiles, as well the outliers. What's interesting here is how many outliers there are at the QB and WR positions, especially how uneven it is for WRs.                                       The density plot shows how the points are distributed by position. This shows a similar story to what we saw in the boxplot but visualizes each of the data points. I suspect the symmetry in the QB position is not unique and is just an artifact of the fact that QBs are heavily represented in the top 100 players each week and if were to expand our dataset we'd see similar distributions for the other positions.                                       Similar exercise to the above but by team. I didn't find a ton interesting here other than Pittsburgh is dominant when it comes to top fantasy players and that Denver and Philadalphia are lacking.                                       This isn't the most useful due to the biased dataset but it does highlight the dominance of some teams compared to others but not much more than that - at least with a quick glance.                                       A bit tough to read due to the volume of teams but paired with the previous one does show that there are a few outliers but many of the distributions are similar.",2,1,2016-09-05,3,"data visualization, fantasy football stats, fantasy football",502,Visualizing fantasy football stats
22,0,While migrating my blog to AMP I wrote a few scripts to automate going from img tags to amp-img tags.,"#meta,#code","{% include setup %} Over Labor Day weekend I migrated my blog to use [AMP](https://www.ampproject.org/) but the first version was definitely a work in progress. One big item I needed to take care of was converting all my images to be AMP compatible by replacing &lt;img&gt; tag with &lt;amp-img&gt; along with the image width and height. I ended up writing a quick Python script to go through each of my posts, find each &lt;img&gt; tag, get the image’s dimensions, and then replace the original tag wit the AMP version. Unfortunately, I ran the script without too much testing and forgot to add closing tags which caused some of the content to go missing.  The solution was to write another script that once again went through every post but instead of replacing every img tag with an amp-img tag it found every amp-img referenced and added a closing tag in case it didn’t have one. These two scripts combined ended up fixing most of the AMP issues but I’m sure there are still a few posts that got warped so if you notice any please let me know.  In the spirit of constantly shipping the code is up on [GitHub](https://github.com/dangoldin/ampification) but is simple enough to not need a ton of polishing. Note that it’s not very robust and has some assumptions based on my blog structure so I would test it thoroughly before applying it to your posts.",2,2,2016-09-08,4,"AMP, img to amp-img, jekyll, amp blog",244,AMP migration scripts
21,0,In 2012 I documented the first autocomplete option for each letter of the alphabet and did the same in 2016.,"#meta,#society","{% include setup %} While going through and making sure each of my old posts was AMP compatible I came across a [post from 2012](/2012/06/07/achieving-browser-autocomplete/) where I tried to list the first autocomplete suggestion for each letter. This naturally made me think of what the results would be if I did the same exercise now. Comparing the information 4 years apart is an interesting way to see how my habits have changed but also provide a glimpse into the evolution of companies, products, and technology. The biggest surprise is how much of the list is work related - it’s somewhat expected given how much time we spend working and how many more cloud services there are but it’s still shocking that almost half the list is work related. The other major realization is that much of my consumption has shifted to mobile - many of the sites that are no longer on the list I actively use on my phone; I may actually use Instapaper, Google Maps, and Twitter more frequently now but it’s mostly on mobile via an app. Given how interesting this exercise was I plan on doing this annually and encourage others to do the same - it’s an extremely simple way to see how technology and our relationship to it changes over time.      2012  2016  Notes      analytics.google.com  amazon.com  I don't care as much as I used to about site metrics but to make up for it I'm shopping more frequently.    bankofamerica.com  betterworks.com  We've been using BetterWorks to manage team and personal OKRs.    cad-comic.com/cad  console.aws.amazon.com  Another work product - need to make sure everything's still up and running.    docs.google.com  drive.google.com  Just a domain change.    eventbrite.com  -  An internal Sentry installation to help us track errors.    facebook.com  football.fantasysports.yahoo.com  Start of football season but not sure what would have replaced this.    glos.si  github.com  Glossi is no longer around but I spend a ton of time on GitHub now.    heroku.com  hellofresh.com  Might be based on recency since I was cancelling my account last week.    instapaper.com  interactivebrokers.com  I'm mostly using the Instapaper app now.    joinblended.com  jira.com  Need to maintain our agility.    klout.com  kafka.apache.org  Apparently I spend a lot of time read Kafka docs.    linkedin.com  localhost:4000  This is Jekyll which powers my blog.    maps.google.com  mint.com  This is an interesting one. I use Google Mpas more frequently than Mint but it's mostly on mobile or typing an address in directly or incognito.    news.ycombinator.com  news.ycombinator.com  One of the few that stayed the same.    optimum.com  opentable.com  We no longer have Optimum and booked a dinner reservation recently.    plus.google.com  -  This is a work domain that's just not very well secured.    questionablecontent.net  questionablecontent.net  Another one that stayed the same.    reader.google.com  -  Another inernal work domain.    startupmullings.com  suntrust.com  I went from blogging about startups to paying a mortgage.    twitter.com  triplelift.atlassian.net  Our Atlassian installation.    udacity.com  usetallie.com  Another work site to submit expense reports.    voice.google.com  vettery.com  A work site to help recruiting.    wixlounge.com  wrike.com  Not many sites starting with w. We tried Wrike out before using JIRA.    xkcd.com  xkcd.com  Another one that stayed the same. I'm loyal to my comics.    youtube.com  youtube.com  Same here. I don't use YouTube much but not many other sites starting with a y.    zerply.com  zillow.com  I'm such an adult.",1,2,2016-09-10,4,"autocomplete, browser, internet, technology",547,Comparing my top sites: 2012 vs now
30,0,AMP comes with a set of restrictions that make it difficult to get Disqus comments integrated. There's a pretty simple way of doing it using amp-iframe and S3.,#code,"{% include setup %} After migrating my blog to AMP the last task was getting [Disqus](https://disqus.com/) working again. The crux of the issue is that in order to improve page performance AMP disallows blanket script tags (which the Disqus integration leverages) but to make up for it comes with a variety of helpers to include officially support functionality. Examples of this include an amp-youtube tag to include YouTube videos and the amp-vimeo tag to include Vimeo videos. As a generic solution, AMP provides the amp-iframe tag which allows you to include a restricted iframe.  Doing the research it turned out there was no out of the box solution but after a bunch of false starts I came across a great [post](https://labs.tomasino.org/disqus-in-amp) by [James Tomasino](https://twitter.com/mr_ino) where he ran into similar issue and came up with a workaround that was simply creating an additional HTML page for each post that contained the appropriate Disqus code which could then be included via the amp-iframe tag. Unfortunately this approach wouldn’t work in my case since amp-iframe requires HTTPS and my blog is solely HTTP due to being hosted on GitHub pages with a custom domain.  The workaround I came up with is to take the script James came up with and make a few tweaks to it that allow it to be hosted on an S3 bucket. I also wanted to avoid having to build an additional comment HTML page for each post for each new post and made a small change that allowed me to pass the relevant details as GET arguments into the comment iframe page. If you’re interested in the implementation, just take a look at the source of this page or check out [https://s3.amazonaws.com/dangoldin.com/amp-disqus.html](https://s3.amazonaws.com/dangoldin.com/amp-disqus.html).",4,1,2016-09-13,3,"disqus, amp, iframe-amp",306,Supporting Disqus in AMP
30,0,Part of being a manager is doing a lot of rote tasks that help improve the team's productivity. I've spent some time automating it with a variety of scripts.,#management,{% include setup %} One of the biggest lessons I learned when I became an engineering manager was how important the basic operational elements. These are all the things that need to get done outside of code and allow the whole team to be as productive as possible and range from reminding people to do code reviews to creating dashboards to highlight key metrics to enforcing an on-call process. These tasks are important yet repetitive so being a good engineer I’ve spent some time automating them. There’s still a long way to go but strong engineers have a mindset that they want to automate as much repetitive work as possible in order to focus on unique and novel challenges.  This attitude can be applied to management as well. By automating the menial stuff you’re able to focus on the tasks that require a human touch. Nearly every product geared towards developers exposes some sort of API which can be used to automate most rote work. The approach I’ve taken so far is extracting data from Redmine and GitHub via their APIs and exposing the results in a simple dashboard powered by [freeboard](https://github.com/Freeboard/freeboard) as well as on Slack. Since every company has a unique setup with their own set of tools and processes it’s difficult to come up with a universal solution but modern day tools make it incredibly easy to get started with some sort of automation.,1,1,2016-09-18,2,"engineering management, automation",240,Automating management
30,0,Apple's Touch ID is great when my fingers are dry but utterly fails when they're wet. It should have enough history to create a wet profile of my finger.,#product,"{% include setup %} Apple’s Touch ID is great but one thing it doesn't handle well is wet fingers. Even if my hands are a little bit sweaty or not completely dry it's difficult to unlock the phone. Yet as soon as they’re dry the phone immediately unlocks. What’s surprising, especially given Apple's focus on delivering the perfect user experience, is that this is still a problem. I'm not familiar with the hardware behind Touch ID but even if there's some sort of warped fingerprint it should be good enough. The fact that there are a few unsuccessful attempts with the wet thumb followed by successful attempt should be enough to develop a profile for the wet version which can be used on future attempts. Modern products succeed by delivering optimized experiences; future products will need to adapt and grow along with us until they become eerily predictive.",0,1,2016-09-23,3,"Apple Touch ID, user experience, thumbprint",149,A smarter Touch ID
28,0,I watched Thursday night football via the Twitter app and it highlights the opportunity Twitter has. It leverages their strength in real time with a great product.,#product,"{% include setup %} In April, Twitter [announced a deal](http://www.bloomberg.com/news/articles/2016-04-05/twitter-said-to-win-nfl-deal-for-thursday-night-streaming-rights) with the NFL to broadcast Thursday night games and I gave it a shot this past Thursday via the Twitter app on my FireTV. The primary motivation was to watch the game but I was also curious to see Twitter’s implementation. I was pleasantly surprised by how smooth and clean the overall experience was: you could watch the entire game without knowing it was via Twitter but the tweets added a please, yet optional, touch. The only real difference between the Twitter app and any other FireTV streaming app was that Twitter augmented the experience with twitter content - tweets, images, and scopes.  This NFL product makes me optimistic about Twitter and does feel as if they finally stumbled unto a product that works and reinforces their strengths. By focusing on live events and building on top of them with content that’s unique to Twitter they have the potential to change the way we consume live TV. At the moment the feed seems to be chronological but if Twitter can figure out how to make it a bit more relevant it can make the feed section standard.  The feed section currently takes up close to a third of the screen which causes the video to be scaled down. The sidebar is an obvious first attempt but I can think of other ways the tweets can be shown - maybe a ticker tape or even a translucent overlay - would make it likelier that people keep the feed on throughout the game.  There’s also a ton of opportunity in opening this platform up to developers. Twitter developed the reputation of betraying the developer community during their growth but this can be a chance to redeem themselves. Imagine being able to build an app that lives within the TV app and can show you how your fantasy team is doing or just displaying a more targeted subset of the tweets or even pulling in additional stats. All Twitter would need to do is provide the platform and the community can build on top of it to deliver custom experiences. The incentive is already there - being able to have an app that’s used while people are watching TV is hugely motivating and will get developers supporting Twitter.  Disclosure: I own a small number of Twitter shares.",1,1,2016-09-25,3,"twitter, nfl, twitter thursday night football",405,NFL Thursdays on Twitter
29,0,After getting frustrated with some meetings being booked across multiple rooms I decided to do something about it and wrote a quick Python script to pinpoint it happening.,#code,"{% include setup %} One of the first things felt by a fast growing company is the lack of meeting space. The first few weeks at a new office it’s wonderful to know you can find a room whenever you need it. Yet after a few months and a bunch of extra people you realize you have to book meetings days in advance. And what makes this worse is seeing more than one room booked for the same meeting.  After seeing this happening I decided to do something about it and wrote a quick script to pull the meeting calendar for every room from [Google Calendar](https://developers.google.com/google-apps/calendar/) and then flag the ones having the same start time, end time, and creator.  This isn’t foolproof since it won’t identify cases where someone books multiple rooms for the same time with different times but it's a solid start and already caught a few cases. The code is up on [GitHub](https://github.com/dangoldin/gcal-shaming) so feel free to take a look and provide suggestions.",2,1,2016-10-01,3,"google calendar, meeting rooms, double booking",173,Shaming meeting room hogs
22,0,Apple introdued a new feature in iOS 10 to alert users that they're connecting to an open and unsafe wifi network.,#meta,"{% include setup %}           While exploring the city earlier today I ended up wandering too close to the Google building and somehow got connected to their guest wifi network, GoogleGuest, and noticed that iOS 10 gave me an “Security Recommendation” notification. My first reaction was that this was an Apple jab at Google but It turns out that iOS 10 introduced a [new feature](https://www.engadget.com/2016/07/22/ios-10-unsecured-networks/) to let people know that they were connecting to an open network. The intent seems to be to warn users that they may not be on a secure connection but it’s a bit hidden away and didn’t actually prevent me from connecting: it was more of an FYI.  It’s clearly a minor feature but I think it reinforces the stance Apple has been taking in favor of [user privacy and encryption](http://www.apple.com/customer-letter/). They’re positioning themselves to be the antithesis of Google and this is a small way of driving that point home.",2,1,2016-10-02,4,"apple, security, privacy, encryption",184,iOS wifi security recommendation
33,0,Advances in technology have reduced friction and made mass surveillance the norm. We need to reintroduce friction in order to protect our privacy and force governments to prioritize their tracking and monitoring.,#society,"{% include setup %} One of the best arguments I’ve heard against mass surveillance is that the marginal cost has dropped to nearly zero which warps the system. Since so much of our world is digital it costs the government nothing extra to collect each additional data point. Given these incentives it’s no surprise that the government was able to get the major companies to provide a dedicated feed of the data they were collecting - modern technology has enabled both the collection and analysis of massive amounts of data.  This infrastructure is something we’ve never had before. In the past surveillance carried a sizable cost - beyond the warrant one would need to either install wiretaps, manually intercept mail, have people followed, and generally hire people to do both the data collection as well as the analysis. These constraints necessitated making tradeoffs and prioritized those that carried the largest risk.  It’s impossible to undo the technological advances and we wouldn’t want to. At the same time we need to do more to introduce friction back to surveillance. The goal isn’t to achieve 100% privacy but to make it costly enough that governments need to think about who and what they’re tracking. The obvious way is to start using end to end encryption - I’m sure isolated cases can be cracked but cracking it at scale would be a monumental task.",0,1,2016-10-09,2,"mass surveillance, security",229,Friction and mass surveillance
33,0,While upgrading our Kafka to 0.10 we ran into issues getting secor to scale. We did a ton of optimization but the final culprit was the version of our Kafka consumer.,"#devops,#data","{% include setup %} Over the past few weeks we rolled out a new data pipeline built around around Kafka 0.10. I plan on writing more about the full project but for this post I wanted to highlight how critical reading the documentation is. One of the first issues we ran into was that [secor](https://github.com/pinterest/secor), a neat application open sourced by Pinterest to allow simple saving of Kafka messages to S3, was consuming extremely slowly. I fastidiously tweaked the Kafka configuration to get as much out of it as I could to no avail. I spent hours experiment with the various secor options to see whether there was a simple solution I was missing. No matter what I tried I was unable to consume more than 50mb/min - despite the fact that both the Kafka cluster and the instance running secor could support an order of magnitude more than that. I confirmed that there was something fishy by running the same exact code on a massive c3.8xlarge instance to see how much better it would fare. And sure enough I still couldn’t get past 50mb/min.      The blue is an c4.xlarge and the orange is a c4.8xlarge. Clearly they should not both be consuming at the same rate. Also, the large spike in the middle is when the offsets start dropping off and secor keeps attempting to catch up.       The flip side is that the uploads to S3 are throttled and drop of when we're behind Kafka.   At this point I was extremely frustrated and figured I might as well revisted the Kafka docs and found this [wonderful gem](http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact):  > The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.  The light immediately went off and sure enough, secor was configured to use a Kafka 0.8 client. As soon as I [upgraded secor](https://github.com/pinterest/secor/pull/262) to use Kafka 0.10 the consumption rate shot up to over 2.5gb/min. Despite feeling incredibly stupid it felt good to finally get to the bottom of it and only wish I read the docs more thoroughly before diving in. The benefit to all this is that I have a much better understanding of how  Kafka, ZooKeeper, and secor need to be configured and the value of actually reading the documentation, something that I still haven’t internalized.      After the upgrade we see a healthy spike of data going in as we're trying to catch up.       Similarly we see us writing it all out to S3.",3,2,2016-10-10,5,"secor, kafka, 0.10, big data, streaming",770,Setting up secor for Kafka 0.10
22,0,"Fighting some writer's block I discovered a new way to write - using a bluetooth keyboard, a lap desk, and my phone.",#meta,"{% include setup %}           The past couple of weeks I’ve had a big case of writer’s block. I haven’t been able to motivate myself to write as much as I used to and when I did get to write it felt more like a chore than a joy. I didn’t know how to break out of it but this past weekend I kicked off the OS X upgrade without realizing how much time it would take.  Since I made a commitment to write two posts a week and I was computerless I had to do something. Lucky for me I have an bluetooth keyboard lying around a neat lap desk with a phone slot so I decided to give it a shot and see what I could muster.  It turned out remarkably well. The small screen made it a lot easier to focus which was magnified by the inability to easy switch to another app - something I’m prone to doing when I’m on an actual computer.  I still need the command line to commit the text and handle the image upload but it was incredibly liberating to write using a keyboard, a lap desk, and a phone. The change of environment itself may have gotten me over the writer’s block but I can also see myself using this setup whenever I travel or am outside. It’s also portable which makes it simple to write where I am.",0,1,2016-10-13,3,"blogging, bluetooth keyboard, phone",257,My new blogging setup
1,0,,,"{% include setup %} While going through my old GitHub repos I discovered that the most starred repo was [twitter-archive-analysis](https://github.com/dangoldin/twitter-archive-analysis), a Python script that would generate a view visualizations of a Twitter archive. I haven’t touched the code in over 3 years and decided to see how it was holding up and whether any of it still worked. After a few false starts getting the necessary packages playing nicely together and updating the code to support Twitter’s new archive format, I was able to get the old code working. Compared to three years ago, the results are surprisingly not that different - I definitely tweet less frequently than I used to and my activity has shifted into being more about replies rather than general tweets.                              No tweets while I'm asleep but tend to be the most active in the evenings.                                       Pretty even distribution but more active on the weekends than the weekdays.                                       Hit my peak in 2013 and have been declining since.                                       I tried to get at the idea of how much I tweet over time and by day - the weekends have remianed steady but my weekday tweeting has dropped off.                                       Definitely not taking advantage of the full 140 characters.                                       The next visualization provides a much better idea of my tweet type distribution.                                       A clear trend to being more about replies and engagement rather than just posting thoughts and ideas.",1,0,2016-10-19,1,,417,Revisiting my Twitter activity
45,0,"There's no way to avoid unsecure devices, especially as we move to the internet of things, and we need to figure a way to make our infrastructure more resilient. One way is to make our routes to the internet - routers and ISPs - more intelligent.",#society,"{% include setup %} After Friday’s DNS DDOS attack I’ve been thinking of approaches that could prevent this from happening in the future. In a perfect world every device would be up to date with the latest updates and it would be difficult to compromise anything that’s connected to the internet. Unfortunately, this is not the case and there’s an ever growing number of devices that are quickly hacked together and sold without any focus placed on security. Akamai did a [study that shows](https://www.wired.com/2016/10/akamai-finds-longtime-security-flaw-2-million-devices/) over 2 million internet connected devices have been compromised which allows them to be used to run DDOS attacks, very similar to the one that took down a big chunk of the internet on Friday. The challenge is that most owners both don’t know and don’t bother to do any security audits when setting up these devices and very likely never upgrade the firmware nor the software to make them more secure.  The solution is either to have much stronger regulation on what’s able to be sold to force manufacturers to secure their devices but I suspect this is a non starter - it’s tough to control the global world and there will always be incentives to deviate. A better solution would be one that assumes the internet will be filled with these malicious devices but can still handle them.  One idea is to make our routers smarter. They’re our homes’ gateway to the internet and improving the way they handle outbound traffic can reduce the impact these faulty devices have. Imagine them being smart enough to know the typical pattern of every connected device and throttle atypical traffic. Or have them serve as a both a cache and a throttler of DNS requests. The risk here is that the router itself becomes compromised or ends up accidentally rejecting valid traffic. I suspect most people have a router that was given to them by their ISP and ISPs have a strong incentive to keep their routers secure. And even if the router does get compromised we can push this sort of “smart throttling” unto the ISPs. In the case of the accidental throttling we’ll either need to deal with a small delay or provide the ability for a human to override the throttling - something that they would not unknowingly do to support a random device.  The solution here is to accept that we will always have bad actors and that we’ll never have total security. In that sort of world the network itself needs to be robust and resilient enough to handle whatever is thrown it’s way. Making the network more intelligent is one way but other ways include building in more resiliency into the protocol itself or making more and more of the internet distributed. This problem will only get worse as our entire homes connect to the internet and we need to find a solution before then.",1,1,2016-10-23,5,"DDOS, ISPs, router, interent of things, connected home",492,Preventing future DDOS attacks
24,0,Sometimes you want to just quickly go from a query to a line chart. I built a simple script to make it easy.,"#code,#dataviz","{% include setup %} Lately I’ve been doing a variety of quick data investigations and they typically follow the same formula: write a query to fetch some simple data, copy and paste into Excel, do a minimal amount of manipulation, plot the results. Often this happens in a sequence where the results of one analysis leads to another one and so forth and so forth until the data has been sliced so many different ways that I’m able to figure out what I was investigating.  Earlier today I did yet another one of these analysis and got annoyed by how repetitive the process was and wrote a quick script to handle simplest case: a single line chart derived from two columns - each representing an axis. The script works by taking tab delimited data via stdin and then using matplotlib to do a standard line chart. There’s a ton of room for improvement but it fits my standard workflow of using [SQL Workbench/J](http://www.sql-workbench.net/) to execute the query and then quickly copy it over to my clipboard in a tab delimited format.  The [code is up on GitHub](https://github.com/dangoldin/python-tools/blob/master/plotter.py) and it can be executed from the command line by piping the raw data directly into the script. If the data is in the clipboard it’s as simple as typing in “pbpaste &#124; ./plotter.py”. Using this approach I was able to generate the image below as well as the Excel version for comparison. The major improvements are cleaning up the styling so it looks nicer as well as supporting multiple series.",2,2,2016-10-26,4,"data visuaization, command line plotting, command line visualization, command line line chart",293,Simple data visualizations from the command line
20,0,The NFL is losing customers in its core market yet is trying to grow with an inferior offering abroad.,#product,"{% include setup %} NFL viewership is [down 10%](http://www.wsj.com/articles/ratings-fumble-for-nfl-surprises-networks-advertisers-1475764108) this season and I understand the desire to grow the brand and the sport [abroad](https://en.wikipedia.org/wiki/NFL_International_Series#Long-term_deals_and_the_NFL.27s_return_to_Mexico:_2016.E2.80.93present). It seems misguided to take a product that’s declining in popularity and rather than fixing the core problems to try to grow it as is. This is akin to a tech startup marketing the hell out of a product that’s unable to retain its existing customers. The proper approach is to nail the product before trying to push it into the market.   In this case it’s actually worse since the NFL is not something isolated and the experience of one fan will influence the experience of another. By having some games played internationally the NFL runs the risk of alienating some fans that have to wake up much earlier to watch the game. The London games have been airing at 9:30 AM EST, which is 1:30 PM in London and 6:30 AM in California. This requires the California fan to be up at dawn to watch the game. And on the flipside, imagine a London fan actually adopting an NFL team: an 8:30 PM EST game would start at 12:30 in London and go on for a few hours.   Given the time difference it’s tough to imagine the NFL expanding internationally in its current form.  The only events with this sort of mass appeal are international competitions - the Olympics and the World Cup come to mind - and they occur every 4 years. They should instead focus on attaining the perfect experience for their existing users - that includes making the league more balanced, ditching the blackout rules and embracing full digital distribution, and actually dealing with the concussion and violence issues. Instead it feels as if the NFL is diluting themselves for a small change at some short-lived growth.",2,1,2016-10-30,2,"nfl, digital sports",320,The NFL abroad
29,0,The Kindle is great but it doesn't sync highlights for ebooks outside of Amazon's store. This is frustrating when trying to help leave notes for a friend's book.,#meta,"{% include setup %} I love to read so it took me a surprisingly long time to get a Kindle. Before then I felt fine either just grabbing a physical book or using a tablet or a phone. LCD displays never bothered me so I figured I might as well get the responsiveness and additional functionality of a tablet rather than a single-use device. But earlier this year I spent some time using my wife’s Kindle and loved the form factor as well as the battery life. I also started to buy a lot more ebooks so finally took the plunge and got myself a Kindle.  If you’re not an avid reader you can get away with a tablet or phone. But if you enjoy reading the Kindle is great and makes it easy to fall into a reading addiction. I’m a bit odd in that I will refuse to write in a book; I remember having a half dozen SAT practice books and rather than circling the choices I would do it in a separate notebook. I don’t know how this habit stemmed but I won’t do any mutilation of the book, including folding a corner to save a spot. I like to think I value the sanctity of a book but I’m sure it’s due to a habit I picked up as a kid.  With a Kindle I don’t have this aversion and get a kick out of highlighting interesting passages or just words I don’t know. I’m also reviewing the draft of a friend’s book and this made it incredibly easy to take notes. Unfortunately, since the book was a draft and was not purchased through Amazon my highlights aren’t accessible through the website which makes it difficult to actually go through and flesh them out.  This seems like an opportunity for Amazon to improve the experience. It may be an anti-piracy decision which offers an inferior experience to pirated books but this seems misguided and ruins the experience for the majority in order to penalize the minority. Amazon is dominant in ebooks and digital publishing and has already won the space; they should be doing everything they can to encourage authors to write and a big part of that is giving them an easy way to get feedback on their drafts.",0,1,2016-11-06,3,"amazon kindle, writing, ebooks",388,Restricted highlighting on the Amazon Kindle
28,0,"Data pipelines are typically complicated and tough to build and maintain. There's a simple, serverless implementation that can be done using AWS's Elastic Load Balancers and Lambda.","#code,#data","{% include setup %} Building a data pipeline can be a massive undertaking that typically requires deploying and configuring a Kafka cluster and then building appropriate producers and consumers that themselves come with dozens of configuration options that need to be tweaked to get the best possible performance. Beyond that one has to set up a coordination service, typically ZooKeeper, to handle a litany of concurrency and failure issues. These days having a data pipeline is a requirement for any data driven business but building a true streaming data pipeline entails a ton of dedicated effort.  An idea I’ve been toying with is a “poor man’s data pipeline” that could be built in a serverless way and can scale to massive volumes. It turns out that a pretty simple data pipeline can be built using two AWS services: Elastic Load Balancer (ELB) and Lambda. This data pipeline doesn’t have the true streaming that Kafka provides but for simple aggregations and a tolerable 5 minute delay it’s extremely cheap and robust.  The way it works is by setting up an Elastic Load Balancer with [access logs enabled](http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html) but not actually associating it with any EC2 instances. Then every time an event is generated it would just be making a request to the ELB with the data specified as query string parameters. Note that this is extremely cheap since at this point we’re not paying for any computer and are just paying for traffic to the load balancer. Note that this is a hack since it will cause every single response to have a 503 status code but can be easily remedied with an simple web service that does no actual processing and responds with a simple 204.  Once we have access logs enabled we set up a Lambda function that gets executed every time a new S3 log file is generated. This lambda function downloads the S3 file and rolls it up via a custom function which can then be setup to export the resulting data wherever it needs to go. Note that at the moment Lambda still has a series of [limits](http://docs.aws.amazon.com/lambda/latest/dg/limits.html) that may prevent this from working at incredibly high volumes but even then one can set up Lambda to make a simple HTTP request to an external service with the log file path which can then be processed.  The [code](https://github.com/dangoldin/poor-mans-data-pipeline) is short and sweet and is up on GitHub along with a guide on getting started. If you have any questions or suggestions I’d love to hear them.",3,2,2016-11-12,4,"data pipeline, aws, lambda, elastic load balancer",435,A poor man's data pipeline
20,0,Using AWS's Lambda and API Gateway we can write recursive functions that work by sending state through HTTP redirects.,#code,"{% include setup %} Two years ago I [toyed around](http://dangoldin.com/2014/12/31/redirect-recursion/) with an odd idea of implementing recursion over HTTP redirects. The idea is that the state is managed through the query string arguments and at each recursive step we just redirect to the URL for the next one. I still can’t think of a legitimate use case for this approach but have been on an AWS [Lambda](https://aws.amazon.com/lambda/) binge lately and wanted to see whether I can get this “redirect recursion” working under Lambda. Turns out it’s incredibly easy.  The only question was exposing the Lambda function to the outside world but AWS offers the [API Gateway](https://aws.amazon.com/api-gateway/) service to make this happen. This also gave me a chance to mess around with the API Gateway for the first time and definitely has me thinking about entire tools and applications that can be done in a “serverless” way.  {% highlight javascript %} # A simple Lambda function to calculate the factorial 'use strict';  exports.handler = (event, context, callback) => {      const done = (err, res) => callback(null, {         statusCode: err ? '400' : '200',         body: err ? err.message : JSON.stringify(res),         headers: {             'Content-Type': 'application/json',         },     });      switch (event.httpMethod) {         // Calculate the factorial         case 'GET':             var n = parseInt(event.queryStringParameters.n,10) || 1;             var a = parseInt(event.queryStringParameters.a,10) || 1;             if (n   20) {                 done(null, {'status': 'try a smaller number'});             } else {                 var url = 'https://rrouzys2ra.execute-api.us-east-1.amazonaws.com/prod/redirect-recursion?';                 var args = 'n=' + (n-1) + '&a=' + (a*n);                 callback(null, {                     statusCode: 302,                     headers: {                         'Location': url + args                     }                 });             }             break;         default:             done(new Error(`Unsupported method ""${event.httpMethod}""`));     } }; {% endhighlight %}        This connects any request to the /redirect-recursion endpoint to the Lambda function.         This shows the URL that needs to be invoked to run the recursion.",5,1,2016-11-13,6,"AWS, lambda, api gateway, redirects, recursion, http",368,Recursive redirects with AWS Lambda
30,0,I revisted an analysis from 2014 where I looked at the HTTP requests being made by the top 100 Alexa sites in order to see what affects page speed.,"#datascience,#dataviz","{% include setup %} A [few years ago](http://dangoldin.com/2014/03/09/examining-the-requests-made-by-the-top-100-sites/) I wrote a simple PhantomJS script to hit the top 100 Alexa domains and track how long it took to load as well as the types of requests it was making. The intent was to try to understand the different factors affecting site speed and how the different sites approached the problem. I rediscovered this script while digging through my old projects this week and thought it would be an interesting analysis to redo this analysis and see how it compared against the data from 2014. The general takeaway is that sites have gotten slower in 2016 compared to 2014 which is likely due to a significant increase in the number of requests they're making.                                                          Average load time.  Pretty similar to last year with most of the top platform sites being incredibly quick. The surprising thing is that on average sites seem to have gotten slower but this can be entirely due to me having a different internet connection - something that on its own is an issue.                                                                          Load time boxplot.  Similar distribution to two years ago but so much more variance. No idea why this would be the case.                                                                          Number of requests.  Many more requests being made in 2016 than in 2014. In 2014 no site made over 1000 requests but in 2016 we see it happening with 3 sites.                                                                          Number of request vs time to load.  Expected and similar results to 2014. The more requests a site is making the longer it takes to load.                                                                          File type frequency.  Pretty similar distribution to 2014 but we do see much higher numbers across the board and a relative decrease in JavaScript and an increase in JSON and HTML.                                                                          File types by url.  Not much here but seems that there's a bit more variety of content types compared to 2014 although still heavily dominated by images.                      As usual, the code’s up on  GitHub  but you'll need to go back in the revision history to get access to the old data files.",8,2,2016-11-18,4,"site speed, javascript, phantomjs, data analysis",557,Comparing the web requests made by the top sites: 2014 vs 2016
26,0,Every once in a while I'll look at my old software projects and am always impressed when old code successfully executes on the first try.,#meta,"{% include setup %} I have over [50 repositories](https://github.com/dangoldin?tab=repositories) on GitHub with the majority being one time projects that were either me exploring a new technology, writing a small script, or doing a quick data analysis and visualization project. Every once in awhile when I’m a bit nostalgic I’ll go through these old projects and mend some of the code.  What’s surprising is discovering old projects and scripts that work as is without me having to do anything to update the underlying code. I’m used to so working with so many open source libraries and cryptic documentation that it’s rare to find a public library that works exactly as you expect. Of course my projects are much simpler than the typical open source library but I find it remarkable that I can get code up and running within a few minutes of a checkout.  This is something that we should aspire to when writing code - writing it in such a way that if someone were to use it in a few years it would be easy to follow and understand while being able to be run without requiring any modification. There’s an urge to constantly improve and extend everything we write but in the world of software it’s possible that we may end up making it worse. Rather than adding bells and whistles to everything we write we should take a step back and think how we would react if we were to discover it in a few years.",1,1,2016-11-19,3,"code, refactor, software engineering",252,Joy of old code
40,0,There are a few services and products that analyze your AWS usage in order to provide some recommendations. They are all just going off of the detailed billing report so I wrote a quick script to provide similar visualizations.,"#code,#dataviz,#devops","{% include setup %} There are a variety of cloud management services that connect to your cloud computing account and analyze your usage in order to offer recommendations that help improve efficiency, security, and reduce your costs. In fact, AWS even provides their own service, [Trusted Advisor](https://aws.amazon.com/premiumsupport/trustedadvisor/), that competes with the external vendors. Unfortunately, these vendors can get expensive quickly. The first useful tier of Trusted Advisor, categorized as Business, has a tiered pricing model based on your existing usage that starts at 10% of your AWS bill and decreases to 3% as you spend past $250k/month. External vendors are cheaper but can still get expensive depending on your bill: [Cloudability](https://www.cloudability.com) starts at 1% of your AWS costs which compared to Trusted Advisor is significantly cheaper is still 1% of your AWS bill.  One option is to sign up for a single month and use that to take the necessary steps to improve your cloud configuration. If your infrastructure is stable month to month this is a simple and cheap way to revamp your setup. But if your infrastructure is constantly evolving you need a way to revisit your environment when necessary.  I spent some time looking at our AWS infrastructure last week and it turns out AWS provides an option to export a [detailed billing report](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html) to S3 which is what the external vendors use to provide their recommendations. AWS offers a few reporting options but the most detailed one contains every resource and tag and in my case was over a million rows and nearly 500MB. There’s a wealth of information here and I wrote a small script to slice, dice, and visualize the data along a few dimensions to help provide some transparency into what is the biggest cost. The code is extremely simple since it’s just grouping and visualizing the data by different dimensions. The only real enhancement I made was to translate the OpsWorks tags into a layer dimension to make the visualization more useful. The two big things I still need to do are provide recommendations around reserved instance usage and do a better job of grouping the usage types since they’re too specific. As usual the [code is up](https://github.com/dangoldin/aws-billing-details-analysis) on GitHub and I’d love to hear any suggestions or feedback. Below are some graphs the script generates but note that I removed the axes labels to avoid revealing our costs and configuration.                                                          By product name.  A simple summary of cost by AWS product/service.                                                                          By product name.  This shows every type of usage AWS has in the billing report. To deal with the long tail the script also generates a plot for the top 25 but one thing I need to do is a better job of grouping these - for example data transfer has different values depending on region and type and I want to consolidate them into a one in order to see total costs due to data transfer.                                                                          By layer and usage type.  To me this is the most interesting one since it's looking at data for multiple dimensions - in this case layer and usage type. The goal here was to see which application/usage pairs result in the largest costs and allow me to prioritize investigation effort. Once again this will be more useful when I do a better job of grouping the usage types.",7,3,2016-11-27,5,"aws, cloud cost management, cloud management, trusted advisor, cloudability",651,Visualizing your AWS costs
27,0,Yesterday I gave a quick talk about tips and tricks to get better as a software engineer - both in the short term and the long term.,"#meta,#management","{% include setup %} Yesterday I had the privilege of giving a talk at [HackReactor](http://www.hackreactor.com/) titled “Things I wish I knew” which was an amalgam of the various themes and topics I’ve been blogging and thinking about. While going through the blog I came up with two themes for the topic - the first was tactics that would make someone a better programmer immediately and the second was how to improve as a developer over time.  ### Short term tips to become a better programmer - **[Generalize at n=3](http://dangoldin.com/2016/04/07/generalize-at-n3/)**. Rather than coming up with the perfect abstract solution right away my rule of thumb is to start thinking about that on your third iteration of solving the same problem. This will ensure you’re solving a problem that will recur while giving you enough perspective to actually develop a useful abstract solution. - **[Think carefully about your database](http://dangoldin.com/2016/02/15/design-your-database-for-flexibility/)**. Compared to changing a database changing code is much simpler. Code is mostly stateless and you don’t need to worry about backfills or migrations. - **[Focus on interfaces, not implementations](http://dangoldin.com/2015/12/02/think-interfaces-not-implementation/)**. Instead of obsessing over the perfect implementation it’s more important to think about how your application works and the way it’s architected. This way you can always change the implementation of a single method or function to make it better without having to gut and rewrite the entire application. - **For dates and times, just use UTC**. A very common refrain online but only worry about timezones when displaying data to users. - **[Use GitHub for documentation](http://dangoldin.com/2016/08/14/integrating-poorly-documented-open-source-libraries/)**. Sometimes Documentation and StackOverflow don’t have exactly what you need. A good resource is to use GitHub’s code search and find actual examples of the relevant code being used.  ### Getting better over time - **[Learn to appreciate DevOps](http://dangoldin.com/2014/12/26/devops-for-the-rest-of-us/)**. Not many people love DevOps but I’m a strong believer in understanding how your code will run and be deployed. It gets you more familiar with the entire lifecycle and allows you to be more creative with your solutions. - **[Have a sample project to learn new languages](http://dangoldin.com/2015/10/11/have-a-go-to-project-when-learning-a-new-programming-language/)**. In addition to tutorials I like having a project that I will implement in new languages to learn them. This repetition highlights the major differences between the languages and allows me to work on a sample application that mirrors my interests. - **[Approach code like you approach the gym](http://dangoldin.com/2016/03/13/approach-work-like-the-gym/)**. We spend more than 8 hours a day working but imagine if we approached it like we do the gym. Sure people that go to the gym every day without a plan are better off than those who don’t go at all but they pale in comparison to those that go with an agenda. How do we turn every line of code we write into something that’s as focused as a workout? - **Read the classics**. Despite being a relatively young field software engineering has had a ton of great books written and rather than spending time reading blog posts (including this one!) it’s worthwhile to go read the classics.",8,2,2016-11-30,5,"software engineering, software development, coding skills, improving, hackreactor",533,Becoming a better developer
31,0,Engineers love getting the latest and greatest code but it's important to read the release notes to make sure the new versions don't have any unintended consequences on your application.,#devops,"{% include setup %} I often find myself upgrading an open source to a newer version but I have a bad habit to only skim the release notes. More often than not an upgrade will work out of the box and you’ll get the immediate benefits of the newer version but every once in a while things blow up and you need to revert or scramble to get a fix out. Reading documentation tends to be dry with only a few relevant parts but when working on large systems it’s paramount to go through and understand the nuances of every upgrade. During my career I’ve run into a variety of issues that could have been avoided by a thorough reading of the release notes. There’s still a chance you’ll miss something and that’s why you should always have a sandbox environment and try to containerize as much as you can. Below are a few examples of issues I’ve run into upgrading various applications over the past few months:  - **Kafka 0.8 to 0.10**. This wasn’t a true upgrade but we wanted to spin up a parallel Kafka cluster that was a significant departure from our previous version. Kafka is a complicated application and we assumed that our code was backwards compatible. This was half-true. The code worked but it took a major performance hit that was [clearly document](http://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact) in the release notes. - **Mongo/PHP**: This was a simple case of not reading the compatibility chart between the different versions of Mongo, PHP, and the associated drivers. If you’re running an old version of PHP you are limited to a subset of drivers that don’t support the latest version of Mongo. Once again this was discovered when messing around on our sand environment. - **Sentry**: Sentry’s a wonderful product that collects errors from every application you’re running and consolidates them into a single, slick UI. We wanted to get the benefit of a few additional plugins and decided to upgrade it to the latest version. Lucky for us there were some significant changes that required us to install a variety of build tools, including the C compiler. Unexpected but quickly remedied.  In our desire to get the latest and greatest we should be taking a step back to weight the benefits and the risks and looking at the release notes is a great way of understanding the potential impact. Even then it’s critical to have a separate environment to test different versions and a plan to roll back since it’s impossible to know what may actually happen on your unique system and configuration.",1,1,2016-12-03,4,"software engineering, upgrading versions, open source, release notes",436,Read the release notes
28,0,Ideally we can write code that's both efficient and expressive but they're often at odds with one another. I wish there was a language that offered both.,#code,| |Array generation | 63.96| |Naive simple | 78.74| |Naive smart | 71.13| |Filter single | 82.19| |Filter multiple | 81.86| |Filter lambda single | 109.44|,0,1,2016-12-06,7,"code, software engineering, efficiency, pandas, for loops, iteration, expressiveness",32,Efficiency vs expressiveness
26,0,"Donating is a great way to pay our success forward and I wanted to share the organizations I donate to - Wikipedia, the ACLU, and the EFF",#meta,{% include setup %} It’s that time of the year when many organizations are ramping up their donation efforts and I wanted to share the organizations I donate money to. I feel incredibly lucky to be where I am and being able to donate to worthy causes is a great way to pay it forward. Everyone is passionate about something and donating to that cause is incredibly worthwhile and valuable.  [Wikipedia](https://www.wikipedia.org/). The need for education is critical to a functioning society and unfortunately this has been magnified recently by the explosion of fake news. Wikipedia is incredible at providing factual information and I find myself visiting it multiple times a day. It’s both education and entertainment since it’s just so easy to get lost in its labyrinth. Out of all the tools and services I pay for Wikipedia offers by far the highest return.   [American Civil Liberties Union](https://www.aclu.org/). Especially over the next few years our rights will be incredibly important and the work the ACLU is doing is a key part in making sure they exist in the future. Democracy is being challenge around the world and keeping it safe is something we need to do for future generations. Rather than losing our liberties one small piece at a time the ACLU needs the ability to engage in the small skirmishes and battles that strengthen our freedoms.   [Electronic Frontier Foundation](https://www.eff.org/). Being in the tech industry I like to think I understand the dangers of technology better than most and the EFF has both an increasingly important and increasingly difficult job ahead. Technology has eliminated friction across the globe and democratized a wealth of information but the flipside is that it makes it incredibly easy for governments and agencies to monitor our digital worlds. This will be a bigger and bigger issue as more and more of our lives are captured digitally and the EFF is the bulwark keeping them secure.,3,1,2016-12-09,6,"donation, wikipedia, american civil liberties union, electronic frontier foundation, aclu, eff",327,It's donation season
